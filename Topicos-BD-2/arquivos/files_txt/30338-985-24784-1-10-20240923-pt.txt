Integrando AvaliaÃ§Ãµes Textuais de UsuÃ¡rios em RecomendaÃ§Ã£o
baseada em Aprendizado por ReforÃ§o
Naan Vasconcelos
naan.vasconcelos@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Davi Reis
davireisjesus@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Thiago Silva
thiagosilva@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
NÃ­collas Silva
ncsilvaa@dcc.ufmg.br
UFMG
Minas Gerais, Brazil
Washington Cunha
washingtoncunha@dcc.ufmg.br
UFMG
Minas Gerais, Brazil
Elisa Tuler
etuler@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
5. Apesar dos novos valores
WebMediaâ€™2024, Juiz de Fora, Brazil
Vasconcelos N. et al.
estarem mais de acordo com as preferÃªncias dos usuÃ¡rios [11, 17],
provocaram um desequilÃ­brio no dilema de exploration/exploitation:
uma reduÃ§Ã£o significativa na entropia de ratings impacta no explo-
ration. Esses resultados apontam que para que MAB possam utilizar
RARS com sucesso, suas estratÃ©gias de exploration precisam buscar
alternativas Ã  entropia.
FUNDAMENTAÃ‡ÃƒO TEÃ“RICA
2.1
Multi-Armed Bandits
Multi-Armed Bandits (MAB) Ã© a principal abordagem de Apren-
dizado por ReforÃ§o para SsR interativa [15]. Trata-se de um modelo
de decisÃ£o sequencial que, continuamente, escolhe uma aÃ§Ã£o ğ‘entre
um conjunto de aÃ§Ãµes A, denominadas arms. A cada seleÃ§Ã£o de
uma aÃ§Ã£o em um ponto ğ‘¡no tempo resulta em uma recompensa
ğ‘…(ğ‘ğ‘¡) âˆˆR. Esses modelos precisam decidir entre: 1) exploitation,
que visa selecionar os arms com as maiores recompensas do pas-
sado; e 2) exploration, que seleciona diferentes arms para obter
mais informaÃ§Ãµes sobre o domÃ­nio e tomar decisÃµes futuras mel-
hores. A escolha entre essas duas opÃ§Ãµes caracterizam o dilema
de exploitation-exploration (i.e. exp-exp) e exige que o modelo seja
capaz de explorar o mÃ¡ximo conhecimento disponÃ­vel enquanto
tambÃ©m explora o espaÃ§o de soluÃ§Ã£o para adquirir ainda mais con-
hecimento sobre o domÃ­nio [32]. Em RS, os itens a serem recomen-
dados sÃ£o modelados como os arms e a recompensa Ã© o feedback do
usuÃ¡rio para essa recomendaÃ§Ã£o (e.g., rating) [19]. Essa resposta dos
usuÃ¡rios Ã©, em geral, coletada e salva continuamente em um con-
junto H (ğ‘¡). Um item Ã© recomendado de acordo com uma regra de
prediÃ§Ã£o ğœ‹, definida como uma funÃ§Ã£o que explora as informaÃ§Ãµes
atuais sobre o usuÃ¡rio: ğ‘–(ğ‘¡) â‰¡ğœ‹(H (ğ‘¡)). Por isso, a estratÃ©gia ideal
deve maximizar as recompensas nas ğ‘‡interaÃ§Ãµes:
ğ‘–âˆ—(Â·) = arg max
ğ‘–(Â·)
ğ‘‡
âˆ‘ï¸
ğ‘¡=1
E [ğ‘Ÿğ‘¢,ğ‘–(ğ‘¡) |ğ‘¡]
(1)
Para definir a utilidade dos itens para cada usuÃ¡rio, as principais
abordagens utilizam uma formulaÃ§Ã£o matricial probabilÃ­stica via
PMF (Probabilistic Matrix Factorization), modelando a distribuiÃ§Ã£o
de rewards pelos fatores latentes de usuÃ¡rios e itens, semelhantes aos
mÃ©todos model-based [27]. A recompensa esperada Ã© quase sempre
modelada como o produto dos fatores latentes do usuÃ¡rio ğ’‘ğ’–com
os fatores do item ğ’’ğ’Š. A funÃ§Ã£o objetivo e dada da seguinte forma:
ğ‘–âˆ—(Â·) = arg max
ğ‘–(Â·)
ğ‘‡
âˆ‘ï¸
ğ‘¡=1
E [ğ‘Ÿğ‘¢,ğ‘–(ğ‘¡) |ğ‘¡] = arg max
ğ‘–(Â·)
ğ‘‡
âˆ‘ï¸
ğ‘¡=1
E [ğ’‘âŠ¤
ğ’–ğ’’ğ’Š(ğ’•) |ğ‘¡]
(2)
Os esforÃ§os atuais estÃ£o concentrados em como otimizar essa
funÃ§Ã£o objetivo, equilibrando o dilema de exp-exp. As abordagens
tradicionais do MAB, como ğœ–-Greedy, UCB e Thompson Sam-
pling, utilizadas no presente estudo, consideram essa funÃ§Ã£o obje-
tivo como regra de prediÃ§Ã£o [26]. A diferenÃ§a entre esses algoritmos
estÃ¡ na maneira como eles controlam o dilema de exp-exp [2, 20â€“22].
Enquanto o ğœ–-Greedy explora a regra de prediÃ§Ã£o com probabili-
dade (1 âˆ’ğœ–), os algoritmos UCB e Thompson Sampling primeiro
medem uma incerteza Î£ em torno das informaÃ§Ãµes disponÃ­veis
sobre o usuÃ¡rio e os itens. Thompson Sampling Ã© um algoritmo
probabilÃ­stico que extrai os vetores de usuÃ¡rios e itens de uma dis-
tribuiÃ§Ã£o normal definida pelas informaÃ§Ãµes atuais disponÃ­veis. No
entanto, mesmo essa abordagem mede a recomendaÃ§Ã£o com base
na combinaÃ§Ã£o de ğ‘ğ‘¢e ğ‘ğ‘–.
2.2
RecomendaÃ§Ãµes Review-Aware
Sistemas de RecomendaÃ§Ã£o Review-Aware [23] partem da premissa
que as preferÃªncias dos usuÃ¡rios sobre itens consumidos podem
ser melhor capturadas por meio das avaliaÃ§Ãµes textuais (reviews)
fornecidas pelos usuÃ¡rios ao invÃ©s dos ratings diretamente assinal-
ados [12, 17]. Essa Ã¡rea vem sendo impulsionada pelos expressivos
avanÃ§os em propostas de arquiteturas de redes neurais aplicadas
a PLN [18]. HÃ¡ um nÃºmero crescente de SsR que vÃªm adaptando
essas propostas de PLN, considerando diferentes abordagens de
arquitetura, tais como redes Attention-Based [14], redes convolu-
cionais temporais (TCN) [5], grafos [31], extraÃ§Ã£o de aspectos [6] e
tÃ³picos [24], anÃ¡lise de sentimento [7] e, mais recentemente, apren-
dizagem contrastiva [29]. Recentemente, em [4] realizou-se uma
revisÃ£o sistemÃ¡tica da literatura dessas abordagens, comparando-as
experimentalmente.
Um algoritmo de destaque nessa avaliaÃ§Ã£o Ã© o HRDR [16], que
transforma os comentÃ¡rios em representaÃ§Ãµes vetoriais por meio
de embeddings. Em seguida, aplica-se uma camada de convoluÃ§Ã£o
2D para capturar padrÃµes locais e, em seguida, um mecanismo de
atenÃ§Ã£o para calcular pesos de atenÃ§Ã£o, focando em aspectos rele-
vantes das representaÃ§Ãµes. As representaÃ§Ãµes ajustadas pelo mecan-
ismo de atenÃ§Ã£o sÃ£o combinadas, resultando em representaÃ§Ãµes
finais dos usuÃ¡rios e itens. De forma semelhante, o CARP [13] tam-
bÃ©m vetoriza os comentÃ¡rios de usuÃ¡rios e itens que sÃ£o processados
por camadas CNN para capturar o contexto e extrair caracterÃ­sticas
importantes. O modelo calcula pesos de atenÃ§Ã£o para diferentes
partes dos comentÃ¡rios usando mecanismos de autoatenÃ§Ã£o. Essas
representaÃ§Ãµes de usuÃ¡rios e itens capturam as variaÃ§Ãµes e aspec-
tos relevantes dos comentÃ¡rios. Outro algoritmo de destaque Ã© o
CARM [14] que tambÃ©m utiliza uma camada CNN sobre repre-
sentaÃ§Ãµes vetoriais para aprender caracterÃ­sticas importantes dos
comentÃ¡rios. As caracterÃ­sticas convolucionais extraÃ­das sÃ£o combi-
nadas com os fatores latentes dos usuÃ¡rios e itens, obtidos atravÃ©s
da PMF. Essas caracterÃ­sticas combinadas sÃ£o entÃ£o passadas por
uma camada linear (fully connected) e uma funÃ§Ã£o de ativaÃ§Ã£o Tanh,
transformando as caracterÃ­sticas aprendidas em uma representaÃ§Ã£o
mais compacta. Por fim, o MAN [28] utiliza redes convolucionais
temporais sobre as representaÃ§Ãµes vetoriais para capturar padrÃµes
locais nos comentÃ¡rios. Uma funÃ§Ã£o de atenÃ§Ã£o calcula uma matriz
de atenÃ§Ã£o entre as representaÃ§Ãµes dos usuÃ¡rios e itens, as quais
sÃ£o novamente processadas por uma camada convolucional tem-
poral para ajustar os pesos da atenÃ§Ã£o. Este processo resulta em
caracterÃ­sticas de interaÃ§Ã£o entre usuÃ¡rios e itens.
Uma caracterÃ­stica comum Ã s abordagens de MAB Ã© que todas
consideram os rewards de forma explÃ­cita, ou seja, por meio das
ratings atribuÃ­das pelos usuÃ¡rios aos itens recomendados. Nesse
trabalho, nossa meta Ã© alterar esses rewards explÃ­citos por valores
implÃ­citos, extraÃ­dos dos reviews dos usuÃ¡rios por meio de aborda-
gens acima descritas.
Integrando AvaliaÃ§Ãµes Textuais de UsuÃ¡rios em RecomendaÃ§Ã£o baseada em Aprendizado por ReforÃ§o
WebMediaâ€™2024, Juiz de Fora, Brazil
Dataset
MusicalInstruments
MusicalInstrumentsMAN
MusicalInstrumentsCARM
MusicalInstrumentsHRDR
MusicalInstrumentsCARP
Measure
Hits
Hits
Hits
Hits
Hits
T
Linear UCB
0.267
0.408
0.584
0.188
0.321
0.484
0.191
0.314
0.463
0.185
0.272
0.446
0.192
0.272
0.461
e-Greedy
0.033
0.062
0.125
0.026
0.053
0.106
0.025
0.051
0.102
0.025
0.051
0.103
0.025
0.047
0.105
TS
0.122
0.187
0.287
0.102
0.163
0.270
0.116
0.187
0.301
0.114
0.181
0.279
0.004
0.015
0.048
Measure
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
T
Linear UCB
0.068
0.145
0.228
0.052
0.131
0.197
0.056
0.121
0.186
0.051
0.105
0.177
0.057
0.128
0.191
e-Greedy
0.031
0.058
0.106
0.025
0.050
0.092
0.025
0.049
0.092
0.024
0.047
0.089
0.023
0.044
0.090
TS
0.101
0.140
0.191
0.084
0.121
0.174
0.089
0.128
0.188
0.092
0.131
0.179
0.004
0.015
0.044
Table 1: Musical Instruments - Efetividade e diversidade das estratÃ©gias MAB aplicadas nas coleÃ§Ãµes originais e suas versÃµes
modificadas. Valores em negrito correspondem aos melhores valores validados estatisticamente por Wilcoxon com p-value = 0.05
Dataset
Tucson
TucsonMAN
TucsonCARM
TucsonHRDR
TucsonCARP
Measure
Hits
Hits
Hits
Hits
Hits
T
Linear UCB
0.133
0.225
0.430
0.062
0.142
0.212
0.080
0.151
0.230
0.082
0.147
0.233
0.072
0.136
0.208
e-Greedy
0.023
0.056
0.118
0.022
0.032
0.060
0.010
0.020
0.049
0.018
0.036
0.072
0.018
0.032
0.064
TS
0.073
0.128
0.223
0.057
0.090
0.143
0.044
0.078
0.128
0.052
0.091
0.147
0.045
0.076
0.133
Measure
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
T
Linear UCB
0.113
0.155
0.265
0.058
0.128
0.170
0.073
0.128
0.167
0.077
0.125
0.171
0.065
0.117
0.156
e-Greedy
0.023
0.054
0.104
0.022
0.032
0.059
0.010
0.020
0.046
0.018
0.034
0.065
0.018
0.031
0.062
TS
0.069
0.113
0.180
0.056
0.087
0.131
0.042
0.071
0.111
0.050
0.084
0.128
0.044
0.072
0.118
Table 2: Tucson - Efetividade e diversidade das estratÃ©gias MAB aplicadas nas coleÃ§Ãµes originais e suas versÃµes modificadas.
Valores em negrito correspondem aos melhores valores validados estatisticamente por Wilcoxon com p-value = 0.05
AVALIAÃ‡ÃƒO EXPERIMENTAL
O objetivo dessa seÃ§Ã£o Ã© responder a pergunta de pesquisa Qual
o impacto do uso de ratings extraÃ­dos de comentÃ¡rios de usuÃ¡rios e
itens em SsR interativos baseados em MAB? Para isso, nossos ex-
perimentos consistem em comparar o desempenho das estratÃ©gias
MAB utilizando as bases de dados originais que contÃ©m ratings
assinalados de forma explÃ­cita, com versÃµes modificadas dessas
mesmas coleÃ§Ãµes, onde os ratings sÃ£o calculados a partir dos comen-
tÃ¡rios textuais.Os modelos de MAB utilizados nos experimentos
estÃ£o destacados na SeÃ§Ã£o 2.1: ğœ–-Greedy, UCB e Thompson Sampling.
Tratam-se das estratÃ©gias que sÃ£o base de funcionamento da grande
maioria das estratÃ©gias atuais. Para a geraÃ§Ã£o das bases de dados
modificadas, os cÃ¡lculos dos ratings foram extraÃ­dos pelas quatro
estratÃ©gias descritas na SeÃ§Ã£o 2.2: MAN, CARM, HRDR e CARP,
escolhidos por apresentarem bons resultados em uma recente e
vasta avaliaÃ§Ã£o experimental [4].
ColeÃ§Ã£o
# UsuÃ¡rios
# Itens
Esparsidade
Amazon - Musical Instruments
27.530
10.620
99.92%
Yelp 2021 - Tucson
8.540
8.867
99,99%
Table 3: VisÃ£o geral das coleÃ§Ãµes utilizadas nas avaliaÃ§Ãµes.
Avaliamos a combinaÃ§Ã£o das estratÃ©gias de MAB e Review-Aware
em duas coleÃ§Ãµes de cenÃ¡rios distintos, uma de comÃ©rcio eletrÃ´nico
e outra de pontos de interesse, conforme descritos na Tabela 3. Em
ambas as coleÃ§Ãµes, todos os itens com rating tambÃ©m possuem um
comentÃ¡rio textual correspondente. Para cada uma delas, foram
geradas outras quatro versÃµes modificadas pelas quatro estratÃ©gias
de Review-Aware.
As trÃªs estratÃ©gias de MAB foram aplicadas sobre as cinco ver-
sÃµes da base (uma original e quatro modificadas). Consideramos
duas mÃ©tricas distintas de avaliaÃ§Ã£o, uma de precisÃ£o e outra de
diversidade: 1) Hits mÃ©trica de precisÃ£o que corresponde ao nÃºmero
de recomendaÃ§Ãµes que pertencem ao histÃ³rico de cada usuÃ¡rio;
2) Users Coverage, mÃ©trica de diversidade que corresponde Ã  por-
centagem de usuÃ¡rios distintos que estÃ£o interessados nos itens
recomendados. Para comparar os resultados alcanÃ§ados com vali-
daÃ§Ã£o estatÃ­stica, adotamos o teste de Wilcoxon, visto a distribuiÃ§Ã£o
nÃ£o normal dos conjuntos de dados de recomendaÃ§Ã£o [4].
3.1
Resultados Experimentais
As Tabelas 1 e 2 apresentam os resultados de nossa avaliaÃ§Ã£o para
as coleÃ§Ãµes Musical Instruments e Tucson, respectivamente. Os resul-
tados remetem a mÃ©dia de Hits e UserCoverage acumulados apÃ³s ğ‘‡
interaÃ§Ãµes dos usuÃ¡rios com o sistema. Tanto em efetividade quanto
em diversidade, os melhores resultados foram alcanÃ§ados pelas es-
tratÃ©gias MAB nas coleÃ§Ãµes originais, nas quais os ratings foram
assinalados de forma explÃ­cita pelos usuÃ¡rios. As diferenÃ§as entre os
resultados alcanÃ§ados nas bases originais e modificadas sÃ£o ainda
maiores Ã  medida que ocorrem mais iteraÃ§Ãµes. A principal razÃ£o Ã©
que as estratÃ©gias MAB aprendem menos sobre os usuÃ¡rios quando
consideram bases com ratings calculados por estratÃ©gias RARs.
Tratam-se de resultados que contrastam com aqueles reportados na
literatura para outros cenÃ¡rios, por exemplo recomendaÃ§Ã£o offline,
que apontam ratings extraÃ­dos a partir dos reviews dos usuÃ¡rios sÃ£o
capazes de elucidar melhor as preferÃªncias dos usuÃ¡rios [12, 17, 28].
WebMediaâ€™2024, Juiz de Fora, Brazil
Vasconcelos N. et al.
(a) DistribuiÃ§Ã£o Acumulativa de Ratings
(b) Popularidade vs. Entropia - Original
(c) Popularidade vs. Entropia - CARM
Figure 1: CaracterizaÃ§Ã£o Comparativa entre a coleÃ§Ã£o original Musical Instruments e suas versÃµes modificadas por RARs.
Para compreender o comportamento das estratÃ©gias MAB nas
coleÃ§Ãµes modificadas, realizamos uma caracterizaÃ§Ã£o comparativa
delas com suas versÃµes originais. Por restriÃ§Ã£o de espaÃ§o, os resulta-
dos serÃ£o reportados apenas para coleÃ§Ã£o Musical Instruments, mas
as conclusÃµes sÃ£o similares na Tucson. Primeiramente, comparamos
a distribuiÃ§Ã£o de ratings, apresentadas na Figura 1(a). Observamos
que enquanto na base original essa distribuiÃ§Ã£o Ã© bem dispersa entre
os valores de 1 a 5, a distribuiÃ§Ã£o estÃ¡ concentrada entre os valores
de 3,5 a 5 nas bases modificadas. Os valores calculados estÃ£o coer-
entes com a literatura [12, 17], que aponta que os mesmos refletem
de forma mais fidedigna as preferÃªncias dos usuÃ¡rios. Todavia, essa
alteraÃ§Ã£o causou um desequilÃ­brio em como as estratÃ©gias MAB
lidam com o dilema exp-exp.
Assim, em nossa segunda caracterizaÃ§Ã£o, comparamos a cor-
relaÃ§Ã£o entre popularidade dos itens, comumente utilizada como
uma estratÃ©gia de exploitation, e a entropia entre os itens, comu-
mente utilizada como uma abordagem de exploration, conforme
apresentado nas Figuras 1(b) - base original - e 1(c) - base modifi-
cada pelo algoritmo CARM (resultados semelhantes obtidos pelos
demais algoritmos de RARS). Conforme podemos observar, na base
modificada essa correlaÃ§Ã£o Ã© muito alta quando comparada Ã  base
original, o que significa que as bases modificadas apresentam uma
baixa entropia, impactando diretamente no Exploration.
Exploration geralmente apresenta uma correlaÃ§Ã£o significativa
entre seu comportamento e a entropia dos itens. Em um cenÃ¡rio sem
entropia, onde os rewards sÃ£o mais previsÃ­veis e nÃ£o hÃ¡ incerteza,
o comportamento dos algoritmos mudam significativamente. O e-
Greedy, por exemplo, rapidamente converge para a exploitation, com
um valor de ğœ–tendendo a zero. O UCB tambÃ©m reduz o exploration,
pois os intervalos de confianÃ§a sÃ£o muito estreitos devido Ã  certeza
das recompensas. No caso do Thompson Sampling, as distribuiÃ§Ãµes
posteriores se tornam muito concentradas em torno do valor ver-
dadeiro da recompensa de cada opÃ§Ã£o. Isso resultaria em menos
amostragem de opÃ§Ãµes nÃ£o Ã³timas, pois a incerteza Ã© praticamente
inexistente. Portanto, a alteraÃ§Ã£o da entropia dos itens nas bases
modificadas impacta diretamente o equilÃ­brio entre exploration e ex-
ploitation, sendo crucial para o design de sistemas de recomendaÃ§Ã£o
baseados em algorimos de aprendizado por reforÃ§o eficazes. Tais
anÃ¡lises respondem a pergunta de pesquisa estudada neste trabalho.
CONCLUSÃ•ES E TRABALHOS FUTUROS
Em Sistemas de RecomendaÃ§Ã£o (SsR), Multi-Armed-Bandits (MAB)
sÃ£o modelos de decisÃ£o sequencial que continuamente escolhe entre
um conjunto de itens, aqueles que maximizam o reward esperado
(e.g., a satisfaÃ§Ã£o do usuÃ¡rio), visando equilibrar a seleÃ§Ã£o entre
itens com as maiores recompensas no passado (exploitation) ou
itens inexplorados (exploration). Esses modelos utilizam como re-
ward valores numÃ©ricos explicitamente assinalados pelos usuÃ¡rios
aos itens. Neste trabalho apresentamos um estudo preliminar do
impacto do uso de ratings extraÃ­dos de comentÃ¡rios textuais por
meio de recomendaÃ§Ã£o Review-Aware (RARS) em SsR interativos.
Comparamos experimentalmente o desempenho de trÃªs estratÃ©-
gias clÃ¡ssicas de MAB utilizando coleÃ§Ãµes de dados com ratings
assinalados de forma explÃ­cita (originais) com suas versÃµes na qual
os ratings eram calculados por meio de quatro estratÃ©gias de RARS.
Os melhores resultados foram obtidos nas bases originais, contrar-
iando com resultados reportados na literatura para outros cenÃ¡rios
alÃ©m de MAB. Nesse sentido, caracterizamos comparativamente as
coleÃ§Ãµes e constatamos uma reduÃ§Ã£o significativa na entropia de
ratings dos itens nas bases modificadas. Em um cenÃ¡rio de entropia
reduzida, a dinÃ¢mica entre exploration e exploitation Ã© drasticamente
simplificada, com os algoritmos rapidamente se concentrando em
exploitation e reduzindo o aprendizado sobre os usuÃ¡rios. Dessa
forma, nosso trabalho abre a possibilidade de novas propostas de
alteraÃ§Ãµes nas estratÃ©gias de exploration, que permitam que as abor-
dagens MAB sejam tambÃ©m capazes de usufruir da habilidade de
estratÃ©gias RARs em capturar melhor as preferÃªncias dos usuÃ¡rios.
TambÃ©m pretendemos estudar como combinar as duas os ratings
explÃ­citos com a informaÃ§Ã£o obtida por meio das RARs.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Fapemig, FAPESP,
CIIA-SaÃºde e AWS.
Integrando AvaliaÃ§Ãµes Textuais de UsuÃ¡rios em RecomendaÃ§Ã£o baseada em Aprendizado por ReforÃ§o
WebMediaâ€™2024, Juiz de Fora, Brazil