Análise da Percepção do Uso de Cigarros Eletrônicos no Brasil por
meio de Comentários no YouTube
Aline Dias1, Richardy R. Tanure1, Jussara M. Almeida2, Helen C. S. C. Lima1, Carlos H. G. Ferreira1
{aline.md,richardy.tenure}@aluno.ufop.edu.br,jussara@dcc.ufmg.br,helen@ufop.edu.br,chgferreira@ufop.edu.br
1Departamento de Computação e Sistemas, Universidade Federal de Ouro Preto, Brasil
2Departamento de Ciência da Computação, Universidade Federal de Minas Gerais, Brasil
2023.
Our methodology involved analyzing production and engagement
metrics, and developing a deep learning-based stance detection mo-
del to estimate people’s stance (approval or disapproval) based on
comments and quantify the temporal dynamics of these attitudes
over the years. Our findings reveal a significant increase in content
production and user engagement, indicating a growing public inte-
rest, with a notable increase in approving comments on the product.
This study fills previous research gaps by offering a comprehensive
and pioneering overview of e-cigarette use and public perception
in Brazil, emphasizing the need for a more informed discussion
among society and regulatory bodies.
WebMedia’2024, Juiz de Fora, Brazil
Dias et al.
de termos específicos que incluem marcas, sabores e outros ter-
mos noticiados e relacionados. Investigamos os comentários para
compreender a opinião pública sobre o uso do cigarro eletrônico,
concentrando-nos na popularidade, interesse, aceitação e rejeição
do produto no contexto brasileiro. Dessa forma, a pergunta central
deste estudo é: Como a população brasileira tem percebido o uso do
cigarro eletrônico ao longo dos últimos anos pela ótica de comentários
associados a vídeos compartilhados no YouTube? Para isso, coletamos
dados associados a 8.932 vídeos, pertencentes a 4.925 canais do
YouTube, cobrindo o período de 2018 a 2023. Nossa coleção consiste
em 543.600 comentários em português compartilhados por 361.015
usuários únicos e associados a vídeos sobre cigarros eletrônicos.
Analisamos métricas de produção e engajamento (# canais, # ví-
deos, # visualizações, # curtidas, etc.) para caracterizar a evolução
das mesmas neste tipo de conteúdo. Em seguida, desenvolvemos
e validamos um modelo para inferir o posicionamento (aceitação
ou rejeição) dos usuários sobre o tema a partir dos comentários,
quantificando a dinâmica do posicionamento ao longo do tempo.
As principais contribuições deste estudo são: i) análise em larga
escala da produção de conteúdo associado ao cigarro eletrônico no
contexto brasileiro; ii) desenvolvimento e validação de um modelo
de aprendizado de máquina para estimar a prevalência da aceitação
ou rejeição dos cigarros eletrônicos com base nos comentários asso-
ciados a vídeos informativos do YouTube; e iii) análise da evolução
temporal do engajamento e do posicionamento dos usuários em
relação aos cigarros eletrônicos no Brasil. Em suma, nosso estudo
fornece uma visão ampla e pioneira sobre o consumo e a percepção
pública dos cigarros eletrônicos no Brasil (sob a ótica de usuários
do YouTube), ressaltando a importância de uma discussão mais
urgente e cuidadosa por parte das entidades responsáveis.
TRABALHOS RELACIONADOS
Nos últimos anos, diversos estudos investigaram as interações on-
line com o cigarro eletrônico, focando em aspectos como propa-
ganda online [10, 29], recomendações em mídias sociais [12, 33],
e interações dos usuários com o produto em diferentes platafor-
mas [12, 30, 40, 45, 46]. Nesta seção, revisamos os trabalhos mais
próximos ao nosso objetivo.
Massey et al. [30] exploraram a exposição a anúncios de vaporiza-
ção em smartphones e seu impacto em jovens adultos, constatando
que o uso de Snapchat, Instagram e Facebook estava ligado a maior
exposição. De forma semelhante, Sun et al. [45] analisaram vídeos
sobre cigarros eletrônicos no TikTok, identificando temas como
comédia, estilo de vida e marketing. Vassey et al. [46] estudaram o
conteúdo de cigarros eletrônicos no Instagram e o efeito da inter-
venção do FDA, observando que, apesar do aumento nos rótulos
de advertência, ainda eram insuficientes. Dashtian et al. [12] usa-
ram modelagem de rede para entender como diferentes termos de
busca no YouTube afetam os resultados relacionados a cigarros
eletrônicos.
Kong et al. [28] avaliaram vídeos de truques com cigarros ele-
trônicos no YouTube, descobrindo que metade dos vídeos eram
patrocinados pela indústria. Bigwanto et al. [6] monitoraram con-
tas do Instagram, destacando a promoção desenfreada dos produtos
devido à falta de regulamentação. Sari Kaunang et al. [38] e Adhi-
kari et al. [1] analisaram tweets para prever opiniões e identificar
tópicos relacionados a cigarros eletrônicos e maconha. Por outro
lado, Hassan et al. [22] usaram o modelo LDA para analisar tweets
sobre EVALI, enquanto Murthy et al. [33] estudaram como idade e
sexo afetam os resultados de busca no YouTube.
Embora esses trabalhos forneçam percepções de grande impor-
tância, destaca-se o baixo volume de dados utilizados, seja em nú-
mero de comentários [22, 38], vídeos analisados [28, 33], termos
analisados [28, 33], ou período de cobertura [22]. Além disso, estes
estudos não focam no cenário brasileiro. Neste sentido, este tra-
balho visa preencher a lacuna deixada pelas pesquisas anteriores,
com uma cobertura temporal abrangente especificamente para o
cenário brasileiro, utilizando um grande volume de dados de vídeos
e comentários, com o objetivo de compreender o posicionamento
do público em relação ao cigarro eletrônico, focando no interesse,
popularidade e aceitação do produto nos últimos anos.
METODOLOGIA
Esta seção descreve a coleta e preparação de dados, bem como o
desenvolvimento do modelo de detecção de posicionamento.
3.1
Coleta e Pré-processamento dos Dados
Para a coleta de dados da plataforma YouTube, utilizou-se a Ap-
plication Programming Interface (API) de dados do YouTube V31
para coletar informações sobre os canais, vídeos e os respectivos
comentários e metadados. Inicialmente, os autores realizaram uma
ampla busca na Web focada nas marcas mais vendidas nos últi-
mos anos, doenças relacionadas, termos e variações, expressões
adotadas, entre outros. As palavras-chave foram criteriosamente
selecionadas com base em notícias, artigos e conteúdos disponíveis
na Web relacionados ao tema deste estudo cobrindo o período do
estudo. Em resumo, foram mapeadas marcas reconhecidas no mer-
cado do cigarro eletrônico, líquidos saborizados e expressões de
interesse sobre potenciais problemas de saúde atrelados ao produto.
As expressões foram consideradas para garantir que a coleta de
dados incluísse vários pontos de vista, desde a aprovação (incluindo
potencial interesse, defesa, recomendação, etc.) até a desaprovação
e críticas do produto. Com isso, os seguintes conjuntos de palavras-
chave finais foram definidos:
Marcas :"Vaporesso", "Elfbar", "Ignite", "Jim pod", "Puff mamma", "Smok",
"Uwell", "Voopoo", "Geekvape", "Oxva", "Aspire", "Lost Vape", "Argus", "Juul",
"Nikbar".
Sabores :"Juice vape", "Sabores vape", "Sabores mais populares de juices para
cigarro eletrônico", "Sabores mais populares de juices", "Juice Joyetech Salt Nic",
"Juice Solace", "Juice Caravela Liquids", "Juice BLVK Unicorn Salt", "Juice Zomo
Salt", "Juice Dream Collab", "Juice Firefly E-juice", "Juice Radiola Juice", "Juice
Zomo Vape - Blueberry Iceburst", "Juice Nasty - Cush Man Mango", "Juice Mr.
Freeze - Strawberry Watermelon Frost", "Juice POP! Vapors - Strawberry Kiwi
e Mango Strawberry (Ice)", "Juice Naked - Hawaiian Pog Ice", "Juice Fantasi",
"Juice Caravela", "Juice Mr. Freeze", "Juice Naked 100", "Juice Nasty".
Expressões : "Quantos cigarros equivalem a um vape?", "Cigarro eletrô-
nico faz mal?", "vape faz mal?", "Cigarro eletrônico vicia?", "vape vicia?",
"Cigarro eletrônico ajuda a parar de fumar", "vape ajuda a parar de fumar",
"Cigarro eletrônico tem mais nicotina?", "vape tem mais nicotina?", "Cigarro
eletrônico ou cigarro normal?", "vape ou cigarro normal?", "Efeitos do cigarro
eletrônico na saúde", "Efeitos do vape na saúde", "Riscos do uso de cigarro ele-
trônico para a saúde", "Riscos do uso de vape para a saúde", "Cigarro eletrônico
causa doenças?", "vape causa doenças?", "Impactos do cigarro eletrônico no
1https://developers.google.com/youtube/v3/getting-started
Análise da Percepção do Uso de Cigarros Eletrônicos no Brasil por meio de Comentários no YouTube
WebMedia’2024, Juiz de Fora, Brazil
pulmão", "Impactos do vape no pulmão", "Dependência de cigarro eletrônico",
"Dependência de vape", "Cigarro eletrônico causa dependência?", "vape causa
dependência?", "Diferenças na dependência de cigarro eletrônico e cigarro tra-
dicional", "Diferenças na dependência de vape e cigarro tradicional", "Cigarro
eletrônico é eficaz para largar o cigarro?", "vape é eficaz para largar o cigarro?",
"Experiências de parar de fumar com cigarro eletrônico", "Experiências de parar
de fumar com vape", "Composição química do cigarro eletrônico", "Compo-
sição química do vape", "Concentração de nicotina em cigarros eletrônicos",
"Concentração de nicotina em vapes", "Diferença entre nicotina no cigarro
eletrônico e no cigarro tradicional", "Diferença entre nicotina no vape e no
cigarro tradicional".
Para garantir uma ampla cobertura temporal, cada palavra-chave
definida anteriormente foi submetida a uma busca por meio da API
do YouTube, sendo restringido o retorno de conteúdos referentes
a cada mês entre 2018 e 2023. Dessa forma, um mesmo termo foi
considerado individualmente em cada mês, permitindo uma análise
temporal mais fiel. Em cada busca mensal, coletamos os cinquenta
vídeos mais relevantes conforme o algoritmo de relevância do You-
Tube, que leva em consideração fatores como tempo de visualização,
engajamento (curtidas, descurtidas, comentários, compartilhamen-
tos), novos inscritos gerados pelo vídeo, título, descrições, tags,
histórico de visualização do usuário. Esta abordagem foi adotada
para que a coleta de dados cubra um período de tempo extenso e
forneça uma visão detalhada das tendências e mudanças na popula-
ridade relacionadas ao tema de estudo. Toda a coleta de dados foi
realizada entre dezembro de 2023 e março de 2024. É importante des-
tacar que a coleta de dados foi configurada para focar no contexto
brasileiro, definindo o idioma de relevância (relevance-language)
como português e o código da região (region-code) como Brasil, de
acordo com o formato ISO 3166-1. Na prática, esses parâmetros
direcionam a coleta para conteúdos em português e recomendados
para o Brasil, garantindo que os dados reflitam o contexto cultural e
linguístico do público brasileiro. No entanto, também é possível que
vídeos em outros idiomas sejam direcionados, já que existe a possi-
bilidade de existência de legendas, o que também atrai comentários
em português do público brasileiro.
Os conjuntos de palavras-chave inicialmente proposto foi ela-
borado visando maximizar, ao máximo, a revocação de conteúdo
relacionado ao tema de estudo. Entretanto, uma avaliação inicial
dos resultados retornados pela coleta apontou um impacto na pre-
cisão. Em outras palavras, muitos vídeos não relacionados a cigarro
eletrônico foram retornados, seja pelo uso de palavras-chave re-
lacionadas a sabores que podem estar associadas a vídeos sobre
comida, pela indisponibilidade de cinquenta vídeos de um dado
tema no mês, ou ainda por outros fatores considerados pelo algo-
ritmo de ranqueamento do YouTube, como relevância, histórico de
visualizações e interações, e engajamento do público. Assim, foi
necessário realizar uma filtragem adicional. Utilizando um conjunto
de palavras-chave baseado na consulta anterior, um vídeo e seus
respectivos dados (comentários, canal, métricas de engajamento,
etc) foram mantidos se seu título contivesse pelo menos uma das se-
guintes palavras-chave2: "cigarro eletrônico", "cigarros eletrônicos", "vape",
"vaper", "vaping", "pod", "mod", "ecigar", "e-cig", "tabaco aquecido", "ejuice",
"juice", "nicotina", "nic", "concentração de nicotina", "juices salt nic", "juices
freebase", "uwell", "Aphrodite", "vaporesso", "elfbar", "elf bar", "ignite", "jim pod",
"puff mamma", "smok", "uwell", "voopoo", "geekvape", "oxva", "aspire", "lost
2Essa comparação foi feita de forma case-insensitive.
Tabela 1: Sumário dos dados analisados.
Descrição
Valor
Número de Canais
4.625
Número de Vídeos
8.932
Número de Comentários
543.600
Número de Usuários
361.015
vape", "argus", "juul", "nikbar", "vaporar", "vapor", "puffs", "atomizador", "atty",
"bocal", "chain-vape", "coil", "clapton coil", "deck", "dli", "drip-tip", "dripper",
"dry-burn", "dry-hit", "estilo velocity", "flood", "flooded", "glicerina", "glicerol",
"gunk", "hot-spot", "kick", "mech", "mesh", "mtl", "nicsalt", "zomo", "pen-style",
"pg", "piteira", "rba", "rda", "rta", "rdta", "spitback", "blvk", "MR. FREEZE", "star-
ter kit", "throat hit", "twisted coil", "wick", "wicka", "eliquid", "Eliquid", "e-liquid",
"E-liquid", "e liquid", "E liquid", "liquid", "nasty cig", "ice apple", "grape ice",
"Apple Ice", "strawberry", "mango", "Watermelon", "Kiwifruit", "Ice", "naked",
"fantasi", "flavor juice", "atomizer", "ajuda a parar de fumar", "parar de fumar",
"eficaz para largar o cigarro".
O próximo passo consistiu em identificar, entre os comentários
dos vídeos selecionados, aqueles que estão em português. Essa tarefa
foi necessária porque muitos vídeos populares em inglês e espanhol
podem ter comentários em português, devido à disponibilização de
legendas e ao direcionamento para o público brasileiro. Notamos
que essa situação é comum, pois os criadores de conteúdo frequente-
mente configuram seus vídeos ou canais para serem recomendados
em diferentes países, aumentando a visibilidade e o engajamento
em múltiplas regiões. No entanto, essa tarefa apresenta desafios,
já que os comentários frequentemente contêm termos em inglês
específicos do contexto, como "vape", "juice", "mango" e "e-liquid".
Para lidar com essa complexidade, os comentários foram pro-
cessados usando dois métodos de detecção de idioma: o modelo
RoBERTa (Robustly optimized BERT approach)3, ajustado para a
tarefa de detecção de idioma, e a biblioteca LangDetector4, uma
ferramenta baseada em algoritmos de aprendizado de máquina, co-
nhecida por sua eficiência e precisão na identificação de idiomas
em textos curtos [47]. Dessa forma, optou-se por utilizar ambos os
métodos para mitigar a presença de falsos negativos, mantendo em
nossa análise todos os comentários identificados como sendo em
português por pelo menos um dos dois modelos.
A Tabela 1 descreve a coleção de dados final, incluindo o nú-
mero de canais, vídeos, comentários em português e usuários que
realizaram tais comentários. Vale ressaltar que, durante a etapa
de coleta dos dados, foram recuperados diversos metadados dispo-
níveis, como o número de visualizações e o número de curtidas.
Essas métricas, que simbolizam engajamento e popularidade, serão
utilizadas como indicadores nos resultados a serem apresentados.
3.2
Modelo de Detecção de Posicionamento
Com a coleção de dados final definida, o próximo passo consiste em
desenvolver e validar um modelo de detecção de posicionamento.
Este modelo supervisionado exige uma amostra de dados rotulados
para treinamento. Dessa forma, o objetivo do modelo é inferir o
posicionamento em relação ao cigarro eletrônico, classificando os
comentários como favorável, inconclusivo ou contrário.
3https://huggingface.co/papluca/xlm-roberta-base-language-detection
4https://pypi.org/project/langdetect/
WebMedia’2024, Juiz de Fora, Brazil
Dias et al.
O processo envolve quatro etapas principais. Primeiro, realiza-
mos a amostragem da coleção de dados para obter uma quantidade
de comentários para compor o conjunto de treinamento do modelo.
Em seguida, discutimos e definimos diretrizes claras para a rotu-
lação dos comentários, assegurando consistência na classificação.
Após a apresentação das diretrizes e esclarecimento de eventuais
dúvidas, a rotulação dos comentários foi realizada por voluntários,
e um índice de concordância foi calculado para verificar a consis-
tência e a precisão dos rótulos atribuídos. Finalmente, treinamos
e validamos o modelo com os dados rotulados, classificando os co-
mentários restantes na coleção conforme os três rótulos definidos,
permitindo assim as análises subsequentes.
Neste estudo, para classificar os comentários e identificar o po-
sicionamento como sendo favorável, contrária ou inconclusiva em
relação ao tema, foram definidas as seguintes diretrizes de rotulação:
Favorável: Inclui comentários que explicitamente apoiam, defendem, ex-
pressam interesse ou compartilham conhecimento sobre cigarros eletrônicos
e produtos associados. Isso abrange, mas não se limita a, discussões sobre
sabores, marcas, bateria, métodos de uso, alusões a termos específicos do
vape, cultura vaping e opiniões que refutam alegações sobre possíveis efeitos
adversos à saúde decorrentes do uso de tais produtos.
Contrário: Abrange comentários que explicitamente condenam, criticam
ou apresentam argumentos contrários ao uso de cigarros eletrônicos e
produtos afins. Isso inclui, mas não se limita a, comentários contrários ao
uso de tais produtos, cultura vaping, ou que evidenciem relação direta do
uso destes produtos com riscos à saúde.
Inconclusivo: Destina-se a comentários que não se enquadram nas cate-
gorias anteriores ou que desviam do tópico proposto. Inclui manifestações
que não permitem uma determinação explícita da postura do usuário em
relação aos cigarros eletrônicos, seus sabores, marcas ou variedades, seja
por falta de informação, ambiguidade ou irrelevância do comentário para a
tarefa em questão.
Após a definição das diretrizes, foi gerada uma amostra aleatória
e estratificada de 30 comentários por mês para cada ano, sendo a
data definida pela postagem do comentário. Isso é importante para
desenvolver um modelo mais generalista, considerando o período
analisado5. Dessa forma, os comentários amostrados foram rotula-
dos manualmente por três indivíduos distintos após a apresentação
das diretrizes de rotulação, cada um responsável por analisar e atri-
buir classificações conforme os critérios definidos. Obtivemos 294
comentários rotulados para a classe contrária, 930 para a classe
inconclusiva e 1674 para a classe favorável. Observou-se um grande
desbalanceamento dos dados, com uma maior produção de conteú-
dos favoráveis, referentes a reviews, propagandas e uso de produtos,
em comparação com conteúdos contrários, como reportagens e
vídeos de médicos alertando e explicando os riscos do uso. Esse des-
balanceamento desafia o aprendizado de modelos de classificação,
uma vez que pode levar a vieses para a classe majoritária. [1]
Para minimizar esse efeito, optou-se por estender a amostra para
rotulação, com foco em aumentar o número de comentários con-
trários. Para isso, foi realizada uma nova amostragem, incluindo
mais 15 comentários por mês, também estratificados e aleatórios.
Nessa abordagem, definimos que os comentários deveriam con-
ter palavras-chave relacionadas a um posicionamento contrário,
sendo elas: "adolescente", "água no pulmão", "agente cancerígeno", "ajuda",
5Não é possível garantir que os comentários selecionados não sejam relacionados a um
mesmo canal ou vídeo, pois estratificar em dois níveis, como data e vídeo, resultaria
em um número excessivo de comentários a serem rotulados.
"alerta", "armadilha", "aroma", "avc", "consequências", "consumo", "criancinha",
"criança", "câncer", "dependência", "desenfreado", "doente", "doença", "edema
pulmonar", "embolia pulmonar", "epidemia", "escola", "evali", "falta de ar", "faz
mal", "febre", "fogo", "fumantes", "hospital", "infantil", "infarto", "inflamação",
"internado", "internação", "intubado", "intubação", "jovem", "jovens", "largar
o cigarro", "larguei", "lesionado", "lesão", "livre", "mal", "malefícios", "matar",
"morrer", "morte", "nunca mais fumei", "não fumem", "não é saudável", "parar
de fumar", "pare de fumar", "parem de fumar", "perfuração pulmonar", "perigo",
"perigoso", "porcaria", "proibir", "proibição", "pulmonar", "pulmão", "raiva", "re-
portagem", "repórter", "risco", "saúde", "tontura", "tosse", "tossindo", "veneno",
"vicia", "vomitando", "vomitei", "vomitou", "vício", "vômito".
Da mesma forma, seguindo as mesmas diretrizes, a rotulação foi
feita nessa amostra adicional. Essa estratégia permitiu aumentar a
representação de comentários que expressam uma posição contrária
em relação ao uso do cigarro eletrônico, permitindo o treinamento
de um modelo com maior capacidade de identificar e classificar
adequadamente o posicionamento dos comentários. É importante
destacar que os comentários selecionados possuem identificadores
únicos, desta forma não houve a inclusão de comentários duplica-
dos, entretanto pode ocorrer que existam mensagens semelhantes,
o que é comum nas mídias sociais, mas isso não compromete a in-
tegridade da nossa coleção detectar os posicionamentos. Em suma,
foram rotulados 3369 comentários, cuja distribuição de rótulos con-
siste em 755 contrários, 915 inconclusivos e 1699 favoráveis. Em
casos em que o comentário foi rotulado de forma diferente por pelo
menos um dos voluntários, definimos o rótulo através da classe
majoritária, ou seja, a classe que recebeu mais votos entre os vo-
luntários. Posteriormente, o coeficiente de concordância Kappa de
Fleiss foi calculado [17], resultando em um valor de 0.76, indicando
um nível de concordância substancial entre os rotuladores.
Para a tarefa de detecção de posicionamentos, utilizamos o mo-
delo BERTimbau Large, pré-treinado em português e considerado
um dos estados da arte para o idioma, realizando o ajuste fino para
esta tarefa específica com base nos dados rotulados [42]. O ajuste
fino é um processo iterativo de treinamento que envolve treinar
um modelo BERT pré-treinado em um conjunto de dados rotu-
lado. Durante esse processo, o modelo incorpora seu conhecimento
linguístico existente com o conhecimento específico da tarefa, ajus-
tando seus parâmetros. Esse procedimento melhora o desempenho
do modelo na tarefa alvo ao aproveitar tanto seu conhecimento
linguístico geral quanto as adaptações específicas da tarefa. Neste
caso, o objetivo é melhorar a separabilidade das classes de posi-
cionamentos nos comentários e criar uma rede capaz de inferir
se o posicionamento de um comentário é favorável, contrário ou
inconclusivo em relação ao uso de cigarros eletrônicos.
Seguindo as recomendações da literatura [11, 20], realizamos
uma busca de hiperparâmetros para ajustar o modelo [44]. Os pa-
râmetros variáveis incluíram taxa de aprendizado (1e-5, 2e-5, 3e-5,
4e-5, 5e-5), probabilidade de dropout (0.1, 0.2, 0.3, 0.4), decaimento
de peso (0.01, 0.1, 1) e até 10 épocas. Utilizamos validação cruzada de
5 partições (5-fold cross-validation) para determinar os melhores pa-
râmetros e avaliar a habilidade de generalização do modelo treinado.
Através desse processo, identificamos que os melhores parâmetros
eram uma taxa de aprendizado de 2e-5, probabilidade de dropout
de 0.3, e decaimento de peso de 0.1, 2 épocas, considerando lotes
de tamanho 4. O modelo final foi então treinado utilizando essas
Análise da Percepção do Uso de Cigarros Eletrônicos no Brasil por meio de Comentários no YouTube
WebMedia’2024, Juiz de Fora, Brazil
Ano
#videos
(a) # vídeos
Ano
#Canais
(b) # canais
Ano
10000
15000
20000
25000
#comentários
(c) # comentários
Ano
0.0
0.2
0.4
0.6
0.8
1.0
# Visualizações
1e9
(d) # visualizações
Ano
50000
100000
150000
# Curtidas
(e) # curtidas
Ano
10000
15000
# Usuários
(f) # usuários únicos que comentaram
Figura 1: Caracterização das métricas de popularidade e engajamento ao longo do tempo.
configurações, e reportamos a média e o intervalo de confiança de
95% das métricas de validação cruzada ao longo das 5 partições.[1]
RESULTADOS
Nesta seção, apresentamos nossos resultados, incluindo a caracteri-
zação do conteúdo, engajamento, e a análise de posicionamento.
4.1
Produção de Conteúdo e Engajamento
Iniciamos nossa análise mensurando a dinâmica da produção de
conteúdo (Figuras 1(a), 1(b) e 1(c)), bem como o engajamento (Fi-
guras 1(d), 1(e) e 1(f)) durante o período analisado. Observando
o número de vídeos criados (Figura 1(a)), notamos um aumento
expressivo na quantidade de vídeos produzidos sobre cigarros ele-
trônicos a partir de 2021. Esse crescimento contínuo reflete um
aumento no interesse pelo tema, com um pico notável em 2023.
A tendência ascendente sugere que a discussão e o compartilha-
mento de informações sobre cigarros eletrônicos se tornaram mais
comuns ao longo do tempo. Da mesma forma, o número de canais
(Figura 1(b)) que publicam conteúdo relacionado ao tema também
apresentou um crescimento consistente, especialmente a partir de
2021. Esse aumento no número de canais indica que mais criadores
de conteúdo têm explorado o tópico, contribuindo para uma maior
diversidade de opiniões e informações disponíveis sobre cigarros
eletrônicos. Já o número de comentários (Figura 1(c)) segue a mesma
tendência de crescimento, com um aumento substancial a partir
de 2021, indicando um maior engajamento dos espectadores com
o conteúdo. Esse crescimento no número de comentários reflete
uma maior interação entre os usuários e os vídeos, sugerindo que o
tema tem gerado mais discussões e debates entre o público. Vale
ressaltar, que os resultados apresentados nesta seção, referem-se
ao conjunto completo dos dados, englobando todas as informações
coletadas durante o período analisado.
Além da produção de conteúdo, também analisamos métricas de
popularidade e engajamento. O número de visualizações (Figura
1(d)) apresenta picos periódicos, especialmente em 2018 e 2020,
indicando momentos de maior interesse pelo tema. Esses picos po-
dem estar associados a eventos específicos, como lançamento de
produtos, cobertura midiática ou outras questões relacionadas aos
cigarros eletrônicos. O número de curtidas (Figura 1(e)) já mostra
uma tendência de crescimento, com um aumento expressivo a partir
de 2021. Isso sugere que os vídeos passaram a atrair mais avaliações
positivas dos espectadores, evidenciando uma aprovação crescente
do conteúdo. Por fim, o número de usuários únicos que comentam
(Figura 1(f)) também cresceu substancialmente a partir de 2021. Esse
aumento indica que mais indivíduos estão se interessando e partici-
pando mais das discussões sobre cigarros eletrônicos, ampliando o
alcance e a diversidade das opiniões expressas nos comentários.
Comparados aos dados das pesquisas da Covitel [31] e da In-
teligência em Pesquisa e Consultoria Estratégica (IPEC) [14], que
mostram um crescimento no uso, experimentação e conhecimento
do cigarro eletrônico em regiões específicas do país, nossos resulta-
dos sugerem que o aumento do conhecimento sobre o tema pode ser
ainda maior do que o estimado. Todas as métricas de popularidade
e engajamento analisadas mostram um crescimento maior do que
o apresentado pela pesquisa ao longo do tempo, especialmente a
partir de 2021. Esse aumento na produção de conteúdo e no en-
gajamento do público, evidencia a crescente relevância do tema
dos cigarros eletrônicos na plataforma YouTube. A análise dessas
métricas sugere que a discussão sobre cigarros eletrônicos têm se
intensificando, com uma quantidade maior de vídeos, canais, visua-
lizações, curtidas e comentários, refletindo um interesse crescente
e uma interação mais ampla com o tema.
Visando compreender o interesse pelas marcas de cigarro ele-
trônico e produtos relacionados, analisamos o número de menções
nos comentários ao longo do tempo. No mapa de calor da Figura
WebMedia’2024, Juiz de Fora, Brazil
Dias et al.
JAN/18
FEV/18
MAR/18
ABR/18
MAI/18
JUN/18
JUL/18
AGO/18
SET/18
OUT/18
NOV/18
DEZ/18
JAN/19
FEV/19
MAR/19
ABR/19
MAI/19
JUN/19
JUL/19
AGO/19
SET/19
OUT/19
NOV/19
DEZ/19
JAN/20
FEV/20
MAR/20
ABR/20
MAI/20
JUN/20
JUL/20
AGO/20
SET/20
OUT/20
NOV/20
DEZ/20
JAN/21
FEV/21
MAR/21
ABR/21
MAI/21
JUN/21
JUL/21
AGO/21
SET/21
OUT/21
NOV/21
DEZ/21
JAN/22
FEV/22
MAR/22
ABR/22
MAI/22
JUN/22
JUL/22
AGO/22
SET/22
OUT/22
NOV/22
DEZ/22
JAN/23
FEV/23
MAR/23
ABR/23
MAI/23
JUN/23
JUL/23
AGO/23
SET/23
OUT/23
NOV/23
DEZ/23
# Comentários por mês
vaporesso
smok
aspire
rgus
juul
ignite
naked
oxva
zomo
nasty
voopoo
elfbar
blvk
fantasi
geekvape
nikbar
uwell
joyetech
mr. freeze
firefly
lost vape
dream collab
caravela liquids
radiola juice
solace
jim pod
puff mamma
pop! vapors
Marcas
0.5
0.0
0.5
1.0
1.5
2.0
Figura 2: Presença de marcas ao longo dos anos ordenado pela popularidade total.
Tabela 2: Desempenho do modelo ajustado.
Classe
P
R
Macro F1
Contrário
0.71 ± 0.02
0.81 ± 0.03
0.76 ± 0.01
Inconclusivo
0.79 ± 0.03
0.73 ± 0.02
0.76 ± 0.02
Favorável
0.85 ± 0.01
0.83 ± 0.03
0.84 ± 0.01
2, o eixo 𝑥representa os meses ao longo do período de 2018 a
2023, enquanto o eixo 𝑦lista as diferentes marcas relacionadas ao
cigarro eletrônico, ordenadas pela popularidade, ou seja, o número
de comentários que fazem menções a elas. Cada célula indica o nú-
mero de comentários mensais, normalizados por coluna por meio
da normalização z-score6. Dessa forma, nosso objetivo é mostrar, a
cada mês, quais foram as marcas mais populares e mencionadas. A
intensidade da cor em cada célula reflete a quantidade de comentá-
rios normalizada, em que células mais escuras indicam um maior
número de comentários.
Em geral, é possível notar diferentes padrões caracterizados
por marcas constantemente populares, outras emergentes e outras
pouco populares. Por exemplo, as marcas Vaporesso e Smok mantêm
uma presença consistente no mercado desde 2018, enquanto mar-
cas como Aspire, Argus, Ignite e Elfbar começam a intensificar sua
popularidade a partir de 2021. Isso sugere que existe um mercado
estabelecido e uma crescente diversidade de interesse por diferen-
tes produtos ao longo do tempo. Esses padrões distintos revelam a
longevidade de certas marcas, bem como a dinâmica de entrada e
crescimento de novas marcas no mercado de cigarros eletrônicos.
6https://en.wikipedia.org/wiki/Standard_score
4.2
Detecção e Análise de Posicionamentos
Relembre que um dos nossos objetivos é inferir os posicionamentos
dos usuários e a percepção da população por meio dos comentários.
Seguindo a metodologia explicada na Seção 3, o modelo foi ajustado
para identificar o posicionamento com base nos dados rotulados.
A Tabela 2 mostra o desempenho final do modelo considerando
as métricas Precisão (P), Revocação (R) e Macro F1-escore (F1)
para cada classe, com o intervalo de confiança (I.C.) de 95% nas 5
partições.
O modelo apresenta um bom desempenho na classe contrária,
com uma precisão média de 0.71, revocação de 0.81 e F1-escore
de 0.76. Isso indica que o modelo é mais confiável na detecção
de comentários contrários, com alta revocação sugerindo que a
grande maioria dos comentários contrários foram identificados
corretamente. É importante destacar que identificar comentários
contrários é mais difícil, pois a maioria do conteúdo no YouTube
sobre o mundo vape é voltada para propaganda, análises e uso de
produtos.
Para a classe inconclusiva, o modelo alcançou uma precisão mé-
dia de 0.79, revocação de 0.73 e F1-escore de 0.76. Embora a precisão
seja alta, a revocação ligeiramente menor sugere que alguns co-
mentários inconclusivos podem não ter sido capturados, o que é
esperado devido à natureza ambivalente desses comentários. Essa
classe é inerentemente desafiadora para o modelo, pois os comen-
tários frequentemente não apresentam um posicionamento claro.
Na classe favorável, o modelo teve o melhor desempenho, com uma
precisão média de 0.85, revocação de 0.83 e F1-escore de 0.84, indi-
cando uma alta capacidade do modelo para detectar comentários
favoráveis com equilíbrio entre precisão e revocação. Em resumo,
o modelo demonstrou um desempenho sólido em todas as classes,
com métricas de F1-escore indicando boa precisão e revocação, e
Análise da Percepção do Uso de Cigarros Eletrônicos no Brasil por meio de Comentários no YouTube
WebMedia’2024, Juiz de Fora, Brazil
intervalos de confiança pequenos, com pouca variação nos resulta-
dos, garantindo a confiabilidade para inferir os posicionamentos
dos usuários. Ressalta-se, ainda, que estudos de detecção de posi-
cionamento, especialmente, em português, têm reportado valores
comumente inferiores [32, 37].
Para mostrar a dificuldade dessa tarefa, alguns comentários clas-
sificados incorretamente são apresentados na Tabela 3. Esses exem-
plos apresentam situações onde os comentários exibem sarcasmo ou
não indicam uma opinião clara, dificultando a classificação correta
pelo modelo. Por exemplo, o primeiro comentário foi classificado
como contrário pelo modelo devido ao uso de palavras de baixo
calão e menção a sintomas negativos relacionados ao uso do cigarro
(“pulmão preto"), embora o comentário sugira que o cigarro eletrô-
nico não faça mal à saúde em comparação ao cigarro tradicional,
expressando assim uma opinião favorável. O segundo comentário
mostra um usuário interessado em adquirir um produto, mas é
pouco discriminativo, pois apenas a menção da marca indica que o
comentário se refere à aquisição, e por tanto, interesse no produto.
Já o terceiro comentário não deixa claro se o usuário está defen-
dendo o uso do cigarro eletrônico ou do cigarro convencional, sendo
classificado como contrário pelo modelo, possivelmente devido aos
termos “médico" e “câncer". O próximo exemplo consiste em uma
pergunta que não deixa claro se é uma ironia, resultando em um
rótulo de inconclusivo. No entanto, o modelo o classificou como
favorável, possivelmente devido ao grande volume de comentários
argumentando que o cigarro eletrônico é similar ao processo de
inalação por nebulizadores. O último exemplo é um comentário
que possui pouco significado semântico e contém ironia, o que
contribuiu para sua classificação incorreta.
01/2018
01/2019
01/2020
01/2021
01/2022
01/2023
12/2023
Data
100000
200000
300000
# Comentários Acumulados
Inconclusivos
Favoráveis
Contrários
Figura 3: Análise temporal dos comentários por classe.
Com o modelo ajustado e validado por meio do ajuste fino e
validação cruzada, estimamos a tendência temporal acumulada dos
comentários para as três classes de posicionamento preditas pelo
modelo ao longo do período analisado, como apresentado na Figura
3. A partir de 2021, observa-se um aumento expressivo nos comentá-
rios de todas as classes, indicando uma intensificação do interesse e
participação do público brasileiro. A grande quantidade de comentá-
rios inconclusivos, apesar de não representarem um posicionamento
claro, indica que muitas pessoas estão interessadas na discussão
sobre o tema. Nota-se ainda um aumento muito maior e mais rápido
de comentários favoráveis do que contrários. Esse fenômeno pode
sugerir uma aceitação crescente dos cigarros eletrônicos entre os
usuários do YouTube, que é a principal plataforma de conteúdo de
vídeo do Brasil.
% Comentários em um dado vídeo (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(x<=X)
Favoráveis (Top 1%)
Contrários (Top 1%)
Favoráveis (Demais)
Contrários (Demais)
Figura 4: CDF do percentual de comentários favoráveis e con-
trários nos vídeos mais (Top 1%) e menos populares (Demais).
Para analisar a prevalência de favoráveis e contrários nos vídeos
mais populares em termos do número de comentários recebidos
(top 1%) e nos demais, calculamos a Função de Distribuição Acu-
mulada do percentual de comentários favoráveis e contrários nos
vídeos, apresentada na Figura 4. Observamos que, para os vídeos
mais populares (Top 1%), o percentual de comentários favoráveis
tende a ser maior do que o percentual de comentários contrários.
Focando nos top 1%, observamos que para 70% dos vídeos, até 40%
dos comentários são favoráveis, enquanto o percentual de comentá-
rios contrários é de aproximadamente 20%. Para os vídeos menos
populares (Demais), essa diferença é ainda maior, com 70% dos
vídeos apresentando até 80% de comentários como favoráveis, en-
quanto o percentual de comentários contrários para os mesmos
70% dos vídeos é de no máximo 30%. Em geral, os vídeos mais po-
pulares, que recebem milhares de comentários, tendem a receber
comentários favoráveis. Esses vídeos podem ser tanto de conteúdo
originalmente favorável ao tema, como por exemplo, reviews de
produtos relacionados, quanto reportagens, onde conteúdo com
argumentos contrários são apresentados. Nos vídeos menos popu-
lares, que representam 99% da base de dados, essa discrepância é
ainda maior, com uma predominância expressiva de comentários
favoráveis.
Investigamos também os canais que mais produziram vídeos
sobre o cigarro eletrônico e identificamos os cinco principais: Mike
Vapes, VapersBrazil, El Mono Vapeador, Wendy V Reviews e VOOPOO.
Em geral, esses canais compartilham experiências com o produto.
Em relação aos canais que mais geraram comentários favoráveis,
destacam-se: Zona do Vapor, VapersBrazil, Olá, Ciência! e Paulo
Jubilut. Os canais Zona do Vapor e VapersBrazil são conhecidos por
apresentar reviews de diferentes modelos e sabores, atraindo um
público que busca informações positivas e recomendações. Já os
canais Olá, Ciência! e Paulo Jubilut abordam o cigarro eletrônico
com uma perspectiva científica e técnica, recebendo comentários
que frequentemente defendem o produto e, portanto, favoráveis. Por
outro lado, os canais que mais receberam comentários contrários são:
Paulo Jubilut, Olá, Ciência!, Zona do Vapor, Drauzio Varella e João
Caetano. Esses comentários contrários vêm geralmente de usuários
que acessam os vídeos para criticar o uso do cigarro eletrônico e/ou
discutir os riscos à saúde. É importante destacar também que alguns
desses canais, como Paulo Jubilut e Olá, Ciência!, recebem tanto
comentários favoráveis quanto contrários, refletindo uma batalha
de debates sobre o uso do produto.
Para compreender os termos mais usados em comentários favo-
ráveis e contrários, analisamos nuvens de palavras dos cinco vídeos
com maior percentual de cada classe. A Figura 5(a) mostra a nuvem
WebMedia’2024, Juiz de Fora, Brazil
Dias et al.
Tabela 3: Exemplos de comentários classificados incorretamente.
Comentário
Real
Predita
Vai tomar no ** bando de burro os cara da Record eu uso cigarro eletrônico a 2 anos e meu pulmão ta
de boa e meu amigo usa cigarro já ta com pulmão preto
Favorável
Contrário
Gente a entrega no site da ignite Brasil demora muito?
Favorável
Inconclusivo
Eu ja não ligo mais, tenho câncer que ta em ultimo estados, os médicos me deram 2 meses, vou usar
ate não conseguir mais andar
Inconclusivo
Contrário
E se eu colocar soro em vez de nicotina vai ser como se eu tivesse fazendo inalação?
Inconclusivo
Favorável
O povo só inventa merda pra se matar kkkkkkkk
Contrário
Inconclusivo
(a) Favoráveis
(b) Contrários
Figura 5: Nuvens de palavras com os top 5 vídeos com maior percentual de comentários de cada classe.
de palavras de comentários favoráveis, onde se destacam termos
como "uso", "vape", "bem", "fumaça" e "vapor". Esses termos indicam
uma associação direta entre a defesa do produto e sua prática de
consumo, sugerindo que o uso do cigarro eletrônico é benéfico à
saúde porque não produz fumaça, mas sim vapor. Por outro lado, a
Figura 5(b) apresenta a nuvem de palavras dos cinco vídeos com
maior percentual de comentários contrários. Entre os termos mais
frequentes estão "dores", "vício", "pulmão", "câncer", "mal" e "saúde",
refletindo os relatos e preocupações com os efeitos adversos à saúde.
Esses termos indicam que os usuários que expressam posições con-
trárias ao uso dos cigarros eletrônicos estão cientes dos potenciais
danos à saúde, contrastando com as opiniões favoráveis observadas
anteriormente.
Em resumo, a análise de posicionamentos revela importantes
implicações sociais sobre a percepção pública do cigarro eletrônico.
Observamos um crescimento notável na produção de conteúdo e
no engajamento dos usuários, refletindo um crescente interesse
pelo tema. A predominância de comentários favoráveis sugere um
forte movimento de defesa e promoção dos cigarros eletrônicos,
mesmo em canais informativos que também recebem muitos comen-
tários contrários. Esses canais, muitas vezes destinados a discutir os
riscos à saúde, acabam sendo alvos de defensores do produto, mos-
trando uma divisão acirrada nas opiniões. As nuvens de palavras
evidenciam essa divisão, com termos relacionados à defesa do uso
do cigarro eletrônico, como "vapor", "uso" e "bem", contrastando
com termos associados a preocupações com a saúde, como "dores",
"pulmão" e "mal".
CONCLUSÃO E TRABALHOS FUTUROS
Este trabalho teve como objetivo investigar a popularidade e a
aceitação dos cigarros eletrônicos no Brasil por meio da análise
de comentários em vídeos conteúdos no YouTube. Considerando
a literatura existente, este trabalho oferece uma análise temporal
mais abrangente e detalhada do debate em torno desse produto no
contexto brasileiro. Nossos principais resultados revelam um au-
mento significativo na produção de conteúdo e no engajamento dos
usuários, refletindo um crescente interesse do público pelos cigarros
eletrônicos. O modelo de detecção de posicionamento desenvolvido
identificou uma prevalência maior de comentários favoráveis do
que contrários nos vídeos coletados, indicando uma aceitação cres-
cente desses produtos entre os usuários da plataforma. Além disso,
a análise temporal destacou a intensificação do debate a partir de
2021, com um aumento substancial nos comentários de todas as
classes analisadas, sugerindo um maior engajamento, interesse e
polarização das opiniões nos últimos anos. A importância desses
achados consiste em uma compreensão mais profunda e inédita
das percepções públicas sobre cigarros eletrônicos no Brasil em
plataformas de vídeos online.
Como trabalhos futuros, planejamos replicar e comparar o es-
tudo com dados de outros países, a fim de identificar possíveis
diferenças e semelhanças em função de aspectos culturais e regula-
mentários. Além disso, investigar a prevalência de desinformação
sobre cigarros eletrônicos junto à uma equipe multidisciplinar.
Análise da Percepção do Uso de Cigarros Eletrônicos no Brasil por meio de Comentários no YouTube
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30102.txt ---

--- INÍCIO DO ARQUIVO: 30103.txt ---
Análise de sentimentos de conteúdo compartilhado em
comunidades brasileiras do Reddit:
Avaliação de um conjunto de dados rotulados por humanos
Giovana Piorino
Universidade Federal de Minas Gerais
giovana.piorino@dcc.ufmg.br
Vitor Moreira
Universidade Federal de Minas Gerais
vitormoreira@dcc.ufmg.br
Luiz Henrique Quevedo Lima
Universidade Federal de Minas Gerais
luiz.quevedo@dcc.ufmg.br
Adriana Silvina Pagano
Universidade Federal de Minas Gerais
apagano@ufmg.br
Ana Paula Couto da Silva
Universidade Federal de Minas Gerais
ana.coutosilva@dcc.ufmg.br
WebMedia’2024, Juiz de Fora, Brazil
Giovana Piorino, Vitor Moreira, Luiz Henrique Quevedo Lima, Adriana Silvina Pagano, and Ana Paula Couto da Silva
TRABALHOS RELACIONADOS
Estudos exploraram a análise de sentimentos em textos de redes
sociais em português brasileiro, tendo alguns deles disponibilizados
publicamente os conjuntos de dados utilizados nos trabalhos.
Dentre eles, os autores [36] realizaram análises textuais e de
sentimento com base em textos em português brasileiro da rede
social Twitter. O trabalho mostra diferentes metodologias para
auxiliar análises textuais com enfoque nessa rede social, como o
Twittómetro e o Amazon Mechanical Turk2.
Outras análises, como em [14], também exploraram a detecção de
tópicos e a análise de sentimentos em textos do Twitter no contexto
brasileiro, com ênfase em temas relacionados à COVID-19. A abor-
dagem de extração de tópicos utilizada foi a LDA (Latent Dirichlet
Allocation), e a análise de sentimentos para os textos em português
obteve auxílio do mUSE(Multilingual Universal Sentence Encoder for
Semantic Retrieval), e do SemEval 2018. Ainda relativo ao Twitter,
os autores em [40] coletaram tweets em português brasileiro de
forma a compor um conjunto de dados 3 de 15.000 tweets, extraídos
entre janeiro e julho de 2017. Para esse estudo, os tweets também
foram classificados com os rótulos positivo, negativo e neutro, por
anotadores cuja anotação obteve métricas de alfa de krippendorf
de 0,529, considerada uma concordância moderada.
Dentre os modelos direcionados à língua portuguesa e à análise
de sentimentos, tem-se o VADER[16](Valence Aware Dictionary for
Sentiment Reasoning), que apresenta uma extensão para a língua
portuguesa chamada LeIA [1] (Léxico para Inferência Adaptada),
a qual rotula textos entre categorias positivo, negativo e neutro,
podendo se adaptar a diferentes contextos, sem se restringir ao
escopo de textos de uma rede social específica.
No que diz respeito a trabalhos relacionados à rede social Red-
dit, os autores [6] utilizaram o modelo GoEmotions baseado em
um conjunto de dados com aproximadamente 58.000 comentários
rotulados manualmente com categorias de emoções, redigidos em
inglês e traduzidos para o português. O estudo também realizou
correlações linguísticas entre as emoções identificadas e os comen-
tários rotulados, e obteve métricas de avaliação das anotações e do
modelo. No entanto, devido à grande quantidade de categorias de
emoções presentes na rotulação, as métricas geraram valores, em
geral, moderados ou fracos para a tarefa de anotação.
Em [17], os autores utilizaram conjunto de dados de textos extraí-
dos do Twitter e do Reddit para avaliar distintas configurações de
pipelines de pré-processamento dos textos em português brasileiro,
passíveis de serem implementadas antes da aplicação de métodos
de modelagem de tópicos. As adaptações avaliadas evidenciaram
melhoras em todas as métricas.
Apesar do recente crescimento da rede social Reddit, ainda há
poucas referências na literatura sobre análises textuais e de tarefas
de anotação de sentimentos em textos dessa rede, principalmente
em português brasileiro. Assim, nosso estudo busca expandir os
recursos de PLN em português brasileiro, fornecendo um conjunto
de dados anotado com sentimentos, juntamente com os resulta-
dos das métricas centrais de avaliação da anotação humana e a
caracterização da linguagem dos textos no conjunto de dados.
2https://www.mturk.com/
3https://bitbucket.org/HBrum/tweetsentbr/src/master/
Tabela 1: Subreddits selecionados e total de postagens e co-
mentários (2022).
Subreddit
Postagens
Comentários
r/brasil
115,876
2,382,928
r/desabafos
115,876
1,487,076
r/futebol
35,826
1,272,009
r/saopaulo
7,308
88,894
r/eu_nvr
12,631
221,348
r/botecodoreddit
7,059
62,999
r/conversas
21,967
355,761
r/investimentos
9,756
156,695
r/tiodopave
2,371
12,106
r/brasilivre
67,301
1,308,441
Total
390,924
7,348,257
METODOLOGIA
Nesta seção primeiramente apresentamos a metodologia utilizada
para coletar o conjunto original de dados. A seguir, descrevemos
a processo de anotação manual de um conjunto selecionado dos
dados. Por fim, apresentamos os métodos usados para a análise
linguística dos comentários anotados.
3.1
Conjunto de Dados
Reddit é uma mídia social online organizada em subcomunida-
des por áreas de interesse ou subreddits, nas quais usuários discu-
tem diferentes assuntos, através de interações do tipo postagem-
comentários, chamadas de threads. Nossa base original de dados
consiste em atividades de usuários (postagens e comentários)4 entre
os meses de janeiro e dezembro de 2022 realizadas nas 10 comuni-
dades brasileiras com maior número de usuários ativos. A Tabela
1 apresenta as principais estatísticas das 10 comunidades selecio-
nadas. Os dados foram coletados a partir da plataforma Pushshift,
que coleta, analisa e arquiva conteúdos do Reddit desde 2015 [4].
Estes dados foram previamente apresentados em [21] e utilizados
para a tarefa de classificação de toxicidade dos comentários com-
partilhados nestes subreddits.
3.2
Anotação dos dados
Para a classificação manual do sentimento associado a cada co-
mentário, foram selecionados 2,000 comentários da base original
coletada, seguindo uma amostra estratificada do total de comen-
tários em cada comunidade analisada. Estes comentários foram
divididos em 4 grupos, com 500 comentários cada, denominados
Grupo1, Grupo2, Grupo3, Grupo4. Cada grupo foi anotado por 3
anotadores distintos.
Os anotadores são estudantes universitários convidados a partici-
par de forma anônima e instruídos a ler e classificar cada comentário
como Positivo, Negativo, Neutro ou Não sei dizer, levando em consi-
deração o sentimento predominante em cada texto. Caso não fosse
possível determinar um sentimento, a opção a ser escolhida deve-
ria ser Não sei dizer. Para auxiliar a identificação do sentimento
predominante, foi sugerido aos anotadores ter atenção especial a
dois pontos: (i) os comentários negativos geralmente manifestam
4O termo ’comentários’ será utilizado abrangendo comentários e postagens.
Análise de sentimentos de conteúdo compartilhado em comunidades brasileiras do Reddit:
Avaliação de um conjunto de dados rotulados por humanos
WebMedia’2024, Juiz de Fora, Brazil
emoções de medo, culpa, mágoa, tristeza, raiva, angústia, ansiedade
e depressão; e (ii) os comentários neutros não apresentam nenhuma
característica que possa levar a sua classificação como negativos
ou positivos.
Ao final do processo de anotação, cada comentário recebeu o
rótulo com o sentimento atribuído pela maioria dos anotadores e
a concordância entre os avaliadores foi medida por três métricas
comumente usadas: Kappa de Fleiss [7], Alpha de Krippendorf [20]
e Concordância Observada [8].
3.3
Classificação Automática de Sentimentos
Para medir a correlação entre a classificação automática e manual
de sentimentos nos comentários amostrados do Reddit, escolhemos
o modelo XLM-RoBERTa (Cross Lingual Language Model - Robustly
Optimized BERT-Pretraining Approach) 5 como nosso baseline, que
está disponível na biblioteca Hugging Face.
O modelo utilizado já havia passado por um ajuste fino baseado
em textos da rede social Twitter em português. A escolha desse
modelo se deve à sua grande base treinada em aproximadamente
10 milhões de tweets na língua portuguesa [3] e a o modelo ser
direcionado à tarefa de análise de sentimentos.
3.4
Análise Textual
Antes de iniciar a análise textual, os 2.000 comentários anotados
foram submetidos a filtros, utilizando-se expressões regulares [13],
com o objetivo de detectar o conteúdo dos comentários a serem
excluídos da análise: endereços de sites, menções a outros usuários,
hashtags, textos citados, datas ou emojis. Risadas expressas em
texto foram removidas, assim como comentários contendo palavras
gramaticais que ocorriam isoladamente e sem valor de informação
para nossa análise, com base na lista de (stopwords) da biblioteca
NLTK[26] e em um modelo do spaCy[37]. Assim, 19 comentários
foram removidos das análises após os filtros.
3.4.1
Razão Type-Token Type-Token Ratio (TTR). Com a tokeniza-
ção dos comentários feita pela biblioteca [25], determinamos a di-
versidade lexical usando a medida TTR. O resultado do TTR advém
do número de tokens distintos dividido pelo número total de tokens
existentes no comentário. Complementamos a análise avaliando o
tamanho (em número de tokens) dos comentários de cada grupo.
3.4.2
Etiquetagem de classe de palavra (Pos Tagging). Para exami-
nar as classes de palavra predominantes nos comentários rotulados,
fizemos o POS tagging [28] com um modelo pré-treinado [37], base-
ado em um treebank anotado de acordo com o padrão das Universal
Dependencies [11]. Esse treebank tem como principal base o traba-
lho de [30].
3.4.3
Reconhecimento de Entidades Nomeadas (REN). Exploramos
as entidades nomeadas através do uso de um modelo pré-treinado do
spaCy. Empregamos novamente o modelo utilizado no Pos Tagging,
sendo o conjunto de dados utilizado para treinar esse modelo o
WikiNER [27]. Essa técnica classifica as entidades em 3 categorias:
PESSOA (PER), LOCALIZAÇÃO (LOC) e ORGANIZAÇÃO (ORG).
Entidades que não se enquadram nessas categorias são classificadas
como DIVERSAS (MISC).
5https://huggingface.co/docs/transformers/model_doc/xlm-roberta
3.4.4
Análise de 𝑛-gramas. . Para complementar as análises lin-
guísticas, realizamos a análise de 𝑛-gramas. Um n-grama é uma
sequência contígua de 𝑛itens de uma determinada amostra de texto.
3.4.5
Classificação de tópicos (BERTopic). Para a extração de tópi-
cos dos comentários utilizamos o modelo BERTopic [15], a fim de
caracterizar os conteúdos mais frequentes dos textos, e como eles se
relacionam com os sentimentos rotulados pelos anotadores e pelo
modelo automático RobERTa. Os comentários foram convertidos
em vetores de representação com auxílio do modelo BERTimbau
[35], no qual há um estágio adicional de ajuste fino direcionado à
similaridade de semântica textual [9] [23] [31].
Para garantir uma modelagem mais consistente dos tópicos foi
realizada a redução da dimensionalidade dos vetores por meio do
UMAP (Uniform Manifold Approximation and Projection for Dimen-
sion Reduction), técnica que melhora agrupamentos subsequentes.
Também foi utilizado o algoritmo HDBSCAN (Hierarchical Density-
Based Spatial Clustering of Applications with Noise), obtendo-se um
agrupamento dos vetores de representação a partir de similaridades
semânticas. Por fim, c-TF-IDF (Class-based Term Frequency-Inverse
Document Frequency) e MMR (Maximal Marginal Relevance) foram
aplicados e ajustados para melhorar a definição de palavras-chave
para os tópicos e para diversificar seu conteúdo semântico, respec-
tivamente. Foram utilizadas recomendações presentes na documen-
tação do modelo6 para o ajuste de tais parâmetros, sendo que para
o UMAP, o número de vizinhos foi ajustado para 10, e o número
de componentes, para 8. Para o HDBSCAN, o número mínimo do
tamanho de agrupamentos é de 10, e número mínimo de amostras,
8. O parâmetro MMR foi atualizado para uma taxa de 0.8.
3.4.6
Rotulações semânticas (PyMUSAS). Para a análise semântica
dos comentários, foi utilizada a ferramenta pyMUSAS, baseada na
estrutura USAS7 (UCREL Semantic Analysis System) adaptada à lin-
guagem Python. Essa classificação apresenta modelos em diferentes
línguas, incluindo o português [29], um dos motivos para seu uso
neste trabalho.
Resumidamente, ela apresenta uma estrutura organizada em có-
digos, que representam categorias semânticas distintas [34]. Cada
comentário pode ser enquadrado em uma ou mais categorias se-
mânticas, fornecendo uma visão ampla e abstrata dos conteúdos
presentes nos comentários e como eles estão relacionados aos sen-
timentos rotulados.
RESULTADOS
Nesta seção, apresentamos os principais resultados obtidos na ava-
liação e caracterização do conjunto de dados anotados.
4.1
Concordância entre anotadores
As métricas para analisar os resultados de concordância entre os
anotadores foram aplicadas aos subconjuntos Todas as anotações,
abrangendo todos os comentários rotulados com as quatro cate-
gorias disponíveis e Apenas sentimentos, abrangendo comentários
rotulados desconsiderando o rótulo Não sei dizer, de forma a verifi-
car o impacto desse rótulo de incerteza nos resultados. A Tabela 2
apresenta os resultados. O Alfa de Krippendorf e o Kappa de Fleiss
6https://maartengr.github.io/BERTopic/index.html
7https://ucrel.lancs.ac.uk/usas/
WebMedia’2024, Juiz de Fora, Brazil
Giovana Piorino, Vitor Moreira, Luiz Henrique Quevedo Lima, Adriana Silvina Pagano, and Ana Paula Couto da Silva
apresentaram valores que podem ser interpretados como concor-
dância moderada entre os anotadores. Já a concordância observada
apresenta valores consideravelmente maiores que as outras métri-
cas, porém não apresenta tanta robustez, por não considerar que
a concordância entre anotadores possa ter acontecido ao acaso.
Em geral, observa-se que a qualidade das métricas melhora consi-
deravelmente ao incluir apenas os comentários rotulados com os
sentimentos e desconsiderar a categoria Não sei dizer.
No que diz respeito à concordância total entre anotadores, isto
é, todos os anotadores indicando o mesmo rótulo, o percentual
de comentários que obtiveram concordância total foi 44.65% no
subconjunto que considera todos os rótulos de anotação. Já no
subconjunto de comentários apenas com rótulos de sentimentos,
a concordância total aumenta para 57%. Alguns exemplos desses
comentários se encontram na Tabela 3.
A fim de estabelecer classificações de sentimentos para análises
posteriores de comparação com modelos automáticos e caracteriza-
ção dos textos, atribuímos às ocorrências de concordância parcial o
sentimento anotado predominante, isto é, a concordância de dois
ou mais anotadores sobre um mesmo rótulo. A Tabela 4 mostra
que quase metade do conjunto de comentários foi majoritariamente
rotulado como negativo, indicando um desbalanceamento de classes
considerável. Já os rótulos positivo e neutro obtiveram proporções
semelhantes. 10,55% dos comentários obtiveram discordância total,
ou seja, cada anotador apontou um rótulo diferente. Este valor de
discordância pode ser o resultado de diferentes perspectivas que
cada anotador pode ter do que é algo positivo ou negativo [24], ou
a presença de conteúdo com teor de sarcasmo ou falta de contexto
adicional para facilitar a atribuição de um sentimento.
A Tabela 5 indica o desempenho de cada trio de anotadores e
suas respectivas métricas, sendo possível observar que a anota-
ção dos comentários pertencentes ao grupo 4 obteve o melhor e
aqueles do grupo 3 o pior desempenho. No entanto, em geral, as
anotações obtiveram métricas de concordância razoavelmente pró-
ximas, apontando para concordâncias médias e moderadas entre
seus respectivos anotadores. De forma complementar, a Tabela 6
apresenta a rotulação de sentimentos para cada anotador, dentro
de cada grupo de comentários.
Uma análise interessante a ser realizada está relacionada ao
grau de incerteza presente na tarefa de rotulação. No total dos
2,000 comentários rotulados, a opção Não sei dizer foi selecionada
por pelo menos um dos três anotadores em 23,05% do conjunto
total. No entanto, apenas em 4,1% dos comentários, dois ou mais
anotadores rotularam o mesmo comentário com Não sei dizer, uma
queda significativa que pode indicar que há uma dificuldade maior
em 2 ou mais anotadores caracterizarem incerteza de sentimento
para um mesmo texto. Um exemplo de texto em que 2 ou mais
anotadores apresentaram incerteza na rotulação é: "Curti muito sua
dupla personalidade, hehe.", o que parece ser uma frase de sarcasmo,
dificultando ainda mais a tarefa de rotulação, mesmo para humanos.
Observamos também grande variação nas categorias escolhidas
pelos anotadores. A Tabela 7 apresenta os resultados das métri-
cas de avaliação de concordância entre anotadores em cada grupo
de comentários. Vemos que para um mesmo grupo de textos, o
anotador 1 assinalou 13,60% dos comentários com Não sei dizer,
enquanto que o anotador 3 atribuiu esse rótulo a apenas 0,40% dos
comentários. Tais resultados reafirmam o apontado na literatura
Tabela 2: Concordância entre anotadores.
Métrica
Todas as anotações
Apenas sentimentos
Kappa de Fleiss
0,40
0,51
Alfa de Krippendorf
0,47
0,53
Concordância observada
0,60
0,70
Tabela 3: Exemplos de comentários que obtiveram concor-
dância total entre os anotadores
Sentimento
Exemplo de comentário
Positivo
Ahh para, eu curto cidadezinha,
as vezes eu vou pra uns lugares desse,
fico uns 2 ou 4 dias, acho super legal.
Negativo
Intervencionismo externo visando ganho
próprio e sem estudar a situação complexa
e possiveis consequencias. Um clássico
dos estados unidos de m*rda
Neutro
Subsidio para quem vender preferencialmente
para o mercado interno ou o contrario,
cobrar mais imposto sobre o produto exportado.
Tabela 4: Porcentagem de comentários para cada agrupa-
mento de rotulação e para a discordância total.
Classificação
Porcentagem
Negativo
48,05%
Neutro
20,95%
Positivo
16,30%
Discordância total
10,55%
Não sei dizer
4,15%
Tabela 5: Métricas de avaliação para concordância entre ano-
tadores para cada grupo desconsiderando o rótulo Não sei
dizer.
Métrica
Grupo1
Grupo2
Grupo3
Grupo4
Kappa de Fleiss
0,41
0,39
0,34
0,44
Alfa de Krippendorf
0,48
0,48
0,40
0,50
Concordância observada
0,60
0,58
0,56
0,64
sobre a subjetividade nas avaliações de sentimento e a dificuldade
dessa tarefa. Adicionalmente, analisando os dados correspondentes
na Tabela 6, observa-se que o grupo 1 apresentou, em geral, maior
classificação de comentários positivos, e o anotador 3 desse grupo
foi o que mais rotulou positivamente, por uma grande margem de
diferença em relação aos demais. Por outro lado, o anotador 2 do
grupo 4 foi o que mais rotulou negativamente, ainda que, em geral,
obteve um proporção de anotações consideravelmente constante
em relação aos outros anotadores de seu grupo, que é o que apresen-
tou as melhores métricas de concordância. O grupo 3, grupo com
as mais baixas métricas de concordância, apresentou disparidades
consideráveis entre as proporções de rotulações, tendo o anotador
1 desse grupo demonstrado discrepância razoável de proporções de
rotulações em relação aos anotadores 2 e 3.
Análise de sentimentos de conteúdo compartilhado em comunidades brasileiras do Reddit:
Avaliação de um conjunto de dados rotulados por humanos
WebMedia’2024, Juiz de Fora, Brazil
Tabela 6: Distribuição de rotulação de sentimentos para cada anotador.
Grupo1
Grupo2
Grupo3
Grupo4
Anotador 1
Anotador 2
Anotador 3
Anotador 1
Anotador 2
Anotador 3
Anotador 1
Anotador 2
Anotador 3
Anotador 1
Anotador 2
Anotador 3
Positivo
22,8%
16,6%
35,4%
19,6%
18,8%
19,8%
17,6%
24,0%
10,0%
15,2%
15,4%
14,8%
Negativo
47,6%
46,8%
38,4%
45,4%
50,2%
44,0%
55,6%
43,6%
48,0%
42,0%
57,2%
46,6%
Neutro
16,0%
28,0%
25,8%
24,4%
21,4%
14,0%
19,4%
27,2%
25,2%
25,8%
27,2%
38,4%
Não sei dizer
13,6%
0,86%
0,4%
10,6%
9,6%
22,2%
7,4%
5,2%
16,8%
17,0%
0,2%
0,2%
Tabela 7: Porcentagem de comentários categorizados como
Não sei dizer por anotador e grupo.
Anotadores
Grupo1
Grupo2
Grupo3
Grupo4
Anotador 1
13,60%
10,60%
7,40%
17,00%
Anotador 2
8,60%
9,60%
5,20%
0,20%
Anotador 3
0,40%
22,20%
16,80%
0,20%
4.2
Comparação com XLM-RoBERTa
O rótulo final de cada comentário foi atribuído com base na con-
cordância de 2 ou mais anotadores. Feita essa atribuição, a fim de
ajustar os rótulos aos que estão presentes no modelo XLM-RoBERTa
(Positivo, Negativo e Neutro), foram removidos comentários em que
houve discordância total entre anotadores e comentários em que a
maioria dos anotadores selecionou Não sei dizer, resultando num to-
tal de 1.706 comentários utilizados para comparação com o modelo
treinado.
Após essa etapa, realizamos a comparação entre os anotadores
e o modelo, que obteve acurácia de 62,37% (porcentagem de co-
mentários em que o rótulo indicado pelo modelo coincidiu com a
anotação humana) e Kappa de Cohen de 0,34, considerado razoavel-
mente fraco [5]. O modelo apresentou as seguintes porcentagens
de sentimentos: 12,49% de comentários positivos, 60,90% de comen-
tários negativos e 26,61% de comentários neutros. A taxa de rótulos
negativos foi consideravelmente superior à da anotação humana,
que é de 48,05%.
A distribuição da concordância entre os grupos foi bem simi-
lar, com variações entre 60% - 65% de concordância do modelo em
relação aos anotadores. O grupo 3 apresentou a menor taxa de
concordância com modelo, com 60,66%, e o grupo 4 apresentou a
melhor taxa, com 65,27%. Tais resultados são análogos aos obtidos
com as métricas de concordância entre anotadores apresentadas
anteriormente, em que os grupos 3 e 4 apresentaram o pior e o
melhor desempenho, respectivamente.
O modelo apresentou uma taxa de concordância para rótulos
negativos razoavelmente alta entre os grupos, variando de 75,10%-
83,33%, enquanto que as taxas de concordância para rótulos posi-
tivos foram as mais baixas entre os grupos, abrangendo uma por-
centagem de rótulos indicados corretamente entre 33,01% e 38,24%.
Isso contrasta com o fato de que sua taxa de rotulações positivas é
similar à taxa dos anotadores, indício de que o modelo exibe grande
dificuldade para identificar corretamente um comentário positivo.
Tal observação se relaciona com os resultados das principais métri-
cas apresentadas na Tabela 8, em que a classe negativo apresentou
valores maiores e mais consistentes que as demais, além do des-
balanceio da amostra citado anteriormente, havendo ocorrências
significantemente maiores de rótulos negativos. Para rotulações
Tabela 8: Métricas de comparação entre anotadores e modelo
XLM-RoBERTa.
Classe
Precisão
Recall
F1-Score
Positivo
0,54
0,35
0,42
Negativo
0,72
0,78
0,75
Neutro
0,44
0,48
0,46
Média Macro
0,57
0,54
0,54
Média Ponderada
0,62
0,62
0,62
Tabela 9: Quantidade de comentários para cada agrupamento
de rotulação e para a discordância total.
Classificação
Quantidade de Comentários
Negativo
Neutro
Positivo
Discordância total
Não sei dizer
para negativo, o grupo 1 apresentou maior taxa de concordância em
relação ao modelo, com 83,33%. Para os rótulos positivo e neutro, o
grupo 4 obteve a maior taxa, com 38,24% e 56,64% respectivamente.
Também buscamos identificar características dos textos em que
o modelo fez uma predição do rótulo errado. Um exemplo de co-
mentário que obteve concordância total entre anotadores, mas que
o modelo errou em sua predição de rótulo, é: "Dai querem vir opinar
no nosso jogo. Americano é f*da... bom que não entendem nada", que
os anotadores indicaram como negativo, mas o modelo considerou
como positivo. Em geral, para as ocorrências de concordância total
dos anotadores, o modelo obteve uma taxa de erros de 38%.
4.3
Caracterização da Linguagem
A comparação dos padrões de linguagem foi realizada por meio do
agrupamento de comentários baseado no rótulo predominante das
3 rotulações feitas pelos anotadores a cada comentário. Utilizou-se
um p-valor < 0,5 em todas as análises para garantir a significância
estatística. A Tabela 9 exibe os dados após a filtragem de textos e,
consequentemente, de comentários que não continham informações
úteis. Esses dados foram utilizados nas análises realizadas.
Razão Type-Token Type-Token Ratio (TTR): Quanto à análise
do TTR, existem diferenças entre as médias de caracteres por co-
mentário nos agrupamentos, em especial, entre o agrupamento
Não sei dizer e os demais. O agrupamento Não sei dizer apresen-
tou a menor média, com 43,13 [29,08, 60,15]. O agrupamento de
comentários neutros teve uma média de 81,41 [69,65, 94,45], já os
agrupamentos de comentários negativos e positivos apresentaram
WebMedia’2024, Juiz de Fora, Brazil
Giovana Piorino, Vitor Moreira, Luiz Henrique Quevedo Lima, Adriana Silvina Pagano, and Ana Paula Couto da Silva
Tabela 10: Porcentagem de etiquetas pos para cada classe
rotulada.
Classificação
NOUN
VERB
ADJ
PROPN
ADV
Negativo
35,51%
30,54%
18,28%
6,28%
4,17%
Neutro
34,97%
27,83%
18,08%
10,07%
3,92%
Positivo
34,49%
32,19%
17,82%
6,31%
3,66%
Não sei dizer
29,84%
26,16%
14,15%
18,41%
2,71%
médias de 98,46 [90,57, 106,66] e 99,13 [80,11, 122,28]. Esses resulta-
dos podem indicar que comentários com sentimentos negativos e
positivos tendem a ser mais longos do que comentários neutros e
aqueles que necessitam de mais contexto para serem interpretados,
categorizados como Não sei dizer. No entanto, ao aplicar o teste es-
tatístico de Mann-Whitney8[38], não foi encontrada uma diferença
entre o agrupamento neutro e o agrupamento positivo. Por outro
lado, ao realizarmos o teste estatístico tanto para compararmos
o agrupamento negativo com o neutro quanto para compararmos
o agrupamento negativo com o positivo, foi demonstrado que há
diferenças significativas entre eles.
Em relação à média e ao intervalo de confiança TTR, os agrupa-
mentos apresentaram os seguintes valores: o agrupamento Não sei
dizer apresenta 0,98 [0,96, 0,99], o agrupamento neutro, 0,97 [0,96,
0,98], o agrupamento negativo, 0,97 [0,96, 0,97] e o agrupamento
positivo, 0,97 [0,97, 0,98]. A análise com o teste de Mann-Whitney
indicou que o único agrupamento com diferença significativa em
relação aos outros foi o agrupamento Não sei dizer. Nos demais re-
sultados, o mesmo teste estatístico será utilizado e, portanto, iremos
omitir o seu nome.
Etiquetagem de classe de palavra (Pos Tagging): A média e o
intervalo de confiança da diversidade de etiquetas POS para cada
agrupamento são os seguintes: o agrupamento Não sei dizer apre-
senta 0,73 [0,66, 0,79], o agrupamento neutro, 0,57 [0,55, 0,60], o
agrupamento negativo, 0,50 [0,48, 0,51] e o agrupamento positivo,
0,56 [0,52, 0,59]. Esses resultados corroboram os obtidos no TTR,
especialmente na diferença entre o agrupamento Não sei dizer e os
demais em termos de diversidade. Vale ressaltar que o agrupamento
negativo possui a menor média.
A Tabela 10 apresenta a representatividade das principais eti-
quetas pos em relação ao total de palavras etiquetadas de cada
agrupamento de comentários. Para um maior aprofundamento, ana-
lisamos a média de palavras classificadas com etiquetas específicas
por comentário, começando pelos adjetivos (ADJ). A média e o
intervalo de confiança para cada agrupamento são os seguintes: o
agrupamento Não sei dizer apresenta 0,93 [0,62, 1,29], o agrupa-
mento neutro, 1,94 [1,58, 2,36], o agrupamento negativo, 2,41 [2,21,
2,62] e o agrupamento positivo, 2,38 [1,90, 2,98]. O agrupamento Não
sei dizer possui a menor média. Ao aplicar o teste estatístico nos
demais agrupamentos, observamos que há diferenças significativas
entre todos eles, a partir de comparações entre negativos e neutros,
negativos e positivos, e positivos e neutros.
Para os substantivos (NOUN), a média e o intervalo de confiança
para cada agrupamento são os seguintes: o agrupamento Não sei
dizer apresenta 1,96 [1,38, 2,65], o agrupamento neutro, 3,76 [3,24,
8O teste Mann-Whitney é um teste não paramétrico utilizado para verificar se dois
grupos de amostras independentes pertencem ou não à mesma população.
4,33], o agrupamento negativo, 4,681 [4,31, 5,06] e o agrupamento
positivo, 4,61 [3,74, 5,66]. Como no caso dos adjetivos, o agrupa-
mento Não sei dizer apresenta a menor média. O teste estatístico
revela diferenças significativas ao confrontarmos o agrupamento
negativo com o positivo e o agrupamento negativo com o neutro,
porém, isso não se prova verdade ao confrontarmos o agrupamento
neutro com o positivo.
A média e intervalo de confiança dos agrupamento para os verbos
(VERB) são os seguintes: o agrupamento Não sei dizer apresenta 1,71
[1,09, 2,52], o agrupamento neutro, 2,99 [2,58, 3,43], o agrupamento
negativo, 4,03 [3,68, 4,38] e o agrupamento positivo, 4,30 [3,45, 5,33].
Observa-se o mesmo padrão para o agrupamento Não sei dizer nas 3
etiquetas. O teste estatístico indica diferenças significativas ao com-
pararmos o agrupamento negativo com o positivo e o agrupamento
negativo com o neutro, mas não mostrou diferenças significativas
ao compararmos o agrupamento neutro com o positivo.
Por fim, o agrupamento de comentários classificados como Não
sei dizer é o único que apresenta mais etiquetas POS de nome
próprio (PROPN) do que de adjetivos (ADJ), como pode ser visto
na Tabela 10. Isso também mostra que esse agrupamento é o que
contém mais nomes próprios.
Reconhecimento de Entidades Nomeadas (REN): Os comentários
classificados como Não sei dizer apresentam uma predominância de
entidades do tipo PER, representando 51% das entidades identifica-
das, seguido por 19% de entidades do tipo LOC, 16% de ORG e 14%
de MISC. Os comentários neutros exibem uma distribuição de 43%
de entidades PER, 25% de LOC, 16% de ORG e 16% de MISC. Já os
comentários positivos mostram 40% de entidades PER, 24% de LOC,
11% de ORG e 24% de MISC. Por fim, nos comentários negativos, 44%
de entidades PER, 35% de LOC, 12% de ORG e 10% de MISC. Esses
dados destacam a predominância de entidades PER no grupo Não sei
dizer, a quantidade de entidades LOC nos comentários negativos e a
presença significativa de entidades MISC nos comentários positivos.
Adicionalmente, considerando os 2000 comentários, nossas aná-
lises mostraram um crescimento no número de entidades menciona-
das de janeiro para fevereiro e de fevereiro para março, em especial,
possivelmente em decorrência da guerra entre Rússia e Ucrânia.
Além disso, existem picos próximos de outubro, coincidindo com
o período eleitoral no Brasil, com exceção do grupo Não sei dizer,
provavelmente por ter poucos comentários, como demonstrado na
Tabela 9.
Análise de 𝑛-gramas: Na análise dos n-gramas, podemos desta-
car os resultados de bigramas dos comentários classificados como
positivo, que frequentemente abordam temas relacionados à vida.
Já para os comentários negativos, evidencia-se lula, bolsonaro. Nos
trigramas de sentimento positivo, temos palavras relacionadas à
conselhos sobre relacionamento (por exemplo sociedade, vê, casais).
os trigramas dos comentários negativos, há a ocorrência da combi-
nação bandido, bandido, morto, possivelmente relacionada a debates
políticos e posicionamentos ideológicos.
Extração de Tópicos (BERTopic): Realizamos a extração de tópicos,
obtendo 15 tópicos correspondentes, ordenados por sua frequência
de ocorrência entre os comentários, apresentados na Tabela 11.
Analisando comentários em que se obteve discordância total entre
os anotadores, os tópicos proporcionalmente mais relacionados
são, em ordem decrescente: 14, 3, 1, 9 e 13. Enquanto os tópicos 3
e 1 são mais genéricos, relacionados a rotina, família e situações
Análise de sentimentos de conteúdo compartilhado em comunidades brasileiras do Reddit:
Avaliação de um conjunto de dados rotulados por humanos
WebMedia’2024, Juiz de Fora, Brazil
Tópico
Negativo
Neutro
Positivo
Sentimento
(a) Sentimentos rotulados por anotadores
Tópico
Negativo
Neutro
Positivo
Sentimento
(b) Sentimentos rotulados pelo modelo
Figura 1: Comparação de frequência de sentimentos para cada tópico entre anotadores e modelo.
do cotidiano, os tópicos 14, 9 e 13 são relacionados a política em
diferentes escopos: o tópico 14 é mais direcionado a ideologias
políticas, sobretudo o nazismo; o tópico 9 relaciona-se a ideia de
fake news, resultado de eleições e partidos políticos, e o tópico 13
relata temas acerca de problemáticas e temas do governo durante a
presidência de Jair Bolsonaro.
Já em relação aos comentários em que houve concordância total
entre anotadores, destacam-se os tópicos 11, 12, 7 e 2. Pelo conjunto
de palavras, em geral, trata-se de tópicos polarizados e que trans-
mitem ideias positivas (tópico 11) ou negativas (tópicos 7,2), além
do tópico 12, que apresenta críticas a governos brasileiros.
Em relação à análise de tópicos, realizamos uma comparação en-
tre os dados anotados por humanos e pelo modelo XLM-RoBERTA,
como pode ser visualizado na Figura 1. Identificamos que os tópicos
5, 12 e 14, relacionados a questões políticas, foram rotulados como
negativos mais pelo modelo do que pela anotação humana. Já o tó-
pico 7, composto por palavrões e palavras que expressam conceitos
negativos no geral, como odeio, pena, horrível, apresentou maior
proporção desse rótulo entre anotadores, comparado ao modelo. Os
tópicos 3 (relacionados a conversas gerais sobre família e rotina)
e 4 (sobre questões financeiras e mercado de trabalho), são razoa-
velmente polarizados entre os anotadores, mas o modelo rotulou
mais como negativos.
Em relação aos comentários em que ao menos um anotador rotu-
lou como Não sei dizer, destacam-se os tópicos 10, 14 e 2. O tópico 10
apresenta 32,6% de seus comentários em que ao menos um anotador
rotulou como Não sei dizer, e apresenta conteúdos relacionados à
crimes, xingamentos e conteúdos sexuais. Já o tópico 14, em que
30,4% de seus comentários apresenta ao menos um rótulo para Não
sei dizer, relaciona-se à questões ideológicas e políticas. Por fim,
o tópico 2, que apresenta 28% de seus comentários com ao menos
um anotador indicando o rótulo, apresenta conteúdos genéricos
relacionados a gírias e relatos do cotidiano. Em relação à comentá-
rios em que todos os anotadores assinalaram como Não sei dizer,
destacam-se 3 comentários nos tópicos 2 e 3, geralmente relaci-
onados a relatos e fatos do cotidiano e gírias. Possivelmente por
apresentarem contextos muito específicos dentro de uma postagem,
são considerados mais difíceis de rotular.
Rotulações semânticas (PyMUSAS): Para os resultados obtidos a
partir da categorização semântica dos comentários, obteve-se, no
total, 163.704 rotulações para níveis semânticos gerais(principais
categorias semânticas, que excluem pontuações, por exemplo), lem-
brando que cada palavra de cada comentário apresenta uma ou mais
rotulações possíveis dentro do domínio das categorias do USAS9.
9https://ucrel.lancs.ac.uk/usas/Lancaster_visual/Frames_Lancaster.htm
Tabela 11: Tópicos e termos mais frequentes.
Tópico
Termos mais Frequentes
pessoa, pessoas, ficar, nada, fazer, aí, coisa, ainda, vida, porque
carro, acho, nunca, vou, uso, desse, lembro, sei, ver, achei
burro, bozo, ai, and, of, p*ca, vem, comida, pode, comentário
nome, filho, criança, banho, banheiro, tomar, p*ta, durante, lembro, deve
dinheiro, pagar, salário, fazer, trabalho, mercado, todos, ganhar, história, sobre
brasil, país, estado, eua, direita, países, rússia, china, nuclear, esquerda
time, goleiro, jogo, gol, futebol, palmeiras, jogador, vasco, paulo, passado
f*da, odeio, mano, tô, p*rra, pqp, tomara, gosto, pena, horrível
palavras, entender, dia, 11, pois, pessoas, falando, palavra, países, comecei
falou, entendi, falei, resultado, fake, hoje, disse, pt, pesquisa, dia
bandido, quer, bunda, p*u, pq, mãos, matar, passou, cima, bola
obrigado, sorte, entendi, comentários, man, respeito, espero, deus, feliz, boa
lula, bolsonaro, bolsonarista, governo, auxílio, gastos, mal, época, presidente, contra
população, política, direito, popular, governo, político, saúde, economia, bolsonaro, passar
socialismo, amp, x200b, hitler, nacional, comunismo, alemães, contrário, dizem, igreja
Considerando todos os comentários, as categorias de maior ocor-
rência são nomes próprios, gírias e palavrões, que compõe 29,64%
das ocorrências totais, termos abstratos, que abrangem ações gerais,
afeto, classificação, avaliação, comparação, posse, importância, faci-
lidade/dificuldade, grau, exclusividade e segurança, que apresenta
17,6%, e termos sociais, que abrangem ações, estados e processos, reci-
procidade, participação, merecimento, traços de personalidade, pessoas,
relacionamentos , família, grupos, obrigação, poder, que apresenta
9,17% do total das ocorrências categorizadas.
Considerando apenas as anotações de sentimentos, observa-se
grande relevância das categorias termos numéricos e julgamentos
de aparência e atributos físicos, como aparência, cor, forma, textura e
temperatura na composição de comentários rotulados como positi-
vos pelos anotadores. Nesse caso, a segunda categoria compõe 6,7%
de todas as rotulações de classes para comentários positivos, contra
1,9% em negativos, e 2,8% em neutros.
Em contrapartida, para os comentários rotulados como negativos,
destacam-se as categorias conceitos de movimento, localização, via-
gem e transporte, bem como conceitos de clima e questões ambientais.
A primeira categoria constitui 9,5% de todas as ocorrências catego-
rizadas em comentários negativos, contra 3,3% para positivos e 5,0%
para neutros. Para comentários rotulados como neutros, destacam-se,
em relação às proporções de comentários negativos e positivos, as
categorias conceitos de ciência e tecnologia, conceitos de dinheiro, ne-
gócios, trabalho e indústria, assim como termos abstratos, que abran-
gem ações gerais, afeto, classificação, avaliação, comparação, posse,
importância, facilidade/dificuldade, grau, exclusividade e segurança.
WebMedia’2024, Juiz de Fora, Brazil
Giovana Piorino, Vitor Moreira, Luiz Henrique Quevedo Lima, Adriana Silvina Pagano, and Ana Paula Couto da Silva
Além disso, a categoria composta por nomes próprios, gírias e
palavrões constitui parte considerável tanto de comentários po-
sitivos(28,65% do total de comentários positivos) quanto para ne-
gativos(29,9%). Assim, conceitos como gírias, palavrões e nomes
próprios podem não ser considerados características predominantes
para determinar o sentimento de um comentário, visto que para am-
bos sentimentos, tais conceitos apresentam presença similar. Essa
questão é abordada ao comparar a anotação humana com o modelo,
que classifica mais tópicos negativamente se forem constituídos
por alguns palavrões, acima da média da rotulação humana para
negativos. Logo, tais padrões semânticos podem levar o modelo
a rotular comentários como negativos excessivamente, devido à
dificuldade de tratar tais padrões no texto.
Para comentários em que houve discordância total entre ano-
tadores, temos as categorias arquitetura, tipos de edifícios e casas,
construções, residência, móveis e acessórios domésticos, conceitos de
dinheiro, negócios, trabalho e indústria e entretenimento em geral,
música, teatro, esportes e jogos. Considerando o total de ocorrências
da categoria arquitetura, tipos de edifícios e casas,construções, resi-
dência, móveis e acessórios domésticos para todos os comentários,
12,38% deles participam da discordância total. Para as categorias
conceitos de dinheiro, negócios, trabalho e indústria e entretenimento
em geral, música, teatro, esportes e jogos, as porcentagens são 10,37%
e 10,10%, respectivamente. Tais categorias compõem as proporções
mais altas de discordância total entre todas as categorias. Esses
resultados indicam certa dificuldade de concordância de anotações
em relação a assuntos específicos que envolvem conhecimento de
mundo do anotador, como arquitetura, entretenimento e mercado
financeiro, por exemplo.
Por fim, a análise de categorias predominantes nos comentários
que os anotadores rotularam com Não sei dizermostra predominân-
cia das categorias conceitos artísticos, artes, artesanato, alimentos, be-
bidas, tabaco e drogas, agricultura e horticultura e educação e estudos.
CONCLUSÕES E TRABALHOS FUTUROS
Os achados da nossa pesquisa corroboram apontamentos na litera-
tura sobre desenvolvimento de conjunto de dados por meio de anota-
ção humana em tarefas que envolvem grande subjetividade, como é
a análise de sentimentos. Um deles diz respeito à concordância entre
anotadores, que, em nosso estudo, se mostrou moderada de acordo
com os resultados do Alfa de Krippendorf e do Kappa de Fleiss.
Também em relação à composição do conjunto de dados, nossos
resultados mostram que quase metade do conjunto de comentários
foi majoritariamente rotulado como negativo, indicando desbalance-
amento de classes considerável, podendo evidenciar um ambiente
mais nocivo de interações.
Ainda em relação aos resultados das métricas de concordância,
as anotações obtiveram valores próximos, apontando para concor-
dâncias médias e moderadas entre seus respectivos anotadores. Em
relação à incerteza, apenas em 4,1% dos comentários, dois ou mais
anotadores rotularam o mesmo comentário com Não sei dizer, o que
revelou dificuldade maior em 2 ou mais anotadores caracterizarem
incerteza de sentimento para um mesmo texto.
Na comparação com os anotadores humanos, o modelo obteve
acurácia de 62,37% e Kappa de Cohen de 0,34, valores considerados
fracos. O modelo rotulou 60,90% dos comentários como negativos,
taxa consideravelmente superior à da anotação humana, que foi
de 48,05%. De fato, determinados tópicos relacionados a questões
políticas foram rotulados como negativos mais pelo modelo do que
pela anotação humana. O modelo também mostrou dificuldade para
identificar corretamente comentários positivos.
A caracterização da linguagem dos comentários revelou que o
tamanho dos comentários categorizados como negativos e positivos
tendeu a ser maior do que o tamanho dos comentários categorizados
como neutros e aqueles categorizados como Não sei dizer. O tamanho
do texto pode impactar a rotulação, uma vez que quanto maior o
contexto, maior a chance de os rotuladores conseguirem fazer uma
interpretação e atribuir um sentimento.
No que diz respeito às classes de palavra mais frequentes em
cada tipo de sentimento, destacam-se os comentários classificados
como Não sei dizer, que apresentaram, tanto predominância de enti-
dades do tipo PER, bem como maior número de etiquetas da classe
nome próprio (PROPN), o que pode sugerir que esses comentários
demandam reconhecimento dessas entidades e, por consequência,
conhecimento de mundo, para poder atribuir um sentimento, pro-
blema que parece ter sido enfrentado pelos anotadores.
A análise de tópicos revelou que para os comentários em que se
obteve discordância total entre os anotadores, figura, em primeiro
lugar, o tópico 14, que é mais direcionado a ideologias políticas.
Este resultado pode ser interpretado em relação aos achados sobre
o desempenho do modelo, que rotulou comentários de questões
políticas como negativos em maior número do que os anotadores
humanos. No que diz respeito aos comentários em que ao menos
um anotador rotulou como Não sei dizer, sobressaíram tópicos rela-
cionados a crimes, xingamentos e conteúdos sexuais e a questões
ideológicas e políticas.
Em termos metodológicos, nosso estudo evidenciou que a qua-
lidade das métricas melhorou consideravelmente ao se separar o
conjunto de dados em dois subconjuntos e incluir apenas os co-
mentários rotulados com sentimentos, desconsiderando a categoria
Não sei dizer. O mesmo aconteceu com o cálculo do percentual de
concordância total dos anotadores sobre um mesmo rótulo, que foi
superior quando desconsiderada a categoria Não sei dizer.
Em consonância com a literatura, nosso estudo corrobora a com-
plexidade da tarefa de criação de conjunto de dados, dado o desafio
de se lidar com níveis de concordância moderados entre anotadores.
Para a consolidação do conjunto de dados, o voto da maioria, ou a
agregação das distintas respostas, é decisório para o rótulo único de
referência que será adjudicado. Em tarefas que envolvem alto grau
de subjetividade, como é o caso da análise de sentimentos, a deci-
são pela maioria reduz a representatividade das diversas opiniões
passíveis de existir em uma população ainda maior. Nesse sentido,
estudos recentes[10, 12] propõem uma mudança em direção a uma
abordagem mais inclusiva de todas as perspectivas dos anotadores
como alternativa à maioria enquanto referência ou ground truth.
Em trabalhos futuros, pretendemos explorar a perspectivização de
forma a mitigar o problema do nível de concordância entre anota-
dores.
Agradecimentos: Este trabalho foi parcialmente financiado pela
FAPEMIG, CAPES e CNPq.
Análise de sentimentos de conteúdo compartilhado em comunidades brasileiras do Reddit:
Avaliação de um conjunto de dados rotulados por humanos
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30103.txt ---

--- INÍCIO DO ARQUIVO: 30104.txt ---
Arquitetura Multicamadas para Coleta e Análise de Dados de
Saúde em Tempo Real
em Ambientes Externos, Integrando Fog Computing e Cloud
Computing
Juan Felipe Souza Oliveira
Instituto Militar de Engenharia
Rio de Janeiro, Brasil
oliveira.juan@ime.eb.br
Paulo Cesar Salgado Vidal
Instituto Militar de Engenharia
Rio de Janeiro, Brasil
vidal@ime.eb.br
Ronaldo Moreira Salles
CIICESI, ESTG, Instituto Politécnico do Porto
Porto, Portugal
rmo@estg.ipp.pt
Marcelo Quesado Filgueiras
Universidade Federal de Juiz de Fora
Juiz de Fora, Brasil
mquesado@uol.com.br
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
Alguns trabalhos realizaram implementações de computação em
névoa e nuvem para o monitoramento de saúde, contudo, poucos
[3][1] exploraram testes e avaliações de integridade dos dados cole-
tados e latência em ambientes externos com alta densidade popula-
cional e interferências, como praias, estádios de futebol e metrôs.
Esses ambientes apresentam desafios adicionais devido à alta quan-
tidade de dispositivos conectados e à instabilidade das redes.
O trabalho em [19] propôs uma arquitetura de monitoramento
de sinais vitais utilizando computação em névoa, chamada CEN.
A arquitetura é composta de 3 (três) camadas (sensores, névoa e
nuvem) e emprega o protocolo de comunicação Message Queuing
Telemetry Transport (MQTT). O protocolo MQTT é caracterizado
por ser mais leve (em termos de processamento) e eficiente e ideal
para transmissão de dados em tempo real em redes com recursos
limitados e instáveis [20], sendo avaliado nos cenários do Estádio
do Maracanã e Praia de Copabacana.
O presente artigo se diferencia de [19] por descrever com mais
detalhes a implementação da arquitetura e por realizar testes em
outros ambientes externos na cidade do Rio de Janeiro, tais como
o metrô, o Estádio Olímpico Nilton Santos (Engenhão) e o trecho
da Baía de Guanabara (entre as cidades de Niterói e Rio de Janeiro).
Esses testes em diferentes cenários tiveram como objetivo demon-
strar a eficiência da arquitetura em condições de instabilidade de
rede.
O artigo está organizado da seguinte forma: Na Seção 2, discuti-
mos os desafios técnicos e operacionais da computação em nuvem e
névoa na área médica. Na Seção 3, revisamos trabalhos relacionados
e comparamos com nossa abordagem. A Seção 4 detalha a arquite-
tura desenvolvida e suas características. Na Seção 5, abordamos a
avaliação das implementações e cenários de testes. Na Seção 6, ap-
resentamos os resultados dos experimentos, incluindo desempenho
em cenários externos, testes de latência e atrasos na emissão de
alertas. Finalmente, na Seção 7, concluímos o artigo e sugerimos
trabalhos futuros.
FUNDAMENTAÇÃO TEÓRICA
2.1
Computação em Nuvem na Área Médica
Aplicações na área da saúde desenvolvidas e baseadas em com-
putação em nuvem possuem alta capacidade de armazenamento e
poder computacional, logo, têm sido conhecidas como formas efi-
cientes de transferir, processar e armazenar dados [22]. No entanto,
existem algumas deficiências dos modelos atuais [6], tais como:
• Infraestrutura de rede: A falta de conectividade com servi-
dores, dados ou aplicações hospedadas na nuvem devido a
falhas nas redes de comunicação, como as redes móveis, rep-
resenta um grande obstáculo. Em contextos médicos, onde o
acesso contínuo a informações é essencial para decisões ráp-
idas, a interrupção de rede pode causar atrasos prejudiciais
no atendimento.
• Largura de banda: O grande volume de tráfego de dados
pode levar os provedores de nuvem a aplicar restrições na
largura de banda para garantir o funcionamento adequado
da rede. Isso pode afetar negativamente o desempenho de
aplicações críticas na área da saúde, como monitoramento
que exigem transferência de grandes volumes de dados em
tempo real. Com restrições de largura de banda, os tempos
de resposta podem ser significativamente aumentados, difi-
cultando o acesso rápido e eficaz às informações necessárias
para decisões imediatas.
• Confiabilidade: Problemas na nuvem podem resultar em
várias implicações. A integridade dos dados é vital para
garantir a precisão das informações e a continuidade dos
serviços. Em situações de falha, pode ocorrer perda ou cor-
rupção de dados, afetando diretamente a consistência de
prontuários médicos e outros registros essenciais. Isso pode
resultar em diagnósticos incorretos ou atrasos no tratamento,
comprometendo a segurança e o bem-estar dos pacientes.
• Tempo de resposta: Aplicações médicas, como sistemas
de monitoramento de pacientes, exigem tempos de resposta
extremamente rápidos para alertar os profissionais de saúde
sobre mudanças críticas no estado do paciente. A latência na
comunicação com a nuvem pode causar atrasos na detecção
e resposta a essas mudanças, potencialmente comprome-
tendo a capacidade dos profissionais de saúde de intervir de
maneira oportuna e eficaz.
Tendo em vista os aspectos observados, modelos de IoT utilizando
apenas a camada de computação em nuvem não necessariamente
oferecem a solução mais viável para aplicativos críticos [3] e en-
frentamento dos desafios de IoT atrelados à área médica [22]. Nesse
contexto, computação em névoa surge como um paradigma de
computação distribuída que estende a capacidade da nuvem para
dispositivos localizados próximos aos usuários ou no limite da rede,
como sensores, câmeras e redes corporais sem fio
2.2
Computação em Névoa na Área Médica
A integração de tecnologias de computação em névoa com disposi-
tivos de saúde, como smartbands e smartphones, apresenta desafios
técnicos e operacionais que devem ser superados para garantir a
eficácia e segurança dos sistemas de monitoramento de saúde [25].
A computação em névoa oferece vantagens significativas, como pro-
cessamento descentralizado e melhoria na capacidade de resposta
em tempo real, mas também traz desafios para uma implementação
bem-sucedida [6][1][25], tais como:
• Segurança e Privacidade dos Dados de Saúde: A proteção
dos dados de saúde é crucial devido à sua sensibilidade. A
comunicação entre a smartband, o smartphone (nó de névoa)
e a nuvem envolve múltiplas etapas suscetíveis a vulnerabil-
idades, exigindo criptografia ponta a ponta e autenticação
robusta para garantir a integridade dos dados.
• Gestão de Conectividade e Interferências: Ambientes
densamente povoados, como praias, estádios e metrôs, ap-
resentam desafios devido à alta densidade de dispositivos
conectados, bem como o relevo do terreno, resulta em con-
gestionamentos e interferências na rede. Utilizar um proto-
colo de comunicação leve como o MQTT pode mitigar esses
problemas.
• Complexidade de Implementação e Integração: Integrar
smartbands, smartphones e camada de computação em nuvem
em uma arquitetura envolvendo a computação em névoa, ne-
cessita da coordenação de múltiplos componentes heterogê-
neos. A interoperabilidade entre dispositivos e plataformas,
Arquitetura Multicamadas para Coleta e Análise de Dados de Saúde em Tempo Real
em Ambientes Externos, Integrando Fog Computing e Cloud Computing
WebMedia’2024, Juiz de Fora, Brazil
como Android e Apple, é essencial, assim como o desenvolvi-
mento de aplicações e plugins especializadas para gerenciar
a comunicação entre dispositivos e a camada de computação
em nuvem.
Em resumo, a integração de dispositivos de saúde com a com-
putação em névoa e nuvem enfrenta desafios críticos: segurança
e privacidade dos dados, gestão de conectividade em ambientes
externos com vários dispositivos conectados, e a complexidade de
integração de sistemas heterogêneos. Superar esses desafios é essen-
cial para uma arquitetura de multicamadas, combinando sensores,
camada de névoa e nuvem.
TRABALHOS RELACIONADOS
Os trabalhos relacionados nesta área de pesquisa exploraram os
conceitos de latência, tolerância a falhas [13], simulação de coleta e
coleta de dados reais em sistemas de monitoramento de parâmetros
de saúde [18]. Esses estudos investigaram abordagens para mini-
mizar a latência na transmissão e processamento de dados, bem
como propuseram técnicas para lidar com falhas e instabilidades
nas redes de comunicações móveis [5][8]. Além disso, utilizaram
simulações para avaliar o desempenho dos sistemas em diferentes
cenários e também realizaram coletas de dados reais para validarem
suas propostas [21].
A revisão dos trabalhos relacionados abordou critérios relevantes
alinhados com a arquitetura desenvolvida neste artigo, tais como:
• Latência: Garantir que o tempo de transmissão e proces-
samento dos dados seja minimizado para proporcionar re-
spostas rápidas e eficientes. Isso exige a otimização da latên-
cia tanto na computação em névoa quanto na nuvem, de
forma a manter a agilidade do sistema.
• Tolerância a falhas: Desenvolver arquiteturas capazes de
suportar falhas em dispositivos, redes ou processos, man-
tendo a operação do sistema sem interrupções. Isso assegura
a continuidade do serviço e a confiança, mesmo em condições
adversas.
• Smartphone como nó da camada de névoa (Fog Node): Os
smartphones desempenham um papel chave na arquitetura de
computação em névoa, atuando como nós periféricos da rede.
Com seu alto poder de processamento e armazenamento,
eles podem realizar tarefas computacionais localmente, re-
duzindo a latência e se conectando de forma eficiente a redes
móveis, Wi-Fi e outros dispositivos próximos.
• Diferentes protocolos de comunicação de rede: Apli-
cação de protocolos, como HTTP, MQTT e CoAP, no contexto
da saúde, explorando diferentes abordagens. Esses protoco-
los oferecem suporte à troca eficiente de dados clínicos, ao
monitoramento remoto de pacientes e à integração com re-
des corporais sem fio, promovendo uma comunicação mais
eficaz entre dispositivos.
• Coleta de dados: validar e calibrar os sistemas de moni-
toramento, permitindo verificar confiabilidade e eficácia do
sistema, e assim otimizar o sistema como um todo com base
nas condições reais de uso.
Os estudos comparativos abordaram os desafios enfrentados
em ambientes internos e externos, considerando a presença ou
ausência de redes de comunicação móveis, como 2G, 3G, 4G e 5G, e a
utilização de Wi-Fi [22][15]. Ambientes internos foram identificados
como oferecendo infraestrutura de rede mais estável, facilitando a
comunicação entre dispositivos de monitoramento de saúde. Em
contraste, ambientes externos apresentam maior variabilidade e
desafios de conectividade.
Soluções foram propostas para lidar com a falta de rede e insta-
bilidades em ambientes externos [21], incluindo armazenamento
temporário de dados, tecnologias de armazenamento local e re-
des ad hoc [9]. Essas estratégias visam garantir a confiabilidade
da coleta de dados em ambientes desafiadores. Em suma, os estu-
dos destacaram a importância de considerar as características de
cada ambiente, a presença ou ausência de redes móveis e Wi-Fi, e
as diferentes gerações de redes móveis no projeto de sistemas de
monitoramento de parâmetros de saúde em diferentes contextos.
As tecnologias e ferramentas computacionais utilizadas em tra-
balhos anteriores [20][21], como banco de dados não-relacionais
NoSQL, sistemas de mensageria e o padrão de projeto em arquitetura
de software Design Pattern, Transactional Outbox, contribuíram para
um aprofundamento e implementação da arquitetura desenvolvida.
Essas contribuições técnicas fornecem melhorias significativas, per-
mitindo uma coleta de dados mais eficiente, confiável e escalável.
Os bancos de dados NoSQL permitem que os sistemas de moni-
toramento de saúde lidem de forma flexível e eficiente com grandes
volumes de dados, especialmente dados heterogêneos e não estru-
turados. Eles possibilitam rápida inserção e consulta de dados em
tempo real, essencial para o monitoramento contínuo de pacientes
[7][21]. É observado também a capacidade de ampliar a escalabili-
dade horizontal, permitindo a expansão do sistema para gerenciar
mais aplicações e dados coletados [4], garantindo uma resposta
rápida e eficaz às necessidades dos pacientes.
Em relação a ferramentas de sistemas de mensageria e streaming
de dados distribuídos para lidar com fluxos contínuos de dados
em tempo real, é empregada a comunicação assíncrona entre os
componentes do sistema, tornando a troca de informações mais
flexível e escalável [14], desempenhando papel fundamental no
monitoramento em ambientes externos, onde as condições de rede
podem ser instáveis. Além disso, do padrão Transactional Outbox
garante a consistência e a atomicidade nas operações de envio
de mensagens [22][21], evitando perdas ou duplicações de dados
durante a comunicação assíncrona.
Por fim, para o tráfego dos dados para a camada de nuvem, estu-
dos recentes demonstram que o protocolo de comunicação Message
Queuing Telemetry Transport (MQTT) apresenta alta eficiência e
tolerância a falhas de conexão [3][5], permitindo reconexão au-
tomática quando há interrupção ou alteração na conectividade,
sendo possível adaptar-se a diferentes níveis de qualidade de rede,
incluindo as de baixa largura de banda e conexões instáveis, como as
de 2G e 3G, ou em situações de sobrecarga da rede devido a vários
dispositivos móveis conectados simultaneamente, otimizando o
tráfego dos dados com base nas condições da rede [22][15].
Apesar dos avanços e contribuições significativas dos trabalhos
analisados, algumas limitações foram identificadas. Primeiramente,
muitos estudos não abordaram de forma abrangente a integração
entre diferentes tecnologias de comunicação, como Wi-Fi e redes
móveis de diferentes gerações (2G, 3G, 4G e 5G), resultando em
possíveis lacunas na continuidade do monitoramento em ambientes
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
com variabilidade de conectividade. Embora alguns trabalhos ten-
ham proposto soluções para tolerância a falhas e armazenamento
temporário de dados, a implementação dessas soluções em cenários
reais ainda carece de testes extensivos e validações práticas. A
maioria das abordagens focou-se em ambientes internos, onde a
estabilidade da rede é maior, deixando uma lacuna significativa em
estudos voltados para ambientes externos com condições adversas
de conectividade.
Outra limitação observada é a subutilização do poder computa-
cional dos smartphones como nós da camada de névoa. Muitos
estudos não exploraram totalmente o potencial destes dispositivos
para executar tarefas computacionais na borda da rede, o que pode
reduzir a latência e melhorar a eficiência do sistema. Essas limi-
tações destacam a necessidade de abordagens mais integradas e
soluções que possam ser aplicadas em uma variedade de cenários
de monitoramento de saúde.
O presente artigo se diferencia ao apresentar uma arquitetura
que será testada em ambientes externos com condições adversas de
conectividade, como praias, estádios de futebol, e transporte público
como metrôs, proporcionando uma avaliação mais abrangente e
prática.
A arquitetura desenvolvida utiliza poder computacional dos
smartphones como nós da camada de névoa (fog nodes), onde são
realizados o pré-processamento, a criptografia e a anonimização
dos dados para conformidade com a LGPD (Lei Geral de Proteção
de Dados).
Por fim, a aplicação de bancos de dados NoSQL e o uso do proto-
colo de MQTT para fluxo contínuo de dados em tempo real de dados,
apresentam uma perspectiva de superar os desafios de escalabili-
dade e eficiência na transmissão de dados. Essas abordagens visam
melhorar a eficiência, reduzir a latência e aumentar a resiliência
do sistema, representando uma contribuição significativa para o
campo do monitoramento de saúde em ambientes externos.
ARQUITETURA DESENVOLVIDA
4.1
Concepção da Arquitetura
As etapas de desenvolvimento e implementação da arquitetura estão
integradas aos princípios fundamentais da Engenharia de Software
e à garantia da Qualidade de Software. Cada fase, desde o levan-
tamento bibliográfico até os testes em ambientes reais, demonstra
uma abordagem metodológica e técnica para garantir um completo
desenvolvimento da arquitetura. O processo inclui: a) levantamento
bibliográfico, b) definição de requisitos funcionais e não funcionais,
c) seleção de tecnologias e frameworks, d) testes preliminares de
variações de redes de comunicações em ambientes externos, e) de-
senvolvimento da arquitetura, f) testes da arquitetura em ambientes
externos, g) avaliação de desempenho da arquitetura.
A arquitetura de monitoramento de saúde inclui três camadas
principais: sensores, computação em névoa e computação em nu-
vem. Os sensores, como smartbands, coletam dados vitais em tempo
real. A computação em névoa, utilizando smartphones como nós
de névoa, realiza o pré-processamento, criptografia, anonimiza-
ção e persistência dos dados em caso de instabilidade de conexão,
além de transmitir dados para a nuvem via MQTT. A computação
em nuvem é responsável pelo armazenamento, análise, geração de
dashboards de acompanhamento e emissão de alertas, permitindo
monitoramento em tempo real.
A partir da conexão Bluetooth entre a smartband e o smartphone
do paciente, será possível realizar a coleta contínua de parâmetros
de saúde, fornecendo informações precisas e atualizadas. Esta ar-
quitetura viabiliza um monitoramento mais eficaz e personalizado.
Ao processar os dados próximos à fonte por meio da computação
em névoa, a arquitetura poderá garantir uma coleta e transmissão
de dados mais eficiente e ágil. Isso pode solucionar desafios rela-
cionados à geração de dados eletrônicos confiáveis, atendimento a
requisitos temporais e tolerância a falhas, coletando parâmetros de
saúde como frequência cardíaca, número de passos e geolocalização
do paciente.
Os dados coletados e processados poderão ser utilizados para
obter uma visão clara da condição de saúde dos pacientes e serão
enviados periodicamente para servidores em nuvem. No contexto
de e-Health, ambulâncias, enfermeiros e médicos poderão acessar
remotamente essas informações para avaliar a condição de saúde
dos pacientes monitorados.
4.2
Implementação
4.2.1
Camada de Computação em névoa. Na Figura 1, é apresen-
tado o diagrama de componentes e ferramentas da arquitetura de
computação em névoa, incluindo a camada de sensores e o aplica-
tivo móvel (Publisher API) desenvolvido para Android e Apple, que
se comunica com a nuvem. Utilizando a conexão Bluetooth entre
a smartband e o smartphone do paciente, são coletados continua-
mente parâmetros de saúde como frequência cardíaca, número de
passos e geolocalização.
Na implementação desta camada, o smartphone atua como nó
da névoa, realizando diversas funções críticas. Primeiramente, ele
realiza o pré-processamento dos dados coletados. No caso de insta-
bilidade de conexão com a nuvem, o smartphone utiliza um banco de
dados local para persistir temporariamente os dados. Isso garante
que nenhuma informação seja perdida durante interrupções na
conectividade.
Os dados processados são enviados para a nuvem via protocolo
MQTT, que assegura a entrega eficiente e resiliente das informações.
Na camada de névoa, além de receber os dados dos sensores, ocorre
o processo de ETL (Extract, Transform, Load), que inclui a extração
dos dados brutos, a aplicação de filtros e o armazenamento local.
Esse processo permite a seleção e processamento apenas dos dados
relevantes, reduzindo a quantidade de informações a serem enviadas
para a nuvem e otimizando o consumo de recursos.
Para gerenciar a persistência e a transmissão dos dados, imple-
mentamos o padrão de projeto Transactional Outbox. Este padrão
permite que, em caso de falha ao se conectar ao broker MQTT na
nuvem, as mensagens sejam armazenadas localmente. Quando a
conexão é restabelecida, o banco de dados local é consultado e os da-
dos armazenados são reenviados automaticamente ao broker MQTT.
Isso assegura que a integridade dos dados seja mantida e que to-
das as informações sejam corretamente processadas e transmitidas,
mesmo em condições adversas de conectividade.
4.2.2
Camada de Computação em Nuvem. Na Figura 2, é apre-
sentado o diagrama de componentes e ferramentas da arquitetura
de computação em nuvem, que complementa a camada de névoa.
Arquitetura Multicamadas para Coleta e Análise de Dados de Saúde em Tempo Real
em Ambientes Externos, Integrando Fog Computing e Cloud Computing
WebMedia’2024, Juiz de Fora, Brazil
Figure 1: Diagrama de Componentes da Camada de Com-
putação em névoa (Autor)
Esta camada, hospedada na AWS (Amazon Web Services), oferece
uma infraestrutura escalável e segura, garantindo conformidade
com a LGPD ao implementar políticas rigorosas de segurança e
privacidade.
Os dados processados e transmitidos via protocolo MQTT pelo
smartphone são enviados para um broker MQTT, que atua como
intermediário na comunicação entre a camada de névoa e a nuvem.
Em caso de desconexão do smartphone, o broker envia logs para um
banco de dados NoSQL através do Amazon SNS (Simple Notification
Service) e dispara Will Messages — mensagens configuradas para
serem enviadas automaticamente quando ocorre uma desconexão
— para a Subscriber API.
A Subscriber API, implementada em Java com o framework
Spring, consome as mensagens do broker MQTT . O Spring facilita
a criação de aplicações Java, proporcionando uma infraestrutura
robusta para gerenciar componentes, configurar funcionalidades
e integrações broker MQTT , RabbitMQ e bancos de dados. A Sub-
scriber API enfileira as mensagens no RabbitMQ, um sistema de
mensageria que permite a comunicação assíncrona e escalável en-
tre os componentes do sistema, e armazena-as no banco de dados
NoSQL. Para garantir a anonimização, a nuvem recebe apenas um
identificador único (ID) associado aos dados coletados, sem infor-
mações pessoalmente identificáveis dos pacientes.
Com a utilização do Prometheus, que é um sistema de moni-
toramento e alerta que coleta e armazena métricas em séries tem-
porais, permitindo flexibilidade na coleta e consulta de dados, é
possível monitorar a saúde do sistema e a integridade dos dados.
O Prometheus coleta dados de vários componentes do sistema, in-
cluindo o broker MQTT, a Subscriber API e o banco de dados NoSQL,
e armazena essas métricas em uma base de dados otimizada para
consultas temporais.
O Grafana é uma plataforma de visualização e análise de código
aberto que se integra ao Prometheus para gerar dashboards inter-
ativos e painéis de controle. Esses dashboards permitem o acom-
panhamento em tempo real dos parâmetros de saúde dos pacientes,
disparam alertas quando métricas específicas atingem determina-
dos limiares e fornecem uma interface de usuário (User Interface)
acessível remotamente por profissionais de saúde. O Grafana per-
mite que os usuários criem visualizações customizadas dos dados
coletados pelo Prometheus, facilitando a detecção de anomalias e a
tomada de decisões informadas.
Por fim, quando o broker MQTT detectar uma desconexão com
o smartphone, ele poderá disparar Will Messages para a Subscriber
API, indicando a desconexão, o nível de bateria do smartphone, a
hora e o local. A Subscriber API poderá então enviar essas men-
sagens para o Telegram para notificar os responsáveis. Esses logs de
eventos poderão ser importantes para a pesquisa, pois permitirão
o monitoramento da continuidade e da qualidade da conexão nos
ambientes externos co. Isso possibilitará a identificação rápida de
problemas de conectividade e a implementação de ações corretivas
imediatas.
Figure 2: Diagrama de Componentes da Camada de Com-
putação em névoa (Autor)
AVALIAÇÃO
Além da arquitetura principal desenvolvida em camadas (névoa e
nuvem), chamada de implementação com computação em névoa
(CEN), também foi desenvolvida uma implementação secundária
chamada de implementação sem computação em névoa (SCEN).
A implementação SCEN utiliza apenas a camada de computação
em nuvem, onde o smartphone envia os dados diretamente para a
nuvem através do protocolo de comunicação HTTP, sem a utilização
da camada de computação em névoa.
Realizaremos a avaliação da arquitetura desenvolvida, compara-
ndo as duas implementações distintas descritas anteriormente. Esta
avaliação visa identificar as vantagens e limitações de cada abor-
dagem, especialmente em termos de confiabilidade e eficiência na
transmissão de dados de saúde.
Por fim, a avaliação incluirá a análise da arquitetura como um
todo, considerando aspectos como latência, emissão de alertas e
capacidade de operar em diferentes ambientes com variabilidade
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
de conectividade. Os critérios de avaliação incluirão a estabilidade
da conexão, a taxa de transferência de dados, a perda de pacotes e
a eficácia na emissão de alertas em tempo real.
5.1
Arquitetura Principal e Implementação
Secundária
A utilização do protocolo HTTP apresenta limitações significativas
em comparação com o MQTT. O HTTP é um protocolo baseado
em requisições e respostas, o que significa que a comunicação é
unidirecional e somente ocorre quando o smartphone envia uma
solicitação e a nuvem responde. Isso pode resultar em maior latência
e menor eficiência na transmissão de dados em tempo real, pois cada
comunicação requer o estabelecimento de uma nova conexão [22].
Além disso, o HTTP não possui mecanismos internos para garantir
a entrega dos dados, o que pode levar à perda de informações em
condições de rede instáveis ou intermitentes [20]. Essas limitações
podem impactar negativamente a confiabilidade e a eficiência da
transmissão de dados críticos de saúde.
Em contraste, o MQTT é projetado para comunicação bidire-
cional contínua, permitindo que os dados sejam enviados e rece-
bidos em tempo real com menor latência. O MQTT oferece garantias
de entrega de mensagens através de diferentes níveis de qualidade
de serviço (QoS), essenciais para a confiabilidade dos dados de saúde
em ambientes com conectividade instável [5].
Na arquitetura, utilizamos o nível de QoS 1, que garante que uma
mensagem seja entregue pelo menos uma vez, com confirmação
de recebimento. Este nível de serviço equilibra confiabilidade e
eficiência, garantindo a entrega dos pacotes de dados mesmo em
caso de falhas temporárias de rede, sem a sobrecarga de múltiplas
transmissões redundantes [5]. Isso mantém uma alta taxa de entrega
de mensagens com baixa latência.
5.2
Critérios de Avaliação
A comparação entre a Computação em Névoa (CEN) e a Sem Com-
putação em Névoa (SCEN), com foco na conexão com o Broker
MQTT abordará limitações identificadas em trabalhos anteriores,
como a falta de integração entre diferentes tecnologias de comuni-
cação e a subutilização do poder computacional dos smartphones.
A arquitetura visa superar essas limitações utilizando smart-
phones como nós da camada de névoa (fog nodes). A aplicação de
bancos de dados NoSQL e o uso do protocolo MQTT para fluxo
contínuo de dados em tempo real são elementos centrais dessa
abordagem.
A comparação entre as implementações CEN e SCEN será con-
duzida com base nos seguintes critérios:
• Estabilidade da Conexão: Avaliar a frequência de desconexões
e a capacidade de reconexão automática.
• Latência: Medir o tempo de transmissão dos dados desde
a coleta até a persistência dos dados nos bancos de dados
presentes na camada de computação em nuvem.
• Taxa de Transferência de Dados: Avaliar a quantidade de
dados transmitidos ao longo dos experimentos.
• Perda de Pacotes: Monitorar os dados perdidos durante a
transmissão.
5.3
Cenários de Teste
As coletas e aferições de dados foram realizadas em três cenários
distintos usando o software Open Signal, que testa sinal de rede,
incluindo download, upload e latência, e permite detectar a presença
e qualidade de sinal e identificar a performance da rede.
• Metrô do Rio de Janeiro, Linha 1: Observou-se variações
constantes na qualidade do sinal, com áreas de ausência
de conectividade e sinal fraco a moderado. Estações subter-
râneas apresentaram alta latência e desconexões frequentes,
comprometendo a continuidade do monitoramento de saúde
em tempo real.
• Estádio Olímpico Nilton Santos (Engenhão), Rio de
Janeiro - RJ: Em dias de jogos, a rede sofre sobrecarga dev-
ido ao grande número de dispositivos conectados, resultando
em instabilidade, atrasos e perdas de dados, impactando neg-
ativamente o monitoramento de saúde.
• Baía de Guanabara, Rio de Janeiro - RJ: Durante o deslo-
camento da barca entre as cidades do Rio de Janeiro e Niterói,
o trajeto apresentou variações na qualidade do sinal devido à
interferência do ambiente marinho e à adaptação à mudança
de áreas de cobertura, resultando em aumento da latência e
desconexões ocasionais.
Estes cenários de teste destacam os desafios significativos na
manutenção de uma conexão de rede estável e confiável em ambi-
entes urbanos e de alta densidade populacional. A análise desses
dados é essencial para avaliar a eficácia das implementações CEN e
SCEN em diferentes condições de conectividade e para identificar
áreas potenciais para melhorias na transmissão de dados críticos
de saúde.
RESULTADOS
Esta seção analisa os resultados de dois experimentos em três
cenários de teste como mostrado na Figura 3. O primeiro exper-
imento avaliou o desempenho das arquiteturas SCEN e CEN no
Metrô e no Estádio Olímpico Nilton Santos (Engenhão), focando na
eficiência em lidar com intermitências de rede. Durante os testes,
pacotes de dados foram enviados a cada segundo para monitorar in-
formações em tempo real. O segundo experimento testou a latência
e emissão de alertas no Engenhão e no deslocamento de barca entre
Rio de Janeiro e Niterói, analisando a comunicação em cenários de
alta densidade e variação de rede.
6.1
Desempenho das Arquiteturas CEN e SCEN
no Metrô
Durante o experimento no metrô, ambas as arquiteturas foram
utilizadas simultaneamente, e identificamos 12 intermitências na
transmissão, variando entre 10 segundos e 2 minutos, ao longo de
90 minutos, totalizando o envio de 5400 pacotes de dados. Essas
intermitências causaram perda de pacotes. A Tabela 1 apresenta os
resultados, destacando o desempenho de cada arquitetura frente às
intermitências.
Essa abordagem SCEN, embora simples via protocolo HTTP,
pode introduzir latência adicional e é mais susceptível a perda de
dados em casos de intermitência na conexão, devido à necessidade
de estabelecer uma nova conexão para cada transação de dados. Os
Arquitetura Multicamadas para Coleta e Análise de Dados de Saúde em Tempo Real
em Ambientes Externos, Integrando Fog Computing e Cloud Computing
WebMedia’2024, Juiz de Fora, Brazil
Figure 3: Coleta de dados nos cenários de teste: (a) Metrô,
(b) Estádio de futebol, e (c) Baía de Guanabara, utilizando a
pulseira Xiaomi Mi Smart Band 4 durante a execução dos
experimentos. (Autor)
Table 1: Comparação de Métricas de Desempenho no Metrô
Métrica
SCEN
CEN
Total de Pacotes
Pacotes Perdidos
Pacotes de Sucesso
Taxa de Sucesso (%)
68.89
96.73
Taxa de Falha (%)
31.11
3.27
valores observados na tabela para SCEN refletem essa dinâmica,
com uma taxa de sucesso de 68.89% e uma taxa de falha de 31.11%,
evidenciando desafios potenciais em termos de latência e perda de
dados durante interrupções na conectividade.
Por outro lado, a arquitetura CEN ao utilizar o protocolo MQTT,
suportando conexões persistentes e permitindo o uso do padrão
Transactional Outbox para registrar os dados coletados localmente
antes de serem enviadas à nuvem em caso de desconexão e in-
termitências da rede. Essa abordagem oferece uma vantagem em
ambientes externos, onde a conectividade demonstrou ser instável.
Os dados da tabela para CEN mostram uma taxa de sucesso de
96.73% e uma baixa taxa de falha de 3.27%, destacando a eficiência
da arquitetura CEN em garantir uma resposta rápida a eventos
críticos com mínima sobrecarga de rede e menor susceptibilidade a
perda de dados.
6.2
Desempenho da Arquiteturas SCEN e CEN
no Estádio Olímpico Nilton Santos
(Engenhão)
Após a avaliação no metrô, um experimento semelhante foi real-
izado no estádio, durante o clássico entre Vasco e Fluminense, em
10/08/2024, com um público de 20.003 torcedores (Figura 3). Durante
as duas horas do evento, foram enviados 7200 pacotes de dados.
Observou-se uma degradação significativa na qualidade das redes
móveis devido à sobrecarga de dispositivos conectados, resultando
em alta latência, taxas de transferência reduzidas e perda de pa-
cotes. A Tabela 2 compara o desempenho de cada arquitetura nesse
cenário.
Table 2: Comparação de Métricas de Desempenho no Estádio
Olímpico Nilton Santos (Engenhão)
Métrica
SCEN
CEN
Total de Pacotes
Pacotes Perdidos
Pacotes de Sucesso
Taxa de Sucesso (%)
69.01
97.25
Taxa de Falha (%)
30.99
2.75
Esses dados destacam a vantagem da arquitetura CEN em am-
bientes desafiadores e de alta demanda, onde a rede enfrenta so-
brecarga e interferências significativas. O Estádio Engenhão, sendo
um local externo e sujeito a diversas condições de rede, representa
um cenário complexo para a conectividade móvel. A arquitetura
CEN demonstrou maior eficiência na entrega de pacotes, com uma
taxa de sucesso de 97.25% e uma taxa de falha de apenas 2.75%. Em
comparação, a arquitetura SCEN apresentou uma taxa de sucesso
de 69.01% e uma taxa de falha de 30.99%. Esses resultados mostram
como a CEN proporcionou menor perda de dados e melhor estabili-
dade de conexão.
6.3
Teste de Latência e Emissão de Alertas
Os testes de latência foram conduzidos no Estádio Olímpico Nilton
Santos (Engenhão) e ao longo do trajeto de barca entre a cidade do
Rio de Janeiro e Niterói, na Baía de Guanabara. O objetivo foi avaliar
a latência do sistema de monitoramento de frequência cardíaca,
configurado para enviar alertas ao detectar frequência cardíaca anô-
mala continuamente por 5 minutos. Medimos o tempo necessário
para processar e transmitir sinais do dispositivo de monitoramento
para o servidor na nuvem, além do atraso na emissão de alertas em
diferentes condições de rede (5G, 4G, 3G).
O sistema de monitoramento é configurado para detectar segmen-
tos temporais nos quais a frequência cardíaca permanece elevada
ou anormal por um período contínuo de 5 minutos. Essa duração
foi escolhida com base em recomendações médicas e trabalhos rela-
cionados, que sugerem que esse período é suficiente para indicar
problemas cardíacos agudos ou crônicos que requerem atenção
médica imediata.
Quando um segmento de 5 minutos com frequência cardíaca ele-
vada é identificado, o sistema emite alertas para notificar os profis-
sionais de saúde ou cuidadores responsáveis pelo acompanhamento
do paciente. Ao analisar esses resultados, também examinamos a
latência das arquiteturas CEN e SCEN, que impactam diretamente
nos atrasos na emissão dos alertas.
Durante ambos os experimentos, foram observadas variações
na qualidade da rede, resultando em intermitências entre 5G, 4G e
3G devido à alta demanda e sobrecarga da infraestrutura de rede e
possíveis interferências. Essas condições reais permitiram medir a
latência e o atraso na emissão de alertas em diferentes cenários de
conectividade.
6.3.1
Resultados dos Testes de Latência. Os resultados mostrados
na tabela a seguir destacam as diferenças de desempenho entre as
arquiteturas:
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
Table 3: Teste de Latência - Cenário Estádio Olímpico Nilton
Santos (Engenhão)
Métrica
CEN
SCEN
Tempo de Resposta Médio 5G (ms)
Tempo de Resposta Médio 4G (ms)
Tempo de Resposta Médio 3G (ms)
Table 4: Teste de Latência - Cenário Baía de Guanabara
Métrica
CEN
SCEN
Tempo de Resposta Médio 5G (ms)
Tempo de Resposta Médio 4G (ms)
Tempo de Resposta Médio 3G (ms)
6.3.2
Cálculo do Atraso Acumulado para Emissão de Alerta. O
atraso acumulado é uma medida do tempo total que se passa desde
a detecção de um evento até a emissão de um alerta, considerando
a latência na rede. Esse atraso é influenciado pela latência de envio
de cada pacote de dados através da rede. A fórmula para calcular o
atraso acumulado em minutos é dada por:
Atraso Acumulado (min) = Tempo de Resposta Médio (ms) × 300
1000 × 60
Os resultados dos testes de latência nos dois cenários, o Estádio
Engenhão e a Baía de Guanabara, estão apresentados nas tabelas a
seguir:
Table 5: Atraso Acumulado em Diferentes Condições de Rede
no Estádio Olímpico Nilton Santos (Engenhão)
Métrica
CEN
SCEN
Atraso Acumulado 5G (min)
0.49
4.36
Atraso Acumulado 4G (min)
1.23
6.57
Atraso Acumulado 3G (min)
1.92
13.04
Table 6: Atraso Acumulado em Diferentes Condições de Rede
no Cenário Baía de Guanabara
Métrica
CEN
SCEN
Atraso Acumulado 5G (min)
1.63
9.76
Atraso Acumulado 4G (min)
3.77
13.49
Atraso Acumulado 3G (min)
6.15
20.06
6.3.3
Análise dos Resultados. Os resultados dos testes indicam que
a arquitetura CEN apresenta tempos de resposta significativamente
menores que a arquitetura SCEN em ambos os cenários analisados.
No Estádio Olímpico Nilton Santos (Engenhão), a arquitetura CEN
obteve atrasos acumulados inferiores em todas as redes testadas
(5G, 4G e 3G) em comparação com a SCEN. Já no cenário da Baía de
Guanabara, os atrasos foram maiores em relação aos experimentos
realizados no estádio, possivelmente devido às condições do ambi-
ente marítimo. O deslocamento da embarcação, a maior distância
das torres de celular e a propagação do sinal sobre a água podem
influenciar a qualidade da conexão, contribuindo para o aumento
da latência observada ao longo do trajeto.
6.3.4
Impacto do Atraso na Emissão de Alertas. O atraso na emis-
são de alertas pode ter um impacto significativo na capacidade de
resposta dos profissionais de saúde a situações críticas. No contexto
do monitoramento contínuo de pacientes, especialmente em ambi-
entes de grande porte e alta densidade como o Estádio, a rapidez
na detecção e resposta a anomalias na frequência cardíaca pode ser
crucial para salvar vidas.
• Arquitetura CEN: Com um atraso acumulado de aproxi-
madamente 2 minutos no pior caso de rede identificado (3G),
a arquitetura permite que os profissionais de saúde sejam
notificados rapidamente sobre uma condição crítica. Essa
agilidade pode ser vital para iniciar intervenções médicas
imediatas e evitar complicações graves.
• Arquitetura SCEN: Em contraste, apresenta um atraso acu-
mulado de aproximadamente entre 13 e 20 minutos no pior
caso de rede identificado (3G). Esse tempo de resposta sig-
nificativamente mais longo pode atrasar a notificação dos
profissionais de saúde, reduzindo a janela de oportunidade
para intervenções rápidas e potencialmente comprometendo
a eficácia do atendimento médico.
CONSIDERAÇÕES FINAIS
A avaliação da arquitetura multicamadas para monitoramento re-
moto de saúde, baseada em computação em névoa e nuvem, com-
provou sua eficácia em cenários de conectividade desafiadora. Os
testes realizados em três ambientes distintos — o Estádio Olímpico
Nilton Santos (Engenhão), o metrô do Rio de Janeiro, e a Baía de
Guanabara — demonstraram que o processamento local nos smart-
phones, atuando como nós de névoa, foi fundamental para garantir a
integridade dos dados e minimizar os impactos de instabilidades na
rede. Essa abordagem se mostrou eficiente para o monitoramento
em tempo real, oferecendo uma solução adaptável em condições
variadas de conectividade.
Apesar de uma pequena perda de pacotes, possivelmente atribuída
a interferências eletromagnéticas e à sobrecarga da rede, os resulta-
dos apontam para a necessidade de uma análise mais detalhada das
condições de rede em cenários com alta mobilidade e congestion-
amento. O protocolo MQTT desempenhou um papel essencial na
adaptação da arquitetura às variações de latência, assegurando a
continuidade da coleta e transmissão de dados, mesmo diante de
redes instáveis.
Com base nos resultados apresentados, a arquitetura CEN oferece
baixa latência na entrega de notificações, otimizando o monitora-
mento dos pacientes e facilitando decisões rápidas e assertivas em
cenários críticos, melhorando a gestão de eventos. Como trabalho
futuro, os autores propõem investigar técnicas para reduzir inter-
ferências e sobrecargas em redes móveis, visando aumentar a confi-
abilidade e eficiência da solução em situações de monitoramento
crítico.
Arquitetura Multicamadas para Coleta e Análise de Dados de Saúde em Tempo Real
em Ambientes Externos, Integrando Fog Computing e Cloud Computing
WebMedia’2024, Juiz de Fora, Brazil
AGRADECIMENTOS
O presente trabalho foi realizado com apoio da Coordenação de
Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) –
Código de Financiamento 001.
This study was financed in part by the Coordenação de Aper-
feiçoamento de Pessoal de Nível Superior - Brasil (CAPES) – Finance
Code 001.

--- FIM DO ARQUIVO: 30104.txt ---

--- INÍCIO DO ARQUIVO: 30108.txt ---
Crianças e Propagandas no TikTok:
identificando publicidade infantil na rede social TikTok
Raíssa Gonçalves Lopes Carvalho
Departamento de Ciência da Computação
Pontifícia Universidade Católica de Minas Gerais
Belo Horizonte, MG, Brasil
raissa.carvalho.1266210@sga.pucminas.br
Humberto Torres Marques-Neto
Departamento de Ciência da Computação
Pontifícia Universidade Católica de Minas Gerais
Belo Horizonte, MG, Brasil
humberto@pucminas.br
WebMedia’2024, Juiz de Fora, Brazil
Carvalho e Marques-Neto
passa grande parte do tempo diante da tela ser influenciado por um
vídeo desse tipo é considerável. É nesse contexto que se insere o
presente trabalho, buscando identificar e analisar a autoridade que
os influenciadores detêm sobre as crianças no TikTok.
Neste trabalho, abordamos a identificação da publicidade no
TikTok sobre o comportamento infantil por meio da implemen-
tação e aplicação de modelos de aprendizado de máquina, para a
identificação de conteúdo publicitário direcionado a crianças. Cria-
mos e pré-processamos um conjunto de dados que inclui vídeos e
metadados extraídos da plataforma, complementado por análises
de descrições, áudios e hashtags. Esse conjunto foi utilizado para
treinar e testar a eficácia de diversos modelos de aprendizado de
máquina na classificação de conteúdos publicitários. Os modelos
aplicados incluem o Naive Bayes [19], Support Vector Machine (SVM)
[10], Logistic Regression [27] e Random Forest [8].
Os principais resultados indicam a capacidade dos modelos em
classificar com precisão os conteúdos publicitários, com o modelo
SVM demonstrando superioridade em desempenho nas métricas de
precisão e generalização. Essa superioridade indica uma capacidade
significativa do modelo em distinguir entre conteúdos publicitários
e não publicitários, ressaltando sua utilidade prática em contextos
de proteção ao público infantil em plataformas digitais.
Este documento está organizado em seções que apresentam os
trabalhos relacionados (Seção 2), detalham desde a metodologia (Se-
ção 3), a implementação e o desenvolvimento dos modelos (Seção 4)
até os resultados obtidos e discussões (Seção 5), proporcionando
uma compreensão aprofundada das complexidades e desafios asso-
ciados à publicidade infantil no TikTok.
TRABALHOS RELACIONADOS
O aumento da participação de crianças nas redes sociais tem ga-
nhado destaque na atualidade e vários estudos têm sido conduzidos
para compreender os possíveis impactos negativos a longo prazo
para essa população. Esta seção apresenta as referências seleciona-
das para proporcionar uma contextualização mais profunda sobre
o assunto.
No Brasil, a legislação classifica a publicidade voltada ao público
infantil como abusiva. O Estatuto da Criança e do Adolescente –
ECA (Lei nº 8.069/1990) [7], o Código de Defesa do Consumidor
(Lei nº 8.078/1990) [6] e a Resolução nº 163 do Conselho Nacional
dos Direitos da Criança e do Adolescente – Conanda [12], definem
a publicidade que se dirige às crianças como considerada abusiva
e ilegal, visto que explora a falta de discernimento e experiência
dessa faixa etária. Complementando essa perspectiva, o Instituto
Alana [1] apresentou uma discussão detalhada sobre a exploração
comercial infantil na plataforma TikTok, destacando uma inicia-
tiva do Instituto através do seu programa Criança e Consumo. Este
programa notificou as empresas TikTok Inc. e Bytedance Brasil
Tecnologia Ltda., exigindo providências quanto à presença de pu-
blicidade dirigida ao público infantil, em contraste com a legislação
vigente e os termos de serviço da plataforma. O documento expõe
as preocupações relacionadas aos impactos do consumismo na in-
fância, destacando a influência prejudicial da publicidade voltada a
esse público, que inclui riscos como a obesidade infantil, o materia-
lismo excessivo e a degradação das relações sociais. A notificação
enfatiza a necessidade de proteger as crianças dessas práticas de ex-
ploração comercial, realçando a urgência em adaptar as estratégias
digitais para resguardar os direitos e o desenvolvimento saudável
da criança e do adolescente. O texto analisado revelou conceitos
fundamentais sobre a exploração comercial infantil na plataforma
TikTok. Além de elucidar as estratégias de publicidade direcionadas
ao público infantil, destacando a atuação significativa de alguns
influenciadores digitais neste contexto.
Com a expansão das redes sociais, a publicidade vai além dos
tradicionais comerciais de televisão, explorando novos territórios e
ampliando o destaque para o trabalho dos influenciadores. O artigo
intitulado “Social Media Influencers (SMIs) in Context: A Literature
Review” [13] aborda de maneira abrangente o impacto dos influen-
ciadores digitais nas redes sociais sobre adolescentes. Este estudo,
uma revisão de literatura exaustiva sobre a interação entre jovens
e SMIs, destaca como esses influenciadores moldam comportamen-
tos e atitudes consumistas, apresentando as dinâmicas complexas
entre adolescentes e a promoção de conteúdo nas mídias sociais.
Os autores discutem os métodos de persuasão utilizados e os pos-
síveis impactos positivos e negativos dessa influência, ressaltando
a necessidade de conscientização dentro do campo de marketing
sobre essas interações.
Ao mesmo tempo em que o impacto dos influenciadores pode ser
visto de um ponto de vista negativo, o artigo de [17], por sua vez,
apresenta o grande fenômeno TikTok, enfatizando sua rápida ascen-
são e sua revolução no consumo e compartilhamento de conteúdo.
Para influenciadores se torna fácil a conexão com sua audiências de
maneira mais orgânica e imersiva. O artigo apresenta que, as em-
presas aproveitam do potencial da plataforma, adaptando-se para
amplificar suas campanhas de marketing e aproveitar sua natureza
viral. No entanto, lutam com a rápida evolução da plataforma e
as mudanças nas preferências do público também apresentam de-
safios, exigindo que as marcas estejam continuamente adaptando
suas estratégias.
Ambos artigos de [13] e [17] trazem difererentes perspectivas,
e, a partir delas, podemos entender o crescimento do TikTok, que,
com sua influência crescente, é uma plataforma que não pode ser ig-
norada no marketing de influenciadores. A busca por um equilíbrio
na plataforma deve ser feita, na qual as empresas devem compensar
a exploração do potencial da plataforma com uma abordagem ética,
especialmente ao engajar com públicos mais jovens e imaturos.
A influência das propagandas infantis não ocorre somente via
TikTok. O artigo de [9] aborda a temática da publicidade infan-
til via Instagram e propõe um classificador baseado em aprendi-
zado de máquina para identificar tais anúncios, aproximando-se
do tema apresentado neste estudo. Os autores coletaram postagens
de influenciadores brasileiros usando a ferramenta Instaloader, ca-
tegorizando manualmente quais dessas postagens eram anúncios
direcionados a crianças. Sete modelos de aprendizado de máquina
foram testados para detectar a publicidade infantil nas redes sociais,
sendo que o Naive Bayes se destacou como um dos mais eficazes.
Concluiu-se que, dada a crescente prevalência de publicidade infan-
til nas mídias sociais, ferramentas como o classificador proposto
são essenciais para monitorar e garantir a proteção dos direitos
das crianças em ambientes digitais. Em paralelo a este tema, o tra-
balho de [14] investiga a eficácia da publicidade infantil presente
em vídeos do YouTube. Utilizando um modelo para caracterizar se
Crianças e Propagandas no TikTok:
identificando publicidade infantil na rede social TikTok
WebMedia’2024, Juiz de Fora, Brazil
um conteúdo é de fato um anúncio infantil no vídeo, os autores
utilizaram personas (bots) que assistiam a vídeos e anúncios do
YouTube. Os autores concluíram que os anúncios em vídeos são cui-
dadosamente elaborados, usando estratégias para captar e manter o
interesse das crianças, impactando diretamente na evolução dessas
crianças. Por fim, a pesquisa também enfatiza a necessidade de uma
maior compreensão e regulamentação dessas práticas publicitárias,
considerando a vulnerabilidade do público infantil e os potenciais
impactos éticos e psicológicos.
A eficácia dos influenciadores é um tema frequentemente abor-
dado em pesquisas. O trabalho [22] apresenta um estudo realizado
pelos autores acerca dos influenciadores do TikTok. Os autores
exploram como a credibilidade dos influenciadores afeta a intenção
de compra dos usuários do TikTok. Para isso, baseiam-se em dados
dos usuários e aplicam o PLS (Partial Least Squares) para análise
de dados, buscando identificar correlações. O PLS é uma técnica
estatística utilizada para modelagem preditiva que busca encontrar
relações entre conjuntos de variáveis dependentes e independentes.
Com resultados positivos, os autores observaram que fatores como
a autenticidade e a expertise do influenciador desempenham um
papel crucial na formação da percepção de credibilidade. Muito
similar a este estudo, temos o trabalho de [25] que examina os fa-
tores que determinam a atratividade dos influenciadores digitais.
Utilizando uma abordagem quantitativa a partir do framework AHP,
os pesquisadores conduziram um levantamento com consumidores
para identificar os principais fatores que determinam a atratividade
e confiabilidade em um influenciador. E, conforme levantado pelo
trabalho de [22], autenticidade, expertise e a capacidade de esta-
belecer conexões com a audiência são elementos primordiais. As
duas pesquisas enfatizam a necessidade de marcas contarem com
influenciadores autênticos e de confiança ao mirar o engajamento e
a conversão de sua audiência no TikTok, destacando a importância
de escolher colaboradores que exibam esses atributos.
Para além das propagandas, as crianças estão constantemente
expostas nas redes sociais. A pesquisa de [4] explora as percepções
e os riscos associados a contatos com desconhecidos no ambiente
virtual, focando especialmente em jovens e adolescentes. Os autores
conduziram entrevistas com jovens procurando entender como
ocorre o contato com estranhos online. Os resultados enfatizam
que o contato é feito, apesar da consciência de muitos sobre os
riscos, colocando-os em posições de vulnerabilidade. As crianças
por fim, mostram interesse em aprender maneiras a fim de evitar
tais contatos. Em paralelo ao tema deste trabalho, os influenciadores
tem o poder de apresentar qualquer tipo de publicidade aos jovens,
que muitas vezes não são supervisionadas pelos pais, aumentando
a vulnerabilidade e risco de cada uma.
Diante dos desafios que as crianças enfrentam nas redes sociais, o
artigo de [21] aborda a crescente preocupação dos pais em relação ao
uso da internet por seus filhos e a demanda por sistemas de controle
parental. Em sua análise, os autores apresentam um programa em
Android, explorando as características essenciais desse sistemas e
sua eficácia em monitorar as crianças. O estudo indica que, mesmo
com alguns desafios, sua relevância já é reconhecida pelos pais.
Ademais, a pesquisa sublinha a união de tecnologia e educação
como um meio primordial para garantir a segurança online das
crianças.
Os trabalhos relacionados apresentados fundamentam-se na aná-
lise compreensiva das dinâmicas de publicidade infantil nas redes
sociais, destacando o papel dos influenciadores e a sutileza das es-
tratégias de marketing dirigidas às crianças. Os estudos revisados
apontam para uma crescente sofisticação na forma como o conteúdo
publicitário é integrado às plataformas favoritas do público infan-
til, como o TikTok. Evidencia-se, assim, a necessidade de métodos
que possam discernir entre entretenimento e publicidade, dada a
complexidade e o impacto dessa publicidade no comportamento e
desenvolvimento infantil. Neste contexto, o trabalho propõe uma
abordagem metodológica utilizando algoritmos de aprendizado de
máquina, que identifica conteúdos publicitários dirigidos a crianças.
Este estudo, portanto, não só dialoga com as preocupações levanta-
das pelos autores citados, mas também avança na aplicação prática
e técnica, contribuindo para o aprimoramento das estratégias de
proteção à infância no ambiente digital.
METODOLOGIA
Para desenvolver a pesquisa proposta, foi adotada uma metodologia
estruturada em etapas. A Figura 1 ilustra de maneira esquemática,
cada uma dessas fases do trabalho. Os parágrafos subsequentes
apresentam uma descrição detalhada de cada etapa, elucidando as
técnicas e os procedimentos empregados ao longo da investigação.
Vale destacar que, durante todo o desenvolvimento do trabalho, foi
utilizada a ferramenta PyCharm, além de Application Programming
Interface (APIs) e bibliotecas externas para auxiliar na implementa-
ção das análises e classificações necessárias.
Figura 1: Metodologia Proposta.
3.1
Coleta de Dados
O escopo deste trabalho concentra-se especificamente na análise
de vídeos que apresentam anúncios protagonizados por crianças
ou adultos, postados no TikTok.
Para a coleta de dados, inicialmente utilizou-se a mesma lista de
influenciadores da plataforma Instagram utilizada em um trabalho
prévio dos autores [9]. Posteriorment, expandiu-se a lista com influ-
enciadores famosos no TikTok, identificados através de pesquisas
na plataforma, focando em contas populares entre o público infantil
e que produzem conteúdo voltado para esse público. Com esses in-
fluenciadores identificados, foi empregado, de forma complementar,
uma ferramenta não oficial disponibilizada pelo site Apify, ”TikTok
Data Extractor“. Esta plataforma foi desenvolvida com o objetivo
de divulgar dados públicos da plataforma, oferecendo dados que
podem ser empregados em pesquisas acadêmicas.
WebMedia’2024, Juiz de Fora, Brazil
Carvalho e Marques-Neto
Com o apoio dessa API, uma base de dados foi estruturada para
este estudo. Nessa base, os influenciadores fornecidos foram consul-
tados com o objetivo de extrair os posts relacionados, permitindo
uma análise detalhada do conteúdo anunciado. A base de dados é
formada por 24.131 exemplos e inclui 328 características.
3.2
Pré-processamento
Com os posts obtidos na etapa anterior, aplicou-se um filtro para
identificar apenas o conteúdo relevante para a pesquisa. Nessa etapa,
buscou-se identificar posts que estavam relacionados à publicidade
infantil. Além disso, foram excluídos vídeos repetidos, sem áudio e
outros elementos textuais, bem como colunas desnecessárias para
o escopo do trabalho. Ainda nessa etapa, foi necessária a adição das
transcrições de cada vídeo.
A identificação de posts não relacionados à publicidade infantil
foi realizada através da análise de elementos textuais extraídos dos
vídeos publicados. Descrições, áudios e hashtags foram utilizados
para caracterizar o conteúdo dos vídeos, e uma busca automática foi
realizada com base em palavras fornecidas por autores de trabalhos
previamente citados [9]. Se os elementos textuais de um vídeo
contivessem uma das palavras coletadas, esse vídeo era marcado
como publicidade.
Após a classificação automática dos vídeos, realizou-se uma aná-
lise manual final para identificar e corrigir possíveis classificações
errôneas. Ao término da etapa, a tabela continha 24.118 exemplos e
32 características.
3.3
Classificação
Finalmente, com os dados pré-processados, foi implementado um
modelo de aprendizado de máquina, buscando identificar as posta-
gens com algum tipo de publicidade infantil. Utilizando técnicas de
classificação, o modelo foi treinado para discernir entre conteúdos
que são e que não são publicidade direcionada às crianças.
Dentre os algoritmos utilizados para essa tarefa, o Naive Bayes
surgiu como uma opção viável devido à sua eficácia com grandes
conjuntos de dados e facilidade de implementação. O SVM também
foi um candidato forte, especialmente pela sua capacidade de lidar
com a alta dimensionalidade típica dos dados de texto. O trabalho
de [11] auxiliou na escolha e entendimento dos modelos.
O sucesso deste modelo dependeu da qualidade e quantidade
do conjunto de dados de treinamento, que incluiu exemplos claros
e diversificados de publicidade infantil e não-infantil. A escolha
final do algoritmo foi influenciada por vários fatores, incluindo
a disponibilidade de dados, a necessidade de interpretabilidade e
os recursos computacionais disponíveis. Experimentos com vários
algoritmos e ajustes de parâmetros foram realizados para identificar
a abordagem mais eficaz para esta tarefa específica. Finalmente, a
classificação do modelo foi incorporada à tabela criada na etapa
anterior.
IMPLEMENTAÇÃO
A implementação deste trabalho foi feita seguindo os passos deta-
lhados em seguida, buscando uma maneira eficiente para classificar
conteúdo publicitário direcionado a crianças no TikTok. Ao longo
da seção, foram realizadas descrições das ferramentas e tecnologias
utilizadas, incluindo especificações sobre as bibliotecas de software
e plataformas de desenvolvimento. Inicialmente, esta seção começa
com uma explanação detalhada do processo de coleta de dados,
abordando as metodologias adotadas para extrair e processar os
vídeos e metadados da plataforma. Posteriormente, discutimos o
pré-processamento dos dados, destacando as técnicas de limpeza
e organização de dados para torná-los adequados para análise. A
etapa de classificação é explorada, apresentando os modelos testa-
dos, a configuração dos experimentos e a justificativa da escolha
do modelo. Por fim, a seção conclui com uma avaliação dos resulta-
dos obtidos, descrevendo como os modelos performaram e quais
informações foram extraídos dessas observações. Cada uma dessas
etapas é fundamentada com exemplos e ilustrações, proporcionando
uma compreensão das técnicas implementadas.
4.1
Coleta de Dados
Esta etapa do estudo envolve a aquisição de dados relevantes para
análise. Houveram duas abordagens principais para a coleta desses
dados: através da API oficial do TikTok ou utilizando uma ferra-
menta de extração de dados de terceiros.
4.1.1
API Oficial do TikTok. O TikTok oferece sua ferramenta ofi-
cial, o "TikTok for Developers", desenvolvida com o objetivo de fo-
mentar a transparência. Especificamente neste trabalho, houve a
tentativa de utilizar a API "Research API", disponibilizada nessa
ferramenta, que permite acesso a informações detalhadas de con-
tas específicas e dados de vídeos mediante solicitações específicas.
No entanto, devido às restrições de acesso dessa API, limitadas a
usuários universitários na América do Norte e Europa, foi feito um
pedido de exceção para este projeto. Infelizmente, esse pedido foi
negado. Como alternativa, a ferramenta TikTok Data Extractor foi
identificada como uma opção viável para a coleta de dados.
4.1.2
TikTok Data Extractor. O TikTok Data Extractor é uma fer-
ramenta de extração de dados não oficial, que extrai mídias e me-
tadados associados a vídeos públicos no TikTok, disponibilizada
pelo site Apify [3]. Essa ferramenta permite a pesquisa de criadores
de conteúdo no TikTok e retorna informações detalhadas de seus
vídeos.
Para a finalidade deste trabalho, realizou-se uma seleção cri-
teriosa de influenciadores com base em pesquisas e em duas lis-
tas pré-definidas, abrangendo distintos grupos de influenciadores,
categorizando-os em:
• Influenciadores Mirins: Seleção de contas de jovens, foca-
das em moda, brinquedos, jogos e entretenimento, incluindo
desafios e brincadeiras. Houve a seleção de 17 influenciado-
res.
• Criadores de conteúdo: Contempla adolescentes e adultos
com conteúdos que atraem crianças e jovens. O propósito é
analisar se essas contas, apesar de não serem voltadas exclu-
sivamente para crianças ou de não produzirem conteúdos
puramente infantis, conseguem captar a atenção desse seg-
mento jovem. Totalizando 25 influenciadores.
A aplicação do TikTok Data Extractor envolveu a inserção dos
nomes dos influenciadores selecionados e a subsequente coleta de
postagens. Os dados obtidos incluem o período de 2020 a 2023, co-
brindo a era da Covid-19 e pós-pandemia. A escolha desse período
específico é importante pois a pandemia da Covid-19 representou
Crianças e Propagandas no TikTok:
identificando publicidade infantil na rede social TikTok
WebMedia’2024, Juiz de Fora, Brazil
um momento único de mudanças sociais e de comportamento, influ-
enciando significativamente o uso das mídias sociais e a criação de
conteúdo, como é apresentado em [15]. Espera-se que, ao analisar
esse período, seja possível compreender melhor como essas mudan-
ças afetaram as tendências de influência digital e o engajamento
dos usuários, particularmente entre os públicos jovens.
Com base na busca dos influenciadores listados e no período
especificado, foram coletados um total de 24.131 exemplos.
4.2
Pré-processamento
A partir da etapa anterior, foi gerada uma tabela extensa composta
por 328 características, contendo dados altamente relevantes para a
pesquisa. No entanto, a análise inicial da tabela revelou a presença
de vídeos sem elementos textuais, colunas irrelevantes e a ausência
de transcrições dos áudios dos vídeos. Portanto, a etapa de pré-
processamento foi crucial neste trabalho.
No processo de pré-processamento dos dados coletados, utilizou-
se o PyCharm, uma IDE (Integrated Development Environment) de-
senvolvida para Python. O PyCharm foi escolhido por suas amplas
funcionalidades, como análise de código, suporte a várias bibliote-
cas e frameworks, e ferramentas para desenvolvimento profissional.
Sua eficiência e facilidade de uso no tratamento de grandes conjun-
tos de dados foram decisivas para a escolha.
Inicialmente, a análise neste estudo concentrou-se principal-
mente nas descrições de texto e hashtags presentes nos vídeos
fornecidos pela API. Contudo, decidiu-se também realizar a análise
dos áudios dos vídeos para enriquecer os dados coletados. Assim,
foi implementada uma etapa para a transcrição de cada vídeo. Além
disso, foram excluídos da análise colunas irrelevantes e os vídeos
que não continham descrição, hashtags ou áudio, bem como aqueles
que apresentavam duplicidades. Esta seleção garante que apenas
dados relevantes sejam considerados, aumentando a qualidade e a
confiabilidade da análise.
A tabela resultante, composta por 32 características distintas e
24.118 exemplos, incluiu uma ampla gama de informações, deta-
lhando aspectos como:
• Vídeo: Detalhes gerais sobre o vídeo, incluindo um identi-
ficador único, descrição ou texto associado, data e hora de
criação, links diretos para o vídeo na plataforma, contagens
de engajamento (curtidas, compartilhamentos, visualizações,
comentários, e se o vídeo foi baixado), e menções a outros
usuários e hashtags associadas;
• Criador do vídeo: Informações detalhadas sobre o autor
do conteúdo, como identificador único, nome de usuário,
apelido, status de verificação, descrição pessoal, link para o
avatar do autor, e estatísticas de engajamento social (número
de seguidores, curtidas recebidas, número total de vídeos
postados, e total de aprovações);
• Áudio do vídeo: Informações sobre a música utilizada no
conteúdo, incluindo nome da música, autor, se é uma música
original ou não, nome do álbum, e um link direto para onde
a música pode ser reproduzida;
Ainda na fase de pré-processamento, uma etapa crucial foi a
distinção entre vídeos publicitários e não publicitários. Essa iden-
tificação foi conduzida por dois métodos complementares: uma
busca automatizada, empregando palavras-chave específicas, e uma
investigação manual. Essa abordagem dupla assegurou uma seleção
mais abrangente e precisa, resultando em um modelo preliminar de
treinamento essencial para o processo subsequente de classificação.
4.2.1
Transcrição de Vídeo. Antes de iniciar a análise das palavras-
chave nos elementos textuais dos vídeos, foi necessário converter os
áudios de cada vídeo em transcrições textuais. Espera-se que a com-
binação das descrições dos vídeos com suas respectivas transcrições
resulte em uma base de dados robusta e adequada para a pesquisa de
palavras-chave. O processo de obtenção das transcrições envolveu
três etapas principais: download do vídeo, transcrição do áudio e o
armazenamento do resultado.
Primeiramente, os vídeos foram baixados utilizando uma bibli-
oteca disponível no Python, que permite baixar vídeos e extrair
informações de diversas plataformas, como YouTube, TikTok e Vi-
meo. Em seguida, cada arquivo de áudio foi submetido à transcrição
utilizando a API Speech-to-Text do Google, que oferece reconheci-
mento de voz de alta precisão, suporta vários idiomas e dialetos,
e possui recursos de filtragem de ruídos, o que ajuda a ignorar a
música de fundo nos vídeos do TikTok.
Dado o volume substancial da base de dados, aproximadamente
24.000 entradas, foi essencial implementar um processo automati-
zado e eficiente em termos de recursos. Todo o procedimento foi
executado através de comandos no PyCharm, utilizando também as
ferramentas de Cloud do Google. Assim, para lidar com os arqui-
vos de áudio gerados a partir dos vídeos, o Google Cloud Storage
(GCS) foi implementado, devido à sua escalabilidade e integração
perfeita com outras soluções do Google Cloud, facilitando o manejo
de grandes volumes de dados de forma segura e acessível. O GCS foi
empregado para armazenar temporariamente os arquivos de áudio
antes da transcrição, o que possibilitou uma gestão eficiente dos
dados e uma integração robusta com a API Speech-to-Text. Buckets
no GCS foram utilizados para organizar e armazenar os arquivos de
áudio extraídos, sendo cada arquivo acessado pela API para transcri-
ção. As transcrições obtidas são então recuperadas e armazenadas
no sistema para análises subsequentes.
Para o armazenamento das transcrições, foi desenvolvida uma
tabela complementar dedicada exclusivamente a esse fim. Com o
objetivo de otimizar o processo de manejo de dados, as transcrições
foram processadas e inseridas na tabela em blocos de 1.000 registros.
Esse método assegura que a tabela complementar seja continua-
mente atualizada de forma eficiente, mantendo a integridade e a
organização dos dados.
O processo resultou na transcrição correta dos vídeos de diver-
sos influenciadores, fornecendo uma base sólida para a pesquisa
subsequente de palavras-chave nos textos transcritos.
4.2.2
Busca automática. A publicidade em conteúdos voltados para
o público infantil tende a apresentar duas características principais:
o emprego de palavras-chave que atraem a atenção das crianças
e a utilização de certos elementos visuais e temáticos ao longo do
vídeo. Neste estudo, a análise concentrou-se exclusivamente na
identificação de termos que denotam apelo infantil nas descrições,
nas transcrições e hashtags dos vídeos.
Para efetuar esta análise, empregou-se um conjunto pré-definido
de palavras-chave, incluindo termos como ’brinquedo’, ’parceria’,
’Mattel’, ’Estrela’, ’l.o.l’, ’surpresa’, ’presente’, ’diversão’, entre ou-
tros. As palavras foram fornecidas por trabalhos complementares
WebMedia’2024, Juiz de Fora, Brazil
Carvalho e Marques-Neto
[9]. Além disso, para garantir a abrangência da pesquisa, foram
utilizadas também as palavras traduzidas para o inglês, resultando
na inclusão de 66 termos no processo de busca, aplicado às colunas
de descrição e hashtags de cada vídeo.
Após a análise de 24.118 posts, constatou-se que 5.131 deles (apro-
ximadamente 20%) apresentaram indicativos de conteúdo publicitá-
rio, conforme identificado pela metodologia empregada.
4.2.3
Busca manual. Durante o processo manual de verificação,
foi analisada uma amostra de 1.200 vídeos previamente etiqueta-
dos como publicitários, representando aproximadamente 20% dessa
base. O objetivo foi identificar e corrigir possíveis equívocos na
categorização inicial. Para tanto, os vídeos foram assistidos indivi-
dualmente e analisados com base nas seguintes questões:
• O influenciador promove a própria marca ou marca exterior
no vídeo?
• O conteúdo publicitário possui linguagem ou tom infantil?
• Se há produtos infantis com marcas identificáveis, eles são o
foco principal da publicação?
Ao revisar a amostra, verificou-se que 70% dos vídeos foram cate-
gorizados adequadamente como conteúdo publicitário. Observou-se
que a hashtag #publi não era comumente empregada, o que servi-
ria como indicativo de conteúdo promocional. No entanto, alguns
influenciadores recorrem a termos estratégicos como "slime"e "Net-
flix"para promover produtos próprios ou conteúdos exclusivos. Em
relação aos vídeos indevidamente marcados, muitos consistiam
apenas em desafios ou entretenimento casual promovidos pelos
criadores de conteúdo. Esta análise ressalta a complexidade na
identificação de publicidade em plataformas digitais, onde as linhas
entre o conteúdo promocional e o entretenimento podem ser tênues
e frequentemente se sobrepõem.
Durante a busca manual, observou-se um fenômeno recorrente.
Muitos influenciadores não indicam explicitamente em suas descri-
ções e hashtags que um vídeo possa ser publicitário, deixando essa
informação implícita no decorrer do vídeo. Nesse contexto, a etapa
de transcrições se mostrou essencial. Um exemplo foi observado
em um vídeo postado por um influenciador analisado. O vídeo apre-
senta uma criança muito feliz por ter “recebido” um brinquedo, sem
deixar explícita a parceria. É necessário também observar os co-
mentários da postagem, onde muitos jovens demonstram interesse
no brinquedo, conforme ilustrado na Figura 2.
4.3
Modelo de Aprendizado Supervisionado
Com a base de dados final formada pelos elementos textuais neces-
sários e a classificação inicial, foram implementados os modelos
de classificação automática. No nosso estudo, investigamos o de-
sempenho de quatro modelos de aprendizagem de máquina, sendo
eles o Naive Bayes, SVM, Logistic Regression e Random Forest, para
identificar vídeos com publicidade infantil na plataforma TikTok,
marcando cada vídeo como publicidade ou não publicidade. A esco-
lha dos modelos foi feita a partir de uma análise de suas vantagens,
priorizando aqueles que se sobressaem na classificação de elementos
textuais e no manejo de grandes volumes de dados.
Utilizamos o vetorizador TF-IDF para extrair características dos
textos, uma abordagem padrão para transformar dados textuais
em um formato que os modelos possam processar eficientemente.
Figura 2: Comentário em uma possível publicidade infantil.
Adaptamos a implementação dos modelos baseados nas recomen-
dações e práticas comuns na literatura de aprendizado de máquina,
garantindo que cada modelo fosse configurado para maximizar sua
eficácia. Para avaliar e comparar os modelos, empregamos uma
técnica de validação cruzada, particionando os dados coletados em
um conjunto de 80% dos dados para treinamento e 20% para teste,
garantindo que o desempenho do modelo seja verificado contra
dados não vistos durante o treinamento. O SVM foi o modelo que
se destacou durante os testes. Durante o treinamento, utilizou-se o
método GridSearchCV especificamente no modelo SVM para otimi-
zar seus parâmetros, resultando em uma regularização com valor
100, um kernel linear e o coeficiente gamma definido como scale. A
escolha desses parâmetros foi guiada por métricas de desempenho,
visando maximizar a precisão e a generalidade do modelo. A partir
dessa otimização final, o modelo se destacou em relação aos outros,
sendo capaz de receber os elementos textuais de um vídeo e retornar
sua classificação, sendo publicidade ou não.
As métricas escolhidas para avaliar o desempenho dos modelos
foram Acurácia, Macro-F1 e Área sob a Curva ROC (AUC). Cada
uma dessas métricas fornece uma visão importante sobre diferentes
aspectos do desempenho do modelo:
• Acurácia mede quão frequentemente o modelo prediz corre-
tamente, seja como publicidade ou não-publicidade.
• Macro-F1 é uma média do F1-Score calculado para cada classe
e fornece uma medida de precisão e recall equilibrada, tra-
tando igualmente as classes minoritárias e majoritárias.
• AUC proporciona uma visão abrangente da capacidade do
modelo de distinguir entre as classes em diferentes limiares,
o que é crucial para ajustar a sensibilidade do modelo a casos
de publicidade infantil.
Essas métricas nos ajudam a entender não apenas a eficácia geral
dos modelos, mas também como eles performam na identificação
precisa e equilibrada de publicidade direcionada ao público infantil.
RESULTADOS
As métricas apresentadas na Tabela 1 forneceram uma visão com-
preensiva do desempenho de cada modelo. O modelo SVM, após
Crianças e Propagandas no TikTok:
identificando publicidade infantil na rede social TikTok
WebMedia’2024, Juiz de Fora, Brazil
Tabela 1: Desempenho dos Modelos de Classificação.
Modelo
Acurácia
Macro-F1
AUC
Naive Bayes
0.84
0.68
0.83
SVM
0.97
0.95
0.99
Logistic Regression
0.92
0.87
0.98
Random Forest
0.95
0.93
0.99
o método GridSearchCV, se mostrou superior, apresentando uma
acurácia e F1-Score particularmente altos, o que indica uma boa
capacidade de generalização e balanceamento entre precisão e sen-
sibilidade. Os outros modelos apresentaram uma boa performance,
mas, não superior ao SVM, como pode ser observado nessa tabela.
Durante uma validação do modelo, a partir de uma base não
classificada, observou-se que o modelo SVM apresentou precisão
de 92%. Uma análise detalhada desses casos sugeriu que certas pa-
lavras no texto desses vídeos podem ter semelhanças com termos
frequentemente encontrados em publicidades, levando a classifica-
ções errôneas. Um exemplo notável da limitação do classificador
ocorre quando influenciadores narram histórias em seus vídeos.
Frequentemente, eles mencionam eventos ou objetos sem intenção
publicitária, no entanto, o classificador, guiado pela presença de
palavras-chave específicas, pode erroneamente identificar tais men-
ções como publicidade. Essa tendência de captar palavras-chave
isoladas, independentemente do contexto narrativo em que são
usadas, destaca uma área significativa para aprimoramento na sen-
sibilidade e precisão do modelo.
A análise das palavras-chave utilizadas em conteúdos de publici-
dade infantil revelou uma frequência significativa de termos especí-
ficos. A Figura 3 apresenta uma Word Cloud que destaca as palavras
mais frequentemente mencionadas nos dados analisados. Termos
como ’desafio’, ’brinquedo’, ’publicidade’, ’sbt’, e ’ad’ aparecem com
destaque, indicando sua prevalência nas campanhas publicitárias
direcionadas ao público infantil. Essa visualização ajuda a identifi-
car os principais temas e tendências nas estratégias de marketing
voltadas para crianças.
Figura 3: Word Clouds
A palavra "desafio"apresenta diferentes implicações no contexto
do TikTok. Ela é amplamente utilizada tanto em vídeos classificados
como publicidade quanto em vídeos não são. Esta alta frequência
reflete a popularidade dos desafios na plataforma, que frequente-
mente incluem danças e brincadeiras entre amigos. No entanto,
muitos influenciadores também utilizam o termo para promover
suas próprias marcas, criando tendências e engajamento entre seus
seguidores. Essa dualidade no uso da palavra evidencia como um
mesmo termo pode servir a propósitos diferentes, dependendo do
contexto em que é empregado.
CONCLUSÃO
O presente estudo criou uma base de dados e implementou um
modelo capaz de classificar publicidade infantil no TikTok, a partir
dos seus elementos textuais, evidenciando o papel crescente desta
plataforma nas atividades diárias do público infantil. O trabalho
enfocou principalmente em identificar e classificar conteúdos pu-
blicitários direcionados a crianças, destacando a necessidade crítica
de discernir e entender como esses anúncios se integram às rotinas
delas. Utilizando uma base de dados, coletada e processada, foram
aplicados modelos de aprendizado de máquina, incluindo Naive
Bayes, SVM, Logistic Regression e Random Forest, para alcançar uma
classificação precisa de vídeos no TikTok, distinguindo de maneira
eficaz, entre conteúdos publicitários dirigidos ao público infantil e
conteúdos não publicitários.
Além disso, a base de dados construída é de extrema importância,
pois fornece um recurso para futuras pesquisas sobre publicidade
infantil nas redes sociais. Ela não só permite a análise de padrões
atuais, mas também oferece um ponto de partida para a implemen-
tação de modelos mais sofisticados e precisos. A base de dados
pré-processada está disponível para uso acadêmico a todos os in-
teressados. Sua distribuição será realizada sob demanda, mediante
contato com os autores deste trabalho.
Os resultados mostraram que o modelo SVM, ajustado com oti-
mização de parâmetros, obteve o melhor desempenho em termos de
acurácia, F1-Score e AUC, mostrando-se eficaz na distinção entre
conteúdos publicitários e não publicitários. No entanto, observamos
limitações no classificador, especialmente em contextos onde influ-
enciadores utilizam termos de maneira ambígua ou não explícita.
Além disso, vale ressaltar algumas limitações encontradas, como:
• Acesso aos Dados: A coleta de dados foi realizada por meio
de uma ferramenta não oficial devido às restrições de acesso
à API oficial do TikTok. Esta limitação pode ter impactado a
abrangência e a representatividade dos dados coletados, uma
vez que a ferramenta não oficial pode não capturar todos os
dados disponíveis na plataforma.
• Volume de Dados: Embora a amostra utilizada seja signifi-
cativa, com aproximadamente 24.000 exemplos analisados, o
volume de dados ainda pode ser considerado limitado quando
comparado ao total de conteúdos disponíveis no TikTok. Isso
pode influenciar a variabilidade e a diversidade dos dados,
afetando a robustez dos resultados.
Este trabalho visa oferecer uma metodologia validada com re-
sultados que sirva de base para futuras pesquisas relacionadas ao
impacto da publicidade digital nas crianças, em especial no TikTok.
Além disso, o estudo pode fornecer a base para a criação de fer-
ramentas de identificação automática de publicidade infantil, que
podem ser utilizadas tanto pelos pais quanto pelo próprio TikTok
para monitorar e proteger os jovens usuários da plataforma.
WebMedia’2024, Juiz de Fora, Brazil
Carvalho e Marques-Neto
Em resumo, este estudo estabelece uma base para a identificação
de publicidade infantil no TikTok, mas reconhece a necessidade de
melhorias contínuas e inovações metodológicas. Durante a elabora-
ção da base de dados, foram realizadas pesquisas sobre a ferramenta
adequada e o conteúdo relevante do TikTok a ser analisado. Posteri-
ormente, a identificação de palavras-chave facilitou a compreensão
dos conceitos associados ao tema. Através das classificações automá-
ticas e manuais, foi possível estabelecer uma conexão mais profunda
com os vídeos analisados, proporcionando uma perspectiva mais
clara sobre como a publicidade pode estar velada. Finalmente, a
análise demonstrou que esta temática segue determinados padrões
textuais, os quais podem ser efetivamente identificados e categoriza-
dos. A combinação de todas essas etapas resultaram em passos para
desenvolver uma ferramenta de detecção de publicidade infantil,
protegendo assim os jovens usuários de conteúdos potencialmente
manipuladores.
Para aprimorar a precisão e a sensibilidade da classificação, tra-
balhos futuros podem focar em várias direções:
• Análise de Contexto: Desenvolver modelos que não se
limitem a analisar palavras isoladas, mas que considerem
todo o contexto em que as palavras são usadas. Isso pode
incluir o uso de técnicas de processamento de linguagem na-
tural (NLP) mais avançadas, como redes neurais recorrentes
(RNNs) ou transformadores, que são capazes de capturar a
sequência e o contexto das palavras.
• Integração de Nomes de Empresas: Expandir a base de
dados de palavras-chave para incluir nomes de empresas
e marcas conhecidas, o que pode ajudar a identificar con-
teúdos publicitários de maneira mais eficaz. Esta inclusão
pode melhorar significativamente a detecção de publicidades
implícitas e parcerias de marca.
• Monitoramento Contínuo: Implementar um sistema de
monitoramento contínuo que atualize a base de dados com
novos conteúdos e ajuste os modelos de classificação com
base em padrões e tendências emergentes na plataforma.
• Aprofundamento na Análise dos Resultados: Concen-
trar esforços para aprimorar a análise dos resultados obtidos,
incorporando uma abordagem detalhada e buscando padrões
presentes nos resultados. Além disso, realizar uma análise
das características com o objetivo de identificar sua relevân-
cia para o resultado da classificação, bem como possíveis
correlações e padrões entre elas.

--- FIM DO ARQUIVO: 30108.txt ---

--- INÍCIO DO ARQUIVO: 30109.txt ---
Cuidado Ubíquo de Pacientes com Doenças Crônicas Através de
um Modelo de Análise do Comportamento Humano
Lucas Pfeiffer Salomão Dias∗
L.P.S. Dias∗
lucaspfsd@gmail.com
Universidade do Vale do Rio dos Sinos (UNISINOS)
São Leopoldo, RS
Jorge Luis Victória Barbosa
J.L.V. Barbosa
jbarbosa@unisinos.br
Universidade do Vale do Rio dos Sinos (UNISINOS)
São Leopoldo, RS
4. A Seção 5 apresenta as
questões éticas envolvendo a avaliação do modelo e por fim na
seção 6, são apresentadas as conclusões e trabalhos futuros.
TRABALHOS RELACIONADOS
Este artigo revisou trabalhos da literatura que tinham por objetivo
desenvolver modelos para inferir comportamentos humanos rela-
cionados às DCNTs. Essa seção aborda esses trabalhos relacionados,
os quais foram selecionados de acordo com a metodologia descrita
na Subseção 2.1.
2.1
Metodologia
Os trabalhos relacionados foram obtidos através de um mapea-
mento sistemático [7]. Este mapeamento seguiu a metodologia de
Petersen et al. [21] a qual executa um processo de pesquisa dividida
em três etapas principais: especificação da string de busca, seleção
WebMedia’2024, Juiz de Fora, Brazil
Dias and Barbosa
das bases de pesquisa e coleta de resultados. A primeira etapa re-
alizou a identificação dos principais termos da área de estudo e
seus sinônimos mais relevantes. Este estudo define os termos prin-
cipais como Behavior, Noncommunicable Diseases e Data Analysis. A
Tabela 1 apresenta esses termos e seus sinônimos relacionados. Os
sinônimos para o termo “Behavior” estão relacionados ao comporta-
mento humano. Por sua vez, os sinônimos para “Noncommunicable
Diseases” foram selecionados por meio de termos que permitissem
a cobertura das variações do termo “DCNT" e das DCNTs mais
comuns [33]. Finalmente, os sinônimos para o termo “Data Anal-
ysis” abrangem tópicos relacionados à ciência de dados. Algumas
bases possuem limites de termos na string de busca e, portanto,
esses termos precisam ter baixa ambiguidade para fazer uma busca
assertiva.
Tabela 1: Estrutura da string de busca
Termos principais
Termos da Busca
Behavior
(behavior OR personality OR habit OR attitude)
AND
Noncommunicable Diseases
(“noncommunicable diseases” OR “chronic diseases” OR “risk
factors” OR “chronic conditions” OR diabetes OR cancer OR
“cardiovascular diseases” OR “respiratory diseases” OR depres-
sion)
AND
Data Analysis
(“data analysis” OR “data analytics” OR “statistical learning”
OR “machine learning” OR “deep learning” OR “data mining”)
Depois de construir a string de busca, a próxima etapa selecionou
as bases de pesquisa para consulta de artigos. Este estudo considerou
as seguintes bases: PsycArticles, PsycInfo, PubMed Central, Bib-
lioteca Digital ACM, Biblioteca Digital IEEE Xplore, Science Direct,
Biblioteca Springer e Wiley.
A etapa de coleta de resultados obteve 41 trabalhos, mas ape-
nas oito artigos estão relacionados à mesma área de estudo desta
pesquisa. Estes trabalhos propuseram modelos para a análise de
comportamentos relacionados a hábitos alimentares e atividades
físicas, para o tratamento e acompanhamento de indivíduos com
DCNTs.
2.2
Descrição dos Trabalhos relacionados
Esta subseção descreve os trabalhos relacionados que foram utiliza-
dos como base de comparação para o desenvolvimento do trabalho
proposto.
Bohanec et al. [3] desenvolveu um sistema de apoio à decisão
chamado HeartMan. Este sistema serve como suporte ao paciente
para auxiliar no tratamento, como gerenciamento de medicamentos,
exercícios, nutrição e automonitoramento.
Meegahapola et al. [13] estudou o nível de consumo alimentar
de 84 estudantes universitários no México por meio do uso de sen-
sores de smartphones e autorrelatos passivos. Este estudo encontrou
relação entre consumo alimentar e fatores de sociabilidade, tipos e
níveis de atividades.
Krzyzanowski et al. [12] desenvolveu um sistema para moni-
torar a atividade física e a ingestão de frutas e vegetais chamado
Rams Have Heart para apoiar um curso de intervenção em doenças
cardiovasculares.
Pérez-Rodríguez et al. [20] propôs uma nova metodologia para
analisar rotinas e recursos comportamentais por contexto para
detectar depressão em estudantes universitários. Por meio de in-
terações sociais, a metodologia identifica e interpreta o compor-
tamento, as percepções e as apreciações dos pacientes e parentes
próximos sobre uma condição de saúde.
Ismail et al. [10] propôs um modelo de saúde baseado em redes
neurais convolucionais para analisar fatores de saúde no ambiente
da Internet das Coisas (IoT). O modelo realiza a classificação dos
dados, selecionando primeiro os fatores mais importantes relaciona-
dos à saúde.
Ramsingh e Bhuvaneswari [24] desenvolveram um algoritmo
para análise de postagens relacionadas ao diabetes em redes soci-
ais. A classificação é realizada por meio da frequência de uso das
palavras com base na pontuação de polaridade de cada frase nos
dados das mídias sociais.
Chen et al. [4] propôs o Smart Web Aid para prevenção de
Diabetes Tipo 2 (SWAP-DM2), um sistema baseado na web para
prevenção e tratamento do diabetes. O sistema identifica hábitos
propensos ao desenvolvimento de diabetes por meio de perguntas
e apresenta material educativo.
Bentley et al. [2] desenvolveu um sistema chamado Health Mashups
para promover mudanças de comportamento. A proposta era iden-
tificar padrões estatísticos entre dados de bem-estar e contexto
em linguagem natural. O sistema coleta manualmente informações
sobre agenda do usuário, atividades físicas, sono, humor e dados
alimentares, entre outros.
Para todos os trabalhos selecionados foram explorados aspectos
de pesquisa, a fim de analisar os seguintes critérios:
• Doença Crônica: este critério visa identificar quais são as
doenças crônicas tratadas pelas obras;
• Mobile: este item tem como objetivo identificar se o usuário
pode acessar em um dispositivo móvel, permitindo atendi-
mento em tempo real e acesso rápido quando necessário;
• Histórico de Contexto: este critério busca determinar foram
armazenadas informações de contexto do usuário ao longo
do tempo para fazer inferências baseadas em contextos ante-
riores;
• Perfil: este critério visa identificar se as obras utilizam perfis
para customizar a experiência do usuário [27]. Considerar a
utilização de recursos que agrupem características e infor-
mações únicas do usuário;
• Análise de Dados: o objetivo deste critério é identificar qual
técnica de análise de dados de comportamento humano o
trabalho aplicou;
• Dados Coletados: esse critério busca determinar quais tipos
de dados são coletados, como atividades físicas, hábitos al-
imentares e número de passos, entre outros dados que o
paciente pode fornecer;
• Modular: este critério visa identificar se os trabalhos possuem
alguma forma de extensão de módulos, podendo agregar
recursos ou novas competências;
• Base de Conhecimento: este critério busca determinar se
o trabalho contém ou não uma base de conhecimento ou
ontologia;
• Análise de Conselhos: este critério visa identificar quais tra-
balhos analisam se os usuários estão praticando comporta-
mentos recomendados.
Cuidado Ubíquo de Pacientes com Doenças Crônicas Através de um Modelo de Análise do Comportamento Humano
WebMedia’2024, Juiz de Fora, Brazil
Tabela 2: Trabalhos Relacionados
Artigos
DCNTs
Móvel
Histórico de
Contexto
Perfil Análise de Dados
Dados Coletados
Modular Base de Conhecimento
Análise de
Conselhos
Bohanec et al. [3]
Insuficiência
Cardíaca
Sim
Não
Sim
DEX (Decision EXpert)
Hábitos alimentares, atividades físicas
Não
Não
Não
Meegahapola et al.
[13]
Obesidade
Sim
Não
Não
Não
Hábitos alimentares, uso de celular,
atividades físicas
Não
Não
Não
Krzyzanowski et al.
[12]
Obesidade
Sim
Não
Não
Não
Hábitos alimentares, atividades físicas
Não
Não
Não
Pérez-Rodríguez et
al. [20]
Diabetes
Não
Não
Não
Natural Language Processing,
Machine Learning, Graph Mining
Hábitos alimentares, atividades físicas
Sim
Knowledge Graph
Não
Ismail et al. [10]
Obesidade
Não
Não
Não
Double-layer CNN structure
Hábitos alimentares, atividades físicas
Sim
CNN-Based
Health
Knowledge Model
Não
Ramsingh and Bhu-
vaneswari [24]
Diabetes
Não
Não
Não
Hybrid NBC TFIDF, Naive Bayes,
MapReduce
Hábitos alimentares, atividades físicas
Sim
Não
Não
Chen et al. [4]
Diabetes
Não
Não
Não
SPSS 16.0
Hábitos alimentares, atividades físicas
Não
Não
Não
Bentley et al. [2]
Geral
Sim
Não
Não
Não
Comportamento diário, atividades
físicas, hábitos alimentares
Não
Não
Não
B-Track
Geral
Sim
Sim
Sim
Data Fusion
Dados relacionados com
comportamentos humanos
Sim
Ontologia
Sim
A Tabela 2 apresenta cada um dos critérios comparando os tra-
balhos relacionados e o modelo. A maioria dos trabalhos possui
pelo menos um dos critérios definidos. Além disso, nenhum dos
trabalhos utilizou os critérios histórico de contextos e análise de
conselhos. Além disso, nenhum trabalho suportou todos os itens
simultaneamente. A análise de aconselhamento tem o papel de com-
parar os comportamentos dos utilizadores com as recomendações
feitas durante o acompanhamento dos utilizadores e analisar ao
longo do tempo se houve uma mudança de comportamento re-
sultante da recomendação feita. Esta característica está presente
apenas no modelo B-Track. Em geral, os estudos focam na edu-
cação de hábitos saudáveis e acompanhando, muitos por meio de
questionários e sensores disponíveis em smartphones.
A análise dos trabalhos relacionados revelou que a contribuição
científica do modelo B-Track é a análise dos comportamentos hu-
manos diretamente associados aos fatores de risco e sua suscetibili-
dade ao desenvolvimento de DCNTs.
MODELO
O B-Track é um modelo para assistência no cuidado das DCNTs
por meio da análise do comportamento humano e sua relação com
os fatores de risco associados às doenças.
3.1
Arquitetura
A arquitetura B-Track é organizada em tipos de componentes, como
módulos, agentes, aplicações e ontologias. A Figura 1 mostra o Dia-
grama de Componentes expandindo-se em um contêiner individual
do B-Track e mostrando os componentes dentro dele.
A Figura 1 apresenta no canto superior esquerdo os módulos da
Mobile Application. Os módulos estão listados a seguir:
• Behavior Tips Module: este item tem como objetivo fornecer
ao usuário dicas para praticar comportamentos preventivos;
• Account Module: este módulo tem como objetivo identificar
os usuários, permitindo-lhes acessar o sistema, visualizar e
editar seus dados gerais;
• Gamification Module: este item busca engajar o usuário na
execução das tarefas, mostra as pontuações e faz com que as
interações da pontuação e os comportamentos dos usuários
mudem;
• Context Module: este módulo tem como objetivo coletar o con-
texto do usuário minuto a minuto, construindo um contexto
histórico que permite ao usuário entender suas atividades e
hábitos;
• Sync Module: o objetivo deste item é enviar dados do banco de
dados local para a API e recuperar dicas de comportamento
e métricas do usuário, enviando-os para o BD local.
A Figura 1 apresenta no canto superior direito os módulos da
Web Application e eles estão listados a seguir:
• Account Module: este módulo tem como objetivo identificar
os profissionais de saúde, permitindo-lhes acessar o sistema,
visualizar e editar seus dados gerais;
• Patient Influence Analysis Module: este item permite ao profis-
sional de saúde compreender os hábitos de cada paciente e o
impacto dos comportamentos;
• Patient Profile Module: este módulo tem como objetivo per-
mitir a visualização do perfil do paciente, das DCNTs e dos
fatores de risco específicos de suscetibilidade do paciente.
Finalmente, a Figura 1 apresenta os controladores da API Appli-
cation na parte inferior central. Os controladores deste aplicativo
estão listados a seguir:
• User Controller: este item permite que a aplicação do usuário
e a aplicação administrativa gerenciem os dados pessoais dos
usuários, como perfis, comportamento humano, contextos,
entre outros;
• Context Controller: este controlador tem como objetivo ajudar
o controlador do usuário e o agente de análise de comporta-
mento a gerenciar o contexto coletado pelo usuário;
• Influence Controller: este item tem como objetivo permitir
que a aplicação administrativa e o agente de monitoramento
de influência gerenciem a influência da dica no comporta-
mento do usuário;
• Behavior Controller: este controlador permite que o agente de
análise de comportamento e o agente de monitoramento de
influência gerenciem comportamentos humanos coletados
dos usuários;
WebMedia’2024, Juiz de Fora, Brazil
Dias and Barbosa
Figura 1: Diagrama de Componentes
• Profile Controller: este item tem como objetivo permitir que o
agente de análise de fatores de risco e a aplicação administra-
tiva gerenciem os perfis dos usuários, onde são feitas DCNTs,
fatores de risco, comportamentos e dicas aos usuários;
• Ontology Controller: este item permite que o agente extensor
de ontologia gerencie consultas de ontologias, adicionando
novos conhecimentos de inferência de comportamentos hu-
manos e fatores de risco.
3.2
Aplicações do modelo
O modelo B-Track é composto por duas aplicações: a Aplicação do
Usuário, que interage com os pacientes, e a Aplicação Administra-
tiva, que interage com os profissionais de saúde. Essas aplicações
são detalhadas nas subseções a seguir.
3.2.1
Aplicação do Usuário. A Aplicação do Usuário permite o
acesso a alguns dados pessoais do usuário, como e-mail, nome,
idade, doença e lista de seus profissionais de saúde. O usuário pode
usar essas informações se sentir piora dos sintomas de DCNT. Inter-
namente, em B-Track, essas informações são armazenadas em um
banco de dados de forma que não seja possível identificar o usuário,
e para o sistema, são reconhecidas através de uma identificação
única.
Portanto, este aplicativo poderá sempre acompanhar o usuário e
recomendar a prática de atividades saudáveis. Portanto, entende-
se como a Aplicação do Usuário de um ou mais dispositivos que
garantam as características descritas acima.
3.2.2
Aplicação Administrativa. O modelo B-Track permite que
agentes públicos e profissionais de saúde obtenham insights sobre
o comportamento dos usuários. Os agentes públicos podem pro-
mover a saúde pública aos seus cidadãos e os profissionais de saúde
podem monitorizar o desempenho dos seus pacientes, configurando
recomendações personalizadas. Assim, o Aplicativo Administrativo
tem como objetivo informar os profissionais de saúde com relatórios
de evolução dos usuários. Este aplicativo apresenta os comporta-
mentos identificados, o nível de intensidade na associação com os
fatores de risco para DCNTs, as recomendações oferecidas e uma
análise da mudança de comportamento.
Diferentemente do Aplicativo do Usuário, o profissional de saúde
utilizará o Aplicativo de Administração somente quando achar
conveniente acessar os dados do usuário ou desejar atualizar seu
perfil.
3.3
Agentes
O modelo proposto possui ações autônomas na análise de con-
texto, análise de fatores de risco, monitoramento da influência das
recomendações e análise de extensão da ontologia. Essas ações
acontecem de forma contínua e simultânea para se adaptarem de
acordo com os hábitos comportamentais dos usuários coletados.
Por esta razão, o modelo B-Track possui agentes.
O agente tem como características ser autônomo, proativo e
inteligente. Esses conceitos são abstratos, permitindo muitas ferra-
mentas para modelá-los. As abstrações da Prometheus Design Tool
(PDT) [17] representam os agentes de B-Track.
A Figura 2 apresenta uma visão geral dos agentes em B-Track,
mostrando os quatro agentes presentes no modelo, nomeadamente
o Behavior Analysis Agent, o Risk Factor Analysis Agent, o Influence
Monitoring Agent e Ontology Extender Agent. Cada um dos agentes
será descrito detalhadamente nas próximas subseções.
3.3.1
Behavior Analysis Agent. Para que B-Track analise adequada-
mente os hábitos comportamentais diários dos usuários, o modelo
precisa estar ciente do contexto deles, bem como se adaptar às mu-
danças neste contexto. A figura 1 mostra os módulos do Behavior
Analysis Agent no canto central esquerdo. O contexto é percebido
por este agente através do Context Module, que recebe da Core API
Cuidado Ubíquo de Pacientes com Doenças Crônicas Através de um Modelo de Análise do Comportamento Humano
WebMedia’2024, Juiz de Fora, Brazil
Figura 2: Visão geral dos agentes do B-Track
os dados de contexto coletados da User Application e constrói um
histórico de contexto para cada usuário específico.
O Módulo de Análise de Dados possui algoritmos de aprendizado
de máquina e fusão de dados, que analisam os históricos de contexto
e reconhecem os comportamentos humanos neles contidos. Por-
tanto, o Behavior Module identifica esses comportamentos humanos
especificamente para cada usuário e envia esses dados para a Core
API.
3.3.2
Risk Factor Analysis Agent. Um dos objetivos do B-Track é
identificar os comportamentos dos usuários que estão associados
aos fatores de risco de DCNTs, portanto esta tarefa é realizada pelo
Agente de Análise de Fatores de Risco. A Figura 1 apresenta os mó-
dulos deste agente no canto inferior esquerdo. Ao identificar os
comportamentos dos usuários, o Risk Factor Module deste agente
realiza a análise através de inferências realizadas pela ontologia
B-Track. Essas inferências identificam a relação entre os compor-
tamentos e os fatores de risco. O NCD Module identifica a quais
DCNTs um usuário está suscetível após a identificação dos fatores
de risco. Assim, o Preventive Behavior Module pode recomendar
comportamentos preventivos a serem realizados pelo usuário.
3.3.3
Influence Monitoring Agent. Além de analisar o comporta-
mento dos usuários para identificar o risco de desenvolver uma
DCNT, o modelo B-Track recomenda a prática de hábitos saudáveis,
portanto o modelo verifica a influência dessas recomendações. A
Figura 1 apresenta os módulos do Influence Monitoring Agent no
canto central direito. A influência acontece comparando os com-
portamentos anteriores dos usuários com os novos. Assim, a expec-
tativa é que os comportamentos não preventivos diminuam e os
preventivos aumentem, indicando que o usuário está seguindo as
recomendações de hábitos saudáveis.
3.3.4
Ontology Extender Agent. O modelo B-Track utiliza como
base de conhecimento uma ontologia chamada B-Track Onto. O
modelo proposto pode adquirir novos conhecimentos sobre a asso-
ciação entre comportamento humano e fatores de risco. A Figura 1
apresenta os módulos do Ontology Extender Agent no canto inferior
direito. A abordagem utilizada por esse agente depende da curado-
ria de um especialista para definir os novos fatores de risco de
DCNTs e associá-los aos comportamentos existentes identificados
pelo modelo B-Track. A tarefa de agregar esse novo conhecimento
à ontologia é de responsabilidade do Ontology Extender Agent.
3.4
B-Track Onto
A ontologia utilizada contempla as classes necessárias para recon-
hecer os comportamentos relacionados aos fatores de risco [22, 23],
como por exemplo consumo de tabaco, sódio ou exposição ao sol,
que foram mapeados pela Organização Mundial da Saúde [33].
A Figura 3 apresenta a ontologia, suas relações entre os conceitos
envolvendo comportamentos humanos, fatores de risco e DCNTs.
A modelagem foi realizada de forma a categorizar comportamentos,
suas frequências, suscetibilidade com os fatores de risco, possibili-
tando novas correlações. Desta forma, a ontologia possui a classe
HumanAction à qual são atribuídas as ações humanas. A frequên-
cia dessas ações são atribuídas à classe HumanActionFrequency. A
classe HumanBehavior representa o comportamento humano na
Ontologia. O comportamento é composto pela associação de uma
ação humana e uma frequência.
A Organização Mundial da Saúde categoriza os fatores de risco
como fatores metabólicos ou modificáveis [32], que são atribuídos
pela classe RiskFactor e suas subclasses Metabolic e ModifiableBe-
havioral. A suscetibilidade do fator de risco acontecer é descrita na
classe Susceptibility e possui 3 subclasses PossibleLowSusceptibility,
PossibleNormalSusceptibility e PossibleHighSusceptibility.
A classe NonCommunicableDisease possui 5 subclasses a qual são
atribuídas as doenças crônicas, sendo elas: Cancer, Cardiac, Mental,
Respiratory e Sanguineous. Uma pessoa sem doenças crônicas é
representada na ontologia pela classe Person. Quando uma pessoa
estiver associada a uma doença crônica, ela é atribuída à subclasse
Patient.
A classe Profile representa os perfis que reúnem pessoas com com-
portamentos similares e possui recomendações de comportamentos
preventivos oriundas da classe Recommendation. O nível de influên-
cia dessa recomendação é representada pela classe InfluenceLevel, e
WebMedia’2024, Juiz de Fora, Brazil
Dias and Barbosa
Figura 3: Visão hierárquica da B-Track Onto.
possui 3 subclasses: AcceptSuggestion, Neutral e NotAcceptSugges-
tion.
No B-Track, um comportamento humano está associado a uma
ação e uma frequência. Além disso, este comportamento possui
uma suscetibilidade relacionada a um fator de risco, que está rela-
cionado com doenças crônicas. As recomendações possuem níveis
de influência e estão associados a perfis de pessoas. A B-Track Onto
infere comportamentos humanos a partir das regras estabeleci-
das, classificando esses comportamentos. Assim, essa classificação
permite identificar os fatores de risco relacionados com estes com-
portamentos humanos de forma automática.
AVALIAÇÃO E RESULTADOS
Para avaliar o modelo proposto neste trabalho, foi implementado
um protótipo baseado no B-Track. Após a criação do protótipo,
foram selecionados 10 pacientes com alguma DCNT para utilizar o
protótipo por pelo menos duas semanas. Esses pacientes utilizaram
o protótipo para informar suas atividades físicas praticadas diaria-
mente e dieta alimentar, preenchendo questionários visuais, infor-
mando quantidade e tipos de cada item. Essas informações foram
utilizadas pelo protótipo para analisar comportamentos humanos e
recomendar comportamentos saudáveis.
A avaliação mostrou a capacidade do modelo em analisar com-
portamentos, identificar comportamentos não preventivos e sugerir
comportamentos benéficos à saúde. Para preservar a identidade dos
pacientes que utilizaram B-Track serão identificados como P1, P2,
P3, P4, P5, P6, P7, P8, P9 e P10.
4.1
Hábitos dos Pacientes
O B-Track coleta hábitos dos pacientes para analisar e classificar
os comportamentos que atenuam (preventivos) ou pioram (não
preventivos) os fatores de risco associados às DCNTs. Usando os
dados de hábitos coletados dos pacientes ao longo de uma semana,
o modelo B-Track determina se um comportamento específico é
"Raramente", "Normal", "Frequente" ou "Excessivo".
Esta definição utiliza tanto a quantidade quanto a frequência de
um hábito. Os hábitos alimentares são categorizados como “Nen-
hum”, “Muito Pouco”, “Pouco”, “Moderado”, “Alto” e “Muito Alto”.
Os hábitos de exercício são categorizados como “Nenhum”, “Mod-
erado” e “Forte”. A frequência é definida pelo número de dias da
semana de ocorrências. "Raramente" é definido como 2 ou menos
ocorrências, "Normal" como 3 ou 4, "Frequente" como 5 ou 6 e "Ex-
cessivo" como 7 dias por semana. A mesma definição de frequência
se aplica tanto à dieta quanto aos hábitos de exercício.
A Figura 4 apresenta os 10 principais comportamentos preven-
tivos dos pacientes, o modelo B-Track define hábitos preventivos
como aqueles que não estão associados a nenhum fator de risco.
Praticar Aeróbico Excessivo foi o hábito mais preventivo com 31%,
seguido por Beber Álcool Raramente e Comer Proteínas do Mar
Raramente com 9%, com Comer Frituras Raramente, Comer Gor-
dura Raramente e Comer Açúcar Raramente com 8%. Comer sal
raramente, comer pimenta raramente, beber suco raramente e beber
leite raramente a 7%.
Figura 4: Comportamentos preventivos dos pacientes
Cuidado Ubíquo de Pacientes com Doenças Crônicas Através de um Modelo de Análise do Comportamento Humano
WebMedia’2024, Juiz de Fora, Brazil
O B-Track coleta hábitos dos pacientes para analisar e classificar
os comportamentos que atenuam (preventivos) ou pioram (não
preventivos) os fatores de risco associados às DCNTs. Usando os
dados de hábitos coletados dos pacientes ao longo de uma semana,
o modelo B-Track determina se um comportamento específico é
"Raramente", "Normal", "Frequente" ou "Excessivo".
Esta definição utiliza tanto a quantidade quanto a frequência de
um hábito. Os hábitos alimentares são categorizados como “Nen-
hum”, “Muito Pouco”, “Pouco”, “Moderado”, “Alto” e “Muito Alto”.
Os hábitos de exercício são categorizados como “Nenhum”, “Mod-
erado” e “Forte”. A frequência é definida pelo número de dias da
semana de ocorrências. “Raramente” é definido como 2 ou menos
ocorrências, “Normal” como 3 ou 4, “Frequente” como 5 ou 6 e “Ex-
cessivo” como 7 dias por semana. A mesma definição de frequência
se aplica tanto à dieta quanto aos hábitos de exercício.
Figura 5: Comportamentos não preventivos dos pacientes
A figura 5 apresenta os 10 principais comportamentos não pre-
ventivos dos pacientes, que estão associados a fatores de risco.
Praticar Aeróbico Raramente foi o hábito mais não preventivo com
34%, seguido de Comer Raramente Grãos e Comer Legumes Rara-
mente com 12%, e Praticar Fortalecimento Muscular Raramente com
10%. Comer sementes de proteínas e nozes raramente e comer base
de soja raramente em 7%, enquanto beber água raramente em 6%.
Pratique o equilíbrio raramente e pratique a flexibilidade raramente
em 5%. Comer Gordura Normal tem a menor representação em 1%.
Em geral, os pacientes tendem a adotar hábitos mais prejudiciais
à saúde relacionados à atividade física do que à dieta alimentar. Isto
pode incluir não praticar exercício suficiente, levar um estilo de
vida sedentário ou não praticar atividade física regular.
4.2
Doenças Crônicas Suscetíveis
O B-Track Onto infere os fatores de risco com base nos compor-
tamentos humanos dos pacientes, permitindo a compreensão das
DCNTs às quais os pacientes estão suscetíveis. A figura 6 apresenta
as 9 principais DCNTs às quais os pacientes são suscetíveis com
base nos fatores de risco relacionados aos seus comportamentos.
A diabetes foi a DCNT susceptível mais prevalente com 21%,
seguida pela doença coronária com 20%, com demência e câncer do
cólon com 15%. O AVC representa 10%, enquanto o câncer gástrico,
as doenças cardiovasculares e a hipertensão representam 5% cada.
A Doença Renal Crônica tem a menor representação, 2%.
Figura 6: DCNTs suscetíveis aos pacientes
4.3
Análises de Hábitos
O modelo B-Track analisa hábitos dos pacientes e os armazena
gerando históricos contextuais, com ações, frequência, comporta-
mento associado aos fatores de risco e quais DCNTs são suscetíveis.
Este histórico de contexto dos usuários foi analisado para entender
o impacto das recomendações nas mudanças de comportamento
dos usuários.
Dos dez pacientes que participaram do experimento, nove de-
les haviam feito mudanças em seus hábitos pouco saudáveis em
algum momento. O paciente P1 não apresentou nenhuma mudança
de comportamento. Os pacientes P2, P3, P5, P7, P9 e P10 tiveram
mudanças de hábitos em curto prazo, enquanto os pacientes P4, P6
e P8 tiveram mudanças em longo prazo.
A experiência sugere uma tendência para melhorias nos padrões
comportamentais, mas é incerto se estas mudanças seriam eficazes
a longo prazo. É necessária uma monitorização adicional dos pa-
cientes durante um período prolongado para determinar a eficácia
destas alterações.
4.4
Avaliação TAM
No modelo TAM, a satisfação do usuário é medida em duas cat-
egorias: utilidade percebida e facilidade de uso percebida [5]. A
utilidade percebida é utilizada para determinar se a tecnologia pro-
posta pode ajudar o usuário a realizar uma atividade de forma mais
adequada, enquanto a facilidade de uso avalia se a tecnologia pode
ser utilizada com o mínimo esforço. Desta forma, o questionário de
avaliação é dividido em duas categorias, uma quanto à sua utilidade
e outra quanto à sua facilidade de uso. O questionário consiste
em itens Likert de cinco níveis: “discordo totalmente”, “discordo
parcialmente”, “indiferente”, “concordo parcialmente” e “concordo
totalmente”.
Tabela 3: Itens relacionados à avaliação da utilidade
Questão
Descrição
O conhecimento de quais hábitos estão associados a fatores de risco é
relevante.
As opções de atividade física são úteis para registar hábitos de exercício
físico.
As opções alimentares são úteis para registrar hábitos alimentares.
A possibilidade de reportar hábitos a qualquer momento é relevante.
Os hábitos saudáveis fornecidos nos materiais educativos são úteis.
Notificações ajudam a aumentar a conscientização sobre os hábitos
diários.
WebMedia’2024, Juiz de Fora, Brazil
Dias and Barbosa
Para avaliar os resultados, foi computado o percentual de se-
leção de cada opção (‘discordo totalmente’, ‘discordo parcialmente’,
’indiferente’, ’concordo parcialmente’ ou ’concordo totalmente’)
por questão. Nesse sentido, será calculada a média dos percentuais
obtidos para cada opção de acordo com a categoria em questão
(utilidade ou facilidade de uso). A média dos percentuais de cada
opção será equivalente ao índice médio de satisfação avaliado pelos
usuários voluntários para cada categoria.
Tabela 4: Itens relacionados à avaliação da usabilidade
Questão
Descrição
Os hábitos associados aos fatores de risco são claramente apresentados.
As opções de atividade física são facilmente utilizadas.
As opções de comida são usadas sem esforço.
As notificações são simples de entender.
O material educativo pode ser consultado facilmente.
O número de notificações solicitando a realização de hábitos é adequado.
As respostas para cada categoria, utilidade e usabilidade, foram
geralmente positivas. Os resultados das médias dos pacientes que
usaram o TAM para questões relacionadas à utilidade do B-Track
são apresentados na Figura 7, mostrando que 61% dos pacientes que
usaram o B-Track concordam completamente que o aplicativo é útil
em seu rotina de tratamento, 22% concordam parcialmente, 7% são
indiferentes, 8% discordam parcialmente e 2% discordam totalmente.
Em relação à usabilidade do B-Track 53% dos pacientes concordam
completamente que o B-Track é fácil de usar, 27% concordam par-
cialmente, 5% são indiferentes, 8% discordam parcialmente e 7%
discordam totalmente (Figura 8).
Figura 7: Utilidade geral percebida pelos pacientes
Figura 8: Usabilidade geral percebida pelos pacientes
Analisando o TAM é possível perceber que, em uma visão geral,
o B-Track obteve uma boa avaliação de utilidade e usabilidade pelos
pacientes.
DECLARAÇÃO DE ÉTICA
A presente pesquisa atende à Resolução 510/2016 do Ministério da
Saúde, que regula a pesquisa com seres humanos no Brasil. O proto-
colo da pesquisa foi aprovado pelo Comitê de Ética em Pesquisa da
Universidade do Vale do Rio dos Sinos (CAAE 67413623.3.0000.5344,
disponível na Plataforma Brasil 1). Todos os participantes assinaram
o Termo de Consentimento Livre e Esclarecido (TCLE), contendo
a especificação sobre os objetivos, riscos e benefícios da pesquisa.
Durante todo o estudo, os pesquisadores estiveram disponíveis
para resolver dúvidas dos participantes, bem como para fornecer
qualquer suporte adicional necessário.
CONCLUSÃO
Os resultados obtidos através dos experimentos indicam que a uti-
lização do B-Track para apoio ao tratamento das DCNTs é possível
e útil ao paciente, porém, ainda é necessária uma avaliação com um
grupo maior de pacientes e por um período mais longo para uma
análise mais detalhada e precisa.
Os pacientes participantes do experimento apresentavam hábitos
associados a fatores de risco com suscetibilidade ao desenvolvi-
mento de DCNTs, como doença coronariana, diabetes e demência.
Alguns pacientes já apresentavam doenças cardíacas, hipertensão e
diabetes, onde seus hábitos poderiam piorar o quadro atual.
Através da análise do comportamento dos pacientes, foram feitas
recomendações de hábitos saudáveis por meio de material educativo.
Os pacientes P4, P6 e P8 obtiveram mudanças em alguns comporta-
mentos por um período mais longo. Para o paciente P1 não foram
observadas alterações e os demais pacientes tiveram alterações
comportamentais por curtos períodos. O paciente P4 alterou seu
consumo de vegetais e alimentos à base de soja de raro para normal,
para P6 notou-se que a frequência da prática aeróbica mudou de
pouco frequente para excessiva e para P8 a frequência da prática de
exercícios aeróbicos mudou de raramente para normal, frequente e
excessivo.
Através da avaliação do TAM foi possível perceber pontos de mel-
horia no B-Track, principalmente nos materiais educativos, onde
20% dos pacientes não acharam útil, na facilidade em encontrar os
fatores de risco associados aos hábitos, onde 20% dos pacientes apon-
taram dificuldades, na usabilidade percebida pelos pacientes nas
opções alimentares, onde 20% relataram dificuldades, e no número
de notificações solicitando a implementação de hábitos, onde 20%
não consideraram adequado.
AGRADECIMENTOS
Os autores agradecem à Fundação de Amparo à Pesquisa do Estado
do Rio Grande do Sul (FAPERGS), à Coordenação de Aperfeiçoa-
mento de Pessoal de Nível Superior - Brasil (CAPES) - Código de
Financiamento 001, ao Conselho Nacional de Desenvolvimento
Científico e Tecnológico (CNPq) e à Universidade do Vale do Rio
dos Sinos (Unisinos) pelo apoio ao desenvolvimento desse trabalho.
Os autores reconhecem especialmente o apoio do Programa de Pós-
Graduação em Computação Aplicada (PPGCA) e do Laboratório de
Computação Móvel (Mobilab) da Unisinos.
1ℎ𝑡𝑡𝑝: //𝑝𝑙𝑎𝑡𝑎𝑓𝑜𝑟𝑚𝑎𝑏𝑟𝑎𝑠𝑖𝑙.𝑠𝑎𝑢𝑑𝑒.𝑔𝑜𝑣.𝑏𝑟
Cuidado Ubíquo de Pacientes com Doenças Crônicas Através de um Modelo de Análise do Comportamento Humano
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30109.txt ---

--- INÍCIO DO ARQUIVO: 30113.txt ---
Estratégias de Undersampling para Redução de Viés em
Classificação de Texto Baseada em Transformers
Guilherme Fonseca
guilhermefonseca8426@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Gabriel Prenassi
prenassigabriel@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Washington Cunha
washingtoncunha@dcc.ufmg.br
UFMG
Minas Gerais, Brazil
Marcos André Gonçalves
mgoncalv@dcc.ufmg.br
UFMG
Minas Gerais, Brazil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
WebMedia’2024, Juiz de Fora, Brazil
Guilherme Fonseca, Gabriel Prenassi, Washington Cunha, Marcos André Gonçalves, & Leonardo Rocha
considerável para melhorias, principalmente em bases de dados
que possuem um elevado grau de desbalanceamento (razão entre
número de documentos da classe majoritária e o número de docu-
mentos da classe minoritária superior a 5).
Existem duas principais abordagens utilizadas para lidar com
o desbalanceamento de dados. O oversampling consiste em criar
novas amostras (geralmente sintéticas) da classe minoritária, para
igualá-la em termos de instâncias à classe majoritária [15]. Essa
abordagem incorre em um aumento significativo no tempo de ger-
ação dos modelos, uma vez que aumenta-se o total de instâncias a
serem consideradas no aprendizado. Mais ainda, a geração sintética
de novos dados, principalmente textuais, pode ser não trivial [12].
A abordagem alternativa para enfrentar o desbalanceamento é o
undersampling (US), foco do presente trabalho, que são técnicas que
reduzem instâncias da classe majoritária para equilibrar as classes.
Até onde sabemos, não existem estudos na literatura que abordam
como os métodos de undersampling interagem com os algoritmos
de CAT do estado da arte baseados em Transformers. Assim, nossa
segunda pergunta de pesquisa é PP2: Métodos de undersampling,
aplicados juntamente com classificadores baseados em Transformers,
são capazes de reduzir o viés dos modelos de classificação? Qual o
impacto dessa combinação na efetividade do modelo?
Para responder a PP2, primeiramente realizamos uma revisão
sistemática sobre os principais métodos de undersampling propos-
tos na literatura. Identificamos e implementamos 14 métodos de
undersampling que estão entre os mais utilizados. Apesar de terem
fins diferentes, as áreas de seleção de instâncias (SI) [8] e undersam-
pling são bastante relacionadas, pois ambas tratam de técnicas que
visam selecionar um subconjunto de dados representativos – o que
as difere são os objetivos da redução. Nesse sentido, adaptamos uma
estratégia de SI, que é considerada estado da arte, para o cenário de
undersampling, o E2SC [6]. Por fim, propomos duas novas estraté-
gias de undersampling, que são contribuições desse trabalho: (1) a
UBR (Undersampling Baseado em Redundância), que se concentra
em remover instâncias da classe majoritária consideradas redun-
dantes (muito similares à outras instâncias); e (2) E2SC-RL, uma vari-
ação do método de SI E2SC que realiza o cálculo das probabilidades
das instâncias serem removidas por meio de Regressão Logística.
Investigamos o desempenho das 17 técnicas de undersampling
em conjunto com o RoBERTa, classificador baseado em Transform-
ers considerado estado da arte em análise de sentimentos [26]. De
fato, benchmarks recentes [10, 26] demonstraram que as diferenças
entre as novas versões desses Transformers (incluindo RoBERTa,
BERT, DistilBERT, BART, AlBERT e XLNet) em diversos conjuntos
de dados utilizados em nossos experimentos são muito pequenas.
Nossos resultados apontaram que os métodos de undersampling
NM1 [28], NM2 [28], E2SC [6], E2SC-RL (nossa proposta) e UBR
(nossa proposta) foram capazes de reduzir o viés do modelo de
classificação, mantendo sua efetividade.
Métodos de CAT baseados em Transformers demandam elevado
custo computacional no processo de aprendizado dos modelos de
classificação, resultando em longos tempos de execução e também
contribuindo significativamente para a emissão de carbono na at-
mosfera [1]. Assim, precisamos investigar o impacto desse novo
passo de pré-processamento na eficiência (i.e. tempo e custo com-
putacional para geração e classificação) dos modelos. Dessa forma,
nossa terceira pergunta de pesquisa é: PP3: Qual o impacto da apli-
cação dessa etapa adicional de pré-processamento (undersampling)
em termos de eficiência? E em termos da emissão de carbono?.
Nossos resultados apontam que o uso de algumas das estratégias
de undersampling, mais especificamente UBR, NM1, E2SC-RL, NM2
e E2SC, que foram capazes não apenas de reduzir o viés sem perda
de efetividade, mas também diminuíram significativamente o tempo
de treinamento dos modelos (50% em média), o que, consequente-
mente, contribuiu para redução na emissão de 𝐶𝑂2 (50%, em média)
durante a geração e execução dos modelos de classificação.
Assim, as principais contribuições deste trabalho são:
• Mapeamento sistemático da literatura sobre métodos de un-
dersampling, identificando e implementando 14 métodos que
estão entre os mais utilizados. Identificamos e adaptamos
também um método de seleção de instâncias para a tarefa
de undersampling;
• Propostas de duas novas estratégias de undersampling;
• Avaliação das técnicas de undersampling em conjunto com
classificadores baseado em Transformers sob três perspecti-
vas: (1) efetividade da classificação; (2) eficiência (tempo); e
(3) capacidade de generalização (viés).
LEVANTAMENTO DAS ESTRATÉGIAS
Nesta seção, detalhamos o processo de revisão sistemática da liter-
atura (RSL) [7, 8] para selecionar quais estratégias de undersampling
e seleção de instâncias que serão avaliadas.
2.1
Métodos de Undersampling
Recorremos ao mecanismo de pesquisa do Google Scholar para sub-
meter a consulta e gerar nosso conjunto inicial de artigos. O Google
Scholar foi escolhido devido à sua ampla cobertura, abrangendo as
principais bibliotecas digitais de editoras como ACM, IEEE e Else-
vier, além de repositórios de pré-impressão como Arxiv. A string de
busca utilizada foi "Undersampling" e, para maximizar a abrangência
da pesquisa, o mecanismo de busca não aplicou nenhum filtro de
local ou ano. Com base nisso, coletamos, inicialmente, um total de
500 artigos únicos que, de alguma forma, utiliza alguma estratégia
de undersampling.
Analisamos manualmente os 500 artigos, procurando identificar
os mais pertinentes para o estudo. Um artigo foi considerado rel-
evante caso utilizasse técnicas de undersampling para reduzir des-
balanceamento, sendo que o método de undersampling empregado
deveria ser explicitamente mencionado (citado). Identificamos 139
artigos relevantes e, a partir deles, enumeramos todas as técni-
cas de undersampling utilizadas, encontrando, ao todo, 32 técnicas
diferentes. Uma tabela completa com uma descrição de todas as
estratégias identificadas está disponível online 1. Optamos por con-
siderar em nossa avaliação aqueles que foram utilizados em mais
de um dos trabalhos relevantes. Faremos nossa avaliação sobre os
seguintes métodos:
- Links de Tomek (TL) [37]: dados dois exemplos 𝑒𝑖e 𝑒𝑗de difer-
entes classes, com 𝑑(𝑒𝑖,𝑒𝑗) representando a distância entre 𝑒𝑖e 𝑒𝑗,
um par 𝐴(𝑒𝑖,𝑒𝑗) é chamado de link de Tomek se não houver nen-
hum exemplo 𝑒𝑙tal que 𝑑(𝑒𝑖,𝑒𝑙) < 𝑑(𝑒𝑖,𝑒𝑗) ou 𝑑(𝑒𝑗,𝑒𝑙) < 𝑑(𝑒𝑖,𝑒𝑗).
1https://github.com/guilherme8426/Undersampling
Estratégias de Undersampling para Redução de Viés em Classificação de Texto Baseada em Transformers
WebMedia’2024, Juiz de Fora, Brazil
Se dois exemplos formam um link de Tomek, então ou um desses ex-
emplos foi classificado manualmente errado ou ambos são exemplos
pertencentes à fronteiras entre as classes e podem ser removidos.
- Condensed Nearest Neighbors (CNN)[17]: O conjunto de da-
dos 𝑆é inicializado com um exemplo da classe majoritária e todos
os exemplos da classe minoritária e um conjunto 𝑇é criado com os
elementos que não pertencem a 𝑆. Cada exemplo de𝑇é classificado
pelo KNN usando 𝑆como conjunto de treinamento. Caso o KNN
acerte a classe do exemplo, ele permanece em 𝑇; caso contrário, o
exemplo é removido de 𝑇e colocado em 𝑆. Esse processo se repete
até que não ocorram mais mudanças no conjunto 𝑆. Ao final, os
elementos de 𝑇são descartados.
- One-Sided Selection (OSS)[20]: Combina o TL e uma variação do
CNN. inicialmente, como no CNN, um conjunto 𝑆é inicializado com
todas as instâncias da classe minoritária e uma da classe majoritária
e um conjunto𝑇com o restante dos elementos, depois as instâncias
de 𝑇são classificadas com o KNN treinado em 𝑆e cada instância
classificada erroneamente é colocada em 𝑆. No final, o TL é utilizado
em 𝑆para identificar pares ambíguos na fronteira da classe.
- Edited Nearest Neighbours (ENN)[39]: insere todas as instân-
cias do conjunto original 𝑇no conjunto de solução 𝑆, utilizando
o KNN de maneira iterativa para classificar todas as instâncias 𝑥
dado que 𝑥∈𝑆e que 𝑥pertença a classe majoritária (considerando
o conjunto {𝑆−{𝑥}} como possíveis vizinhos). Por fim, remove as
instâncias classificadas incorretamente.
- Repeated Edited Nearest Neighbours (RENN)[36]: ENN apli-
cado sucessivamente até que não seja possível remover mais pontos.
- ALL k-NN [36]: ENN aplicado sucessivamente, mas, a cada apli-
cação, o número de vizinhos a serem considerados aumenta.
- Neighbourhood Cleaning Rule (NCR)[22]: Utiliza o KNN para
classificar todas as instâncias da base de dados. Caso a classe pre-
vista seja diferente da classe real e a instância pertença à classe
majoritária, a instância é eliminada. O NCR classifica também as
instâncias da classe minoritária. Se a classificação estiver incor-
reta, o método elimina os vizinhos mais próximos da instância que
pertencem à classe majoritária.
- Near Miss (NM)[28]: três métodos de undersampling são propos-
tos. O NearMiss-1 (NM1) remove as instâncias da classe majoritária
que têm a menor distância média entre as k instâncias da classe
minoritária. O NearMiss-2 (NM2) seleciona os elementos da classe
majoritária cuja distância média para os k pontos mais distantes
da classe minoritária é a mais baixa. Já o NearMiss-3 (NM3) calcula,
para cada instância da classe minoritária, as k instâncias da classe
majoritária mais próximas e as mantém na base de dados.
- SBC [41]: Todo o conjunto de treino é dividido em N clusters. Para
cada um dos clusters, o número de instâncias a serem selecionadas
é calculado com base no número de amostras da classe majoritária
e da classe minoritária que existem no cluster. Após isso, exemplos
da classe majoritária são selecionados aleatoriamente. Por fim, o
algoritmo combina as instâncias selecionadas de cada cluster com
as da classe minoritária para formar um novo conjunto.
- IHT [35]: Utiliza um classificador (c) para obter o instance hard-
ness (IH) de cada instância. O IH de uma instância é dado por
𝐼𝐻(< 𝑥𝑖,𝑦𝑖>) = 1 −𝑝(𝑦𝑖|𝑥𝑖,𝑐) onde 𝑝(𝑦𝑖|𝑥𝑖,𝑐) denota a probabil-
idade, gerada pelo classificador c, da instância 𝑥𝑖pertencer à classe
𝑦𝑖. O IHT seleciona amostras da classe majoritária com baixa proba-
bilidade de pertencerem à classe majoritária para serem removidas.
- CC-NN [24]: As instâncias da classe majoritária são divididas em
N clusters, com N sendo o número de instâncias da classe minoritária.
Após isso, o vizinho mais próximo do centróide de cada um dos
clusters que pertença à classe majoritária é escolhido para compor,
junto com as instâncias da classe minoritária, o conjunto final.
- OBU [38]: Utiliza o Fuzzy c-means para dividir os dados em 2 clus-
ters, onde o cluster que tiver mais instâncias da classe minoritária
é chamado de 𝐶𝑀. Depois disso, o algoritmo remove todas as in-
stâncias da classe majoritária cujo grau de pertencimento para o
CM é menor que 𝛼(hiperparâmetro).
2.2
Métodos de seleção de instância
Apesar de terem fins diferentes, as áreas de seleção de instâncias
(SI) e undersampling (US) são relacionadas, pois tratam de técni-
cas que visam selecionar um subconjunto de dados a ser usado no
treinamento do modelo e que cumpram seus objetivos: (1) no caso
de SI, melhorar a eficiência sem perda de efetividade; e (2) no caso
de US, reduzir o viés da classe majoritária, mantendo a efetividade.
Apesar dos objetivos finais serem diferentes, partimos da hipótese
de que há uma relação subjacente entre ambas as tarefas, principal-
mente para métodos de SI baseados em redução de redundância [8],
que podem ser adaptados para remoção de instâncias redundantes
da classe majoritária. Essa hipótese norteia a concepção do nosso
novo método de US - UBR - descrito na próxima seção e também nos
motivou a selecionar e adaptar trabalhos de SI para undersampling.
Uma varredura na literatura de SI aplicada a CAT nos revela
que o método E2SC [6, 32] é o estado da arte. O método funciona
em duas etapas. Na primeira, calcula as probabilidades de cada in-
stância ser removida. Estas probabilidades são obtidas por meio da
confiança do classificador KNN, que é calibrado (classificador cujas
previsões de probabilidade de classe correspondem bem à acurácia
do classificador) [34]. Na segunda etapa, o E2SC tenta estimar qual
a taxa de redução ótima para a base de dados. Após isso, as instân-
cias são amostradas aleatoriamente, ponderadas pela probabilidade
encontrada no primeiro passo. Para o nosso trabalho, realizamos
uma modificação do E2SC, chamada E2SC_RL, que segue o mesmo
princípio do E2SC, porém, em vez do KNN como classificador, uti-
lizaremos Regressão Logística (RL). Optamos por essa abordagem
ser um classificador igualmente calibrado e possuir baixo custo com-
putacional, inferior ao KNN. Portanto, consideramos em nossos
experimentos o E2SC e o E2SC_RL, ambos adaptados para remover
apenas instâncias da classe majoritária. Além de todos as estraté-
gias apresentadas nesta seção, apresentamos a seguir nossa nova
proposta de estratégia de undersampling.
MÉTODO PROPOSTO
Nesta seção, apresentamos nossa segunda contribuição neste tra-
balho, uma nova abordagem denominada UBR (Undersampling
Baseado em Redundância). Essa abordagem busca inspiração em
técnicas de SI. Um par de documentos é considerado redundante se
apresenta alta similaridade entre si. Nossa hipótese é que manter
apenas uma das instâncias no conjunto de treinamento é suficiente,
pois a presença de ambas não trará aumento significativo na apren-
dizagem do modelo. Ao se concentrar na redução de redundância
na classe majoritária, obtemos um potencial de redução do des-
balanceamento e, consequentemente, do viés para essa classe. O
Algoritmo 1 detalha o pseudocódigo do UBR.
WebMedia’2024, Juiz de Fora, Brazil
Guilherme Fonseca, Gabriel Prenassi, Washington Cunha, Marcos André Gonçalves, & Leonardo Rocha
Algoritmo 1: Algoritmo UBR
Input: X, 𝛼
Output: instanciasSel
1 𝑋𝑀𝑎𝑗←𝑜𝑏𝑡𝑒𝑟𝐼𝑛𝑠𝑡𝑎𝑛𝑐𝑖𝑎𝑠(𝑋,𝑐𝑙𝑎𝑠𝑠𝑒= 𝑚𝑎𝑗𝑜𝑟𝑖𝑡𝑎𝑟𝑖𝑎);
2 𝑋𝑀𝑖𝑛←𝑜𝑏𝑡𝑒𝑟𝐼𝑛𝑠𝑡𝑎𝑛𝑐𝑖𝑎𝑠(𝑋,𝑐𝑙𝑎𝑠𝑠𝑒= 𝑚𝑖𝑛𝑜𝑟𝑖𝑡𝑎𝑟𝑖𝑎);
3 𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑖𝑎𝑠𝑆𝑒𝑙←𝑋𝑀𝑖𝑛;
4 𝐷←𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑖𝑎𝐴𝑝𝑟𝑜𝑥𝑖𝑚𝑎𝑑𝑎𝑁𝑉𝑖𝑧𝑖𝑛ℎ𝑜𝑠(𝑋𝑀𝑎𝑗) ;
5 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠←𝑋𝑀𝑎𝑗;
6 𝑁←
𝑋𝑀𝑎𝑗
−∥𝑋𝑀𝑖𝑛∥;
7 while 𝑁> 0 do
𝐴, 𝐵←𝑃𝑎𝑟𝑀𝑎𝑖𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟(𝑋𝑀𝑎𝑗, 𝐷) ;
if (∥𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐴]∥> 𝛼)𝑂𝑅(∥𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐵]∥> 𝛼) then
𝑐𝑜𝑛𝑡𝑖𝑛𝑢𝑒;
end
if 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐴] ≠𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐵] then
𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐴] ←𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐴] Ð𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠[𝐵] ;
𝑑𝑒𝑙𝑒𝑡𝑎𝑟(𝑐𝑙𝑢𝑠𝑡𝑒𝑟[𝐵]) ;
𝑁= 𝑁−1 ;
end
17 end
18 𝑒𝑠𝑐𝑜𝑙ℎ𝑖𝑑𝑜𝑠←𝑒𝑠𝑐𝑜𝑙ℎ𝑒𝑅𝑒𝑝𝑟𝑒𝑠𝑒𝑛𝑡𝑎𝑛𝑡𝑒(𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠) ;
19 𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑖𝑎𝑠𝑆𝑒𝑙←𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑖𝑎𝑠𝑆𝑒𝑙Ð𝑒𝑠𝑐𝑜𝑙ℎ𝑖𝑑𝑜𝑠;
Dado 𝑋como o conjunto total de instâncias de treinamento, ini-
cialmente dividimos 𝑋em dois conjuntos, 𝑋𝑀𝑎𝑗e 𝑋𝑀𝑖𝑛, onde 𝑋𝑀𝑎𝑗
consiste em instâncias de 𝑋pertencentes à classe majoritária, e
𝑋𝑀𝑖𝑛consiste em instâncias de 𝑋pertencentes à classe minoritária.
Para cada instância de 𝑋𝑀𝑎𝑗, calculamos a similaridade de cosseno
da instância para os seus K vizinhos mais próximos e colocamos
em uma lista ordenada 𝐷.
Por questões de otimização de tempo e de uso de memória, uti-
lizamos uma versão aproximada do KNN [33]. Após isso, passamos
a considerar cada instância de 𝑋𝑀𝑎𝑗como um cluster individual e
realizamos 𝑁iterações, onde 𝑁= 𝑋𝑀𝑎𝑗−𝑋𝑀𝑖𝑛. Em cada iteração,
buscamos em 𝐷o par de instâncias 𝐴e 𝐵pertencentes a 𝑋𝑀𝑎𝑗que
apresenta a maior similaridade e que estão em clusters distintos e
cujos clusters não sejam maiores que 𝛼(hiperparâmetro do método
que controla a quantidade de vizinhos a ser avaliada), com o obje-
tivo de unir os clusters aos quais 𝐴e 𝐵pertencem. Ao final, temos
|𝑋𝑀𝑖𝑛| clusters dentro do conjunto 𝑋𝑀𝑎𝑗, dos quais será selecionado
aleatoriamente um representante para compor, juntamente com o
conjunto 𝑋𝑀𝑖𝑛, o novo conjunto de treinamento.2
dataset
# docs
# majoritária
# minoritária
RD
Nome
sentistrength_twitter_2L
2,289
1,340
1.41
A
vader_amazon_2L
3,610
2,128
1,482
1.44
B
english_dailabor_2L
1,227
1.51
debate_2L
1,979
1,249
1.71
sentistrength_youtube_2L
2,432
1,665
2.17
E
sentistrength_rw_2L
2.19
F
vader_twitter_2L
4,196
2,897
1,299
2.23
G
tweet_semevaltest_2L
3,060
2,223
2.66
H
sentistrength_digg_2L
2.72
sentistrength_myspace_2L
5.32
J
sentistrength_bbc_2L
6.60
K
digital_music_2L
162,989
158,985
4,004
39.71
Tabela 1: Coleções de dados utilizadas nos experimentos. A
coluna “nome” contém como a base vai ser referenciada.
2Nossa técnica é limitada a problemas de classificação binários. Deixamos para o futuro
a extensão da abordagem para problemas multi-rótulo.
CONFIGURAÇÃO EXPERIMENTAL
4.1
Base de dados
Consideramos 12 datasets, com diferentes níveis de desbalancea-
mento. A Tabela 1 mostra os datasets com o número de documentos,
número de documentos pertencentes à classe majoritária e à mi-
noritária, o nome pelo qual vamos nos referir a base de dados ao
longo do trabalho e a razão de desbalanceamento (RD)[31], métrica
que demonstra o quão desbalanceado é uma base de dados (quanto
maior for a RD mais desbalanceado é a base de dados). O RD é
calculado como sendo 𝑅𝐷= 𝑐𝑙𝑎𝑠𝑠𝑒𝑚𝑎𝑗𝑜𝑟𝑖𝑡𝑎𝑟𝑖𝑎
𝑐𝑙𝑎𝑠𝑠𝑒𝑚𝑖𝑛𝑜𝑟𝑖𝑡𝑎𝑟𝑖𝑎.
4.2
Método de Classificação de Texto
Consideramos métodos de CAT baseados em Transformers BART
[23], RoBERTa [25] e BERT[11], que atualmente se apresentam
como os melhores entre os métodos de classificação utilizados na
literatura para tarefa de Análise de Sentimento [8]. Para ajustar os
hiperparâmetros, utilizamos a mesma metodologia discutida em [8].
Assim, fixamos a taxa de aprendizado inicial como 5 × 10−5, o
número máximo de épocas como 20 e a paciência como 5 épocas. Por
fim, realizamos um grid search em max_len (150 e 256) e batch_size
(16, 32 e 64), pois esses valores especificados impactam diretamente
na eficiência e efetividade do modelo. Utilizamos, também, 6 classifi-
cadores tradicionais: KNN, Random Forest [3], Regressão Logística
(RL) [40], SVM [2], XGBoost (XGB) [4] e LightGBM (LGBM) [19].
4.3
Métricas e Protocolo Experimental
Nossa avaliação é feita sob três perspectivas: (1) efetividade da
classificação; (2) capacidade de generalização dos modelos (viés);
e (3) eficiência (tempo e emissão de 𝐶𝑂2). A efetividade é avaliada
utilizando a Macro Average F1 (MacroF1). A capacidade de general-
ização é medida pela métrica TPRGap apresentada em [9], definida
na Equação 1, onde 𝑇𝑃𝑅(𝑖) é true positive rate da classe 𝑖, 𝑇é o
número total de classes, 𝑁é o fator de normalização, que é igual
ao número de pares de classes que comparamos  𝑇
.
𝑇𝑃𝑅𝐺𝑎𝑝=
∑︁
𝑖,𝑗𝜖𝑇
|𝑇𝑃𝑅(𝑖) −𝑇𝑃𝑅(𝑗) |
𝑁
(1)
A eficiência é medida com base no custo de cada método em termos
do tempo total necessário para construir o modelo e realizar as clasi-
ficações. O Speedup é calculado como 𝑆= 𝑇𝑤𝑜
𝑇𝑤, onde 𝑇𝑤é o tempo
total gasto na construção do modelo, mais o tempo da classificação,
usando alguma abordagem de undersampling, e𝑇𝑤𝑜é o tempo total
gasto na execução (modelo e classificação) sem a fase de undersam-
pling. A emissão de𝐶𝑂2 é o equivalente de dióxido de carbono gasto
para treinamento de um modelo e classificação baseado em [21].
Os experimentos foram realizados na AWS. Para as bases de
dados de A até K, as etapas undersampling, que demandam estri-
tamente processamento em CPU, utilizaram uma instância do tipo
c6a.4xlarge e a classificação, que demanda hardware especializado
(GPU), instâncias do tipo g4dn.xlarge. Para a base L, que é bem
maior que as demais, demandou um poder computacional maior e
ambas as etapas foram executadas em instâncias do tipo g5.4xlarge.
As bases de dados foram divididas utilizando o método de validação
cruzada com 5 partições (base L) ou 10 partições (demais bases).
As comparações foram realizadas utilizando o método estatístico
Teste-T com correção de Bonferroni [8].
Estratégias de Undersampling para Redução de Viés em Classificação de Texto Baseada em Transformers
WebMedia’2024, Juiz de Fora, Brazil
RoBERTa
BART
BERT
SVM
LR
RF
XGB
LGBM
KNN
dataset
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
Macro F1
TPRGap
A
88.6(0.7)
0.063
89.3(1.1)
0.048
84.3(1.7)
0.050
71.8(2.5)
0.187
71.4(2.5)
0.228
67.0(2.4)
0.370
64.9(2.4)
0.417
63.2(3.2)
0.375
64.3(3.1)
0.478
B
89.0(0.7)
0.065
88.3(1.4)
0.079
86.9(0.8)
0.057
72.1(1.5)
0.257
72.9(1.5)
0.209
68.6(1.8)
0.422
67.6(1.4)
0.247
69.4(2.7)
0.250
65.9(2.8)
0.498
93.3(1.1)
0.041
93.7(1.2)
0.036
89.1(2.2)
0.063
79.3(3.1)
0.111
80.4(2.1)
0.117
75.7(2.8)
0.221
75.0(1.5)
0.171
68.9(5.9)
0.208
76.9(2.1)
0.166
89.3(1.2)
0.076
89.1(1.1)
0.085
85.5(2.0)
0.146
76.4(2.1)
0.213
75.6(3.6)
0.223
73.3(3.0)
0.271
71.5(1.8)
0.302
72.8(3.4)
0.296
72.7(3.2)
0.330
E
89.7(1.9)
0.096
88.9(1.7)
0.117
86.1(1.8)
0.134
79.0(1.7)
0.215
78.6(2.0)
0.245
72.6(4.0)
0.257
71.1(1.7)
0.472
71.7(3.7)
0.385
73.3(3.4)
0.263
F
87.3(3.4)
0.126
88.0(3.3)
0.128
80.9(2.5)
0.202
69.1(3.4)
0.421
68.2(2.8)
0.471
59.3(4.1)
0.698
63.1(5.0)
0.582
65.8(4.2)
0.454
58.8(2.2)
0.698
G
94.2(1.0)
0.160
93.7(1.0)
0.053
88.0(1.3)
0.108
82.0(1.0)
0.238
80.4(1.4)
0.320
72.6(1.9)
0.526
74.0(1.4)
0.476
74.0(2.8)
0.372
75.4(1.3)
0.454
H
90.1(1.5)
0.102
90.1(1.6)
0.099
86.4(1.9)
0.125
70.9(2.0)
0.404
71.9(1.7)
0.399
64.5(2.0)
0.615
64.0(1.9)
0.660
63.9(4.4)
0.633
60.8(1.5)
0.654
83.8(5.0)
0.190
81.6(5.6)
0.198
79.1(3.6)
0.290
67.0(5.6)
0.539
63.0(6.8)
0.619
54.9(5.1)
0.731
56.7(5.2)
0.665
56.4(5.3)
0.598
55.2(6.4)
0.800
J
83.2(3.4)
0.333
83.0(5.8)
0.283
79.8(4.9)
0.350
68.1(3.8)
0.610
63.1(6.4)
0.733
59.8(4.6)
0.809
59.2(3.5)
0.803
58.4(10.5)
0.763
56.5(4.4)
0.875
K
81.0(4.5)
0.350
78.0(6.1)
0.410
76.4(4.4)
0.447
50.5(5.0)
0.944
53.3(4.7)
0.914
54.3(5.7)
0.888
53.4(5.3)
0.898
55.7(11.2)
0.833
46.4(0.1)
0.998
87.8(2.5)
0.308
88.6(1.2)
0.296
85.3(0.7)
0.383
78.7(0.3)
0.601
78.2(0.7)
0.562
78.9(0.4)
0.561
72.0(0.3)
0.698
73.6(0.7)
0.668
60.9(0.5)
0.866
Média:
0.159
0.153
0.196
0.395
0.420
0.531
0.533
0.486
0.590
Tabela 2: Resultados de Macro-F1 e TPRGap dos classificadores. Células em negrito são os maiores valores numéricos para uma
base de dados e células em verde são estatisticamente equivalentes à classificação de maior valor numérico.
ANÁLISE DOS RESULTADOS
5.1
PP1: Comparação entre métodos de CAT
Para responder a PP1 (Como algoritmos estado da arte recentes,
baseados em Transformers, são afetados pelo desbalanceamento de
classes em tarefas de análise de sentimentos? Há espaço para melho-
rias?), comparamos os classificadores tradicionais KNN, RF, RL,
SVM, XGB e LGBM com os baseados em Transformers RoBERTa,
BART e BERT.
A Tabela 2 apresenta os resultados de MacroF1 e TPRGap para
estes classificadores. Como primeira análise, podemos destacar a
superioridade, em termos de efetividade, dos classificadores basea-
dos em Transformers quando comparados aos classificadores tradi-
cionais – estes foram inferiores (estatisticamente) aos Transformers
em todas as 12 bases de dados consideradas, resultado condizente
com a literatura [7]. Já dentre os classificadores baseados em Trans-
formers, o RoBERTa e o BART se destacam, ambos com resultados de
classificação estatisticamente equivalentes em todas bases. O BERT,
por sua vez, é estatisticamente equivalente ao RoBERTa e ao BART
em apenas 4 das 12 bases de dados. Por fim, quando analisamos
os valores numéricos absolutos de cada classificador, o modelo
RoBERTa produz os maiores valores de MacroF1 em 8 coleções
enquanto o BART o faz em 4 delas, reforçando a ideia da literatura
[8] de que o RoBERTa produz resultados condizentes com o estado
da arte atual de análise de sentimentos. Por isso, nas análises das
próximas seções, passaremos a utilizar apenas o RoBERTa.
Focando agora no viés (TPRGap médio) das abordagens (quanto
menor TPRGap, menor viés), observamos que os Transformers ap-
resentam menor viés quando comparados aos métodos de CAT
tradicionais. Para os classificadores tradicionais, o método que
obteve os melhores valores foi o SVM com um TPRGap médio
de 0.395, o que é mais que o dobro do TPRGap médio dos métodos
baseados em Transformers que conseguiram 0.159 (RoBERTa), 0.153
(BART) e 0.196 (BERT). Esse resultado é muito interessante e até
onde sabemos não foi reportado na literatura - a boa capacidade
dos Transformers de lidar com desbalanceamento de dados.
Apesar disso, os Transformers ainda apresentam resultados de
TPRGap alto em bases de dados que têm um desbalanceamento
elevado, como é o caso das bases J, K e L, com RD igual a 5.32, 6.60
e 39.71, respectivamente. Isso nos mostra que ainda há espaço para
melhorias, isto é, espaço para usar técnicas capazes de reduzir o
viés dos modelos Transformers.
Portanto, os classificadores baseados em Transformers conseguem
gerar modelos que, além da efetividade estado da arte, apresentam
um viés consideravelmente menor quando comparados aos classi-
ficadores tradicionais. Além disso, observamos que, mesmo para
os Transformers, ainda há espaço considerável de melhoria. Esse
espaço é explorado a seguir.
5.2
PP2: Efetividade e Viés de CAT com
undersampling
Na Tabela 3, apresentamos os resultados de efetividade obtidos por
meio da aplicação dos métodos de US juntamente com o classifi-
cador RoBERTa. A coluna “NoUnder” apresenta o resultado sem o
undersampling do conjunto de treinamento. Um ponto importante
é que, para todos os métodos que permitem hiperparametrização
no que tange a quantidade de instâncias a serem removidas (UBR,
E2SC, E2SC_RL, NM1, NM2, IHT e CC_NN), limitamos a remoção
de no máximo 50% da base de dados, pois trabalhos recentes rela-
cionados à seleção de instâncias [6] apontam que esse é o limite
empírico de redução onde ainda é possível não ocorrer perdas na
efetividade. Os outros métodos não foram modificados, seguindo
a política de remoção própria.
Podemos observar que os métodos UBR, NM1, E2SC_RL, NM2,
E2SC, TL e OSS conseguem empate estatístico com a classificação
sem undersampling (i.e., com o treino completo desbalanceado) em
todas as coleções analisadas. Isso demonstra que todas as técnicas
listadas acima tem a capacidade de balancear a base sem causar
perdas de efetividade. Os demais métodos não obtiveram bons re-
sultados em relação aos anteriores, perdendo em 4 (IHT), 3 (OBU,
SBC, RENN e ALLKNN), 2 (NM3) ou em 1 (NCR, ENN) base(s),
respectivamente. Os métodos CNN e CC_NN são estatisticamente
equivalentes ao US em 11 de 12 bases de dados, porém, no maior
conjunto de dados (L), ambos tiveram um tempo de undersampling
que ultrapassou o tempo de treinamento do modelo de classificação
sem o undersampling, sendo, portanto, desconsiderados para essa
análise devido a sua impraticabilidade.
Na Tabela 4 apresentamos os resultados para a métrica TPRGap,
a qual mede o viés dos modelos. A coluna “NoUnder” apresenta o
resultado sem o undersampling, enquanto que as demais apresen-
tam o TPRGap dos modelos com undersampling. As cores do fundo
das células representam o quanto os modelos conseguiram reduzir
o viés do modelo comparados ao “NoUnder”. Ou seja, quanto maior
o tom de verde, maior a redução do viés, e quanto mais vermelho,
WebMedia’2024, Juiz de Fora, Brazil
Guilherme Fonseca, Gabriel Prenassi, Washington Cunha, Marcos André Gonçalves, & Leonardo Rocha
dataset
NoUnder
UBR
NM1
E2SC_RL
NM2
E2SC
NM3
IHT
OBU
SBC
NCR
TL
OSS
RENN
ALLKNN
ENN
CNN
CC_NN
A
88.6(0.7)
88.6(0.8)
88.9(0.8)
88.8(1.1)
89.0(0.8)
88.6(1.2)
88.5(1.0)
87.7(1.2)
85.1(1.4)
87.6(1.5)
87.2(2.0)
89.2(1.2)
88.3(0.6)
85.6(0.9)
87.2(2.1)
85.6(0.9)
88.2(1.5)
88.7(1.4)
B
89.0(0.7)
88.7(1.1)
88.3(1.2)
89.1(1.1)
88.2(0.7)
88.6(1.4)
51.3(13.5)
87.8(1.5)
85.6(1.4)
87.9(1.2)
89.4(1.4)
88.7(0.9)
88.7(0.9)
83.1(8.9)
70.2(5.1)
83.4(9.0)
88.1(2.0)
90.0(1.2)
93.3(1.1)
94.3(1.4)
94.2(1.5)
93.4(1.3)
94.1(1.1)
93.8(1.6)
93.9(1.4)
92.0(1.2)
92.3(1.6)
89.3(3.3)
92.4(1.4)
93.4(1.7)
94.3(1.5)
92.5(1.7)
91.7(2.0)
92.5(1.7)
93.7(1.1)
94.5(1.4)
89.3(1.2)
87.6(1.5)
88.0(1.5)
88.7(1.7)
86.3(1.5)
88.1(1.7)
87.7(2.0)
84.3(3.2)
80.7(1.6)
87.7(1.2)
86.7(1.5)
88.4(1.5)
89.1(1.4)
83.4(2.1)
83.6(3.4)
83.4(2.1)
88.2(1.0)
81.7(13.8)
E
89.7(1.9)
87.9(1.6)
88.4(1.8)
89.4(1.7)
89.3(1.9)
89.1(1.9)
89.0(1.7)
82.3(1.4)
88.2(2.3)
79.2(4.2)
68.7(7.2)
89.9(1.6)
89.9(1.6)
55.2(3.5)
58.6(3.5)
55.2(3.5)
89.6(1.5)
89.3(1.6)
F
87.3(3.4)
82.3(3.5)
83.1(4.4)
85.4(3.5)
86.7(3.7)
88.6(3.6)
86.3(2.9)
80.6(2.2)
77.1(5.1)
86.9(3.0)
87.4(2.8)
88.4(4.0)
88.2(3.8)
81.6(3.1)
84.6(5.2)
87.7(3.6)
86.7(3.2)
84.1(3.2)
G
94.2(1.0)
92.7(0.9)
92.6(1.1)
93.1(1.2)
92.5(1.2)
92.6(1.1)
92.9(1.3)
87.9(1.1)
86.7(2.4)
92.0(0.9)
93.2(1.0)
93.4(1.4)
93.8(1.1)
91.5(1.0)
93.1(0.9)
92.9(0.8)
93.0(1.0)
93.0(1.0)
H
90.1(1.5)
88.6(1.6)
89.7(2.1)
88.5(1.5)
89.2(1.8)
89.3(1.8)
88.9(1.6)
83.0(2.0)
88.3(1.7)
88.1(0.9)
90.1(1.5)
90.3(1.1)
90.6(1.6)
86.8(1.9)
88.7(1.8)
89.2(1.3)
89.8(1.8)
88.8(1.5)
83.8(5.0)
82.9(4.5)
80.5(4.3)
80.6(4.7)
81.3(4.1)
81.3(4.7)
81.0(3.9)
74.1(4.5)
77.6(3.3)
81.0(4.6)
84.0(4.9)
87.2(4.7)
85.4(3.7)
75.8(4.4)
77.3(5.5)
81.2(5.3)
83.0(5.0)
82.4(4.6)
J
83.2(3.4)
80.1(4.2)
81.0(4.7)
81.5(5.2)
80.8(5.5)
82.9(5.0)
79.4(5.3)
74.5(3.7)
81.8(6.0)
60.2(2.9)
84.5(4.6)
83.2(4.8)
84.5(4.6)
80.7(3.9)
82.4(4.1)
84.1(3.3)
81.5(3.6)
79.9(9.5)
K
81.0(4.5)
79.3(3.1)
78.0(4.1)
78.7(4.2)
75.0(5.0)
79.2(4.7)
74.0(3.2)
73.3(4.7)
71.7(4.4)
71.0(4.5)
79.8(5.1)
78.7(4.6)
77.2(5.0)
79.4(5.0)
77.4(6.1)
77.8(5.9)
77.8(4.3)
76.7(3.7)
87.8(2.5)
81.9(4.8)
87.1(1.3)
88.7(0.3)
85.1(3.5)
88.7(0.3)
51.0(1.7)
60.3(1.5)
77.3(9.1)
30.9(1.7)
80.1(21.4)
80.0(21.4)
72.7(26.5)
69.2(3.5)
67.9(2.6)
79.8(21.2)
-
-
Tabela 3: Macro-F1 do RoBERTa utilizando as abordagens de undersampling. Células em negrito são os maiores valores numéricos
para uma base de dados. Células em verde representam resultados que são estatisticamente equivalentes à classificação sem
undersampling (NoUnder). Células com “-” representam métodos que resultaram em um tempo superior ao tempo de classificação
da mesma base sem o undersampling e, portanto, desconsiderado.
dataset
NoUnder
UBR
NM1
E2SC_RL
NM2
E2SC
NM3
IHT
OBU
SBC
NCR
TL
OSS
RENN
ALLKNN
ENN
CNN
CC_NN
A
0.063
0.010
0.004
0.002
0.005
0.003
0.011
0.072
0.114
0.039
0.077
0.035
0.036
0.141
0.098
0.141
0.047
0.011
B
0.065
0.043
0.026
0.034
0.013
0.033
0.692
0.075
0.100
0.012
0.048
0.060
0.060
0.188
0.445
0.166
0.027
0.023
0.041
0.002
0.003
0.000
0.018
0.007
0.022
0.059
0.019
0.149
0.038
0.029
0.025
0.061
0.062
0.061
0.002
0.011
0.076
0.005
0.043
0.019
0.019
0.006
0.020
0.107
0.097
0.013
0.032
0.052
0.042
0.143
0.108
0.143
0.006
0.070
Média
0.061
0.015
0.019
0.014
0.014
0.012
0.186
0.078
0.083
0.053
0.049
0.044
0.041
0.133
0.178
0.128
0.021
0.029
E
0.096
0.037
0.012
0.001
0.010
0.031
0.026
0.197
0.023
0.245
0.403
0.093
0.090
0.634
0.593
0.634
0.032
0.024
F
0.126
0.006
0.047
0.029
0.054
0.025
0.036
0.132
0.144
0.024
0.043
0.108
0.134
0.111
0.034
0.010
0.000
0.011
G
0.160
0.002
0.003
0.006
0.001
0.001
0.012
0.122
0.094
0.017
0.018
0.063
0.062
0.037
0.007
0.004
0.012
0.012
H
0.102
0.030
0.000
0.006
0.021
0.020
0.001
0.158
0.007
0.035
0.028
0.085
0.077
0.072
0.021
0.007
0.016
0.024
0.190
0.037
0.027
0.040
0.036
0.006
0.063
0.212
0.066
0.029
0.018
0.118
0.131
0.158
0.136
0.046
0.006
0.037
Média
0.135
0.023
0.018
0.017
0.025
0.017
0.027
0.164
0.067
0.070
0.102
0.093
0.099
0.202
0.158
0.140
0.013
0.022
J
0.333
0.148
0.176
0.222
0.193
0.221
0.111
0.085
0.264
0.352
0.256
0.336
0.304
0.216
0.248
0.262
0.112
0.247
K
0.350
0.182
0.209
0.215
0.210
0.226
0.064
0.059
0.173
0.047
0.237
0.361
0.400
0.287
0.324
0.324
0.193
0.329
Média
0.342
0.165
0.192
0.218
0.202
0.224
0.087
0.072
0.219
0.199
0.247
0.349
0.352
0.251
0.286
0.293
0.153
0.288
0.308
0.182
0.207
0.214
0.208
0.216
0.198
0.067
0.245
0.619
0.434
0.443
0.579
0.045
0.042
0.403
-
-
Média Total
0.159
0.057
0.063
0.066
0.066
0.066
0.105
0.112
0.112
0.132
0.136
0.149
0.162
0.174
0.177
0.183
-
-
Tabela 4: TPRGap dos modelos gerados pelo classificador RoBERTa em conjunto com as abordagens de undersampling utilizando
o classificador RoBERTa. Quanto mais verde a célula, maior a redução do viés. Quanto mais vermelho, maior o aumento do viés.
maior o agravamento do viés. Para auxiliar essa análise, dividimos
nossas bases em 4 grupos com relação a seu grau de desbalancea-
mento (RD). No primeiro grupo são as bases de dados que têm um
RD até 2 (bases A, B, C, D), o segundo contém as bases com RD
maiores que 2 e menores que 5 (bases E, F, G, H e I), o terceiro aque-
las com RD maior que 5 e menor que 10 (bases J e K). Por fim, no
último grupo, temos apenas a base de dados L, com um RD de 39.71.
Em relação ao viés médio total calculado, os métodos que con-
seguiram a maior redução de viés dos modelos em todas as 12
coleções foram UBR (viés total médio de 0.057), NM1 (0.063), E2SC_RL
(0.066), NM2 (0.066) e E2SC (0.066), com uma redução de quase 3
vezes comparada ao NoUnder (0.159).
Os métodos RENN, ALLKNN, ENN, que já estão entre os piores
em termos de efetividade, também desempenham mal em relação
ao enviesamento do modelo, aumentando o viés médio. Os méto-
dos TL e OSS, apesar de apresentarem bons resultados em termos
de efetividade, demonstram um baixo desempenho em relação ao
enviesamento, piorando o modelo em alguns casos (em 3 datasets
cada) e tendo um TPRGap médio total próximo ao NoUnder – 0.149
(TL) e 0.162 (OSS).
Por fim, observamos também que o método UBR – que obteve os
melhores resultados quanto a média total de TRPGap – se mostra
bastante eficaz individualmente por base, principalmente naquelas
mais desbalanceadas. Por exemplo, nas bases do grupo 3 (bases K
e J) e 4 (L), o UBR reduziu o viés do modelo NoUnder em cerca de
duas vezes, estando sempre muito próximo do menor viés geral
alcançado por qualquer método para essas bases. Observação sim-
ilar é válida para os outros dois grupos: UBR sempre aparece entre
os melhores resultados. O E2SC também é bastante competitivo,
apresentando os melhores resultados médios para os grupos 1 e 2
e estando, também, entre os melhores nos demais grupos.
Sumarizando, conseguimos, com as análises reportadas acima,
responder positivamente à PP2 (Métodos de undersampling, apli-
cados juntamente com classificadores baseados em Transformers, são
capazes de reduzir o viés dos modelos de classificação? Qual o impacto
dessa combinação na efetividade do modelo?), pois os métodos UBR,
NM1, E2SC_RL, NM2 e E2SC são capazes de significativamente re-
duzir o viés do modelo, sem perda de efetividade em todas as bases.
5.3
PP3: Eficiência de CAT com undersampling
Analisamos aqui os métodos de US em relação à sua eficiência,
buscando verificar qual o impacto dessa nova etapa de pré-proces-
samento dos dados no tempo total e na emissão total de 𝐶𝑂2 prove-
niente do treinamento dos modelos. A Tabela 5 apresenta o speedup
produzido pelos métodos de US. Conforme mencionamos na Seção
4.3, o speedup é calculado pela razão entre o tempo total gasto
na construção do modelo, mais o tempo da classificação, usando
alguma abordagem de undersampling pelo tempo total gasto na
execução (modelo e classificação) sem a fase de undersampling.
Estratégias de Undersampling para Redução de Viés em Classificação de Texto Baseada em Transformers
WebMedia’2024, Juiz de Fora, Brazil
dataset
UBR
NM1
E2SC_RL
NM2
E2SC
NM3
IHT
OBU
SBC
NCR
TL
OSS
RENN
ALLKNN
ENN
CNN
CC_NN
A
1.209
1.076
1.094
1.135
1.178
1.096
1.181
1.374
1.216
1.210
0.929
0.961
1.376
1.230
1.415
1.243
1.193
B
1.203
1.163
1.273
1.114
1.099
1.588
1.230
1.342
1.364
0.988
0.962
0.990
1.513
1.452
1.476
1.005
1.186
1.315
1.237
1.400
1.096
1.253
1.247
1.115
1.214
1.638
1.223
0.873
0.885
1.039
1.120
1.070
1.361
1.202
1.558
1.469
1.712
1.450
1.456
1.493
1.402
1.282
1.637
1.516
1.185
1.150
1.854
1.447
1.966
1.519
1.500
E
1.450
1.312
1.569
1.312
1.361
1.400
1.182
1.381
1.769
1.607
0.937
0.948
1.611
1.641
1.663
1.176
1.467
F
1.534
1.299
1.371
1.450
1.666
1.544
1.627
1.491
1.507
1.413
1.068
1.065
1.416
1.632
1.504
1.601
1.529
G
1.527
1.530
1.518
1.374
1.354
1.414
1.298
1.329
1.444
1.122
0.946
0.995
1.220
1.159
1.270
1.310
1.521
H
1.749
1.659
1.950
1.619
1.788
1.757
1.668
1.607
1.728
1.319
1.055
1.067
1.771
1.587
1.395
1.494
1.867
1.657
1.923
1.635
1.601
1.609
1.519
1.634
1.331
1.564
1.401
1.034
1.014
1.516
1.751
1.537
1.510
1.612
J
1.487
1.730
1.617
1.523
1.876
2.128
1.535
1.302
2.861
0.877
1.003
0.800
0.998
0.977
0.990
2.283
1.608
K
1.695
1.691
1.802
1.621
1.741
3.054
1.602
1.411
3.433
1.487
1.107
0.972
1.220
1.341
1.335
2.377
1.442
2.662
2.709
2.903
2.867
3.039
39.486
2.103
1.777
12.498
1.038
0.892
1.318
0.946
1.713
1.230
-
-
Média
1.587
1.566
1.654
1.513
1.618
4.811
1.465
1.404
2.722
1.267
0.999
1.014
1.373
1.421
1.404
-
-
Tabela 5: Resultados de Speedup no custo total (tempo) para geração dos modelos utilizando o classificador RoBERTa em
conjunto com as abordagens de undersampling. Quanto mais verde a célula, maior a redução no tempo total de treinamento
em relação a abordagem sem undersampling. Quanto mais vermelho, maior o tempo.
dataset
NoUnder
UBR
NM1
E2SC_RL
NM2
E2SC
NM3
IHT
OBU
SBC
NCR
TL
OSS
RENN
ALLKNN
ENN
CNN
CC_NN
A
55.537
45.884
51.580
50.740
48.897
47.114
50.650
46.966
40.362
43.922
45.838
59.770
57.725
40.322
45.119
39.218
40.892
44.769
B
83.539
69.377
71.793
65.550
74.933
75.923
52.539
67.865
62.154
59.487
84.470
86.764
84.280
55.137
57.481
56.526
66.378
66.234
34.265
26.042
27.693
24.463
31.241
27.324
27.460
30.724
28.192
20.567
27.991
39.227
38.706
32.962
30.580
32.013
24.429
27.903
93.391
59.893
63.529
54.531
64.380
64.097
62.535
66.566
72.778
56.550
61.574
78.756
81.160
50.329
64.525
47.462
57.948
61.228
E
55.470
38.202
42.243
35.322
42.254
40.692
39.576
46.877
40.118
30.730
34.474
59.146
58.448
34.394
33.765
33.315
41.039
35.803
F
36.458
23.745
28.052
26.563
25.116
21.848
23.591
22.375
24.414
23.945
25.780
34.099
34.192
25.725
22.307
24.220
22.403
23.493
G
171.805
112.413
112.145
113.060
124.901
126.778
121.423
132.204
129.121
116.158
152.980
181.434
172.539
140.629
148.092
135.182
117.260
108.297
H
78.329
44.724
47.164
40.116
48.316
43.737
44.516
46.895
48.635
41.999
59.281
74.156
73.297
44.120
49.254
56.096
43.587
39.587
22.538
13.581
11.704
13.769
14.060
13.986
14.816
13.768
16.906
14.135
16.061
21.784
22.209
14.847
12.854
14.647
14.468
13.676
J
21.994
14.773
12.700
13.592
14.429
11.708
10.321
14.307
16.869
7.486
25.071
21.912
27.473
22.023
22.498
22.193
9.291
13.296
K
23.693
13.934
13.980
13.114
14.579
13.568
7.724
14.747
16.740
6.504
15.897
21.369
24.348
19.383
17.626
17.710
9.510
15.948
2,552.335
942.877
938.264
876.957
879.055
834.978
60.623
1,209.117
1,414.486
117.465
2,324.505
2,760.191
1,831.865
1,210.973
1,245.247
1,943.847
-
-
Tabela 6: Emissão de Carbono (𝐶𝑂2) para geração dos modelos utilizando o classificador RoBERTa em conjunto com as
abordagens de undersampling. Quanto mais verde a célula, maior a redução da emissão em relação a abordagem sem undersampling.
Podemos observar que, com exceção do TL, todos os métodos,
em média, conseguiram manter ou reduzir o tempo em comparação
com o RoBERTa aplicado aos dados originais (sem undersampling).
Os métodos UBR, NM1, E2SC_RL, NM2 e E2SC, que nas análises
anteriores se mostraram superiores quantos aos critérios de efe-
tividade e redução do enviesamento, conseguem também um bom
desempenho no critério de eficiência. Os speedups alcançados, re-
spectivamente, de 1.587, 1.566, 1.654, 1.513 e 1.618 são muito bons.
Juntamente com o NM3 (4.811) e SBC (2.722), esses são os métodos
com maior ganho de speedup. Contudo, vale lembrar que o NM3 e o
SBC geram perdas de efetividade para algumas coleções (Tabela 3)
ao produzir os respectivos ganhos de speedup.
Por fim, na Tabela 6, apresentamos os valores de emissão de 𝐶𝑂2
(em g) produzido pelos métodos de undersampling. Assim como
causaram perda de eficiência, os métodos TL e OSS também pro-
duzem um aumento de emissão de 𝐶𝑂2 para algumas bases. Já
todos os demais métodos, em menor ou maior grau, geram alguma
redução de emissão. Para as bases de dados de A a K, a emissão de
carbono é muito pequena quando comparada a base L, que é ordens
de magnitude maior que as demais. Por essa mesma razão, o tempo
de processamento de L é muito maior, mesmo ela sendo executada
em uma máquina de maior poder computacional. Por isso, nossa
análise nesse critério focará nessa base. Ao analisar a emissão dos
métodos de undersampling para a base de dados L, observamos que
os métodos UBR (emissão 942.877 g 𝐶𝑂2), NM1 (938.264 g 𝐶𝑂2),
E2SC_RL (876.957 g 𝐶𝑂2), NM2 (879.055 g 𝐶𝑂2) e E2SC (834.978
g 𝐶𝑂2), os mesmos métodos destacados nas análises anteriores,
também conseguem reduzir mais que pela metade a emissão em
comparação com o NoUnder (2,552.335 g 𝐶𝑂2). Traduzindo esses
números para exemplos ilustrativos, podemos dizer que a emissão
antes era equivalente a 2.73 meses de uma árvore sequestrando car-
bono ou a emissão de 14.40 km percorridos por um carro [21]. Já a
emissão após o undersampling, considerando o UBR como exemplo,
seria equivalente a 1.03 meses de sequestro de carbono feito por
uma árvore ou a emissão de de 5.41 Km percorridos por um carro
de passageiros. Se individualmente essa parece ser uma redução pe-
quena, se considerarmos milhões de máquinas ao redor do mundo
rodando processos similares todos os dias, dezenas ou centenas de
vezes, essa redução pode passar a ser considerável.
Voltando à nossa PP3 (Qual o impacto da aplicação dessa etapa
adicional de pré-processamento (undersampling) em termos de efi-
ciência? E em termos da emissão de carbono?), temos que os métodos
UBR, NM1, E2SC_RL, NM2 e E2SC conseguem reduzir o enviesa-
mento do modelo, mantendo a efetividade e reduzindo o tempo de
treinamento dos modelos e, consequentemente, as emissões de 𝐶𝑂2.
5.4
Discussão Final
Dadas as análises apresentadas nesta seção, onde os métodos de
undersampling foram avaliados quanto à sua efetividade, sua capaci-
dade reduzir o enviesamento dos modelos e sua eficiência, podemos
concluir que os melhores métodos de undersampling analisados
foram UBR, NM1, E2SC_RL, NM2 e E2SC. Todos eles conseguiram
reduzir o enviesamento do modelo para a classe majoritária sem
WebMedia’2024, Juiz de Fora, Brazil
Guilherme Fonseca, Gabriel Prenassi, Washington Cunha, Marcos André Gonçalves, & Leonardo Rocha
produzir nenhum tipo de perda de efetividade global (em termos
de MacroF1). Esses métodos também apresentaram uma redução
no tempo de treinamento do modelo com uma consequente re-
dução na emissão de 𝐶𝑂2. Das estratégias que se destacaram, duas
foram originalmente propostas neste trabalho - UBR e E2SC_RL.
Em particular a UBR foi aquela que apresentou os resultados mais
consistentes de redução de enviesamento sem perda de efetividade,
com um bom speedup e redução de emissão de 𝐶𝑂2, sendo nossa
recomendação final para o problema, se tivermos de fazer uma.
CONCLUSÕES E TRABALHOS FUTUROS
O impacto do desbalanceamento de classes, relacionado ao viés de
um classificador para a classe majoritária, em estratégias do estado
da arte de CAT baseadas em Transformers, tem sido pouco discutido
na literatura [5]. Neste trabalho, apresentamos uma avaliação detal-
hada de métodos de undersampling (US) aplicados em conjunto com
algoritmos baseados em Transformers na tarefa de Análise de Sen-
timento. Primeiramente, realizamos uma análise comparativa entre
métodos de classificação baseados em Transformers e tradicionais
sob duas perspectivas: efetividade e enviesamento. Essa análise rev-
elou que, além de mais efetivos, como conhecidos, os Transformers
são capazes de lidar de forma mais adequada com o problema do viés
que os algoritmos tradicionais. Nossos resultados experimentais
também indicaram que ainda existe espaço para melhoria, princi-
palmente em bases de dados com um desbalanceamento maior.
Baseado em um mapeamento da literatura sobre undersampling,
selecionamos e implementamos os 14 métodos mais utilizados, além
de adaptarmos diretamente um método de seleção de instâncias
para a tarefa de undersampling devido a conexões entre as tarefas.
Propomos, também, duas novas estratégias de US: E2SC_RL e UBR,
totalizando em um conjunto de 17 métodos a serem comparados.
Uma avaliação experimental vasta, utilizando esses 17 métodos
e 12 bases de dados revelou que um conjunto de cinco métodos de
undersampling – UBR (nossa proposta), NM1, E2SC_RL (nossa pro-
posta), NM2, E2SC – foram capazes de reduzir o viés dos modelos
de CAT quando comparados com modelos sem undersampling sem
perdas de efetividade (qualidade da classificação). Além disso, esses
mesmos métodos produziram uma redução significativa no tempo
de treino e na emissão de 𝐶𝑂2 no treinamento dos modelos Trans-
formers. Entre esses 5 métodos, o UBR apresentou os resultados
mais consistentes considerando todos os critérios analisados.
Como trabalhos futuro, visamos estender o presente estudo con-
siderando, também, outros cenários de CAT, como multiclasses
e/ou hierárquico, além de avaliar outros algoritmos de CAT além
do RoBERTa, tais como BERT e BART. Ademais, considerando que
a eficácia dos LLMs recentes em comparação com modelos anteri-
ores baseados em Transformer, como o RoBERTa, para análise de
sentimentos e propósitos de CAT ainda não está clara [10], e que,
quando LLMs superam alguns Transformers de 1a e 2a geração, os
ganhos são tipicamente de apenas alguns pontos percentuais [10],
consideramos incerto se esses ganhos marginais se traduzem em
benefícios práticos em aplicações do mundo real. Desta forma, em
trabalhos futuros, planejamos realizar uma análise completa sobre
o custo-benefício dos LLMs em relação aos Transformers de 1a e 2a
geração, de modo a habilitar a aplicação dos métodos de undersam-
pling como etapas de pré-processamento de LLMs recentes.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Fapemig, FAPESP,
CIIA-Saúde e AWS.

--- FIM DO ARQUIVO: 30113.txt ---

--- INÍCIO DO ARQUIVO: 30117.txt ---
Impacto da Pandemia na Discussão sobre Saúde Mental:
O Caso do Discord no Brasil
Pedro Bento
DCC–UFMG
Belo Horizonte, MG, Brasil
pedro.bento@dcc.ufmg.br
Arthur Buzelin
DCC–UFMG
Belo Horizonte, MG, Brasil
arthurbuzelin@dcc.ufmg.br
Yan Aquino
DCC–UFMG
Belo Horizonte, MG, Brasil
yanaquino@dcc.ufmg.br
Isis Carvalho
DCC–UFMG
Belo Horizonte, MG, Brasil
isisferreira@dcc.ufmg.br
Pedro Dutenhefner
DCC–UFMG
Belo Horizonte, MG, Brasil
pedroroblesduten@ufmg.br
Lucas Dayrell
DCC–UFMG
Belo Horizonte, MG, Brasil
lucasdayrell@dcc.ufmg.br
Caio Santana
DCC–UFMG
Belo Horizonte, MG, Brasil
caiosantana@dcc.ufmg.br
Victoria Estanislau
DCC–UFMG
Belo Horizonte, MG, Brasil
victoria.estanislau@dcc.ufmg.br
Gisele L Pappa
DCC–UFMG
Belo Horizonte, MG, Brasil
glpappa@dcc.ufmg.br
Debora Miranda
FM–UFMG
Belo Horizonte, MG, Brasil
debora.m.miranda@gmail.com
Virgilio Almeida
DCC–UFMG
Belo Horizonte, MG, Brasil
virgilio@dcc.ufmg.br
Wagner Meira Jr
DCC–UFMG
Belo Horizonte, MG, Brasil
meira@dcc.ufmg.br
WebMedia’2024, Juiz de Fora, Brazil
Bento, Buzelin, Aquino et al.
um local onde os usuários se sentem à vontade para discutir
saúde mental em contextos variados.
Este estudo busca explorar essas questões e identificar pos-
síveis tendências ou correlações; para isso, as seguintes per-
guntas foram formuladas, a fim de nos ajudar a entender em
que dimensões o Discord evoluiu nos últimos anos.
Q1: Como a utilização do Discord no Brasil se alterou ao
longo do tempo? Qual a influência da pandemia?
Q2: Em que medida a discussão sobre o tópico saúde mental
mudou no Discord durante o período analisado?
Para responder às perguntas formuladas, coletamos men-
sagens textuais de uma amostra representativa de grupos pú-
blicos brasileiros no Discord, conforme definido pelos termos
de serviço da plataforma. Seguimos rigorosamente as consi-
derações éticas em todas as etapas, garantindo a exclusão de
qualquer informação privada e/ou que permita a identificação
de indivíduos. Utilizando a API oficial do Discord, consegui-
mos coletar dados desde a primeira mensagem registrada em
todos os grupos analisados até o início de 2024.
Junto com especialistas da área de psiquiatria, definimos
palavras-chave relacionadas à saúde mental com o objetivo
de identificar mensagens e indivíduos nesses contextos. Esses
termos foram agrupados em categorias que abordam diferen-
tes tópicos relacionados à saúde mental, para nos ajudar a
entender melhor a evolução dessa rede social. A análise reali-
zada inclui a verificação do comportamento dos usuários no
Discord, identificando o seu crescimento, a quantidade de men-
sagens e os períodos do dia com mais atividade. Além disso,
examinamos discussões sobre saúde mental e suas possíveis
implicações nos usuários.
Com essas análises, pretendemos compreender como o uso
do Discord no Brasil pode apontar perspectivas sobre a mu-
dança nos níveis de interação entre jovens online, afetadas
principalmente pela pandemia, e como essa população lida
com temas relacionados à saúde mental. Ao trazer luz a even-
tuais problemas relacionados ao uso da plataforma, esperamos
não apenas contribuir para a compreensão acadêmica dessa
rede, mas também mapear as discussões cujo tema tange a
saúde mental nela contidas, a fim de incentivar a criação de
políticas e estratégias de monitoramento mais eficazes.
TRABALHOS RELACIONADOS
Nesta seção, começamos revisando trabalhos que exploraram
plataformas de mensagens, especialmente o Discord. Em se-
guida, examinamos pesquisas anteriores sobre saúde mental e
o impacto da pandemia no uso de redes sociais.
2.1
Plataformas de Mensagens
Diversos estudos exploraram os mais variados usos do Dis-
cord. Pesquisas investigam sua aplicação no contexto edu-
cacional [17, 31], enquanto outros estudos focam no uso da
plataforma para tráfico de drogas [30]. Adicionalmente, uma
análise comparativa entre Discord, WhatsApp e Telegram des-
tacou a caracterização de grupos, a avaliação de postagens e
os aspectos de privacidade e segurança em cada rede [12]. Em
um contexto diferente, uma investigação conduzida por meio
de um dos fóruns que fornecem links para grupos públicos do
Discord [7] revelou a presença de uma variedade de grupos
extremistas [11]. Outra contribuição foi o desenvolvimento
de um conjunto de dados de mensagens juvenis classificadas
como odiosas, que pode ajudar a melhorar a identificação de
discursos de ódio [10], enquanto um último discutiu desafios
específicos da moderação em canais de voz [14].
2.2
Saúde mental nas redes sociais
As redes sociais são ferramentas de socialização muito im-
portantes na atualidade, exercendo grande influência em seus
usuários, principalmente os mais jovens. Elas constituem, in-
discutivelmente, veículos de auto-expressão e socialização de
grande alcance. De acordo com [1], diversos estudos observam
uma relação entre o uso de redes sociais e o aumento de sofri-
mento mental e de comportamento autodestrutivo e suicida
em jovens [16]. [1] Ainda reporta que as mídias sociais podem
afetar a autoimagem e os relacionamentos interpessoais dos
adolescentes através da comparação social e interações nega-
tivas, incluindo o cyberbullying; além disso, o conteúdo das
mídias sociais frequentemente envolve a normalização e até a
promoção de automutilação e suicídio entre os jovens [27].
Por outro lado, as redes sociais também abrem um espaço
de discussão sobre saúde mental[2, 29]. Essas plataformas pos-
sibilitam que pessoas com características em comum (como
por exemplo a luta contra algum tipo de sofrimento mental)
compartilhem experiências, se expressem de forma honesta -
sem ter que se censurar - e sejam acolhidos por outras pessoas
que passaram ou passam pelas mesmas situações [28]. Esse
sentimento de acolhimento pode não ser encontrado pela pes-
soa em suas interações presenciais, o que faz com que esse
espaço possa ter um efeito ainda pouco estudado e que seja
valioso por si só.
2.3
Impactos da pandemia online
Diversos estudos exploraram o impacto da pandemia na vida
das pessoas, e, dentre eles, muitos focaram nas mudanças na
interação das pessoas no mundo virtual. Um dos estudos [18]
revelou que o uso das mídias sociais pela Geração Z (nascidos
entre 1995 e 2010) exacerbou o medo da pandemia e gerou
fadiga por conta da sobrecarga constante de informações (de
diversas qualidades e veracidades).
Uma pesquisa com usuários das redes sociais revelou que
essa mesma faixa etária, durante o período da pandemia, pas-
sava quase o dia todo online, dando preferência a consumo
passivo de conteúdo do que a comunicação, e que o tempo
de tela dessa população aumentou significativamente nesse
período[13].
Outro trabalho importante nesse sentido foi [23], que es-
tudou os efeitos psico-sociais da pandemia na saúde mental
usando dados de mídias sociais de 2020, e concluiu que a ex-
pressão de sintomas relacionados à saúde mental cresceu muito
se comparado ao mesmo período no ano anterior. As pessoas
expressaram todo tipo de preocupações neste período, desde
Impacto da Pandemia na Discussão sobre Saúde Mental:
O Caso do Discord no Brasil
WebMedia’2024, Juiz de Fora, Brazil
sobre desafios pessoais e profissionais até sobre medidas de
precaução contra a pandemia [21].
2.4
Oportunidade de Pesquisa
Embora plataformas como WhatsApp e Telegram tenham sido
amplamente estudadas recentemente [15, 20, 22, 26], o Dis-
cord permanece relativamente inexplorado, especialmente no
contexto brasileiro. Motivados por esses fatos, este trabalho
busca explorar essas oportunidades, com um enfoque especial
na discussão relacionada à saúde mental.
COLETA E PREPARAÇÃO DE DADOS
Esta seção explica o processo da coleta de dados de grupos
públicos do Discord.
3.1
Considerações Éticas
Inicialmente, para realizar a coleta dos dados, certas precau-
ções foram tomadas. Todos os dados foram coletados de grupos
considerados objetivamente públicos pelos termos de uso do
Discord, que são assinados por todos os usuários1. Os dados
foram anonimizados e não serão disponibilizados de modo a
evitar qualquer exposição e nenhuma análise foi feita de forma
que a identificação do usuário que a enviou seja possível. Toda
a metodologia de coleta é descrita na próxima subseção, vi-
sando a reprodutibilidade e a transparência do nosso trabalho.
É importante destacar que a coleta de dados do Discord foi
feita utilizando procedimentos semelhantes aos descritos na
literatura atual [25].
3.2
Coleta e Filtragem dos Dados
Coletamos mensagens de grupos públicos brasileiros no Dis-
cord, seguindo as políticas da plataforma2. Esses grupos são
divididos em dois tipos distintos: (i) aqueles destacados no
Discovery3, um recurso oficial do aplicativo para buscar no-
vos grupos para ingressar; e (ii) aqueles com links de convite
publicamente disponíveis em sites externos e fóruns online.
Inicialmente, desenvolvemos um web scraper com o objetivo
de extrair links de convite para os grupos da plataforma, das
fontes mencionadas anteriormente. Dessa forma, coletamos os
links de convite para todos os grupos em português brasileiro
apresentados no Discovery [8], bem como todos os links de
convite disponíveis nos quatro sites não oficiais mais populares
de compartilhamento de links do Discord. A Tabela 1 detalha
a quantidade de grupos coletados de cada fonte.
Embora não seja possível alcançar todos os grupos públicos
disponíveis na plataforma, a variedade e o volume significativo
de grupos coletados garantem que nosso conjunto de dados
seja uma amostra representativa e diversificada dos grupos
publicamente acessíveis no Brasil.
1https://discord.com/terms
2https://discord.com/safety/360043709612-our-policies
3https://discord.com/guild-discovery
Tabela 1: Distribuição do número de links de grupos
coletados para todas as fontes públicas utilizadas.
Fonte
# de Links
Discovery
1.086
Disboard
1.200
Discadia
Top.gg
Discords
Total de grupos únicos
2.097
A coleta das mensagens, por sua vez, foi realizada utilizando
um crawler baseado na API oficial do Discord 4. Uma vez con-
figurado, o software acessou grupos públicos da plataforma
através dos links de convite e salvou todas as mensagens de
texto compartilhadas em seus canais desde a criação do grupo,
até o momento da coleta. Para cada mensagem, o crawler re-
tornou informações sobre o timestamp, nome de usuário do
autor, ID único do autor e conteúdo.
Para garantir análises temporalmente consistentes, restrin-
gimos nosso estudo apenas aos grupos ativos - definidos como
aqueles com pelo menos 10 mensagens enviadas em seus ca-
nais de texto durante todos os meses, entre Janeiro de 2018 e
Janeiro de 2024. Isso nos permitiu obter um volume de dados
que abrange os períodos pré, durante e pós-pandemia. Após a
filtragem, nosso dataset final consiste em 128 grupos, com um
total de 441.683.794 mensagens de texto enviadas por 1.787.300
usuários diferentes.
METODOLOGIA
Esta seção descreve a metodologia utilizada, que adota uma
abordagem quantitativa e qualitativa, em uma pesquisa ex-
ploratória e descritiva, para analisar a evolução do uso da
plataforma Discord, identificar e categorizar termos sensíveis
relacionados à saúde mental, e compreender o impacto da
pandemia de COVID-19 nas discussões dos usuários.
4.1
Utilização do Discord
Inicialmente, buscamos compreender como a plataforma Dis-
cord evoluiu ao longo dos anos em termos de interação dos
usuários. Para isso, focamos em três métricas principais: a
quantidade de mensagens enviadas, o número de usuários ati-
vos e os horários de pico de uso da plataforma. O objetivo foi
identificar mudanças de padrões de uso ao longo do tempo.
Para isso, os dados foram processados utilizando os timestamps
de cada mensagem, permitindo a separação mês a mês.
4.2
Identificação de Termos Sensíveis
Observar e coletar dados sobre a discussão de temas relacio-
nados à saúde mental em redes sociais é um desafio complexo,
especialmente ao tentar compreender o estado emocional dos
usuários em uma análise que abrange toda a plataforma. Para
4https://discord.com/developers/applications
WebMedia’2024, Juiz de Fora, Brazil
Bento, Buzelin, Aquino et al.
Tabela 2: Termos selecionados pelos especialistas, divididos por categoria.
Categoria
Termos
Depressão
"depress", "deprim", "sozinho", "estres", "stress", "eu sou um lixo", "desesper", "vou chora", "quero
chora", "solita", "solidão", "sinto vazio", "vazio existencial", "vou de arrasta", "exaust emocionalmente",
"sinto vazio", "vazio existencial", "me odeio", "n aguento mais", "sinto inutil", "sou inutil", "desampar",
"me sinto inutil", "sinto perdido", "desmotiva", "não aguento mais", "cansado de tudo", "exaustão
emocional", "dor emocional", "sem futuro", "chorar todo dia", "sem vontade de viver", "tristeza
profunda", "sobrecarga emocional", "vida sem sentido", "sem motivacao", "isolamento", "sinto-me
perdido", "não sinto nada", "morto por dentro", "esgotado mentalmente", "triste", "sentimento de
culpa", "inutilidade", "sinto culpado", "medo intenso", "fadiga emocional", "sem saida", "fim da linha",
"vazio", "perda de interesse", "impotencia", "desvalorização", "indesejado", "sentindo inutil"
Suicídio
"suicid", "smt", "sct", "se mata", "vou me matar", "quero morrer", "quero me matar", "enforc", "se mata",
"se corta", "vo me mata", "vou me m", "pular da ponte", "pular da pte", "13 porque", "13 motivo",
"selfharm", "sufoc", "vontade de morre", "0 vontade de viver", "zero vontade de viver", "automutilação",
"autoflagelação", "querer morrer", "auto-mutilacao"
Ansiedade
"ansied", "ansios", "panico", "angusti", "nervo", "sufoc", "medo", "preocup", "tens", "hiperventilação",
"sensação de desastre", "taquicardia", "fobia", "sudorese", "desconforto no peito"
Psicose
"esquizo", "paranoia", "alucina", "delir", "distúrbios do pensamento", "vozes na cabeça", "visões", "perda
de contato com a realidade", "perda de realidade"
Transtornos
"transt personalidade", "transtorno de personalidade", "anorex", "bulim", "transtorno alimentar",
"transtorno mental", "transtorno bipolar", "TDAH", "transtorno obsessivo-compulsivo", "TOC",
"distúrbio de conduta", "dislexia", "transtorno de déficit de atenção", "transtorno dissociativo",
"transtorno de pânico", "transtorno de estresse pós-traumático", "tbp"
Questões Gerais
"insegur", "neur", "edtwt", "shtwt", "saude mental", "doenca mental", "doente mental", "doença mental",
"boderline", "ptsd", "hiperfixa", "autis", "abus", "dificuldade social", "insonia", "trauma", "violen", "altas
habilidades", "tea", "TEA", "bully", "terapia", "conselho", "suporte emocional", "estigma mental",
"psicoterapia", "medicação psiquiátrica", "aconselhamento", "crise", "intervenção", "recuperação",
"apoio psicológico", "diagnóstico", "consulta psicológica", "psico"
tratar essa dificuldade, optamos por utilizar uma lista de termos
específicos, desenvolvida em colaboração com especialistas da
área de psiquiatria. Esses termos incluem palavras e expres-
sões comumente usadas em discussões sobre saúde mental,
com tópicos como depressão, ansiedade e suicídio. A lista foi
criada com base em literatura especializada e na experiência
clínica dos colaboradores. Em nossas análises, nos concentra-
mos em comparar a frequência e a variação temporal do uso
desses termos, identificando quais são mais prevalentes em
determinados períodos.
Apesar das limitações inerentes, como a dependência da
precisão na identificação de termos relevantes e a ausência de
contexto, esse método nos permite recuperar de forma eficaz
os tópicos de mensagens específicos referentes à saúde mental.
Isso proporciona uma visão mais detalhada do conteúdo discu-
tido pelos usuários, oferecendo uma compreensão ampla sobre
a incidência de diferentes assuntos relacionados a esse tema
e sua variação diante de fatores externos, além das preocupa-
ções e estados emocionais predominantes na plataforma. A
fim de identificar as mensagens que mencionam tais palavras,
aplicamos um algoritmo de busca na base de dados, que foi
projetado para encontrar menções exatas e derivações comuns
dos termos listados, garantindo uma cobertura abrangente de
todas as mensagens que contém os termos determinados.
4.3
Categorização dos Termos
Outro importante passo em direção às análises almejadas foi a
categorização dos termos, em coerência com o conhecimento
dos profissionais da área da psiquiatria. Eles foram organizados
nas seguintes categorias: “Depressão”, “Ansiedade”, “Suicídio”,
“Psicose”, “Transtornos” e “Questões Gerais”. Cada termo foi
alocado à categoria em que mais se encaixava às condições psi-
quiátricas correspondentes, enquanto os termos relacionados a
“Questões Gerais” foram aqueles que não se enquadravam em
um quadro psiquiátrico específico, mas ainda eram relevantes
para a discussão sobre saúde mental.
As mensagens identificadas são então categorizadas com
base nos termos encontrados: caso uma mensagem contenha
um termo de uma categoria específica, ela é classificada como
pertencente a essa categoria. Se uma mensagem contém termos
de múltiplas categorias, ela é categorizada como pertencente
Impacto da Pandemia na Discussão sobre Saúde Mental:
O Caso do Discord no Brasil
WebMedia’2024, Juiz de Fora, Brazil
a todas as categorias correspondentes. Todos os termos e cate-
gorias foram revisados por especialistas da psiquiatria, para
garantir sua corretude de uso, e estão disponíveis na Tabela 2.
4.4
Período da Pandemia do COVID-19
Compreender o contexto da pandemia é um aspecto crucial
deste trabalho, uma vez que foi um período de estresse mundial.
Apesar da clareza do teor do evento, apontar datas específicas
para caracterização do início e fim é de grande complexidade,
visto que variadas cidades e estados tiveram diferentes experi-
ências. Porém, para fins de análise, foi necessário definir um
período representativo da pandemia no Brasil. O intervalo do
início de Março de 2020 até o fim de Abril de 2022 foi sele-
cionado, dado que os períodos de isolamento e lockdown se
iniciaram em Março de 2020, e o Ministério da Saúde consi-
dera Abril de 2022 como o término oficial da pandemia no
Brasil[3]. Para podermos comparar os dados em cada período,
todos os dados anteriores a Março de 2020 são considerados
“pré-pandemia” (desde Janeiro de 2018), enquanto tudo que
vem depois de Abril de 2022 é considerado “pós-pandemia”
(até Janeiro de 2024).
RESULTADOS
Esta seção apresenta os resultados das nossas análises acerca
do uso do Discord durante a pandemia de COVID-19, focando
no número de mensagens, usuários ativos e conteúdo relacio-
nado à saúde mental. Investigamos a distribuição temporal das
mensagens e a retenção de usuários após o fim do isolamento
social.
5.1
Caracterização da Rede
Para entender a evolução do uso do Discord, é essencial anali-
sar a quantidade de mensagens enviadas e o número de usuá-
rios ativos na rede ao longo do tempo, conforme ilustrado
na Figura 1, que mostra um aumento acentuado tanto no nú-
mero de mensagens quanto no de usuários ativos durante a
pandemia de COVID-19. No início de 2020, observa-se um
pico significativo na quantidade de mensagens enviadas por
mês, acompanhado por um crescimento similar no número de
usuários ativos, que coincide com o início das medidas de qua-
rentena no Brasil. Esse aumento abrupto na atividade sugere
que, durante o isolamento social, os usuários de mídias sociais
buscaram o Discord como uma plataforma de comunicação
que poderia suprir a falta de interações sociais nesse período
de isolamento.
Além disso, o gráfico também mostra que, após o término do
período pandêmico em Abril de 2022, houve uma diminuição
gradual tanto no número de mensagens quanto no número
de usuários ativos. Esse declínio pode ser interpretado como
uma normalização dos hábitos de uso da plataforma, à me-
dida que as interações presenciais voltaram a ser possíveis.
No entanto, é notável que, enquanto o número de mensagens
retornou a valores semelhantes aos registrados anteriormente,
o número de usuários ativos permaneceu consideravelmente
mais alto do que no período pré-pandemia. Isso indica que
Figura 1: Distribuição do número total de mensagens e
usuários ativos por mês, de Janeiro de 2018 a Janeiro de
2024. A área sombreada em vermelho indica o período
da pandemia.
Figura 2: Mapa de calor representando a distribuição
do número de mensagens enviadas por hora do dia, de
Janeiro de 2018 a Janeiro de 2024. A área delimitada em
vermelho indica o período da pandemia.
muitos dos novos usuários que se adaptaram a plataforma, jun-
tamente com aqueles que apenas aumentaram seu uso durante
a pandemia, continuaram a utilizar a plataforma mesmo após
o seu fim, demonstrando uma retenção significativa e uma
possível mudança permanente nos padrões de comunicação
na plataforma, e talvez até no mundo digital como um todo.
Outro ponto importante para compreender a mudança nos
padrões de uso da plataforma é a análise das horas ativas
dos usuários. A Figura 2 ilustra a distribuição do número de
mensagens enviadas por hora do dia como uma proxy para a
atividade dos usuários ao longo do dia e destaca as mudanças
nos padrões de uso durante o período pandêmico. O gráfico
mostra um aumento significativo na atividade do Discord du-
rante o período das 3h às 9h da manhã, uma faixa horária
que anteriormente apresentava pouca ou nenhuma atividade.
Esse aumento no uso pode ser atribuído às mudanças nos há-
bitos dos usuários durante a pandemia de COVID-19: medidas
como home office e aulas remotas fizeram com que as pessoas
passassem grande parte do seu tempo online, o que facilitou a
WebMedia’2024, Juiz de Fora, Brazil
Bento, Buzelin, Aquino et al.
utilização de plataformas como o Discord em horários não usu-
ais. Tal efeito fica ainda mais claro ao analisar o grande volume
de mensagens durante o horário comercial, principalmente
entre 9 e 12 horas.
Por outro lado, de maneira semelhante ao que aconteceu
com o número de usuários, após o término da pandemia, essa
tendência de aumento na atividade nas primeiras horas da
manhã foi parcialmente mantida. Observa-se que os períodos
de pouca ou quase nenhuma atividade diminuíram em compa-
ração ao período pré-pandêmico, sugerindo que muitos usuá-
rios que adotaram novos hábitos de uso durante a pandemia
os mantiveram mesmo após o fim das restrições. A retenção
dessa atividade fora do horário convencional demonstra como
a pandemia pode ter provocado mudanças permanentes nos
comportamentos sociais e nas rotinas diárias dos usuários do
Discord.
5.2
Análise do Discurso
Para entender melhor as interações relacionadas à saúde men-
tal e comportamento autodestrutivo, inicialmente analisamos
a relevância dos termos sensíveis em relação a todas as men-
sagens que continham esses termos. Para isso, utilizamos a
medida de Term Frequency-Inverse Document Frequency (tf-idf),
que avalia a importância de uma palavra em um documento
(neste contexto, mensagem) em relação a um corpus. Isso é
feito dividindo a frequência de um termo nas mensagens em
relação à sua frequência no corpus como um todo. Dessa forma,
termos comuns a todos os documentos recebem um peso me-
nor que aqueles mais específicos ao contexto.
A Figura 3 apresenta as nuvens de palavras compostas pelos
termos que obtiveram os maiores valores de tf-idf para cada
um dos períodos analisados. Dividindo entre pré, durante e
pós-pandemia, é possível notar uma diferença expressiva entre
a ocorrência dos termos por período.
No período pré-pandemia, os termos mais relevantes são
“Se mata” e “Vazio”. A maior parte dos demais termos que
aparecem na nuvem de palavras consistem em termos autode-
preciativos, mostrando que estes já eram bastante utilizados
na época. Em especial, o primeiro termo é preocupante, dado
seu teor imperativo e agressivo, enquanto o termo “Vazio” tem
comportamento bem similar nos 3 períodos analisados. Assim,
é possível estabelecer um comportamento geral, do período
pré-pandemia como sendo mais (auto)agressivo.
Durante a pandemia, a palavra mais relevante é “Insônia”.
De acordo com a psiquiatria, a insônia é frequentemente preci-
pitada por um estressor significativo na vida, mostrando uma
clara mudança de comportamento que pode ser transitória
ou de longo prazo e que em longo prazo frequentemente re-
flete o estado psiquiátrico do indivíduo [9]. Outros pontos de
interesse são o aumento da relevância da palavra “Trauma”,
antes não tão evidente, cujo comportamento de aumento per-
sistiu no pós-pandemia, e o crescimento do termo “Stress”,
característico deste período de incertezas.
Após o período pandêmico, as palavras de maior tf-idf foram
“Trauma”, “Vazio” e “Tea”. Em especial, “Tea” aqui não nos diz
Figura 3: Nuvens de palavras contendo os termos mais
relevantes relacionados à saúde mental em três períodos:
antes, durante e após a pandemia.
tanta coisa, visto que pode tanto significar Transtorno do Es-
pectro Autista, quanto a expressão “tea” (chá, em inglês). Vale
mencionar também que o termo “Trauma” não tem necessari-
amente a conotação psiquiátrica de trauma. Isso sugere uma
transição significativa nos problemas e percepções individuais
ao longo do tempo, assim como sua familiaridade com termos
relacionados com saúde mental. A prevalência de “Trauma”
como uma das palavras principais sugere que, mesmo após a
resolução de algumas dificuldades imediatas da pandemia, os
efeitos psicológicos de longo prazo continuam a afetar a popu-
lação, que continua a falar sobre isso, deixando a saúde mental
como tema recorrente de discussões no Discord. Por exemplo,
termos relacionados à categoria “Depressão” aparecem como
tema mais banal e frequente. “Ansiedade” e “Suicídio” tam-
bém surgem, mas em menores frequências, mostrando menor
familiaridade ou conforto dos usuários em falar sobre esses
temas.
Impacto da Pandemia na Discussão sobre Saúde Mental:
O Caso do Discord no Brasil
WebMedia’2024, Juiz de Fora, Brazil
Tabela 3: Estatísticas de usuários ativos, em porcentagem e valor absoluto.
Pré Pandemia
Per Pandemia
Pós Pandemia
Número Absoluto de Usuários
309.290
724.280
416.150
Número Absoluto de Usuários (Termos)
17.834
46.037
24.664
% de Usuários que Citaram os Termos
5,76%
6,35%
5,92%
Número Absoluto de Mensagens
87.177.800
246.259.000
102.762.000
Número Absoluto de Mensagens com Termos
939.810
3.513.560
1.495.510
Mensagens com Links
1.132.760
4.155.450
2.288.930
Mensagens com Links (Termos)
31.464
87.024
58.537
% de Mensagens com Links Dentre as Mensagens Totais
1,29%
1,68%
2,22%
% de Mensagens com Links Dentre as Mensagens com Termos
3,34%
2,47%
3,91%
Outra análise pertinente é a comparação entre os usuários
que discutem termos relacionados à saúde mental. Conforme
demonstrado na Tabela 3, observa-se que houve um aumento
na porcentagem de usuários que mencionaram termos asso-
ciados à saúde mental. Se olharmos os números absolutos, o
acréscimo foi de 30 mil usuários. É interessante notar que,
mesmo após o término da pandemia, esse valor se manteve
elevado, sem retornar aos níveis anteriores.
Além disso, a Tabela 3 caracteriza quantitativamente um
interessante achado comportamental no Discord. Apesar da
pandemia de COVID-19, em termos relativos, não impactar
consideravelmente no uso de links nas mensagens do Discord,
notamos que mensagens sobre o tema de saúde mental, coleta-
das pela busca de termos selecionados, apresentam um taxa
de envio de links maior do que mensagens gerais, chegando a
3,9% após a pandemia.
Com o objetivo de analisar o comportamento temporal da
discussão associada ao tópico de saúde mental na rede, para
além da análise qualitativa, apresentamos a porcentagem de
mensagens que contém os termos especificados em relação
às mensagens totais ao longo do tempo, conforme a Figura
4. Nesse sentido, observa-se um nítido crescimento relativo
da quantidade de mensagens sobre temas de saúde mental
após Março de 2020 - inicio da pandemia de COVID-19. Além
disso, destaca-se a manutenção dos níveis atingidos mesmo
após o fim do período pandêmico, apesar da queda em valores
absolutos da quantidade de mensagens totais, indicando que o
assunto se estabeleceu mesmo com a mudança de cenário.
Para aprofundar a compreensão sobre o conteúdo dessas
discussões, desmembramos a visualização geral dos termos
em seis categorias distintas, conforme apresentado na seção
de agrupamento dos termos, cujo resultado é apresentado na
Figura 5. Em primeiro lugar, verificamos que termos da catego-
ria “Depressão” tiveram um ligeiro crescimento em relação ao
período pré-pandemia, que se manteve até mesmo após Abril
de 2022. No caso da categoria “Suicídio”, houve um aumento
gradual da porcentagem após o isolamento social. Quanto às ca-
tegorias “Psicose”, “Transtornos” e “Questões Gerais”, também
ocorreu um ligeiro crescimento, embora com porcentagens
mais acentuadas. Por fim, no caso da categoria “Ansiedade”, é
possível perceber uma tendência de crescimento independente
Figura 4: Porcentagem de mensagens contendo termos
sensíveis ao longo do tempo. A área sombreada em ver-
melho indica o período da pandemia.
do período analisado, mostrando que é um tema de discus-
são que se torna cada vez mais popular entre os usuários do
Discord.
Finalmente, cabe uma discussão sobre a ocorrência de des-
vios atípicos na distribuição das porcentagens desses gráficos.
Ao final do ano de 2017, houve o lançamento da série “13 Rea-
sons Why” da Netflix, que aborda temas sensíveis relacionados
a suicídio, sendo uma possível justificativa acerca da alta nos
termos relacionados ao assunto neste período.
Tendo em mente as análises feitas, nota-se que o cresci-
mento da discussão sobre saúde mental é uma característica
duradoura e com potencial de se estabelecer na rede, além de
sofrer notória influência de eventos relevantes relacionados.
Quanto a classificação das mensagens com termos sensíveis
em grupos distintos, é fundamental identificar e entender quais
delas ocorrem simultaneamente de forma mais recorrente na
rede. Ainda que as co-ocorrências sejam um fenômeno pouco
observado nessa base de dados, constituindo apenas 1,52% das
mensagens totais, é interessante observar esse subconjunto
e verificar se as relações observadas de fato se traduzem em
relações no discurso do usuário. A Figura 6 mostra que, das
co-ocorrências, aquela entre “Depressão” e “Ansiedade” é domi-
nante, sendo seguida pelo par “Depressão” e “Questões Gerais”.
WebMedia’2024, Juiz de Fora, Brazil
Bento, Buzelin, Aquino et al.
Figura 5: Porcentagem de mensagens contendo termos
sensíveis ao longo do tempo, separadas por categoria. A
área sombreada em vermelho indica o período da pan-
demia.
Considerando que ansiedade e depressão sejam os diagnós-
ticos mais prevalentes populacionalmente, é compreensível
que apareçam entre os mais comuns. O caráter comórbido fre-
quente e as inter-relações entre os dois diagnósticos explicam a
alta frequência de co-ocorrência entre os termos apresentados
no discurso.
Figura 6: Porcentagem de mensagens contendo co-
ocorrência entre pares de termos em relação ao número
total de mensagens sensíveis.
DISCUSSÃO E CONCLUSÃO
Neste estudo, exploramos as dinâmicas da plataforma Discord
no Brasil, especialmente em relação aos impactos na temática
das discussões sobre saúde mental promovidas pelos usuários,
com um enfoque na pandemia. Por meio de uma análise so-
bre as mensagens enviadas ao longo dos anos, conseguimos
identificar padrões de uso e comportamento que evidenciam o
crescimento da atividade na plataforma e transições significa-
tivas no teor das discussões durante o período pandêmico.
Nossas perguntas focaram na caracterização da plataforma
Discord durante a pandemia e na análise do uso de termos
relacionados à saúde mental, esses últimos definidos por um
especialista. Observamos que a plataforma cresceu significati-
vamente nos períodos de isolamento, com aumentos notáveis
no número de usuários, mensagens e tempo de uso. No período
pós-pandêmico, embora o número de mensagens tenha retor-
nado a níveis anteriores, a retenção de usuários permaneceu
alta, indicando um impacto duradouro da pandemia na base
de usuários do Discord.
Com o auxílio de profissionais de psiquiatria, definimos ter-
mos relacionados à saúde mental e os categorizamos conforme
padrões médicos para facilitar a análise. Todas as categorias
apresentaram aumento expressivo durante e após a pande-
mia, com 2023 sendo o ápice das incidências. Esse contraste
revela que, apesar do restabelecimento do padrão de uso das
redes sociais, houve uma mudança significativa no discurso
dos usuários. O número absoluto de mensagens retornou aos
níveis pré-pandêmicos, mas os termos relacionados à saúde
mental permaneceram elevados ou aumentaram ainda mais.
Isso indica que uma análise superficial e puramente quanti-
tativa das redes sociais pode não refletir a verdadeira mudança
no conteúdo. Falar de saúde mental, em especial de quadros
mais prevalentes, como ansiedade e depressão, pode expressar
que a rede espelha condições apresentadas pela população.
Portanto, é evidente que o conteúdo da plataforma mudou.
O aumento significativo de termos específicos à saúde mental
dificilmente ocorreu por acaso, especialmente em um universo
de meio bilhão de mensagens.
Por último, este estudo apresenta limitações. As palavras
chave foram filtradas mas não necessariamente contextualiza-
das, e podem, em alguns casos, tratar de termos mais genéricos
e e não necessariamente usados para se referir a problemas
de saúde mental. Esse problema pode ser minimizado utili-
zando representações semânticas das mensagens, como aque-
las geradas por modelos de embeddings. Além disso, análises
detalhadas da rede e suas propriedades são essenciais para
entender melhor o que se passa na plataforma e para prevenir
que ela se torne um espaço de discussões prejudiciais sobre
saúde mental.
ACKNOWLEDGMENTS
Este trabalho foi parcialmente financiado por CNPq, CAPES,
FAPEMIG, e pelo projeto CIIA-Saúde.
Impacto da Pandemia na Discussão sobre Saúde Mental:
O Caso do Discord no Brasil
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30117.txt ---

--- INÍCIO DO ARQUIVO: 30120.txt ---
Jogos Digitais Sérios usados para o Exercício de Habilidades do
Pensamento Computacional em Crianças
com Transtorno do Espectro Autista
Katherin Felipa Carhuaz Malpartida
Instituto de Ciências Matemáticas e de Computação
Universidade de São Paulo (USP)
São Carlos, São Paulo, Brasil
katherincm@usp.br
Kamila Rios da Hora Rodrigues
Instituto de Ciências Matemáticas e de Computação
Universidade de São Paulo (USP)
São Carlos, São Paulo, Brasil
kamila.rios@icmc.usp.br
WebMedia’2024, Juiz de Fora, Brazil
Katherin Felipa Carhuaz Malpartida and Kamila Rios da Hora Rodrigues
processo de design do JDS e a primeira avaliação desse e, por fim, a
Seção 5 discorre sobre as considerações finais e os trabalhos futuros.
REFERENCIAL TEÓRICO
Esta seção descreve os principais conceitos que fundamentam este
trabalho, entre eles: Pensamento Computacional; Deficiência Int-
electual e Transtorno do Espectro Autista; End-User Development
– EUD (ou Desenvolvimento pelo Usuário Final). A compreensão
da inter-relação entre esses conceitos é essencial para o desenvolvi-
mento de ferramentas educacionais inclusivas, permitindo que o PC
seja exercitado por crianças com TEA ou DI, por meio de jogos cri-
ados e personalizados pelos próprios profissionais (e.g. professores,
pedagogos, etc.), utilizando plataformas de EUD.
2.1
Pensamento Computacional (PC)
Jeannette Wing popularizou o termo PC em 2006, descrevendo-o
como um processo que envolve desde a estruturação do raciocínio
até o comportamento humano reflexivo para a resolução de proble-
mas. O PC pode ser ainda observado em processos como a leitura,
escrita e aprendizado da matemática, sendo parte da capacidade
analítica desde a infância do indivíduo. Wing destaca que o PC pode
ser aprendido por todos e não apenas por profissionais da área da
Computação [38].
2.1.1
Pilares do Pensamento Computacional. Diversos autores, como
Wing e Brackmann [4, 22, 38] e instituições, como Learning [3] e o
Centro de Inovação para a Educação Brasileira (CIEB) 2, propõem
que o PC pode ser ensinado a partir de quatro pilares fundamentais
[29], os quais são:
• Decomposição: envolve a divisão de problemas complexos
em partes menores e gerenciáveis para facilitar sua solução.
• Reconhecimento de Padrões: identifica semelhanças entre
problemas e soluções para obter resultados mais eficiente.
• Abstração: consiste na análise e categorização de dados para
destacar informações relevantes e organizar estruturas que
simplifiquem a solução de problemas.
• Algoritmo: engloba os pilares anteriores, definindo um
plano ou conjunto de instruções necessárias de forma clara
e organizada para alcançar um objetivo específico.
Vale destacar que a comunidade científica ainda não estabele-
ceu uma definição formal para o PC, visto que é uma disciplina
emergente e em desenvolvimento. Apesar disso, é amplamente re-
conhecido como um campo em constante evolução e com um futuro
promissor [16, 36].
2.2
Deficiência Intelectual (DI)
A DI é caracterizada por limitações significativas no funcionamento
cognitivo e na adaptação comportamental, abrangendo áreas como
habilidades práticas, interpessoais e conceituais. Essa condição
geralmente se manifesta nos primeiros estágios de desenvolvimento,
antes dos 18 anos [1]. A DI implica uma limitação no desenvolvi-
mento das funções essenciais para compreender e interagir com o
ambiente, como é observado em condições como o TEA, a Síndrome
de Down e outros [24].
2https://cieb.net.br
O Transtorno do Espectro Autista (TEA), é descrito como um
distúrbio do neurodesenvolvimento complexo e geneticamente het-
erogêneo, cujas características se dão por dificuldades na interação
social, dificuldades no reconhecimento de expressões faciais e pelos
padrões de comportamentos repetitivos e/ou estereotipados [2].
Pessoas com TEA podem exibir uma variedade de sintomas que
não são específicos ao transtorno, incluindo diferenças nas habili-
dades cognitivas, na linguagem expressiva, nos padrões iniciais e
nas comorbidades psicopatológicas. Essas variações podem ajudar
na identificação de subtipos dentro do TEA [2]. Assim, há uma
ampla gama de sintomas e comportamentos entre indivíduos com
TEA, tanto específicos, quanto não específicos ao transtorno, o que
demanda uma abordagem personalizada para cada caso.
Nesse contexto, a proposta de incorporar o PC nas práticas
pedagógicas assume um papel fundamental, requisitando, em primeiro
lugar, um entendimento aprofundado das características individu-
ais da pessoa com deficiência. Essa abordagem permite avaliar a
pertinência da aplicação de cada pilar do PC, buscando ampliar as
opções e os níveis de complexidade progressivamente. A prática
pedagógica deve envolver a resolução de problemas cotidianos,
caso contrário, não será possível o alcance de níveis elevados de
desenvolvimento pela pessoa com deficiência [30].
2.3
End-User Development (EUD)
O EUD, é uma abordagem que pode ser definida como um conjunto
de métodos, técnicas e ferramentas que têm como objetivo permitir
que usuários finais, que não possuem experiência em programação
ou que não sejam especialistas em Computação, por exemplo, de-
senvolvam e adaptem sistemas conforme suas necessidades, sejam
elas profissionais, educacionais ou de lazer [17]. O EUD se refere
geralmente à “participação ativa dos usuários finais no processo de
desenvolvimento de software” [7].
Podem ser identificados dois tipos de atividades do usuário final
a partir de uma perspectiva de Design Centrado no Usuário (DCU):
1) Customização ou Parametrização, são atividades em que os
usuários escolhem entre comportamentos alternativos ou opções
de interação já presentes na aplicação. Em sistemas adaptativos,
essa personalização é realizada automaticamente pelo sistema com
base no comportamento do usuário; 2) Criação e modificação
de programa, inclui atividades que envolvem criar ou alterar um
artefato de software existente [17].
O jogo experimental descrito neste trabalho será incluído em uma
plataforma de autoria de jogos, idealizada para que profissionais de
domínio que não a Computação, tais como a Educação e a Saúde,
possam criar e personalizar seus jogos digitais a partir de templates
pré-definidos. É fundamental que esses profissionais (mesmo sem
ter experiência técnica avançada) possam criar e personalizar os
jogos de maneira fácil e intuitiva. Essa adaptação permitirá que
os jogadores (população de interesse dos profissionais) recebam
desafios adequados ao seu nível de conhecimento e habilidade.
TRABALHOS RELACIONADOS
A fim de contextualizar este trabalho no cenário atual, foi realizada
uma pesquisa bibliográfica em bases de dados como ACM Digital
Library, Engineering Village, IEEE Xplore, entre outras. Inicialmente,
foram considerados três tópicos principais para a realização da
Jogos Digitais Sérios usados para o Exercício de Habilidades do Pensamento Computacional em Crianças
com Transtorno do Espectro Autista
WebMedia’2024, Juiz de Fora, Brazil
pesquisa: Pensamento Computacional, Jogos Digitais Sérios e TEA.
No entanto, foram escassos os estudos que abordaram especifi-
camente a intersecção dos três temas supracitados. Desta forma,
optou-se por focar nos temas Pensamento Computacional e Jogos
Digitais Sérios, a fim de identificar um maior número de estudos e,
posteriormente, classificar aqueles voltados para o público infantil
com TEA. Astring de busca utilizada foi (("computational thinking")
AND ("digital games" OR "educational games" OR "serious games"
OR "learning games" OR gamification OR "game based learning")).
Como resultado, foram identificados estudos relevantes; no entanto,
dois deles foram selecionados com base em critérios de qualidade.
Esses estudos se destacam por abordar de forma abrangente todas
as fases do desenvolvimento de jogos sérios para o aprimoramento
do PC em criancas com TEA, desde o design e a implementação até
a avaliação com o público-alvo. Esses estudos são descritos a seguir.
3.1
CodaRoutine
O trabalho apresenta o processo de design e implementação de um
jogo sério para crianças no espectro autista. O objetivo do jogo é de-
senvolver habilidades de resolução de problemas e ensinar conceitos
básicos de programação (e.g. sequência, condicional e interação)
[13]. O jogo é constituído por três níveis de dificuldade e cada um
tem três etapas. Ao longo do jogo, são apresentadas tarefas rela-
cionadas às atividades cotidianas das crianças, tais como preparar
a lancheira para a escola, preparar a mochila para o próximo dia
de aula e decorar uma árvore de natal. Os cenários onde o jogador
interage são a cozinha, o quarto e a sala de estar.
Os resultados evidenciaram que a maioria das crianças teve con-
trole do jogo, compreendendo as tarefas e considerando o jogo
divertido e interessante. A partir dessas conclusões, os autores
afirmaram que, com a exposição adequada, o jogo pode se tornar
uma ferramenta efetiva no ensino de conceitos de programação e
habilidades de resolução de problemas para crianças com TEA [13].
No entanto, é fundamental identificar algumas limitações iner-
entes à implementação do jogo. Primeiro, o jogo foi desenvolvido
apenas em versão Web e não oferece suporte para plataformas
móveis, o que pode restringir seu acesso e uso, considerando a
crescente preferência por dispositivos móveis em contextos educa-
cionais. Além disso, a falta de opções de personalização em relação à
configuração dos cenários, fases e níveis pode limitar a capacidade
de adaptação do jogo a diferentes contextos de aprendizado e a
diversidade de públicos-alvo.
3.2
Pensar e Lavar
Trata-se de um jogo digital educacional que visa desenvolver, de
forma intrínseca, o PC em crianças neurotípicas e com deficiência
intelectual. O jogo tem como foco o processo de lavagem de roupas
e ao longo das três fases o jogador deve realizar tarefas como a
separação das peças de roupas, em seguida o processo de lavagem
e, por fim, guardar as peças já lavadas. Cada fase trata os pilares
do PC e permite que o jogador desenvolva habilidades como o
raciocínio lógico e crítico, resolução de problemas, abstração, entre
outras [11]. O jogo também considera um conjunto de diretrizes de
acessibilidade que incluem: interface de fácil entendimento, textos
claros, linguagem simples, feedback, progressão gradual dos níveis,
elementos motivadores entre outros [12].
Uma das limitações identificadas no jogo é que esse requer insta-
lação individual em cada computador em que se deseja utilizá-lo.
Não há versões Web e móvel, limitando a flexibilidade de acesso,
considerando especialmente a diversidade de dispositivos utilizados
em ambientes educacionais. Embora o jogo permita a configuração
das fases e níveis, esse não é flexível em relação à configuração
dos cenários, podendo não atender plenamente às necessidades de
personalização e adaptação para diferentes contextos de ensino e
especificidades de cada jogador.
As pesquisas descritas mostram resultados promissores indi-
cando que crianças com deficiência intelectual podem desenvolver
habilidades relacionadas ao PC por meio de jogos sérios. Sendo
assim, este trabalho visa o design e a avaliação de um jogo digi-
tal sério focado no exercício das habilidades do PC. Um aspecto
diferenciador é a adoção da plataforma RUFUS, que possibilitará
aos profissionais da Educação usar o jogo aqui projetado, ou ainda
criarem seus próprios jogos de forma personalizada, conforme os
objetivos de aprendizagem de cada criança, sendo assim um re-
curso generalizável. A próxima seção detalha o processo de design
e avaliação do jogo experimental projetado neste trabalho.
PLATAFORMA DE AUTORIA PARA JOGOS
DIGITAIS
A plataforma de autoria RUFUS viabiliza a criação de jogos digitais
sérios por parte de profissionais de áreas como a Saúde e Educação.
Essa plataforma é composta por uma interface Web de autoria,
voltada para o planejamento e criação dos jogos por parte dos
profissionais, usando templates pré-definidos, aliada a um aplica-
tivo móvel, destinado à interação direta dos jogadores, usuários
alvo dos profissionais da Saúde e/ou Educação. Na interface Web, os
profissionais podem: cadastrar pacientes/alunos e seus familiares,
criar jogos e ajustar elementos como conteúdo visual ou feedback a
ser fornecido durante o jogo. A plataforma oferece atualmente cinco
mecânicas (também denominadas de templates) de jogo: perguntas e
respostas (quiz), encaixe (puzzle), coleta de itens (plataforma), narra-
tivas (storytelling) e narrativa invertida. Por meio da interface Web,
os profissionais são orientados durante o processo da criação dos
jogos e os jogadores acessam o jogo criado utilizando o aplicativo
móvel da plataforma, mediante credenciais. Suas interações durante
o jogo são registradas e transmitidas ao sistema Web, o qual gera
relatórios sobre o desempenho do jogador. Tais relatórios podem ser
analisados pelos profissionais, permitindo intervenções pontuais e
otimização do tratamento ou prática pedagógica. [8, 9, 31]
A plataforma de autoria será usada no contexto deste trabalho
para materializar o jogo experimental aqui proposto. Portanto, uma
nova mecânica deverá ser criada e avaliada com os usuários alvo
aqui definidos (profissionais da Educação Especial de uma insti-
tuição parceira e sua população de interesse). A seção a seguir
descreve o processo de criação e avaliação do novo jogo.
4.1
Jogo para Pensamento Computacional
Para guiar o processo de design e avaliação do protótipo de jogo ide-
alizado nesta pesquisa, foi empregada a abordagem SemTh [10], que
visa garantir a ativa contribuição de partes interessadas, sobretudo
de especialistas de diferentes áreas que não apenas a Computação,
no processo de criação da solução no contexto de jogos sérios. Além
WebMedia’2024, Juiz de Fora, Brazil
Katherin Felipa Carhuaz Malpartida and Kamila Rios da Hora Rodrigues
disso, a SemTh busca viabilizar a comunicação entre as partes in-
teressadas por meio da definição de etapas e atividades/artefatos a
serem desenvolvidas. A abordagem propõe quatro etapas fundamen-
tais: Clarificação do problema de design, Modelagem da Interação,
Materialização do Design e Avaliação. Em cada etapa a abordagem
disponibiliza um conjunto de atividades a serem conduzidas e é
possível realizar ciclos de iteração entre as etapas.
Este projeto empregou as etapas da SemTh, que respaldou o
avanço em duas frentes do projeto: 1) no design de um jogo especí-
fico para o contexto e público aqui tratados, 2) na identificação de
elementos generalizáveis, a partir do jogo específico, para projetar
uma mecânica de jogo que deverá ser implementada na plataforma
de autoria de jogos do grupo de pesquisa do laboratório Intermídia
ICMC/USP, permitindo que outros jogos possam ser criados na
mesma mecânica, para contextos distintos e por outros profission-
ais [19]. Essa última frente avança os projetos na área de End-User
Development [25] do grupo de pesquisa.
A seção a seguir descreve as atividades realizadas em cada etapa
da SemTh instanciada neste projeto para a criação do jogo para o
contexto supracitado.
4.2
Etapa 1 - Clarificação do problema de design
Esta etapa se concentra na compreensão do cenário de aplicação do
jogo e na formulação de uma abordagem adequada para a sua con-
cepção. Inicialmente, foi realizado um estudo da literatura comple-
mentado por achados de recursos disponíveis em outros repositórios,
tais como plataformas e lojas de jogos digitais. Os trabalhos encon-
trados evidenciaram plataformas e ferramentas como Scratch 3 ,
Blocly Games 4 entre outras, através das quais foi promovido o
desenvolvimento de competências ligadas ao PC [15, 20, 28, 37].
O estudo buscou identificar jogos que promovem o desenvolvi-
mento do PC e habilidades de aprendizagem em crianças neurotípi-
cas e com DI. Os principais jogos identificados e considerados neste
trabalho foram aqueles descritos na Seção 3. Posteriormente, foram
selecionados elementos que passariam a compor a estrutura do jogo
digital experimental aqui nomeado de “Nossa Rotina”. Diretrizes
de acessibilidade também foram consideradas para a elaboração do
protótipo [11, 26].
O objetivo educacional do jogo proposto neste projeto está rela-
cionado ao conceito do PC e à resolução de problemas, incluindo o
desenvolvimento das competências e habilidades que englobam a
Decomposição, Reconhecimento de Padrões, Abstração e Algorit-
mos. Esses pilares foram considerados nas pesquisas de criação de
jogos digitais encontrados na revisão da literatura e supracitados
[11, 13]. Em relação às competências e habilidades que serão exerci-
tadas por meio do jogo, está sendo considerada como referência
o currículo apresentado pelo Centro de Inovação para a Educação
Brasileira (CIEB)5 [29]. Para o desenvolvimento dessas habilidades,
o jogo deverá apresentar atividades relacionadas à rotina diária do
público-alvo.
As atividades da vida diária foram selecionadas a partir de uma
pesquisa exploratória em estudos e recursos disponíveis na Web.
Foram encontradas lojas digitais com brinquedos educacionais
3https://scratch.mit.edu/
4https://blockly.games/
5https://cieb.net.br/
voltados ao treinamento da rotina com crianças. Os dois jogos
selecionados foram: Quadro Educativo 6 Infantil e Jogo Magnético
Minha Rotina 7. Esses jogos consideram que as atividades da rotina
são metas (ex.: acordar, tomar banho, escovar os dentes) compostas
de elementos ou peças (ex.: pente, sabonete, creme dental) que a
criança deve organizar segundo o seu dia a dia. Os jogos são utiliza-
dos por profissionais, como terapeutas e professores, para auxiliar
na organização da rotina das crianças com a família e em espaços
de educação e consultórios.
Em relação aos requisitos de acessibilidade, incluindo aqueles
específicos para pessoas com DI, foram considerados os trabalhos
de Dutra [12] e Pagani [26].
O trabalho de Dutra apresenta 16 diretrizes para DI e o de Pagani
traz 10 categorias com diretrizes específicas para projetar interfaces
com o foco em pessoas com TEA.
Entre os requisitos elicitados para o jogo "Nossa Rotina", destaca-
se aqueles relacionados à EUD e acessibilidade. A lista completa
está disponível no trabalho de Malpartida e Rodrigues [19]:
1. Fornecer diferentes graus de dificuldade. Os desafios devem
avançar conforme o aumento das habilidades do jogador;
2. Incorporar elementos engajadores como pontuação e vidas;
3. Permitir a personalização do jogo (ex.: escolha de person-
agens, fases e níveis de dificuldade);
4. Ter uma interface simples, facilitando a compreensão e min-
imizando a inclusão de muitos elementos na tela;
5. Apresentar uma interface padronizada (ex.: cores, ícones,
símbolos, etc.);
6. Empregar botões de controle como: Ajuda, Pausa, Voltar e
Cancelar – Evitar direcionamentos automáticos.
Os requisitos identificados na literatura, e a partir de outros jogos,
deverão ser validados por profissionais da Educação da instituição
parceira deste projeto, por meio de oficinas participativas e sessões
de brainstorming.
É importante destacar que este projeto tem aprovação de comitê
de ética em pesquisa, com número de protocolo 76853723.3. 0000.5504.
Todas as etapas em que estava prevista a participação de pessoas,
foram aprovadas por tal comitê.
4.3
Etapa 2 - Modelagem da Interação
Nesta etapa, a abordagem SemTh sugere o uso de uma Linguagem
de Modelagem de Domínio Específico para Aplicações Terapêuticas
[14]. Essa linguagem emprega representações gráficas para objetos
multimídia (ex.: imagem de fundo, efeito sonoro, texto); selos (ex.: F
para opções de flexibilidade, P para pontuação, OT para definir os
objetivos terapêuticos, entre outros) e agrupamentos (ex.: cenários e
sub-cenários). O uso desses elementos facilita a comunicação entre
profissionais multidisciplinares. Esses elementos podem usados
durante uma atividade de brainstorming ou práticas de Design
Participativo [34].
Para a primeira versão do jogo experimental foram consideradas
atividades da vida diária das crianças, tais como escovar os dentes
e tomar banho. Os itens de cada atividade são os componentes
necessários para desenvolver tais atividades, sendo eles: escova
de dente, creme dental, sabão, etc. Em cada fase do jogo, serão
6https://brinquedosbabebi.com.br
7https://nigbrinquedos.com.br
Jogos Digitais Sérios usados para o Exercício de Habilidades do Pensamento Computacional em Crianças
com Transtorno do Espectro Autista
WebMedia’2024, Juiz de Fora, Brazil
desenvolvidos os pilares do PC (abstração, decomposição, recon-
hecimento de padrões e algoritmo); um pilar primário por cada
fase, sendo os outros complementares para a resolução das tarefas
estabelecidas.
Uma primeira modelagem das telas do jogo proposto foi real-
izada [19]. A Figura 1 ilustra a modelagem da primeira fase para
o desenvolvimento do PC, relacionado ao pilar primário - Recon-
hecimento de padrões. Nessa tela, o jogador deve identificar os
itens e colocá-los no lugar correto (ação D). O selo de flexibilidade
(selo F) corresponde à escolha que jogador fará, as possibilidades
foram configuradas pelo profissional mediando a interação com o
jogo, e considerando a quantidade de itens que serão visualizados
pelo jogador (podendo ser 2, 4 ou 6 itens). A quantidade de itens
está vinculado ao objetivo terapêutico (selo OT!), pois indicará a
habilidade do jogador para identificar tais itens. É possível notar
ainda que esta tela do jogo prevê uma imagem de fundo. Ao final
de cada fase é apresentada uma mensagem de feedback positivo
(visual e auditivo), previamente configurada pelo profissional. Sobre
o feedback negativo (visual e auditivo), o jogador terá cinco tentati-
vas para resolver o jogo, caso não consiga, uma mensagem poderá
ser exibida sugerindo jogar novamente (caso o profissional julgue
pertinente ao objetivo do jogo). As ações devem ser salvas para
geração de relatórios (selo R). A pontuação do jogador é coletada,
tanto para itens colocados em locais corretos, como não (selo P) e
estão associados a uma ação do jogador (A).
Figura 1: Modelagem da tela do jogo - Fase 1. Fonte: Malpar-
tida and da Hora Rodrigues [19]
Todas as ações do jogador serão armazenadas para gerar re-
latórios (selo R), e estarão disponíveis a posteriori para o profissional
mediador, a fim de que os mesmos analisem o tempo, a pontuação,
os erros e as escolhas das crianças durante o jogo.
De modo similar, as fase dois e três foram modeladas. Tais mod-
elagens são ilustradas no trabalho de Malpartida e Rodrigues [19].
Na fase dois, o pilar trabalhado é a Decomposição e o jogador deve
identificar e selecionar os itens que correspondem a cada atividade
exibida. Na fase três é trabalhado o pilar Algoritmo e o jogador deve
identificar a ordem dos itens para cada atividade, estabelecendo
uma sequência organizada. O pilar de Abstração é desenvolvido
implicitamente durante todas as fases do jogo.
4.4
Etapa 3 - Materialização do Design
Seguindo as etapas da SemTh, e considerando os resultados das
etapas anteriores, uma primeira materialização do jogo foi realizada.
Trata-se de um protótipo de média fidelidade feito na ferramenta
Figma. Nesta etapa é recomendada ainda a criação do Game Design
Document (GDD), em que se descreve as características do jogo,
baseadas nos resultados do levantamento dos requisitos.
As subseções a seguir descrevem os aspectos fundamentais do
jogo, bem como suas fases e como o mesmo deverá ser implemen-
tado na plataforma de autoria de jogos do grupo de pesquisa em
sua versão final, a ser disponibilizada aos jogadores e profissionais
envolvidos.
4.4.1
Aspectos fundamentais do jogo. O jogo digital sério deste
projeto visa auxiliar no desenvolvimento de habilidades de com-
preensão de conceitos, generalização e abstração em crianças no
espectro autista, por meio do emprego dos pilares do PC. Os itens
e atividades do jogo correspondentes à rotina diária das crianças,
deverão ser configurados na plataforma Web de autoria do grupo
de pesquisa. Seguindo as diretrizes de acessibilidade e requisitos
coletados, o jogo foi desenhado para ter uma interface simples e
padronizada, incluindo botões de controle, como ajuda e pausa.
O jogo é composto por 3 fases, com 3 níveis de dificuldade, os
quais incorporam elementos engajadores como a pontuação, um
número determinado de vidas e feedback imediato. O jogador tem a
possibilidade de escolher seu personagem, as fases e os níveis que
deseja jogar (opções previamente configuradas pelo profissional
mediador). As configurações mencionadas serão realizadas pelo
profissional mediador, por meio da interface Web, viabilizando
a personalização para atender necessidades específicas de cada
jogador [19].
4.4.2
Fases do jogo. Cada uma das três fases do jogo representa
itens e atividades que são parte da rotina diária das crianças. Na
Fase 1 (Reconhecimento de padrões), o jogador deve relacionar os
itens de higiene pessoal com suas sombras correspondentes. A Fase
2 (Decomposição) mostra itens de higiene pessoal e atividades da
rotina, e o jogador deve analisar quais são os itens correspondem à
atividade exibida. Na Fase 3 (Algoritmo), o jogador deve observar a
atividade apresentada, escolher os itens que correspondem a essa
atividade e colocá-los por ordem lógica de aplicação. A Figura 2
ilustra a tela da Fase 1 do jogo, materializada em um primeiro
protótipo de média fidelidade, seguindo a modelagem criada na
segunda etapa da SemTh (ver Figura 1). As telas de materialização
das fases 2 e 3 são ilustradas no trabalho de Malpartida e Rodrigues
[19].
4.5
Etapa 4 - Avaliação
Essa etapa da SemTh visa identificar e corrigir eventuais problemas
ou erros antes da implementação do jogo, assim, a equipe pode
retroceder às etapas anteriores com o intuito de promover correções
ou inclusões de novos requisitos, até a conclusão do fluxo delineado
pela abordagem [10].
A SemTh sugere o uso de um conjunto de instrumentos que
podem ser usados para avaliar os jogos. Para este trabalho, foi
utilizado o método Cognitive Walkthrough (CW) [27], um método
de inspeção que avalia a usabilidade analisando o percurso que
WebMedia’2024, Juiz de Fora, Brazil
Katherin Felipa Carhuaz Malpartida and Kamila Rios da Hora Rodrigues
Figura 2: Tela prototipada da Fase 1 - Reconhecimento de
Padrão. Fonte: Malpartida and da Hora Rodrigues [19]
um usuário supostamente faria para alcançar seu objetivo quando
interagindo com a interface de um sistema interativo. Foi utilizada
uma variante do CW, chamada de Cognitive Barriers Walkthrough
(CBW), voltada a verificar a facilidade de aprendizado de jogos
digitais [32].
Além disso, entrevistas semiestruturadas e dois questionários
(de usabilidade e de resposta emocional) foram utilizados na etapa
de avaliação. Os questionários são: 1) SUS - System Usability Scale
– um instrumento que mede a percepção dos usuários em relação
à facilidade e eficácia da interação com um sistema específico [6];
2) SAM - Self-Assessment Manikin – um instrumento de resposta
emocional que utiliza imagens pictográficas e avalia três dimensões:
Satisfação, Motivação e Sentimento de controle [5] do usuário.
A avaliação do Protótipo do Jogo na Interface Móvel foi realizada
na instituição parceira ACORDE 8. Essa instituição oferece assistên-
cia especializada a crianças e adolescentes no espectro autista. É
importante novamente destacar que o projeto foi aprovado pelo
comitê de ética em pesquisa, com a inclusão e ciência de tal institu-
ição, garantindo a conformidade com os padrões éticos e legais de
uma pesquisa responsável.
A seguir, são descritas as atividades realizadas durante a avali-
ação do protótipo, conforme sugere o CBW [32].
1. Preparação. Foram definidas tarefas e ações necessárias
para a avaliação com profissionais da instituição, bem como
todos os documentos e termos necessários para a condução.
A tarefa atribuída aos participantes foi jogar o jogo prototi-
pado no Figma nos níveis fácil, médio e avançado; verificar
a pontuação alcançada e as vidas obtidas. Buscou-se definir
tarefas que permitissem aos profissionais explorar as princi-
pais ações disponíveis ao jogador no jogo — versão mobile
(exemplo na Figura 2);
2. Participantes. Participaram da avaliação duas professoras
(aqui chamadas de P) da instituição. As professoras foram
convidadas pela coordenadora pedagógica da instituição.
Eram duas mulheres, especialistas em Pedagogia, com exper-
iência mínima de cinco anos em atividades pedagógicas com
crianças no espectro autista. Uma tem experiencia no uso de
tecnologias para esse público-alvo e nenhuma das duas tem
experiência no uso de jogos digitais para fins educacionais;
3. Condução e Coleta de Feedback. Foi coordenada uma re-
união presencial com as participantes na instituição. Du-
rante a reunião, foram descritos o contexto da pesquisa,
8https://institutoacorde.org.br/
seus objetivos e como seria realizada a avaliação. As par-
ticipantes leram e assinaram formulários de consentimento,
questionários de perfil e autorização de uso de imagens e
dados. Em seguida, foram convidadas a interagir com o jogo
por meio da ferramenta Figma. A avaliação ocorreu individ-
ualmente com cada participante. Durante a interação, elas
foram incentivadas a compartilhar suas impressões, e respon-
deram às questões do método CBW (ver Tabela 1) durante a
execução da tarefa pré-definida. A ferramenta Google Meet
foi utilizada para observar e registrar a interação de cada par-
ticipante com o protótipo. Após a interação com o protótipo,
as participantes responderam aos questionários SAM e SUS
e, por fim, foi realizada uma entrevista semiestruturada. As
avaliações duraram aproximadamente 40 minutos. A Figura 3
ilustra as participantes interagindo com o protótipo do jogo;
Tabela 1: Questões do CBW [32].
Inicial-Q: O usuário conseguiria expressar a tarefa
a ser realizada?
Perguntas: (repita para cada ação.)
Q1: O usuário tentaria atingir o efeito correto?
Q2: O usuário se manteria focado na tarefa?
Q3: O usuário perceberia que a ação correta está
disponível?
Q4: O usuário conseguiria associar a ação correta
com o efeito que está tentando atingir?
Q5: Caso a ação correta seja realizada, o usuário
perceberia que está progredindo para concluir a
tarefa?
Final-Q: O usuário perceberia incentivo para
continuar a tarefa?
Figura 3: Participantes avaliando o protótipo do jogo.
4. Consolidação dos resultados. Durante a avaliação foram
encontrados alguns problemas no protótipo, tais como: 1) A
ação de pontuar o jogador era muito sutil (apontou P1); 2) O
feedback deveria ter som (apontaram P1 e P2) para melhor
percepção e inclusão do jogador; 3) As instruções da tarefa
do jogo deveriam ter áudio (apontaram P1 e P2) para fins de
inclusão. As participantes também indicaram: “Não é evidente
a ordem dos elementos na tela” (por P1), “A disposição dos
elementos para colocar uma sequência precisa ficar mais clara”
(por P1) ou “Os elementos como o sabonete devem estar o mais
próximo possível do que as crianças usam” (por P2) [uma
referência à questão estética do sabonete]. Para esses tipos
de questões apontadas, foi explicado às participantes que na
versão configurável do jogo na plataforma Web de autoria,
Jogos Digitais Sérios usados para o Exercício de Habilidades do Pensamento Computacional em Crianças
com Transtorno do Espectro Autista
WebMedia’2024, Juiz de Fora, Brazil
Tabela 2: Resultados do questionário SUS.
Participante
Pontuação
Resultado
P1
Excelente
P2
72.5
Bom
elas poderão personalizar todos esses aspectos, uma vez que
em tal plataforma o profissional quem faz upload de imagens,
textos, áudios e mensagens de feedback ao usuário.
Os resultados do questionário SUS, disponíveis na Tabela 2,
apontaram que as duas participantes avaliaram a usabili-
dade como Aceitável, definindo uma pontuação acima da
média sugerida pelo questionário SUS de 68 pontos. Para o
questionário SAM (ver Tabela 3), os domínios de Satisfação,
Motivação e Sentimento de Controle, tiveram resultados pos-
itivos em ambas as participantes.
Tabela 3: Resultados do questionário SAM.
Domínio
P1
P2
Satisfação
Motivação
Sentimento de Controle
Durante as entrevistas semiestruturadas, as participantes
expressaram opiniões positivas, mas também sugeriram al-
gumas correções para os problemas encontrados, como: 1)
Adicionar som de palmas ou parabéns quando o jogador
ganha um ponto, para deixar mais claro (por P1); 2) Ao final
de cada nível, mostrar os pontos ganhos (por P1); 3) Caso o
jogador tenha problemas de frustração, as vidas podem ser
substituídas por outros elementos motivacionais. Por exem-
plo, ao completar um nível, o jogador poderá receber 4 pontos
e três estrelas como reforço positivo (por P2); 4) Adicionar
áudio às instruções ajudaria jogadores não alfabetizados ou
disléxicos (por P1 e P2);
5. Relatório dos resultados. Durante a avaliação do pro-
tótipo foram identificados alguns problemas significativos,
descritos acima, que podem impactar a experiência do usuário.
Abaixo, estão listados de forma sintetizada, os principais
problemas encontrados, bem como as sugestões de melho-
rias e correções propostas pelos participantes:
– Pontuação sutil: As participantes observaram que a pontu-
ação do jogador é muito sutil, dificultando a compreensão
do progresso durante o jogo. A sugestão de melhoria é au-
mentar a visibilidade da pontuação por meio de elementos
visuais e auditivos mais claros e destacados;
– Feedback sem som: As participantes indicaram que o feed-
back no jogo não possui som (na versão prototipada), o
que pode afetar a percepção do jogador em relação aos
acertos e erros durante as atividades. A sugestão de mel-
horia na versão implementada é incluir sons de feedback,
como palmas ou parabéns, para indicar o desempenho do
jogador de forma mais clara e motivadora;
– Instruções sem áudio: As instruções no jogo não tinham
áudio no protótipo, o que pode dificultar a compreensão
e a participação de jogadores não alfabetizados ou com
dislexia, segundo as participantes. Para melhorar esse as-
pecto, na versão implementada, pode ser adicionado áudio
às instruções para auxiliar na compreensão e na partici-
pação de jogadores com diferentes habilidades de leitura;
– Motivação e reforço positivo: Diante da possibilidade de
um jogador se sentir frustrado devido à limitação de vidas
no jogo, podem ser implementados elementos adicionais
de motivação e reforço positivo, como a inclusão de es-
trelas ou outro elemento motivador ao finalizar um nível.
Isso pode ajudar a reforçar comportamentos desejáveis e
manter o interesse dos jogadores.
É importante destacar que as professoras validaram os requisi-
tos do jogo relacionados, por exemplo, a apresentar atividades de
vida diária de forma lúdica e por meio dos pilares do pensamento
computacional, fornecer diferentes graus de dificuldade, incorporar
elementos engajadores como pontuação e vidas, e permitir a per-
sonalização do jogo (ex.: escolha de personagens, fases e níveis de
dificuldade).
Destaca-se ainda, que as mesmas participantes também avaliaram
os protótipos da interface Web de criação dos jogos na plataforma
Web de autoria. Os resultados das avaliações das profissionais estão
sendo analisadas junto à equipe de desenvolvimento da plataforma.
Essa análise levou a ajustes na proposta, na modelagem e na mate-
rialização do jogo.
Uma versão preliminar do jogo já está implementada para as
fases 1 e 2. A Figura 4 ilustra o jogo atualmente para essas fases.
O jogo também avalia a experiência do jogador de forma lúdica e
simples, conforme ilustrado na Figura 5.
A estética dos elementos de interface é escolhida pelo profis-
sional criando tal jogo na interface Web. Atualmente a plataforma
dispõe de 8 famílias de assets [21]. A família usada nas imagens
das Figuras 4 e 5 é denominada de "céu" e traz elementos lúdicos
sobre unicórnios. A Figura 6 ilustra mais detalhes dessa família,
bem como a paleta de cores adotada nela.
Após a conclusão da implementação do jogo, serão conduzidas
novas avaliações com os profissionais da instituição parceira, bem
como estudos de caso longitudinais com crianças e adolescentes
de tal instituição, a fim de verificar a efetividade do jogo para o
objetivo pedagógico ao qual foi criado.
Destaca-se que o protótipo da interface Web da plataforma de
autoria, local em que os profissionais irão criar o jogo do pensa-
mento computacional – com as imagens de seu interesse e para os
jogadores de seu interesse, também foi modelada, materializada e
avaliada pelos profissionais da instituição parceira. Tal interface
também está em fase de implementação, mas, este conteúdo não é o
foco deste artigo. Entretanto, para fins de entendimento, a Figura 7
ilustra o protótipo de duas das telas de criação do jogo na interface
Web da plataforma.
CONSIDERAÇÕES FINAIS
Este artigo descreveu parte das atividades de um projeto mais amplo,
que visa disponibilizar um jogo digital sério de apoio a profissionais
da Educação no exercício de habilidades relacionadas aos pilares do
PC em crianças no espectro autista. Estão sendo adotadas práticas
do Design Participativo [34], de modo que partes interessadas no
projeto possam participar do processo de construção da solução.
WebMedia’2024, Juiz de Fora, Brazil
Katherin Felipa Carhuaz Malpartida and Kamila Rios da Hora Rodrigues
[a]
[b]
Figura 4: Versão preliminar do jogo funcional - Fase 1[a],
Fase 2[b].
Figura 5: Versão preliminar da avaliação da experiência do
jogo.
Figura 6: Elementos da família de elementos gráficos usados
no jogo ilustrado nas Figuras 4 e 5.
A literatura apresenta outros jogos similares, no entanto, o difer-
encial desta proposta é permitir que o profissional personalize o
Figura 7: Exemplos da tela de configuração de Níveis do
jogo na interface Web de autoria. Fonte: Malpartida and
da Hora Rodrigues [19]
jogo para adequar às necessidades específicas de cada jogador. Além
disso, os dados de interação com o jogo são enviados aos profission-
ais para que os mesmos possam analisar e conduzir intervenções
que julgarem necessárias, por exemplo.
Quanto às limitações e desafios da pesquisa, é possível consid-
erar que residem principalmente no público-alvo dos profissionais
e também desta pesquisa, cuja participação requer atenção especial
em termos de suporte, comunicação e adaptação às necessidades
individuais. Além disso, a participação das crianças deve ser con-
sentida pelas mesmas e há um desafio em conquistar a confiança
desse público, bem como em mantê-lo engajado. Além disso, a etapa
de avaliação se concentrará especificamente na interação das cri-
anças com o jogo e no exercício de habilidades do PC, sem avaliar
diretamente o desenvolvimento dessas habilidades ao longo do
tempo. Essas limitações, embora desafiadoras, representam uma
oportunidade de pesquisa mais profunda, com uma abordagem
interdisciplinar e centrada no usuário.
Como trabalho em andamento, o grupo está aperfeiçoando a im-
plementação do jogo prototipado que já traz mudanças agregadas a
partir da avaliação das interfaces, uma delas é a possibilidade de
inserir feedback sonoro nas instruções do jogo. Uma vez concluída
a implementação, serão realizados estudos de caso longitudinais
(médio prazo) com crianças no espectro do autismo da mesma insti-
tuição, para avaliar o exercício de habilidades de PC, especialmente
no contexto das atividades de vida diária.
AGRADECIMENTOS
Este trabalho foi financiado pela Coordenação de Aperfeiçoamento
de Pessoal de Nível Superior – Brasil (CAPES) – Código de Finan-
ciamento 001. Os autores agradecem aos profissionais e alunos dos
grupos de pesquisa e equipe de desenvolvimento do projeto RUFUS
durante o processo de design deste estudo, bem como pelos comen-
tários e sugestões de melhoria. Da mesma forma, agradecem aos
professores da instituição parceira que avaliaram o protótipo do
jogo para dispositivos móveis. Agradecem também à Comissão de
Cultura e Extensão (CCEx) do Instituto de Ciências Matemáticas e
de Computação (ICMC) da USP, pelo apoio financeiro por meio de
editais de projetos de extensão.
Jogos Digitais Sérios usados para o Exercício de Habilidades do Pensamento Computacional em Crianças
com Transtorno do Espectro Autista
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30120.txt ---

--- INÍCIO DO ARQUIVO: 30121.txt ---
Middleware para Aplicações Distribuídas de Vídeo com Suporte à
Computação na Borda na Indústria 4.0
Otacílio de A. Ramos Neto
otacilio.ramos@ifpb.edu.br
Instituto Federal da Paraíba (IFPB)
Guarabira/PB, Brasil
Rafael C. Chaves
rafael.chaves@polodeinovacao.ifpb.edu.br
Instituto Federal da Paraíba (IFPB)
João Pessoa/PB, Brasil
Alysson P. Nascimento
alysson.nascimento@polodeinovacao.ifpb.edu.br
Instituto Federal da Paraíba (IFPB)
Guarabira/PB, Brasil
Ruan D. Gomes
ruan.gomes@ifpb.edu.br
Instituto Federal da Paraíba (IFPB)
João Pessoa/PB, Brasil
0.
In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe-
dia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.
© 2024 SBC – Brazilian Computing Society.
ISSN 2966-2753
Em ambientes industriais, os dispositivos de captura de vídeo fre-
quentemente possuem capacidade de processamento insuficiente,
o que torna necessário transmitir as imagens para dispositivos com
recursos de hardware mais adequados. Nesse cenário, é essencial
que a rede ofereça uma alta taxa de transmissão para permitir a
distribuição adequada de vídeos em tempo real. Geralmente, as re-
des cabeadas são a opção mais viável. No entanto, uma alternativa
menos invasiva, mais flexível e que pode atender às necessidades
de taxa de bits e latência são as redes 5G privadas [17], que permi-
tem reduzir a complexidade da implantação de novos dispositivos
de vídeo, além de viabilizar a conexão de sistemas que possuem
alguma mobilidade. Além disso, para aplicações de controle e classi-
ficação na indústria, há restrições na latência de respostas, exigindo
que o processamento dos dados seja o mais próximo possível da
borda da rede [13]. Dessa forma, é importante que a localização do
equipamento responsável pelo processamento seja a mais próxima
possível do local de atuação, dificultando o uso da computação em
nuvem (Cloud Computing). Nesse contexto, a computação na borda
(Edge Computing) se mostra uma alternativa mais adequada, sendo
que a combinação de Edge Computing e redes 5G pode oferecer
vantagens significativas na indústria 4.0, como altas taxas de bits,
baixa latência, instalação pouco invasiva e computação próxima
da aplicação, sem a necessidade de estar fisicamente integrada ao
equipamento [17].
Para desenvolver um sistema de análise de vídeo que atenda aos
requisitos de diferentes aplicações de visão computacional, é neces-
sário implementar um sistema distribuído que integre os elementos
de captura, transmissão e processamento. Para uma implantação
adequada, esse sistema deve possuir características fundamentais,
como transparência, abertura e escalabilidade [16]. Transparência,
neste contexto, significa que os componentes do sistema são inte-
grados de forma que haja uma ilusão de que todos operam em uma
única maquina. A abertura, por sua vez, refere-se à capacidade do
sistema de facilitar a integração de novos componentes com pouca
ou nenhuma alteração. Uma forma de atingir a transparência em
sistemas distribuídos é a partir da utilização de um middleware,
que é uma camada de software que atua como intermediária entre
múltiplos componentes do sistema. Dependendo da sua implemen-
tação, essa camada pode facilitar a integração de novas aplicações,
promovendo um sistema distribuído mais aberto.
Nesse contexto, o middleware proposto neste trabalho apresenta
diversas características importantes para sistemas de análise de
WebMedia’2024, Juiz de Fora, Brazil
Otacílio de A. Ramos Neto, Rafael C. Chaves, Alysson P. Nascimento, and Ruan D. Gomes
vídeo. Entre elas, destacam-se a capacidade de replicar o fluxo
de vídeo de um produtor para múltiplos clientes, disponibilizar os
vídeos para aplicações através de dispositivos virtuais (/dev/video*) e
utilizar um protocolo de transmissão próprio que permite a entrega
de vídeos em altas resoluções, com diferentes lógicas de priorização
para a entrega dos quadros.
Por meio do middleware, diferentes aplicações de análise de vídeo
podem ser implementadas, seja consumindo um mesmo fluxo ou
fluxos distintos de vídeo, sem que seja necessário lidar com a com-
plexidade inerente à distribuição dos fluxos de vídeo e o acesso con-
corrente ao mesmo fluxo por diferentes aplicações, seja na mesma
máquina ou em máquinas distintas. Além disso, o middleware ofe-
rece funcionalidades que permitem automatizar de maneira mais
fácil a implantação ou migração das aplicações ao longo de uma
infraestrutura de computação distribuída, de modo a atingir aos
requisitos necessários de velocidade de processamento e latência.
Nesse sentido, a aquisição dos fluxos de vídeo a partir das aplicações
ocorre como se estas estivessem lendo a partir de um dispositivo
de vídeo correspondente a um equipamento conectado localmente
à máquina, independente do local onde a aplicação executa na
infraestrutura de computação distribuída.
Foram realizados experimentos para verificar o funcionamento
do middleware em um cenário inicial de computação na borda.
Verificou-se que ele consegue distribuir fluxos com resoluções de
até 4K em tempo real, sem provocar sobrecarga significativa, man-
tendo estabilidade na taxa de quadros entregue para as aplicações
responsáveis por analisar o vídeo. No experimento, foram avaliados
cenários utilizando UDP e TCP e também considerando as duas
lógicas distintas suportadas pelo middleware para a priorização da
entrega dos quadros (i.e. Last In, First Out e First In, First Out).
O restante deste artigo está organizado da seguinte forma: na
Seção 2 é descrita uma breve fundamentação teórica, abordando
conceitos de Edge Computing e protocolos pra transmissão de ví-
deo; na Seção 3 são descritos os trabalhos relacionados a este artigo
encontrados na literatura; na Seção 4 o middleware proposto neste
artigo é descrito; na Seção 5 são descritos os resultados obtidos
a partir de experimentos para avaliar o desempenho e o funcio-
namento do middleware; finalmente, na Seção 6 são descritas as
conclusões e as perspectivas de trabalhos futuros.
FUNDAMENTAÇÃO TEÓRICA
A implementação do conceito de Cloud Computing viabilizou o
uso sob demanda da infraestrutura de data centers, permitindo o
processamento intensivo de dados, provendo escalabilidade e dispo-
nibilizando hardware robusto [4]. Nesse contexto, a computação em
nuvem tem sido aplicada no processamento massivo de dados, prin-
cipalmente em sistemas de análise de vídeo que fornecem insights
valiosos a partir da aplicação de técnicas de visão computacional.
No entanto, essa abordagem enfrenta desafios significativos em
razão da quantidade de dados gerados. Como exemplo, uma câmera
com resolução HD (High Definition) pode produzir até 200 GB de
vídeo não comprimido por dia [8]. Essa grande quantidade de da-
dos, se transmitida em larga escala, pode levar a problemas mais
frequentes de sobrecarga de rede e de hardware nas centrais de
processamento [18]. Além disso, a computação em nuvem impõem
limitações para aplicações de análise de vídeo devido à latência adi-
cionada durante a transmissão dos dados e questões relacionadas à
privacidade [7, 13].
O surgimento do conceito de Edge Computing representa uma
evolução desse paradigma, atuando como um complemento à com-
putação em nuvem e provendo recursos para processamento inten-
sivo na borda da rede [13]. Essa estratégia é especialmente eficiente
para o processamento de dados de mídia em tempo real, pois alivia
a pressão sobre os recursos da nuvem e reduz a latência de trans-
missão [2, 7]. Por outro lado, também existem desafios ao utilizar
a computação na borda. Os dispositivos mais adequados para o
processamento de vídeo são as Graphics Processing Units (GPUs) [8],
projetadas principalmente para execução em servidores ou desktops.
Embora existam GPUs para aplicações embarcadas, como as da
linha Jetson da NVIDIA1, sua capacidade limitada (quando compa-
rada com outras GPGPUs para servidores) e custo ainda impedem
que sejam uma solução definitiva. Como alternativa, o processa-
mento pode ser transferido para servidores de borda, o que exige
a transmissão dos vídeos através da rede local. Isso exige que a
rede de acesso dos dispositivos seja capaz de prover a taxa de bits
necessária e não adicione uma latência excessiva.
Em relação às tecnologias de transmissão de vídeo, diversos pro-
tocolos têm sido estudados e desenvolvidos, se diferenciando quanto
aos objetivos e funcionalidades implementadas. Protocolos como
o DASH (Dynamic Adaptive Streaming over HTTP) [14] e o HLS
(HTTP Live Streaming) [10] foram construídos sobre o protocolo
HTTP, com o objetivo de priorizar a escalabilidade e a qualidade
da experiência do usuário, ambos apresentando funcionalidades de
adaptação da taxa de bits de acordo com o estado da rede. Por outro
lado, o WebRTC [5] é um conjunto de protocolos e APIs, desen-
volvido para oferecer suporte completo a navegadores e permitir
a realização de videoconferências. Embora esses protocolos sejam
amplamente utilizados, eles foram projetados para a transmissão e
exibição de vídeos na internet, e não são adequados para o cenário
considerado neste artigo, que possui foco na distribuição de vídeo
para processamento de sinais e visão computacional.
Quanto aos protocolos utilizados para transmissão em tempo
real, pode-se destacar o RTMP (Real Time Messaging Protocol) [15],
que é construído sobre o TCP e, devido a isso, herda as característi-
cas de controle de congestionamento e confiabilidade, o tornando
ideal para cenários de transmissão para a nuvem ou em redes locais
sobrecarregadas; no entanto, pode não aproveitar todo o potencial
em outros cenários. Nesse contexto, surgem os protocolos cons-
truídos sobre UDP, que priorizam transmissões de baixa latência.
Entre eles, pode-se citar o RTP (Real-time Transport Protocol) [11],
um protocolo bem conhecido e amplamente utilizado, e o SRT (Se-
cure Reliable Transport) [12], que é um protocolo recente e bastante
promissor. O SRT é implementado sobre o protocolo UDP e provê
mecanismos para controle de fluxo e controle de congestionamento,
entre outras funcionalidades.
Apesar de RTP e SRT serem alternativas interessantes, optou-se
neste trabalho pela implementação e utilização de um protocolo
próprio com o objetivo de permitir maior flexibilidade na configura-
ção do protocolo de transporte utilizado, bem como na estratégia de
priorização na entrega dos quadros, seja FIFO (First-In, First-Out) ou
1https://www.nvidia.com/pt-br/autonomous-machines/embedded-systems/
Middleware para Aplicações Distribuídas de Vídeo com Suporte à Computação na Borda na Indústria 4.0
WebMedia’2024, Juiz de Fora, Brazil
LIFO (Last-In, First-Out). Essa flexibilização permitiu avaliar o im-
pacto de cada uma das configurações no desempenho do middleware
em um cenário inicial de computação na borda. Em trabalhos futu-
ros pretende-se realizar um estudo experimental para comparar o
desempenho das estratégias implementadas no middleware descrito
neste artigo com os protocolos RTP e SRT.
TRABALHOS RELACIONADOS
Com a crescente adoção do conceito de Indústria 4.0 nos proces-
sos produtivos, diversas pesquisas têm abordado a integração de
sistemas de visão computacional nas linhas de produção. Além
disso, o surgimento e utilização de novos paradigmas como Edge
Computing e Cloud Computing têm viabilizado o desenvolvimento
de sistemas de análise de vídeo de forma distribuída. Nesse sen-
tido, nos parágrafos seguintes são descritos alguns trabalhos que
foram considerados relevantes e que reforçam a importância do
tema abordado no presente artigo.
Em [20] é apresentada uma revisão da literatura sobre o cres-
cente papel da visão computacional nos processos da indústria de
manufatura. Os autores descrevem o fluxo de aplicação dessas tec-
nologias na linha de produção, abrangendo etapas como aquisição
de imagens, aplicação de algoritmos e atuação com base nas infor-
mações obtidas. Além disso, são discutidas as principais técnicas
utilizadas, tais como detecção de características, reconhecimento
de objetos, segmentação e modelagem 3D. Por fim, são abordadas
diversas aplicações nas áreas de modelagem de produtos, planeja-
mento, produção, controle de qualidade, montagem e transporte.
Embora o artigo não trate diretamente sobre aspectos de implemen-
tação de sistemas de análise de vídeo, ele contribui para o reforçar
a importância de tais sistemas na indústria atual.
Em [1] é descrito um conjunto de aplicações que formam um
pipeline para análise de vídeos em tempo real. O sistema é composto
por diversos módulos que gerenciam os fluxos das câmeras conec-
tadas e realizam diversas ações, tais como identificar os melhores
parâmetros (p.ex: taxa de quadros, resolução e posição das câmeras)
e gerenciar os recursos disponíveis por meio da redistribuição do
processamento entre a nuvem e a borda. Para realizar a prova de
conceito o sistema foi aplicado no monitoramento de cruzamen-
tos de ruas, identificando o volume de tráfego e alertando sobre
padrões anômalos de trânsito. Como resultado, os autores obser-
varam que o sistema identificou com 95% de acurácia o volume de
tráfego nos locais monitorados, utilizando menos de 15% do poder
de processamento de um servidor quad-core para cada fluxo criado.
Apesar de abordar desafios como transmissão de fluxos de vídeo e
redistribuição de processamento, o artigo não descreve como tais
funcionalidades foram implementadas. Porém, a apresentação de
propostas de aplicações nesse sentido reforça a necessidade de mid-
dlewares que facilitem a construção de sistemas de análise de vídeo
de forma distribuída.
Em [6] é descrito um middleware para distribuição de vídeo para
aplicações de processamento de vídeo na borda, baseado no para-
digma produtor-consumidor, com armazenamento de dados. Uma
característica importante do middleware é o acesso através de uma
API simples, permitindo que a aplicação consumidora configure pa-
râmetros e resoluções dos fluxos de vídeo, equilibrando a latência de
transmissão e a acurácia dos métodos utilizados. Para caracterizar
o comportamento da latência na rede, os autores implementaram
um testbed utilizando duas NVIDIA Jetson TX2, um servidor de
borda x86 equipado com uma GPU e um ponto de acesso Wi-Fi
(802.11ac 5GHz). Durante os testes, identificaram que a latência
cresce quase linearmente em relação ao tamanho dos frames e que
a transmissão de dois fluxos simultâneos pode aumentar a latência
em até 164%. O middleware proposto por [6] se destaca por possi-
bilitar o ajuste da qualidade da transmissão e acesso por meio de
uma API simplificada. Por outro lado, o middleware apresentado
no presente artigo oferece a transmissão de forma transparente,
permitindo que as aplicações consumidoras recebam os fluxos como
se fossem câmeras conectadas fisicamente à máquina que realiza o
processamento do fluxo.
Em [8] é descrito o sistema Vision Edge IoT (VEI), um Edge ga-
teway multi-cloud projetado para aplicações de análise e proces-
samento de vídeo na borda. O VEI atua como um middleware de
distribuição de vídeo e também como um gateway para a publica-
ção dos resultados em serviços de nuvem, por meio de uma API
simplificada. O middleware do VEI é baseado em um sistema de pu-
blicação/assinatura (Pub/Sub), permitindo a replicação de um fluxo
de vídeo para múltiplas aplicações clientes. Os autores realizaram
diversos experimentos para avaliar o desempenho do VEI utilizando
diferentes aplicações de processamento de vídeo, incluindo testes
com múltiplos clientes simultâneos. Os resultados demonstraram
que a latência permaneceu estável mesmo com a carga adicional de
múltiplos clientes.
O middleware apresentado em [8] possui semelhanças com a so-
lução proposta neste trabalho, porém, difere no formato da interface
disponibilizada. Enquanto o VEI permite sua utilização através de
chamadas de API no código da aplicação cliente, a solução proposta
no presente artigo permite o acesso ao fluxo de vídeo da rede como
uma câmera diretamente conectada à máquina consumidora. Uma
vantagem dessa abordagem é permitir a utilização de aplicações
clientes sem que seja necessária nenhuma modificação, não impor-
tando se ela está localizada no dispositivo de captura, na nuvem
ou no servidor de borda. Além disso, a replicação dos vídeos por
meio de dispositivos virtuais oferece outra vantagem, uma vez que é
possível instanciar múltiplos clientes para processamento do vídeo
em uma mesma máquina, sem que haja a necessidade de transmi-
tir o fluxo individualmente para todos os clientes. No middleware
proposto no presente artigo apenas um fluxo é transmitido para
cada máquina, e o middleware replica esse fluxo para os dispositivos
virtuais que são acessados pelos clientes distintos.
As soluções encontradas em nosso levantamento não implemen-
tam os conceitos de transparência de localização e transparência
de concorrência em um sistema distribuído. De forma sucinta, a
transparência de localização é a capacidade que um sistema dis-
tribuído possui de ocultar a localização geográfica de um recurso
fazendo parecer que ele é local. Já a transparência de concorrência
é a capacidade de ocultar que um recurso é compartilhado por vá-
rios usuários ao mesmo tempo [16]. Para isso, as fontes de vídeo
remotas devem se apresentar como se fossem câmeras locais na
mesma máquina onde executa a aplicação cliente, sendo esse o
principal diferencial da solução proposta no presente artigo. Da
mesma forma, para que o sistema de distribuição de vídeo possua
transparência de concorrência, vários clientes devem poder acessar
WebMedia’2024, Juiz de Fora, Brazil
Otacílio de A. Ramos Neto, Rafael C. Chaves, Alysson P. Nascimento, and Ruan D. Gomes
o fluxo de vídeo ao mesmo tempo, seja em uma mesma máquina
ou em máquinas diferentes.
DESCRIÇÃO DO MIDDLEWARE PROPOSTO
O primeiro passo para a criação da solução foi decidir se a funci-
onalidade de distribuição de vídeo seria integrada aos drivers dos
dispositivos de captura ou implementada como um software sepa-
rado. Considerando que adicionar essa funcionalidade aos drivers
contraria seu propósito principal (funcionar como interface entre o
hardware e o sistema operacional) e que outros recursos de rede,
como sistemas de arquivos, são compartilhados por meio de soft-
ware externo (p.ex: o Network File System - NFS), optou-se por criar
um middleware, que captura as imagens a partir das interfaces for-
necidas pelos drivers e as distribui para as máquinas clientes usando
interfaces padronizadas do sistema operacional.
O middleware proposto neste artigo foi criado com o objetivo de
prover duas funcionalidades principais. A primeira é a transmissão
e distribuição de vídeos satisfazendo os conceitos de transparên-
cia de localização e transparência de concorrência. A segunda é
a flexibilidade com relação à entrega dos quadros, com a opção
de priorizar a entrega do quadro mais recente em detrimento da
retransmissão de quadros mais antigos (lógica LIFO) ou utilizando
uma abordagem em que quadros antigos não são descartados de
imediato quando um quadro novo torna-se disponível (lógica FIFO).
Estas funcionalidades influenciaram as decisões de projeto tanto da
arquitetura quanto do protocolo de comunicação.
4.1
Arquitetura e Modos de Funcionamento
O middleware é o responsável por integrar todos os componentes de
um sistema de análise de vídeo, sendo sua principal característica a
flexibilidade para se adaptar às diferentes estruturas de distribuição.
Esse sistema é composto por três módulos fundamentais, como
ilustrado na Figura 1: o Capture, instalado no dispositivo de captura
para encapsular os quadros em pacotes e enviar o fluxo de vídeo
ao servidor de distribuição; o Decapture, instalado na máquina de
processamento para remontar os quadros e disponibilizar o fluxo
de vídeo para as aplicações através de dispositivos de vídeo virtuais
(/dev/video*); e o servidor de distribuição, que funciona como o
núcleo do middleware, e faz a distribuição dos fluxos de vídeo.
Figura 1: Arquitetura do middleware.
Por meio dos dispositivos de vídeo virtuais, as aplicações podem
consumir os fluxos de forma simples, como se estivessem lendo
de uma câmera local, devido à abstração provida pelo middleware.
Diferentes aplicações também podem consumir o mesmo fluxo
em simultâneo para realizar diferentes tipos de processamento.
Como exemplo, a Figura 2 mostra um fluxo sendo recebido em
um computador e aberto em simultâneo pelos softwares VLC e
Scilab, por meio dos dispositivos de vídeo virtuais criados pelo
middleware. Essa flexibilidade, aliada à possibilidade de replicação e
distribuição dos fluxos de vídeo para diferentes máquinas, permite
uma maior facilidade no desenvolvimento e execução das aplicações.
Isto é importante, por exemplo, em cenários de Indústria 4.0, que
empregam recursos de computação na borda para o processamento
de vídeo.
Figura 2: Fluxo sendo aberto em simultâneo por dois softwa-
res distintos utilizando a abstração provida pelo middleware.
Em uma arquitetura de Edge Video Analysis (EVA), representada
na Figura 3, parte do middleware (Decapture e Servidor de Distri-
buição) está instalada no servidor de borda para processamento,
enquanto o Capture está instalado no dispositivo que provê o fluxo
de vídeo. Por exemplo, em um cenário de redes 5G privadas, a
máquina de borda poderia ser a mesma máquina que executa as
funções de rede da RAN (Radio Access Network), o que permiti-
ria minimizar a latência em cenários em que há a necessidade de
integração com sistemas de controle. Na figura, considerou-se a
execução do Decapture na mesma máquina que executa o servi-
dor de distribuição. No entanto, o Decapture poderia ser executado
em outras máquinas conectadas na mesma rede local, permitindo
assim a execução distribuída das aplicações na infraestrutura de
computação na borda.
Por outro lado, em uma arquitetura de Cloud Video Analysis
(CVA), representada na Figura 4, o servidor de distribuição pode es-
tar localizado tanto na nuvem quanto na rede local dos dispositivos
de captura, enquanto o módulo de Decapture deve estar presente
em uma máquina na nuvem. Também é possível realizar o pro-
cessamento dos fluxos combinando infraestrutura de computação
na borda, para tarefas que requerem baixa latência e geração de
mensagens de volta para os dispositivos finais, e computação na
nuvem para tarefas que não requerem baixa latência.
Como um exemplo de utilização do middleware, pode-se consi-
derar uma aplicação para controle de qualidade da produção na
indústria, em que um conjunto de câmeras são posicionadas ao
longo do processo produtivo com o objetivo de identificar defei-
tos ocorridos durante o processo. Os fluxos obtidos a partir dessas
câmeras podem ser processados em servidores na borda e gerar co-
mandos de controle (p.ex: para separação automatizada de produtos
defeituosos). Por meio da utilização do middleware descrito neste
artigo, as aplicações que realizam a análise dos frames podem ser
implementadas de maneira mais simples, sem se preocupar com a
complexidade relacionada à captura do vídeo no dispositivo final e
Middleware para Aplicações Distribuídas de Vídeo com Suporte à Computação na Borda na Indústria 4.0
WebMedia’2024, Juiz de Fora, Brazil
Figura 3: Arquitetura para soluções que utilizam Edge Com-
puting.
Figura 4: Arquitetura para soluções que utilizam Cloud Com-
puting.
à distribuição do vídeo para processamento nos servidores de borda.
Assim, o foco maior no desenvolvimento da aplicação se concentra
nos algoritmos de visão computacional. Além disso, pode-se execu-
tar diferentes instâncias das aplicações em diferentes máquinas de
modo a aumentar o desempenho (p.ex: cada instância processa uma
região dos quadros ou aplica um algoritmo diferente para identifica-
ção de defeitos). Por meio do middleware torna-se possível também
automatizar a instanciação das aplicações e a configuração neces-
sária para que os fluxos de vídeo sejam encaminhados para cada
instância, garantindo a criação dos dispositivos virtuais de vídeo a
serem abertos pelas aplicações nas máquinas de processamento.
4.2
Protocolo de Comunicação
Optou-se pelo desenvolvimento e utilização de um protocolo pró-
prio para avaliar a influência de diversas configurações no desem-
penho do middleware. Entre elas estão a estratégia de priorização
na entrega dos frames, que pode ser importante para sistemas de
controle em tempo real, e a flexibilidade na escolha de protocolo de
transporte (UDP ou TCP), permitindo a adaptação do middleware
para cenários diversos, a depender dos requisitos das aplicações e
das características da rede.
Frequentemente, as ações dos dispositivos de controle devem ser
baseadas nas informações mais recentes. Nesse contexto, um proto-
colo que realize a entrega dos quadros utilizando uma abordagem
First in, First Out (FIFO) pode introduzir um atraso no sistema de-
vido à entrega dos quadros sempre na mesma ordem em que foram
capturados e enviados. Em contraste, existe a opção de recuperar
os quadros com uma abordagem Last In, First Out (LIFO), o que
garante que os quadros mais recentes disponíveis serão priorizados
para envio às aplicações.
Em cenários de processamento na borda, usualmente em redes
sem sobrecarga, a utilização do UDP pode prover baixa latência
sem prejudicar a entrega dos pacotes. Por outro lado, em cenários
onde o processamento é feito na nuvem ou onde há uma sobrecarga
da rede, o TCP pode ser mais indicado, devido às características de
controle de congestionamento.
O procolo de comunicação implementado possui dois tipos de
pacotes com tamanho total de 1472 bytes. O pacote do Tipo 1 pos-
sui um cabeçalho com cinco campos: assinatura (2 bytes), canal de
comunicação (1 byte), tipo do pacote (1 byte), tamanho da carga
útil (2 bytes), número de sequência (8 bytes), segundo do instante
de envio (8 bytes) e nanossegundo do instante de envio (8 bytes). O
pacote do Tipo 2 possui os mesmos campos do pacote do Tipo 1 e
mais dois campos: número do fragmento (2 bytes) e número total de
fragmentos (2 bytes), que são utilizados no procedimento de remon-
tagem de quadros que passaram pelo processo de fragmentação por
serem maiores que a carga máxima de um pacote do Tipo 1. Uma
ilustração do formato dos pacotes é apresentada na Figura 5.
Figura 5: Campos do protocolo de comunicação.
O algoritmo de envio funciona da seguinte forma: inicialmente
ele testa se o bloco de dados cabe em um único pacote do Tipo 1.
Se sim, ele registra o instante de tempo e envia o pacote do Tipo 1,
caso contrário ele calcula quantos pacotes do Tipo 2 são necessários
e executa um laço para enviar todos os fragmentos do quadro em
pacotes do Tipo 2. Os campos de sequência, tipo, canal, tv_sec,
tv_nsec e assinatura também são preenchidos antes do envio.
A recepção dos pacotes é feita por uma thread que fica bloqueada
aguardando a chegada de dados da rede. No momento em que ela
recebe um pacote, ele é imediatamente lido, para evitar descarte, e
adicionado na fila de recepção. A remontagem dos quadros é feita
na thread principal, utilizando um algoritmo baseado em uma janela
deslizante sobre o número de sequência dos quadros, à medida que
os pacotes são entregues.
A fila usada para armazenamento durante a remontagem cresce
conforme os pacotes de novos quadros são recebidos. Caso a fila
atinja seu tamanho máximo, ela começa a “deslizar” descartando
quadros antigos até liberar espaço suficiente para adicionar no final
da fila o novo quadro cujo pacote inédito foi recebido. Se um pacote
recebido for de um quadro mais antigo que o quadro mais antigo
atualmente na fila ele é descartado. Se o pacote for de um quadro
que está na janela da fila, ele é adicionado na posição do quadro
correspondente. Quando um quadro é completado ele é enviado
WebMedia’2024, Juiz de Fora, Brazil
Otacílio de A. Ramos Neto, Rafael C. Chaves, Alysson P. Nascimento, and Ruan D. Gomes
para o decodificador e todos os mais antigos que ele são removidos.
Por fim, caso o pacote seja de um quadro mais novo do que o mais
recente, a fila cresce para acomodar o novo quadro. Se em algum
momento a fila de remontagem entrar em um estado em que todas
as suas posições estão ocupadas, então o quadro mais antigo é
descartado para liberar espaço para a remontagem de um novo
quadro.
A Figura 6 ilustra a janela de quatro quadros no processo de
remontagem. Nela, os quadros 1 e 2 já foram remontados ou des-
cartados e não estão na janela. Os quadros 3, 4 e 6 já receberam
3, 2 e 1 pacotes de dados, respectivamente. O quadro 7 ainda não
teve nenhum pacote entregue. Caso algum seja entregue antes do
quadro 3 ser completado, este será descartado para que a janela
possa deslizar para a direita e acomodar o novo quadro cujo pacote
foi recebido.
Figura 6: Ilustração da fila de remontagem deslizando sobre
o número de sequência dos quadros.
AVALIAÇÃO DE DESEMPENHO E
FUNCIONAMENTO
A avaliação do sistema foi feita a partir de três métricas de desem-
penho: quadros por segundo (fps), taxa de bits do fluxo de vídeo
e latência dos quadros. Escolhemos quadros por segundo por ser
usada como métrica para hardware de processamento vídeo, taxa
de bits por se usada como métrica de capacidade de transmissão
de uma rede e latência por indicar quanto tempo o quadro levou
para ser transmitido. Além dessas métricas, também foi realizada a
contagem dos quadros descartados para verificar o funcionamento
do algoritmo de priorização de quadros. A avaliação foi feita para
as quatro configurações possíveis de transmissão do middleware:
TCP-FIFO, TCP-LIFO, UDP-FIFO e UDP-LIFO, para 14 diferentes
resoluções de imagem. O uso de FIFO ou LIFO diz respeito à forma
como o algoritmo de remontagem retira os pacotes da fila de re-
cepção. No caso do FIFO os quadros são remontados priorizando
a ordem em que são enviados, já no caso do LIFO os quadros são
remontados com priorização dos pacotes dos quadros mais recentes
que foram recebidos.
A bancada de testes foi organizada para uma aplicação de EVA,
conforme a Figura 7, em que as imagens foram capturadas a uma
taxa de 28 quadros por segundo no formato MJPEG por uma câ-
mera Logitech BRIO ligada a uma placa Jetson Nano, executando
Ubuntu 18.04. Os quadros foram transmitidos para um notebook
com arquitetura AMD64, executando o sistema operacional Ubuntu
22.04. O notebook faz o papel de servidor de processamento en-
quanto a placa Jetson Nano executa os componentes do middleware
responsáveis pela captura e distribuição dos fluxos de imagens para
os clientes.
Figura 7: Arquitetura usada na avaliação da capacidade de
distribuição e remontagem dos quadros.
As medições foram feitas durante 10 minutos para cada configu-
ração, sendo que as máquinas mantiveram seus relógios ajustados
por meio do software ptp4l. Para todos os cenários foi considerada
uma captura a partir da câmera a 28 fps.
O resultado das medições para a métrica de quadros por segundo
(fps) é apresentado na Figura 8. A partir desta figura, percebe-se
que o sistema exibiu um desempenho muito próximo para todas as
configurações, sem diferenças relevantes, desde a resolução mais
baixa (160x120, QSIF) até a mais alta (4096x2160, DCI4K). Neste caso
o sistema conseguiu entregar os quadros no destino com a mesma
taxa de quadros em que elas foram capturadas, demonstrando que,
para essa taxa de quadros, o middleware não adiciona sobrecarga
significativa.
Figura 8: Taxa de quadros por segundo para cada configura-
ção.
A Figura 9 exibe os resultados para a métrica de taxa de bits.
Tal como na métrica de quadros por segundo, o middleware exibe
desempenho semelhante para todas as configurações, sendo que
as variações estão dentro da margem de tolerância calculada como
sendo o desvio médio em relação ao valor médio da taxa de bits
para uma mesma resolução.
Os resultados para a métrica de latência na entrega dos quadros
são apresentados na Figura 10. Para este último caso, nota-se que
existe uma diferença perceptível para a transmissão de quadros em
resoluções iguais ou maiores que Full-HD nas configurações diferen-
tes. Esta diferença advém do aumento considerável na quantidade
Middleware para Aplicações Distribuídas de Vídeo com Suporte à Computação na Borda na Indústria 4.0
WebMedia’2024, Juiz de Fora, Brazil
Figura 9: Taxa de bits por segundo para cada configuração.
de pixels de uma resolução para outra, impactando no algoritmo de
remontagem dos quadros.
Comparando os resultados exibidos no gráfico, percebe-se que
as configurações usando UDP apresentam uma menor latência até
a resolução 2560x1440, com diferenças mais significativas para as
resoluções 1920x1080 e 2560x1440. Para as resoluções 3840x2160 e
4096x2160, o TCP apresentou melhores resultados. Para as resolu-
ções menores que 1920x1080 o uso de FIFO ou LIFO, tanto com TCP
como UDP, não apresentou diferenças significativas na latência.
Entretanto, para o uso de TCP, em resoluções maiores ou iguais a
1920x1080 a remoção por LIFO apresentou vantagens perceptíveis.
Já para UDP o uso de FIFO nestas resoluções proporcionou uma
latência média menor. No entanto, ao considerar o desvio padrão,
não é possível afirmar que o uso de FIFO ou LIFO provoca diferença
significativa na latência. A latência menor do TCP nas resoluções
maiores é um resultado que foge do esperado, de modo que uma
análise mais cuidadosa será realizada em trabalhos futuros para
entender melhor este resultado.
Figura 10: Latência para cada configuração.
No geral, percebe-se que no cenário avaliado, o middleware funci-
onou adequadamente e conseguiu entregar no destino um fluxo de
vídeo com a mesma taxa de quadros capturada a partir da câmera,
para todas as resoluções avaliadas, incluindo a DCI 4K. No entanto,
existem algumas diferenças em termos de latência que são visíveis
na Figura 10 para as diferentes configurações. Em cenários de distri-
buição mais complexos, considerando redes com possibilidades de
rotas distintas e a distribuição dos vídeos para múltiplas máquinas,
diferenças mais significativas entre os resultados obtidos para cada
uma das quatro configurações possíveis poderão ser observadas.
Isso será avaliado em trabalhos futuros.
O outro aspecto que o experimento de medição de desempenho
analisou é a capacidade do middleware de descartar quadros que
são mais antigos que os quadros mais novos e já remontados. Os
resultados obtidos são apresentados nas figuras 11, 12 e 13, sendo
que não existe gráfico correspondente para a configuração TCP-
FIFO porque neste caso o controle de congestionamento do TCP, a
retransmissão de pacotes e a retirada dos pacotes da fila de recepção
na mesma ordem em que foram enviados, fazem com que o des-
carte de quadros pelo middleware seja zero em todas as resoluções,
ficando a cargo da aplicação cliente realizar descartes caso seja
necessário.
Figura 11: Número de quadros descartados para 10 minutos
de transmissão usando TCP-LIFO.
Figura 12: Número de quadros descartados para 10 minutos
de transmissão usando UDP-FIFO.
Observando as figuras 11, 12 e 13 percebe-se claramente o funci-
onamento do mecanismo de descarte. Na configuração UDP-FIFO
só houve descarte nas três maiores resoluções e o número foi muito
baixo (≈0, 05% de quadros descartados no período de 10 minutos).
Isso mostra que nesse caso o descarte só ocorreu quando os quadros
não puderam ser remontados devido à perda de pacotes na rede. Já
nas configurações 11 e 13 tem-se um número maior de descartes,
sendo o máximo ocorrido na resolução 1920x1080 no TCP-LIFO
WebMedia’2024, Juiz de Fora, Brazil
Otacílio de A. Ramos Neto, Rafael C. Chaves, Alysson P. Nascimento, and Ruan D. Gomes
Figura 13: Número de quadros descartados para 10 minutos
de transmissão usando UDP-LIFO.
(≈2, 98% de quadros descartados). Esse resultado se deve ao fato de
que os pacotes são retirados da fila de recepção priorizando o mais
recente, fazendo com que o sistema remonte o quadro mais recente
primeiro e descarte os quadros mais antigos. Em geral, para esse
cenário simples de experimentação, não foi possível estabelecer um
padrão para o comportamento desta métrica de acordo com a re-
solução ou o modo de configuração. Diferenças mais significativas
devem ser observadas em cenários mais complexos de avaliação, o
que será alvo de trabalhos futuros.
CONCLUSÕES E TRABALHOS FUTUROS
Este artigo apresenta um novo middleware para dar suporte ao
desenvolvimento de aplicações distribuídas de vídeo, tendo como
foco aplicações para Indústria 4.0 em um cenário de computação
na borda. O middleware é capaz de capturar, replicar, distribuir e
entregar fluxos de vídeo utilizando interfaces de vídeo virtuais gené-
ricas (/dev/video*) do sistema operacional Linux. Além disso, foram
apresentadas as métricas de desempenho, como taxa de quadros
por segundo, taxa de bits e latência, obtidas em uma bancada de
testes com um cenário inicial de computação na borda.
Observou-se que para todas as resoluções, incluindo a resolução
4K, o middleware entregou para as aplicações uma taxa de quadros
estável e muito próxima à capacidade máxima da câmera (≈28 fps),
para todas as configurações de uso permitidas pelo middleware (i.e.
TCP-FIFO, TCP-LIFO, UDP-FIFO e UDP-LIFO). Esses resultados
indicam uma baixa sobrecarga causada pelo mecanismo de distri-
buição. Dessa forma, o middleware permite o desenvolvimento de
aplicações de análise de vídeo de maneira simplificada, ao mesmo
tempo em que garante a distribuição e o acesso simultâneo por
múltiplas aplicações aos fluxos de vídeo gerados. As aplicações
precisam apenas abrir o dispositivo de vídeo virtual criado pelo
middleware para ter acesso ao fluxo de vídeo de entrada.
Como trabalhos futuros, o sistema será testado em duas novas
bancadas de teste conectadas por meio de uma rede 5G privada e
uma rede Wi-Fi 6, em cenários de Indústria 4.0. Por fim, também
serão avaliados outros cenários de distribuição de processamento
para investigar o desempenho e a escalabilidade do middleware em
cenários mais complexos de distribuição, considerando múltiplos
servidores de borda para processamento e redes de transporte mais
complexas.
AGRADECIMENTOS
Este trabalho é parcialmente apoiado pela EMBRAPII e pelas empre-
sas Cisco, Prysmian e MPT Cable. Os autores também agradecem
ao CNPq (305536/2021-4), ao IFPB e ao Polo de Inovação do IFPB.

--- FIM DO ARQUIVO: 30121.txt ---

--- INÍCIO DO ARQUIVO: 30123.txt ---
O Impacto de Estratégias de Embeddings de Grafos na
Explicabilidade de Sistemas de Recomendação
André Levi Zanon
andrezanon@usp.br
Universidade de São Paulo
São Carlos, São Paulo, Brasil
Leonardo Rocha
lcrocha@ufsj.edu.br
Universidade Federal de São João
del-Rei
São João del Rei, Minas Gerais, Brasil
Marcelo Garcia Manzato
mmanzato@icmc.usp.br
Universidade de São Paulo
São Carlos, São Paulo, Brasil
WebMedia’2024, Juiz de Fora, Brazil
André Levi Zanon, Leonardo Rocha, and Marcelo Garcia Manzato
versão 2 (ExpLOD v2) [20] e o Proposed Property-based Explana-
tion Model (PEM) [10]). O objetivo dessa análise é responder nossa
segunda pergunta de pesquisa: QP2: As estratégias baseadas
em embeddings de grafos são, de fato, capazes de gerar ex-
plicações melhores em SsR em comparação às abordagens
sintáticas?.
Portanto, as principais contribuições deste trabalho são:
• Análise comparativa entre estratégias de embedding de gra-
fos e algoritmos sintáticos na qualidade das explicações ge-
radas em SsR;
• Análise comparativa do impacto de diferentes estratégias de
embedding de grafos na qualidade das explicações geradas
para SsR;
• Criação de um arcabouço completo para avaliação de estraté-
gias de geração de explicações para SsR baseadas em grafos
de conhecimento.
Focando primeiramente na QP1, nossos resultados deixam claro
que modelos bilineares, capazes de representar relações mais com-
plexas entre nós e arestas, impactaram positivamente nas métricas
de qualidade de explicação. Com relação à QP2, observamos que
enquanto métodos sintáticos priorizam a recência dos itens e popula-
ridade de atributos escolhidos nas explicações, estratégias de embed-
dings conseguem balancear o trade-off entre a popularidade e diver-
sidade de atributos de itens nas explicações mostradas aos usuários.
O artigo está estruturado da seguinte forma: A Seção 2 revisa
trabalhos relacionados a algoritmos de SsR explicáveis e baseados
em GCs. A Seção 3 detalha o arcabouço de avaliação focado em
reprodutibilidade, incluindo a configuração experimental, métricas,
conjuntos de dados e algoritmos. A Seção 4 discute os resultados,
e a Seção 5 resume as descobertas.
TRABALHOS RELACIONADOS
Uma vez que GC fornecem metadados estruturados sobre itens, eles
têm sido utilizados para gerar recomendações precisas e explicáveis
em diversas arquiteturas de recomendação. Explicações em GC são
geradas a partir da associação entre itens interagidos e recomen-
dados com atributos compartilhados. Na literatura existem duas
abordagens de algoritmos explicativos pós-hoc ou agnósticas ao
modelo com CG: uma em que as recomendações são reordenadas
com base nas melhores explicações para um item recomendado e
outra em que somente as explicações são geradas [22].
Considerando algoritmos de reordenação agnósticos ao modelo
utilizando GC, [2] utilizou três métricas de otimização - atuali-
dade de itens interagidos, popularidade e diversidade de atributos
extraídos de caminhos de explicação do GC - para reordenar reco-
mendações. Por sua vez, [34] reordenou recomendações avaliando
a relevância dos atributos extraídos de caminhos de explicação ao
comparar a frequência de associações de atributos com itens intera-
gidos e com o catálogo de itens. Além disso, [14] gerou explicações
por meio de extração de aspectos e análise de sentimento, aumen-
tando a precisão da recomendação ao incorporar avaliações textuais
como regularizador de algoritmo de recomendação. No entanto, nes-
ses trabalhos, as abordagens propostas avaliaram explicações com
base exclusivamente em métricas como precisão e diversidade.
Considerando arquiteturas GC pós-hoc para gerar explicações,
[19] criou um algoritmo chamado ExpLOD, que alavanca explica-
ções de um GC com base em um grafo bipartido que conecta itens
interagidos com recomendados pelos atributos que compartilham.
As explicações são classificadas com base em uma adaptação da
métrica Term-Frequency Inverse Document Frequency (TF-IDF), onde
os nós de item são documentos e os nós de atributo são termos. Este
trabalho foi avaliado comparando a explicação proposta com infor-
mações extraídas do GC em um experimento online, onde o método
alcançou percepção do usuário melhorada considerando objetivos
de explicação. [20] estendeu o ExpLOD [19], adicionando atributos
mais amplos da hierarquia GC. O mesmo experimento online foi
conduzido, mas as explicações do ExpLOD foram comparadas com
a nova versão proposta. Os usuários preferiram explicações com
atributos mais amplos ao analisar sob a perspectiva de objetivos de
explicação. Mais recentemente, [10] propôs o Modelo de Explicação
Baseado em Propriedades (PEM), uma função de pontuação que clas-
sifica atributos com base em suas conexões com itens interagidos
e o catálogo completo de itens. PEM superou a segunda versão do
ExpLOD em experimentos online, estabelecendo-se como o estado-
da-arte para algoritmos explicativos de GC agnósticos ao modelo.
Entretanto, essas abordagens são sintáticas e não consideram in-
trinsecamente a estrutura e o caminho do GC para gerar explica-
ções. Para suprir essa lacuna, em [35] foi criado um algoritmo de
explicação agnóstico ao sistema de recomendação utilizando repre-
sentações vetoriais de GC e comparado com abordagens sintáticas
utilizando métricas offline de qualidade de explicação. No entanto,
diferentes maneiras de gerar representações vetoriais de grafos não
foram exploradas visto que somente um modelo para a geração dos
embeddings de grafos foi utilizado no algoritmo de explicação.
A avaliação de explicações também tem recebido atenção, já que
as explicações são principalmente avaliadas com base em testes de
usuários online, que são demorados e custosos para validar as estra-
tégias. Em [6], métricas offline foram implementadas para avaliar
recomendações explicáveis, no entanto, avaliando a robustez dos
algoritmos explicativos medindo o número de itens que podem ser
explicáveis para os usuários e o número de interações do usuário
relacionadas às explicações. Por outro lado, alguns trabalhos tam-
bém consideram a diversidade e relevância dos atributos exibidos
nas explicações [2, 27] embora a relação entre tais métricas e testes
online seja definida. Além disso, métricas offline não são padroni-
zadas, e trabalhos que avaliam com estudos de usuários carecem
de avaliação quantitativa.
Assim, SsR explicáveis frequentemente não avaliam as explica-
ções de forma quantitativa e qualitativa, já que modelos agnósticos
de reordenação com GC contribuem com métricas de precisão e
diversidade. Por outro lado, explicações com GC agnósticas ao mo-
delo são avaliadas com testes de usuário online, que são custosos e
limitados à avaliação do número de participantes e, como resultado,
não são extensivamente avaliados de maneira offline.
MATERIAIS E MÉTODOS
3.1
Visão Geral
A fim de responder a QP1 e analisar como diferentes algoritmos
de embeddings de grafos impactam na geração de explicações ag-
nósticas a modelo em SsR, apresentamos um arcabouço que gera
O Impacto de Estratégias de Embeddings de Grafos na Explicabilidade de Sistemas de Recomendação
WebMedia’2024, Juiz de Fora, Brazil
explicações para recomendações a partir de embeddings de grafos
gerados pelos algoritmos TransE [18], ComplEx [32] e RotatE [29].
O caminho mostrado como a explicação escolhida é dado pela maior
similaridade entre dois embeddings: o do caminho e o do usuário. O
embedding do caminho é composto pela soma dos embeddings dos
nós e arestas que conectam um ou mais nós de itens interagidos
pelo usuário com um nó de item recomendado. Já o embedding do
usuário é composto pela somatória dos embeddings dos nós dos
itens interagidos. Na Seção 3.2 detalhamos as estratégias baseadas
em embeddings.
Para responder a QP2, comparamos as abordagens de embedding
de grafos com três algoritmos de estado-da-arte para explicações sin-
táticas em SsR. Nosso objetivo é verificar se abordagens baseadas em
embeddings performam melhor que abordagens sintáticas. Os algo-
ritmos sintáticos implementados foram: ExpLOD [19], ExpLOD ver-
são 2 (ExpLOD v2) [20] e o Proposed Property-based Explanation Mo-
del (PEM) [10]. Os três utilizam estratégias para balancear a quanti-
dade de referências que nós de atributos possuem entre itens intera-
gidos e recomendados para escolher o caminho mais relevante para
uma explicação. Na Seção 3.3 detalhamos essas abordagens. Na Se-
ção 3.4 detalhamos as métricas propostas por [2] que medem a diver-
sidade de atributos, a popularidade dos mesmos e a recência de itens
dentro das explicações para avaliar a qualidade das explicações.
As estratégias avaliadas são pós-hoc e, portanto, independem
dos SsR. Em nossa avaliação consideramos sete algoritmos de re-
comendação baseados em diferentes abordagens: Mais Popular [7]
para recomendações não personalizadas, o algoritmo PageRank
Personalizado [19] aumentado com o grafo Wikidata para reco-
mendações baseadas em grafo, User-KNN [24] para algoritmos
de vizinhança, Embarrassingly Shallow AutoEncoder (EASE) [28] e
Bayesian Personalized Ranking Matrix Factorization (BPR-MF) [23]
para algoritmos não neurais, e Neural Collaborative Filtering (NCF)
[16] para arquiteturas baseadas em redes neurais. Utilizamos a bi-
blioteca CaseRecommender [8] para implementar os algoritmos
Mais Popular, User-KNN e BPR-MF. As implementações dos outros
recomendadores, juntamente com os algoritmos de explicação, mé-
tricas e consultas para extrair as triplas para a construção do GC,
estão disponíveis em um repositório de código aberto1, sendo
essa uma de nossas contribuições nesse trabalho.
Nas avaliações, consideramos os conjuntos de dados MovieLens
100k [15] e da LastFM [4] para as explicações das Top-5 recomenda-
ções de seis algoritmos de SsR para todos os usuários. Assim, para
os itens mais bem ranqueados de cada algoritmo de recomendação,
todos os sete (quatro de embedding e três sintáticos) algoritmos de
explicação agnósticos ao modelo foram executados a fim de obter as
métricas de qualidade de explicação. Considerando as orientações
de reprodutibilidade em SsR propostas [12] e robustez da avalia-
ção de explicação destacados [30], e para garantir uma avaliação
rigorosa, aplicamos seis algoritmos de SsR de diferentes famílias,
utilizando 90% do conjunto de dados para treinamento e 10% para
teste para gerar as explicações.
Utilizamos um GC da Wikidata Linked Open Data (LOD) para os
domínios de filmes e artistas musicais para implementar todos os al-
goritmos avaliados. Os dados processados permaneceram com 99%
das interações originais para o conjunto de dados MovieLens 100k e
1https://github.com/andlzanon/lod-personalized-recommender
89% para o conjunto de dados LastFM. Um resumo das estatísticas do
MovieLens 100k e do LastFM antes e depois do pré-processamento
e as informações do GC estão disponíveis na Tabela 1.
MovieLens
LastFM
Conjunto
Original
usuários
1,892
itens
9,724
17,632
interações
100,836
92,834
Conjunto
Processado
usuários
1,875
itens
9,517
11,641
interações
100,521
83,017
Wikidata
GC
entidades
78,703
34,297
triplas
295,787
134,197
tipos de
arestas
Tabela 1: Estatísticas dos conjuntos de dados originais e pro-
cessados, e informações do GC quanto ao número de entida-
des, triplas e quantidade de arestas.
3.2
Abordagens Baseadas em Embedding
Explicações em GC agnósticas a modelo tem por objetivo descobrir
qual o caminho mais relevante que conecta um item interagido a
um recomendado. A equação 1 define formalmente esse objetivo
em que para todos os caminhos 𝑐no conjunto de caminhos 𝐶que
conectam um item interagido a um recomendado, uma função de
agregação (𝑎𝑔𝑔) - como média e soma - é aplicada sob a relevância
𝑟𝑒𝑙de cada nó 𝑛em 𝑐.
𝑎𝑟𝑔𝑚𝑎𝑥(∀𝑐∈𝐶: 𝑎𝑔𝑔(𝑟𝑒𝑙(𝑛)∀𝑛∈𝑐)
(1)
A Figura 1 ilustra a abordagem de embeddings de GC agnóstica
ao modelo. Os nós interagidos pelo usuário no GC são os azuis, os
nós de atributo são representados em amarelo, e o nó em vermelho
é o do item recomendado. As mesmas cores se aplicam aos vetores
que representam os embeddings desses nós. O vetor preto repre-
senta um embedding de relações entre nós, representadas por uma
seta de mesma cor na Figura 1.
Dois embeddings são necessários para calcular a relevância 𝑟𝑒𝑙
de um caminho de explicação: o do usuário e o do caminho. O em-
bedding do usuário, que é calculado por um pooling de soma dos
embeddings dos itens interagidos; e o embedding do caminho, que,
por sua vez, é um pooling de soma de todos os embeddings de itens,
atributos e relações do caminho que conecta um nó de item intera-
gido pelo usuário a um nó de item recomendado no GC. As Equações
2 e 3 exibem o cálculo para cada um dos embeddings onde 𝐼é o
conjunto de nós de itens interagidos pelo usuário, e 𝑃é o conjunto
de nós de itens, relações e atributos em um caminho. O método
𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔retorna o embedding do nó passado como parâmetro.
Os caminhos foram extraídos usando o algoritmo de Dijkstra [9].
𝑒𝑚𝑏𝑒𝑑(𝑢𝑠𝑒𝑟) =
∑︁
𝑖∈𝐼
𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔(𝑖)
(2)
𝑒𝑚𝑏𝑒𝑑(𝑝𝑎𝑡ℎ) =
∑︁
𝑛∈𝑃
𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔(𝑛)
(3)
WebMedia’2024, Juiz de Fora, Brazil
André Levi Zanon, Leonardo Rocha, and Marcelo Garcia Manzato
Figura 1: Estrutura do modelo proposto. Os nós azuis
representam os itens interagidos pelo usuário, os nós
amarelos representam nós de atributo, e o nó vermelho é
o nó do item recomendado. O mesmo esquema de cores se
aplica aos vetores gerados pelos algoritmos de embeddings.
Vetores pretos representam o embeddings de arestas. O
símbolo Í representa uma operação de pooling de soma, e
∼é a similaridade de cosseno entre dois embeddings, 𝑟𝑒𝑙é
a saída da função de similaridade de cosseno.
O caminho de explicação escolhido é aquele com a maior simi-
laridade com o embedding do usuário entre todos os caminhos que
conectam pelo menos um item interagido ao item recomendado.
Nesse sentido, a similaridade de cosseno do embedding do usuário
com os embeddings do caminho computa essa proximidade. O com-
primento máximo do caminho foi definido como 5, e o número de
itens interagidos por explicação, de 3, seguindo a mesma configura-
ção dos baselines. O valor da similaridade é representado na Equação
4, onde 𝑒𝑚𝑏𝑒𝑑(𝑢𝑠𝑒𝑟) é o embedding do usuário e 𝑒𝑚𝑏𝑒𝑑(𝑝𝑎𝑡ℎ) é o
embedding do caminho.
𝑠𝑖𝑚(𝑢𝑠𝑒𝑟, 𝑝𝑎𝑡ℎ) =
𝑒𝑚𝑏𝑒𝑑(𝑢𝑠𝑒𝑟).𝑒𝑚𝑏𝑒𝑑(𝑝𝑎𝑡ℎ)
||𝑒𝑚𝑏𝑒𝑑(𝑢𝑠𝑒𝑟)||.||𝑒𝑚𝑏𝑒𝑑(𝑝𝑎𝑡ℎ)||
(4)
A seleção do caminho de explicação respeita a Equação 5, em que,
para todos os caminhos de explicação dentro do conjunto 𝑃𝑎𝑡ℎ𝑠
que ligam um nó de item interagido a um nó de item recomendado
por atributos compartilhados entre eles, o embedding do caminho
com a maior similaridade de cosseno com o embedding do usuário
é o escolhido.
𝑎𝑟𝑔𝑚𝑎𝑥(∀𝑝𝑎𝑡ℎ∈𝑃𝑎𝑡ℎ𝑠𝑠𝑖𝑚(𝑢𝑠𝑒𝑟, 𝑝𝑎𝑡ℎ))
(5)
O Algorithm 1 é o pseudocódigo do algoritmo de explicação
baseado em embedding e utiliza de três parâmetros: os itens de
histórico do usuário (𝑝𝑟𝑜𝑓𝑖𝑙𝑒_𝑖𝑡𝑒𝑚𝑠), o item recomendado a ser
explicado (𝑟𝑒𝑐_𝑖𝑡𝑒𝑚) e o modelo de embbedding de grafos treinado
(𝑒𝑚𝑏𝑒𝑑_𝑚𝑜𝑑𝑒𝑙). Na linha 2 o embbedding do usuário é criado a
partir da soma dos embbedding dos nós dos itens interagidos e que
são retornados pelo modelo de embbedding de grafos.
Em seguida, na linha 3, todos os caminhos de um item interagido
ao recomendado são obtidos utilizando o algoritmo de Dijkstra [9].
Como no algoritmo proposto o embbedding do usuário é comparado
aos embbeddings dos caminhos, as linhas 4 e 5 inicializam variáveis
que serão responsáveis por armazenar a similaridade máxima atual
e o caminho de embbedding mais similar ao do usuário.
As linhas 6 a 14 representam a geração dos embbeddings dos
caminhos e a comparação com o embbedding do usuário. Dessa
forma, para cada caminho, o embbedding é gerado somando os
embeddings dos nós que o compõem. Portanto, 𝑐.𝑛𝑜𝑠() retorna
uma lista de nós do caminho 𝑐e 𝑒𝑚𝑏𝑒𝑑_𝑚𝑜𝑑𝑒𝑙(𝑐.𝑛𝑜𝑠()) retorna os
embbeddings desses nós. Em seguida, a similaridade de cosseno
entre os embbeddings do usuário e do caminho é realizada na linha
8. Quando esse valor é maior que o máximo, tanto o valor máximo,
quanto o caminho que corresponde a essa similaridade máxima, são
atualizados nas linhas 10 e 11. Na linha 14 o algoritmo retorna o
caminho de embbedding com maior similaridade com o embbedding
do usuário.
Algorithm 1 Geração de Explicação com Embeddings
1: function Caminho_Embedding(profile_items, rec_item, em-
bed_model)
2:
𝑢𝑠𝑒𝑟_𝑒𝑚𝑏𝑒𝑑←𝑠𝑢𝑚(𝑒𝑚𝑏𝑒𝑑_𝑚𝑜𝑑𝑒𝑙(𝑝𝑟𝑜𝑓𝑖𝑙𝑒_𝑖𝑡𝑒𝑚𝑠))
3:
𝑐𝑎𝑚𝑖𝑛ℎ𝑜𝑠←𝑑𝑖𝑗𝑘𝑠𝑡𝑟𝑎(𝑝𝑟𝑜𝑓𝑖𝑙𝑒_𝑖𝑡𝑒𝑚𝑠,𝑟𝑒𝑐_𝑖𝑡𝑒𝑚)
4:
𝑚𝑎𝑥←−1
5:
𝑐𝑎𝑚_𝑚𝑎𝑥←[]
6:
for c in caminhos do
7:
𝑐𝑎𝑚𝑖𝑛ℎ𝑜_𝑒𝑚𝑏𝑒𝑑←𝑠𝑢𝑚(𝑒𝑚𝑏𝑒𝑑_𝑚𝑜𝑑𝑒𝑙(𝑐.𝑛𝑜𝑠()))
8:
𝑠𝑖𝑚←𝑐𝑜𝑠𝑠𝑒𝑛𝑜(𝑐𝑎𝑚𝑖𝑛ℎ𝑜_𝑒𝑚𝑏𝑒𝑑,𝑢𝑠𝑒𝑟_𝑒𝑚𝑏𝑒𝑑)
9:
if 𝑠𝑖𝑚> 𝑚𝑎𝑥then
10:
𝑚𝑎𝑥←𝑠𝑖𝑚
11:
𝑐𝑎𝑚_𝑚𝑎𝑥←𝑐
12:
end if
13:
end for
14:
return 𝑐𝑎𝑚_𝑚𝑎𝑥
15: end function
A proposta de embedding foi feita gerando embeddings dos GCs
extraídos da Wikidata LOD para os domínios de filmes e artísti-
cos das bases de dados MovieLens e LastFM, respectivamente. A
escolha dos algoritmos de embeddings utilizados foi realizada con-
siderando as diferentes famílias de algoritmos. Enquanto o TransE
[18] e RotatE [29] utilizam a abordagem translacional, o algoritmo
ComplEX [32], por sua vez, é um algoritmo bilinear. O algoritmo
TransE foi comparado ao RotatE para verificar se a evolução do
estado da arte na representação de embeddings de grafos em uma
família de algoritmos, melhora métricas de qualidade de explicação.
Modelos translacionais se baseiam no conceito de coordenadas
cartesianas [5, 36] em que, considerando uma tripla (ℎ,𝑟,𝑡), onde
ℎe 𝑡são nós no GC e 𝑟é a relação que conecta os dois nós, uma
transformação linear que representa a distância entre esses elemen-
tos gera a função de custo a ser minimizada, assim, ℎ+ 𝑟≈𝑡. O
RotatE usa a equação de distância 𝑑𝑟(ℎ,𝑡) = ||ℎ◦𝑟−𝑡||, já o TransE
utiliza a função 𝑑𝑟(ℎ,𝑡) = ||ℎ+𝑟−𝑡||. O símbolo ◦denota o produto
elemento a elemento. Modelos bilineares, por sua vez, medem a
similaridade de ℎ, 𝑟e 𝑡utilizando produtos interno e operações
multiplicativas, geralmente envolvendo formas bilineares [17].
Para o treinamento dos modelos de embedding de grafos foi rea-
lizada a otimização de parâmetros em que a taxa de aprendizado (𝜆)
O Impacto de Estratégias de Embeddings de Grafos na Explicabilidade de Sistemas de Recomendação
WebMedia’2024, Juiz de Fora, Brazil
foi variada entre os valores 0.1 e 0.001, e o tamanho do batch (𝐵) para
atualização dos parâmetros foi de 128 e 256. O tamanho do embed-
ding (𝐾) também foi otimizado entre 200 e 400. A utilização ou não
de amostragem negativa também foi variada. Quando existia a amos-
tragem negativa, 10 exemplos negativos por amostra positiva foram
gerados para um batch. A quantidade de épocas para o treinamento
foi fixa de 40 para todos os modelos. Como o modelo ComplEX [32]
usa o algoritmo de Stochastic Gradient Descent com AdaGrad [11]
como otimizador, a taxa de aprendizado é ajustada no treinamento.
As triplas do GC foram divididas em conjuntos de treinamento,
validação e teste na proporção de 0.8, 0.1 e 0.1, respectivamente.
No GC extraído da Wikidata para os itens do conjunto de dados
MovieLens, 235,466 triplas foram utilizadas para treino, 29,434 para
validação e 29,433 para teste. Já para o GC extraído para o LastFM,
a separação treino, validação e teste foi de 101,516, 12,690 e 12,689,
respectivamente. A biblioteca Pykeen [1] foi utilizada para imple-
mentar os modelos de embeddings de grafo. A acurácia do modelo
é medido pela métrica de Hit Rate que mede o quão bem o modelo
encontra o nó que completa uma tripla. Assim, dado um embedding
de nó ℎe um embedding de relação 𝑟, o modelo deve encontrar o em-
bedding de nó 𝑡correto correspondente a tripla (ℎ,𝑟,𝑡) presente no
grafo. As métricas de Hit Rate no conjunto de teste para os modelos
de melhores parâmetros para os embeddings de GC extraído da Wi-
kidata para o MovieLens e LastFM estão apresentadas na Tabela 2.
Os melhores modelos obtiveram os seguintes parâmetros: Para
o conjunto de dados MovieLens 100k, no modelo TransE 𝐾foi de
200, 𝜆de 0.001, 𝐵de 256 e sem a presença de amostragem negativa;
para o modelo ComplEX 𝐾foi de 400, 𝐵de 128 e com a presença de
amostragem negativa; no modelo RotatE 𝐾foi de 200, 𝜆de 0.001, 𝐵
de 128 e com a presença de amostragem negativa. Para o conjunto
de dados LastFM, no modelo TransE 𝐾foi de 200, 𝜆de 0.001, 𝐵
de 256 e sem a presença de amostragem negativa; para o modelo
ComplEX 𝐾foi de 200, 𝐵de 128 e sem a presença de amostragem
negativa; no modelo RotatE 𝐾foi de 200, 𝜆de 0.001, 𝐵de 128 e com
a presença de amostragem negativa.
TransE
RotatE
ComplEX
MovieLens
H@1
0.0317
0.0982
0.0115
H@3
0.0956
0.1595
0.0209
H@5
0.1282
0.1927
0.0261
H@10
0.1727
0.2418
0.0362
LastFM
H@1
0.0570
0.1852
0.0029
H@3
0.1135
0.2725
0.0076
H@5
0.1481
0.3154
0.0118
H@10
0.1998
0.3735
0.0219
Tabela 2: Métricas do conjunto de teste para diferentes al-
goritmos de embeddings de grafos para o GC dos conjuntos
de dados MovieLens e LastFM. 𝐻@𝑛é a métrica de Hit Rate
do modelo para completar corretamente uma tripla conside-
rando os 𝑛nós mais próximos.
3.3
Abordagens Sintáticas
Três algoritmos sintáticos foram implementados para comparação
com os quatro resultados obtidos por meio da aplicação do mé-
todo de embedding de grafos no arcabouço da descrito na Seção 3.2.
Diferentemente de métodos baseados em embeddings, abordagens
sintáticas utilizam a ocorrência do nó do atributo relacionado ao
nó do item para determinar sua relevância.
O método ExpLOD [19] classifica propriedades no Grafo de Co-
nhecimento (GC) usando uma abordagem adaptada de TF-IDF. A
relevância de um atributo é determinada pela frequência de re-
ferências ao atributo tanto de itens interagidos quanto de itens
recomendados em relação ao total de referências ao atributo entre
todos os itens. A Equação 6 ilustra o cálculo para o valor de rele-
vância de um atributo 𝑝, onde 𝑛𝑝,𝐼𝑢representa o número de links
do conjunto de itens interagidos pelo usuário 𝐼𝑢para o atributo 𝑝.
Da mesma forma, 𝑛𝑝,𝐼𝑟denota o número de links conectando o atri-
buto 𝑝aos itens recomendados 𝐼𝑟, e 𝐼𝐷𝐹(𝑝) significa a Frequência
Inversa do Documento de 𝑝, calculada como log( |𝐶|
𝑛𝑝,𝐼𝐶), onde |𝐶|
representa o número total de itens no catálogo e 𝑛𝑝,𝐼𝐶é o número
total de itens referenciando o atributo 𝑝. Os valores 𝛼e 𝛽são pesos
e foram definidos como 0.5 de acordo com [19].
𝑠𝑐𝑜𝑟𝑒_𝑒𝑥𝑝𝑙𝑜𝑑(𝑝, 𝐼𝑢, 𝐼𝑟) = (𝛼
𝑛𝑝,𝐼𝑢
|𝐼𝑢| ) + (𝛽
𝑛𝑝,𝐼𝑟
|𝐼𝑟| ) ∗𝐼𝐷𝐹(𝑝)
(6)
A Equação 7 exibe o cálculo para classificação de atributos do
ExpLOD v2 [20], que é muito similar à sua versão anterior; mas
também abrange atributos mais amplos. Por exemplo, considere o
filme ’La La Land’ de 2016, classificado com o atributo ’romance’
no GC do Wikidata. Essa classificação implica que o filme também
está associado a atributos mais amplos como ’relacionamento inter-
pessoal’ e ’amor’, já que ’romance’ é uma subclasse desses atributos.
Como resultado, para atributos do GC mais amplos 𝑏que têm sub-
classes, a relevância é a soma da Equação 6 para todos os atributos
𝑝𝑏𝑖no conjunto de 𝑃𝑐(𝑏) que são filhos de 𝑏, multiplicado pelo IDF
da classe mais ampla (𝐼𝐷𝐹(𝑏)). Portanto, os algoritmos ExpLOD
classificam atributos que são populares entre o conjunto de itens
interagidos, mas raros no conjunto de itens do catálogo.
𝑠𝑐𝑜𝑟𝑒_𝑒𝑥𝑝𝑙𝑜𝑑(𝑏, 𝐼𝑢, 𝐼𝑟) =
|𝑃𝑐(𝑏) |
∑︁
𝑖=1
𝑠𝑐𝑜𝑟𝑒_𝑒𝑥𝑝𝑙𝑜𝑑(𝑝𝑏𝑖, 𝐼𝑢, 𝐼𝑟) ∗𝐼𝐷𝐹(𝑏)
(7)
O mecanismo de pontuação usado no Modelo de Explicação Ba-
seado em Propriedades (PEM) é representado pela Equação 8 e,
diferentemente dos algoritmos ExpLOD, considera o número de
itens interagidos que referenciam o atributo em vez do número de
links. Para pontuar um atributo 𝑝, primeiramente, considera-se o
número de itens interagidos que referenciam o atributo |𝐼(𝑝, 𝐼𝑢)|,
onde 𝐼𝑢representa o conjunto de itens com os quais o usuário in-
teragiu. Esse valor é então normalizado pelo número total de itens
com os quais o usuário interagiu, denotado por |𝐼𝑢|.
Além disso, a equação considera o número de itens no catálogo
𝐶conectados ao atributo |𝐼(𝑝,𝐶)|. Semelhante ao termo anterior,
esse valor é normalizado pelo número total de itens no catálogo, de-
notado por |𝐶|. Finalmente, o logaritmo do número total de itens no
WebMedia’2024, Juiz de Fora, Brazil
André Levi Zanon, Leonardo Rocha, and Marcelo Garcia Manzato
catálogo conectados ao atributo log(|𝐼(𝑝,𝐶)|) é calculado para am-
plificar a importância de atributos relativamente raros no catálogo.
𝑠𝑐𝑜𝑟𝑒_𝑝𝑒𝑚(𝑝, 𝐼𝑢, 𝐼𝑟,𝐶) = |𝐼(𝑝, 𝐼𝑢)|/|𝐼𝑢|
|𝐼(𝑝,𝐶)|/|𝐶|
∗log(|𝐼(𝑝,𝐶)|)
(8)
Para os todos algoritmos o caminho com a maior média de rele-
vância de atributo é escolhido como explicação. Além disso, o com-
primento máximo do caminho foi definido como cinco, e o número
máximo de itens interagidos para um item recomendado foi três.
3.4
Métricas
A métrica de Recência da Interação de Conexão (𝐿𝐼𝑅) mede a recên-
cia dos itens interagidos pelo usuário que formam uma explicação;
a Popularidade da Entidade Compartilhada (𝑆𝐸𝑃) mede a populari-
dade dos atributos exibidos em explicações para um único usuário
e Diversidade do Tipo de Explicação (𝐸𝑇𝐷) o número de atributos
diferentes nas explicações. Assim, as métricas propostas por [2]
para avaliar a qualidade de explicação definem que explicações de
qualidade encontram caminhos diferentes (𝐸𝑇𝐷), mas utilizando
atributos populares (𝑆𝐸𝑃) e conectando itens recomendados com
itens recém interagidos (𝐿𝐼𝑅). Todas as métricas variam entre 0 e
1 em que 1 é o valor ótimo, exceto por ETD pode ser maior que 1
caso o caminho tenha mais que um atributo.
As Equações 9 e 10 representam as métricas de 𝐿𝐼𝑅e 𝑆𝐸𝑃, res-
pectivamente. Essas métricas são calculadas com base na média de
equações de média móvel exponencialmente ponderada normali-
zada para cada item interagido e atributo dentro de uma explicação.
Para 𝐿𝐼𝑅, os valores dos itens interagidos (𝑝𝑖) são calculados
usando seus respectivos carimbos de data/hora (𝑡𝑖), que são norma-
lizados pelo método min-max para variar entre 0 e 1. A natureza
recursiva da função garante que o valor de uma propriedade 𝑖de-
pende de 𝑖−1, com valores ordenados em ordem crescente de
carimbos de data/hora. Assim, 𝐿𝐼𝑅(𝑝1,𝑡1) equivale a 𝑡1. O parâ-
metro 𝛽é tipicamente definido como 0,3, como sugerido em [2].
Consequentemente, 𝐿𝐼𝑅atribui valores mais altos a explicações
que conectam recomendações com itens interagidos mais recentes.
𝐿𝐼𝑅(𝑝𝑖,𝑡𝑖) = (1 −𝛽) ∗𝐿𝐼𝑅(𝑝𝑖−1,𝑡𝑖−1) + 𝛽∗𝑡𝑖
(9)
Na Equação 10, a métrica de Popularidade da Entidade Compar-
tilhada (𝑆𝐸𝑃) quantifica a popularidade dos atributos considerando
o número 𝑣𝑖de nós de itens conectados ao atributo 𝑒𝑖. A ordenação
e a normalização min-max também são aplicadas ao número de
referências que um atributo têm a outros nós. Consequentemente,
𝑆𝐸𝑃(𝑒1, 𝑣1) corresponde a 𝑣1, representando o atributo com o me-
nor número de referências no GC. Valores altos de 𝑆𝐸𝑃indicam
que os atributos em explicações são populares.
𝑆𝐸𝑃(𝑒𝑖, 𝑣𝑖) = (1 −𝛽) ∗𝑆𝐸𝑃(𝑒𝑖−1, 𝑣𝑖−1) + 𝛽∗𝑣𝑖
(10)
Finalmente, a Equação 11 define a métrica de Diversidade do
Tipo de Explicação (𝐸𝑇𝐷), que quantifica a diversidade de atributos
em explicações associadas a recomendações. Ela calcula a razão do
número de propriedades na lista recomendada |𝜔𝐿𝑢| para o mínimo
entre o comprimento da lista de recomendação𝑘e o número total de
possíveis atributos 𝜔𝐿que poderiam formar uma explicação. 𝐸𝑇𝐷
fornece uma visão sobre a variedade de atributos apresentados em
explicações e ajuda a avaliar se o algoritmo de explicação tende a
favorecer atributos repetitivos. Valores mais altos de 𝐸𝑇𝐷indicam
uma maior diversidade de atributos.
𝐸𝑇𝐷(𝑆) =
|𝜔𝐿𝑢|
𝑚𝑖𝑛(𝑘, |𝜔𝐿|)
(11)
RESULTADOS
Para responder às questões de pesquisa QP1 e QP2, executamos os
algoritmos sintáticos ExpLOD, ExpLOD v2, PEM e a abordagem de
embedding com os algoritmos TransE, RotatE e ComplEX para todos
os usuários dos conjuntos de dados MovieLens 100k e LastFM para
os cinco principais itens recomendados dos algoritmos de recomen-
dação Mais Popular, BPR-MF, PageRank, UserKNN, EASE e NCF
considerando as métricas LIR, ETD e SEP. Os resultados para outras
métricas, tais como precisão e diversidade, e exemplos de explica-
ções gerados por cada um dos algoritmos estão disponíveis online2.
A Tabela 3 e Tabela 4 correspondem à média e o desvio padrão
das métricas de qualidade de explicação SEP, ETD e LIR para todos
os usuários considerando os conjuntos de dados MovieLens 100k
e LastFM, respectivamente, em relação aos cinco principais itens
recomendados de cada algoritmo. As duas primeiras colunas são
relativas ao algoritmo de recomendação executado e a métrica de
qualidade, em seguida, os resultados do método proposto consi-
derando três algoritmos de embedding diferentes estão nas três
primeiras colunas e os resultados dos três métodos sintáticos estão
nas últimas três colunas. Os valores em negrito são os mais altos
entre os algoritmos, e os valores sublinhados são os mais baixos.
Distinguimos os valores mais altos e mais baixos, porque os obje-
tivos de explicação e os atributos de qualidade exibem um trade-off.
Os atributos dos itens são distribuídos seguindo uma long-tail em
que alguns atributos são muito comuns entre poucos itens [13] e,
assim, quanto maior a quantidade de atributos mostrados em expli-
cações aos usuários, maior também é a probabilidade de um atributo
menos popular de ser escolhido em uma explicação [2, 3, 31].
A fim de responder a QP1 e diferenciar o impacto de diferentes al-
goritmos de embeddings na geração de explicações, as três primeiras
colunas das tabelas se referem os resultados das qualidade de ex-
plicações do método agnóstico ao modelo utilizando os algoritmos
de embedding de grafos TransE [18], RotatE [29] e ComplEX [32].
Considerando os métodos TransE [18] e RotatE [29], que perten-
cem a família de algoritmos translacionais de embedding de grafos,
as métricas de HitRate no conjunto de treinamento do modelo de
embedding de grafos na Tabela 2 evidenciam que o algoritmo RotatE
[29] é uma evolução no estado da arte para o modelo translacional
TransE [18]. Nesse sentido, no conjunto de dados LastFM esta evolu-
ção do estado-da-arte refletiu na evolução das métricas de qualidade
de explicação, em que tanto ETD quanto SEP obtiveram resultados
melhores para o método com o algoritmo RotatE em comparação ao
TransE. No entanto, o mesmo não ocorreu no dataset do MovieLens,
em que a melhoria das métricas de HitRate de um mesmo tipo de
modelo não necessariamente refletiu na evolução de métricas de
qualidade de explicação. Assim, mesmo modelos mais simples de
embedding de grafos conseguem gerar representações vetoriais que
refletem em explicações de qualidade.
2https://tinyurl.com/zfscfshv
O Impacto de Estratégias de Embeddings de Grafos na Explicabilidade de Sistemas de Recomendação
WebMedia’2024, Juiz de Fora, Brazil
TransE
RotatE
ComplEX
ExpLOD
ExpLOD v2
PEM
Mais Popular
LIR
0.03147 ± 0.12
0.0275 ± 0.11
0.0234 ± 0.09
0.0945 ± 0.15
0.0834 ± 0.14
0.0320 ± 0.07
ETD
0.6718 ± 0.21
0.6842 ± 0.20
0.9537 ± 0.31
0.5809 ± 0.19
0.5947 ± 0.19
0.9390 ± 0.12
SEP
0.52104 ± 0.18
0.6869 ± 0.13
0.5625 ± 0.15
0.6322 ± 0.14
0.6107 ± 0.12
0.1430 ± 0.13
Page Rank
LIR
0.0310 ± 0.11
0.0312 ± 0.12
0.0241 ± 0.10
0.0939 ± 0.15
0.0872 ± 0.15
0.0323 ± 0.08
ETD
0.7335 ± 0.21
0.6570 ± 0.22
0.9305 ± 0.31
0.5563 ± 0.20
0.6043 ± 0.19
0.9389 ± 0.12
SEP
0.4662 ± 0.20
0.7022 ± 0.15
0.5557 ± 0.16
0.6250 ± 0.16
0.5606 ± 0.15
0.1095 ± 0.11
UserKNN
LIR
0.03327 ± 0.12
0.0352 ± 0.12
0.0234 ± 0.10
0.1010 ± 0.14
0.0914 ± 0.13
0.0322 ± 0.08
ETD
0.7055 ± 0.24
0.6849 ± 0.22
0.9859 ± 0.33
0.6593 ± 0.19
0.6409 ± 0.19
0.9452 ± 0.11
SEP
0.5202 ± 0.23
0.2311 ± 0.15
0.6103 ± 0.16
0.5803 ± 0.16
0.5275 ± 0.14
0.131 ± 0.12
BPR-MF
LIR
0.0408 ± 0.16
0.0298 ± 0.11
0.0244 ± 0.09
0.1048 ± 0.14
0.0945 ± 0.13
0.0306 ± 0.07
ETD
0.6826 ± 0.26
0.6934 ± 0.24
0.9855 ± 0.32
0.6891 ± 0.19
0.6937 ± 0.19
0.9583 ± 0.10
SEP
0.5755 ± 0.23
0.2151 ± 0.15
0.5013 ± 0.16
0.6113 ± 0.14
0.5428 ± 0.14
0.1400 ± 0.12
EASE
LIR
0.0312 ± 0.12
0.0295 ± 0.11
0.0209 ± 0.09
0.1000 ± 0.14
0.0912 ± 0.14
0.03193 ± 0.08
ETD
0.7345 ± 0.24
0.6751 ± 0.23
0.9724 ± 0.31
0.6204 ± 0.20
0.6328 ± 0.19
0.9459 ± 0.11
SEP
0.4952 ± 0.49
0.6583 ± 0.17
0.6089 ± 0.16
0.5743 ± 0.17
0.5274 ± 0.15
0.1350 ± 0.13
NCF
LIR
0.0320 ± 0.13
0.0244 ± 0.09
0.0199 ± 0.08
0.1181 ± 0.13
0.1035 ± 0.12
0.0380 ± 0.08
ETD
0.6966 ± 0.29
0.8080 ± 0.25
1.0100 ± 0.32
0.8432 ± 0.16
0.8161 ± 0.16
0.9885 ± 0.05
SEP
0.6122 ± 0.22
0.2511 ± 0.14
0.3955 ± 0.14
0.5868 ± 0.14
0.5350 ± 0.13
0.1613 ± 0.11
Tabela 3: Tabela das métricas LIR, ETD e SEP para explicações dos cinco principais itens recomendados no conjunto de
dados MovieLens 100k. Valores em negrito são os mais altos e valores sublinhados são os mais baixos entre os algoritmos de
recomendação.
TransE
RotatE
ComplEX
ExpLOD
ExpLOD v2
PEM
Mais Popular
LIR
0.0104 ± 0.06
0.0100 ± 0.06
0.0123 ± 0.08
0.0182 ± 0.09
0.0189 ± 0.11
0.0143 ± 0.08
ETD
0.8247 ± 0.27
0.9319 ± 0.25
1.1394 ± 0.28
0.7023 ± 0.17
0.4927 ± 0.22
0.9212 ± 0.12
SEP
0.5084 ± 0.22
0.6617 ± 0.21
0.5181 ± 0.16
0.7097 ± 0.17
0.7537 ± 0.23
0.1214 ± 0.08
Page Rank
LIR
0.0108 ± 0.07
0.008 ± 0.06
0.0125 ± 0.07
0.0191 ± 0.10
0.0212 ± 0.12
0.0134 ± 0.07
ETD
0.7683 ± 0.25
0.8704 ± 0.30
1.0769 ± 0.32
0.6100 ± 0.20
0.5447 ± 0.19
0.9440 ± 0.10
SEP
0.4820 ± 0.18
0.6359 ± 0.17
0.5022 ± 0.18
0.6501 ± 0.21
0.7164 ± 0.21
0.1209 ± 0.09
UserKNN
LIR
0.0102 ± 0.07
0.0090 ± 0.05
0.0117 ± 0.07
0.0183 ± 0.10
0.0191 ± 0.11
0.0158 ± 0.08
ETD
0.7760 ± 0.26
0.8688 ± 0.31
1.0624 ± 0.32
0.5335 ± 0.20
0.5355 ± 0.18
0.9106 ± 0.14
SEP
0.5071 ± 0.19
0.6088 ± 0.18
0.4540 ± 0.18
0.5288 ± 0.26
0.2810 ± 0.22
0.1417 ± 0.10
BPR-MF
LIR
0.0110 ± 0.06
0.0096 ± 0.06
0.0113 ± 0.07
0.0191 ± 0.10
0.0204 ± 0.11
0.0164 ± 0.08
ETD
0.8403 ± 0.26
0.9219 ± 0.31
1.1021 ± 0.31
0.6145 ± 0.21
0.6196 ± 0.19
0.9450 ± 0.11
SEP
0.5312 ± 0.18
0.6002 ± 0.17
0.5984 ± 0.18
0.5605 ± 0.23
0.6302 ± 0.19
0.1759 ± 0.12
EASE
LIR
0.0102 ± 0.07
0.0092 ± 0.05
0.0111 ± 0.07
0.0188 ± 0.11
0.0194 ± 0.11
0.0154 ± 0.08
ETD
0.7934 ± 0.25
0.8753 ± 0.32
1.0707 ± 0.32
0.5474 ± 0.20
0.5585 ± 0.18
0.9246 ± 0.13
SEP
0.5026 ± 0.19
0.5870 ± 0.19
0.4639 ± 0.18
0.5307 ± 0.25
0.2861 ± 0.21
0.1466 ± 0.10
NCF
LIR
0.0098 ± 0.06
0.0106 ± 0.06
0.0131 ± 0.07
0.0182 ± 0.09
0.0162 ± 0.08
0.0162 ± 0.08
ETD
0.9409 ± 0.27
1.0318 ± 0.32
1.2057 ± 0.29
0.7775 ± 0.18
0.7867 ± 0.18
0.9589 ± 0.09
SEP
0.6302 ± 0.17
0.6509 ± 0.16
0.6153 ± 0.17
0.5904 ± 0.19
0.5514 ± 0.20
0.2748 ± 0.15
Tabela 4: Tabela das métricas LIR, ETD e SEP para explicações dos cinco principais itens recomendados no conjunto de dados
LastFM. Valores em negrito são os mais altos entre os algoritmos de recomendação e valores sublinhados são os mais baixos.
Nesse sentido, o algoritmo ComplEX [32], da família de algorit-
mos bilineares de embedding de grafos, obteve os resultados mais
consistentes para ambas as bases. Particularmente este algoritmo
obteve ETD acima de 0.65 e SEP acima de 0.45 para todas os con-
juntos de dados e algoritmos. Isso acontece, pois esses modelos
utilizam formas bilineares para gerar as representações vetoriais de
grafos, e isso possibilita a modelagem de padrões mais complexos
entre nós e arestas. Diferentemente, modelos translacionais geram
embeddings por meio da aproximação dos vetores realizando trans-
lações, limitando sua expressividade. Dessa forma, respondendo a
QP1, a utilização de um tipo de modelo de embedding que captura
WebMedia’2024, Juiz de Fora, Brazil
André Levi Zanon, Leonardo Rocha, and Marcelo Garcia Manzato
relações mais complexas entre nós e arestas refletiu na melhora de
métricas de qualidade de explicação.
Para responder a QP2 e comparar as diferenças entre abordagens
sintáticas e semânticas, consideramos também os modelos ExpLOD
[19], ExpLOD v2 [20] e PEM [10] (nas três últimas colunas da Tabela
3 e Tabela 4). Neste contexto, é possível perceber que, para ambos
os conjuntos de dados, os algoritmos sintáticos, especialmente os
algoritmos de ExpLOD e do ExpLOD v2, foram as melhores entre
todos os outros algoritmos para as métricas LIR e SEP. Por outro
lado, as abordagens de embeddings foram superiores na métrica
ETD de diversidade.
No caso do algoritmo RotatE, por exemplo, para os algoritmos
de Mais Popular e PageRank no conjunto de dados MovieLens e
UserKNN, BPR-MF, EASE e NCF no LastFM, as métricas de SEP e
ETD foram melhores quando comparadas à algoritmos sintáticos.
Isso indica a capacidade de métodos de embedding de fornecer ca-
minhos de explicação diversos para várias recomendações e manter
um certo nível de popularidade de atributos. No entanto, algoritmos
de embeddings também demonstraram menores níveis de LIR, o
que pode ser atribuído à sua metodologia de treinamento. Embora
identifique caminhos diversos para explicações de diferentes re-
comendações, o algoritmo incorpora o item interagido apenas no
processo de soma de pooling para gerar os embeddings do caminho
de explicação e do usuário. Em contraste, os três métodos sintáti-
cos priorizam a quantidade de nós de itens interagidos que estão
conectados a nós de atributos no GC para a escolha do caminho
mais relevante a ser mostrado como explicação.
A exceção é o algoritmo PEM, que reflete o comportamento de
trade-off entre diversidade e popularidade de atributos. Este al-
goritmo alcança uma diversidade muito alta, mas, em contraste,
possui a menor SEP para todas as métricas. Isso acontece porque, ao
contrário dos algoritmos ExpLOD, que são baseados em TF-IDF, o
PEM normaliza o número de itens que referenciam um atributo aos
itens interagidos pelo catálogo. Como o catálogo de itens é extenso,
o PEM é mais suscetível a exibir mais itens diversos.
Portanto, algoritmos sintáticos definem a relevância do caminho
de explicação como um trade-off entre o número de conexões dos
nós de atributos com nós de itens. Nesses algoritmos a explicação es-
colhida contém nós de atributos que estão conectados a muitos nós
de itens interagidos pelo usuário, mas pouco conectados a nós do
conjunto inteiro de itens. Em particular, os algoritmos ExpLOD [19]
e ExpLOD v2 [20] priorizam a popularidade, enquanto o PEM [10]
prioriza a diversidade. Já os modelos de embeddings são treinados na
tarefa de completar triplas dos grafos. Assim, considerando um nó
ℎe uma relação 𝑟, algoritmos de embedding de grafos aprendem a
encontrar o nó correto 𝑡da tripla (ℎ, 𝑟, 𝑡) do GC. Consequentemente,
algoritmos de explicação que empregam representações vetoriais
são balanceados considerando as métricas de SEP e ETD.
Respondendo a QP2, observamos que métodos sintáticos estão
mais propensos a serem influenciados pela popularidade, porque
escolhem explicações considerando o número de ligações de um
nó de atributo aos nós de itens. Por outro lado, os métodos base-
ados em embeddings escolhem caminhos de explicação a partir da
similaridade da representação vetorial de nós e arestas do grafo.
Logo, para estas estratégias, a popularidade de nós de atributos em
nós itens não interfere decisivamente na escolha da explicação a
um item recomendado, sendo, dessa maneira, mais equilibrados e,
consequentemente, melhores que métodos sintáticos.
CONCLUSÕES
Neste trabalho foi desenvolvida uma abordagem comparativa e
reprodutível, analisando o efeito de diferentes algoritmos de em-
beddings de grafos na geração de explicações de qualidade em algo-
ritmos agnósticos a modelos em SsR, utilizando GC. Dessa maneira,
por meio de três métricas de explicabilidade: a recência dos itens
e diversidade e popularidade de atributos, as explicações de cada
algoritmo foi avaliada para duas bases de dados e seis SsR.
Foi verificado que abordagens baseadas em embeddings conse-
guem balancear melhor a popularidade e diversidade de atributos
comparando com abordagens sintáticas. Além disso, métricas de
treinamento de diferentes métodos de embedding não necessaria-
mente refletem na melhoria nas métricas de qualidade de explicação.
Como trabalhos futuros, modelos geracionais podem ajudar na oti-
mização multi-parâmetro e considerar todas as três métricas de
qualidade, visto que algoritmos sintáticos de explicação priorizam
popularidade ou diversidade de atributos e modelos de embedding
não priorizam a recência dos itens.
AGRADECIMENTOS
Os autores agradecem à CAPES, CNPq, Fapesp, AWS e Fapemig
pelo financiamento e apoio a esta pesquisa.

--- FIM DO ARQUIVO: 30123.txt ---

--- INÍCIO DO ARQUIVO: 30125.txt ---
Quando as Avaliações Viram Bombas: Explorando a Dinâmica do
Review Bombing nos Jogos no Metacritic
Marcus Vinicius Guerra Ribeiro
marcus.guerra@ufv.br
Universidade Federal de Viçosa - UFV
Florestal, Minas Gerais
Clara Andrade Pimentel
clara.pimentel@cefetmg.br
CEFET-MG
Belo Horizonte, Minas Gerais
Philipe de Freitas Melo
philipe.freitas@ufv.br
Universidade Federal de Viçosa - UFV
Florestal, Minas Gerais
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
ato do review bombing, assim como para identificar as principais
motivações por trás destes ataques.
Como resultado, pudemos observar que jogos que sofrem de
RB possuem características únicas, além das notas baixas, que nos
permite diferenciar dos demais jogos na plataforma, como o volume
gigantesco de comentários, maior engajamento dos usuários em co-
mentários negativos, comentários mais tóxicos e com uso de termos
específicos. Também encontramos nos comentários ataques direcio-
nados à empresa e aos desenvolvedores, um apelo ao uso de termos
técnicos para dar maior validade às críticas, além de motivações
carregadas de discursos de ódio, como sexismo e homofobia.
TRABALHOS RELACIONADOS
Avaliar e identificar review bombing é uma tarefa complexa. Os
usuários almejam criar a impressão de que os comentários e notas
negativos são orgânicos, e não recorrem diretamente ao discurso de
ódio ou a comentários diretos sobre o ataque, porque assim as pla-
taformas conseguiriam banir com facilidade o conteúdo que viola
os termos de uso. Desse modo, os usuários manipulam as ferramen-
tas disponíveis nas plataformas digitais de maneira coordenada,
como as notas em sites de críticas ou botões de dislike, como os do
YouTube [19] [4].
Observando as relações entre o número de vendas e as notas
atribuídas às obras, é possível notar que as avaliações de produtos
ou mídias exercem uma influência direta e significativa sobre o
comportamento dos consumidores, influenciando diretamente o
volume de vendas [4] [2]. Especialmente no contexto das mídias
eletrônicas, conforme demonstrado por [9], parece existir uma rela-
ção geométrica entre o número de vendas desses jogos e suas notas
no Metacritic, destacando que pontuações mais altas promovem
um aumento de vendas. Também existem indícios que destacam as
limitações das pontuações atribuídas no site como representações
completas e precisas das experiências dos jogadores [13]. O estudo
de [28] traz um interessante contraste entre as avaliações feitas por
amadores e por profissionais no Metacritic, onde se demonstra que
amadores tendem a fornecer avaliações mais polarizadas e favo-
ráveis a obras antigas, mesmo as avaliando surpreendentemente
muito tempo após o lançamento. Em contraste, especialistas geral-
mente emitem avaliações menos polarizadas, concentrando-se em
aspectos técnicos e contextuais dos jogos e tendem a avaliá-los logo
após o lançamento.
Os resultados obtidos por [31] revelam a ineficácia da maioria
dos classificadores de sentimento na avaliação precisa de resenhas
de jogos. Uma análise mais detalhada apontou duas causas princi-
pais para as classificações incorretas: a presença de resenhas que
destacam tanto os pontos positivos quanto os negativos, bem como
resenhas que fazem comparações entre diferentes obras, o que pode
confundir o classificador.
Diferente dos trabalhos apresentados, este se distingue, pois
utiliza técnicas de clusterização e busca caracterizar padrões de
review bombing. Assim como também foi realizada uma análise de
como os usuários interagem e comentam com jogos que sofrem
ataques, e a relação deles com toxicidade e discurso de ódio.
METODOLOGIA
O Metacritic reúne críticas de uma variedade de mídias de entreteni-
mento, incluindo filmes, programas de televisão, álbuns de música
e jogos. Na plataforma, os usuários podem fornecer uma pontuação
com ou sem comentário para mídias específicas. No sistema de
avaliação, os avaliadores são categorizados como crítica especiali-
zada ou usuários amadores. Os críticos fazem parte de uma seleção
cuidadosa e sua média ponderada resulta em um valor numérico
singular, chamada de Metascore. Enquanto isso, as pontuações dos
usuários amadores contribuem para o UserScore. Para a análise dos
ataques de review bombing na plataforma, dada a ausência de uma
API própria, foi necessário o desenvolvimento de um coletor e a
criação de um novo conjunto de dados foi considerada necessária
para o estudo.
3.1
Coleta de Dados
Para obter os dados, foi implementado um web scrapper, conforme
mostrado na Figura 1, que manipula o navegador através do uso
do Selenium 2 e coleta os dados nas páginas do Metacritic. O pri-
meiro passo envolve a navegação pelo catálogo da plataforma. Este
catálogo contém uma lista completa dos jogos disponíveis no site.
A navegação é feita percorrendo todas as 6068 páginas existentes,
manipulando a URL para iterar por todas as páginas.
A cada página visitada, realiza-se uma extração do HTML pre-
sente. Durante essa etapa, todos os títulos dos jogos são coletados e
armazenados. O objetivo dessa etapa é compilar uma lista de todos
os jogos disponíveis. Uma vez obtidos todos os títulos, o próximo
passo é acessar cada URL individualmente. Isso é realizado concate-
nando o título do jogo com a URL base do Metacritic, formando uma
URL no formato https://www.metacritic.com/game/gameName,
na qual são extraídas todas as informações desejadas. Dessa maneira
são coletadas as seguintes informações sobre os jogos: (1)plataforma,
(2)classificação etária, (3)título, (4)editora, (5)desenvolvedor, (6)data
de lançamento, (7)gênero, (8)notas, (9)quantidade de notas positi-
vas, negativas e neutras. Quanto aos usuários, tanto críticos quanto
amadores, foram coletados seus (i) comentários, (ii) notas, (iii) no-
mes, (iv) data dos comentários, (v) plataforma da qual o comentário
foi feito e o (vi) tipo de usuário. Cada jogo é caracterizado por 27
atributos distintos, abrangendo desde informações sobre usuários e
seus comentários até detalhes intrínsecos das mídias, como data de
lançamento e desenvolvedores. Além disso, implementamos outras
métricas como a variância entre as notas dentro de um jogo, a re-
lação entre o 𝑈𝑠𝑒𝑟𝑆𝑐𝑜𝑟𝑒/𝑀𝑒𝑡𝑎𝑠𝑐𝑜𝑟𝑒, contagem de palavras-chave
relacionadas ao contexto de discurso de ódio, totalizando mais 8
atributos derivados.
Como resultado, coletamos 145.523 jogos e 1.017.504 comen-
tários distintos. Para garantir a qualidade dos dados, foi elaborada
uma filtragem, onde foram removidas todos os jogos que não ti-
nham nenhum comentário, ou que não possuíssem UserScore ou
Metascore, ou que faltasse algum dado relevante para análise. Com
isso, o conjunto de dados final contém 12.200 jogos propriamente
avaliados no Metacritic e os mesmos 1.017.504 comentários dis-
tintos.
3.2
Rotulando Jogos alvos de Review Bombing
Dado nosso objetivo de estudo de identificar ataques de RB, tam-
bém precisamos estabelecer um conjunto bem definido de jogos
que foram alvos de ataques. Para levantar os jogos alvos de RB,
2Available at: https://selenium-python.readthedocs.io/
Quando as Avaliações Viram Bombas: Explorando a Dinâmica do Review Bombing nos Jogos no Metacritic
WebMedia’2024, Juiz de Fora, Brazil
Figura 1: Diagrama geral da coleta dos dados dos jogos do Metacritic.
rotulamos manualmente os jogos baseado em fontes externas. Para
isso, foram consultados três sites distintos e bem citados na comu-
nidade gamer, que produziram matérias sobre casos notórios de
review bombing [25, 33, 34]. A seleção dos candidatos foi realizada
com o seguinte critério nessas fontes: um jogo é marcado como
review bombing caso alguma fonte tenha explicitamente mencio-
nado que o jogo foi alvo de um ataque no Metacritic; ou caso mais
de uma única fonte tenha simplesmente citado o jogo. Além disso,
também foram incluídos manualmente outros casos de destaque na
comunidade como: Horizon Forbidden West [27], Battlefield V [18],
e Diablo Immortal [5].
ANÁLISE DOS DADOS
Após limpar e analisar o dataset, partimos para uma análise mais
aprofundada dos dados. Nesta seção, utilizamos técnicas de ciência
de dados, como derivação de features, utilização de modelos de
aprendizado de máquina e clusterização para podermos compre-
ender quais características definem um ataque de review bombing.
Nesta seção, portanto, analisamos como as características dos jogos
no Metacritic (e.g., comentários, curtidas, toxicidade, sentimento)
se comportam quando os jogos sofrem ataques, comparadas àquelas
dos que não sofreram. Dessa forma, conseguimos mensurar as ati-
vidades relacionadas à prática de review bombing online e discutir
as motivações e implicações de tais ataques para as comunidades
online.
4.1
Engajamento no Metacritic
Ao analisar os dados, é possível perceber como a seção de jogos do
Metacritic é composta. Onde apenas 8.3% dos jogos tem tanto um
comentário, um userScore e um Metascore. Além disso, conforme a
Figura 2a, mais de 60% dos jogos possuem menos de 10 avaliações.
Isso evidencia que uma parte significativa dessa seção é vazia, com
inúmeros jogos pouco avaliados, ou com um engajamento muito
baixo. Também é importante ressaltar o limite superior do gráfico de
Total de Comentários
0.0
0.2
0.4
0.6
0.8
1.0
Probabilidade Cumulativa
(a) CDF Comentários Por Jogo
Número de Comentários por Usuário
0.0
0.2
0.4
0.6
0.8
1.0
Probabilidade Cumulativa
Amadores (669277)
Profissionais (348227)
(b) Comentários por Usuário
Figura 2: Estrutura geral dos dados do Metacritic
10.000 comentários. Esse limite é imposto pela própria plataforma
como o número máximo de comentários visíveis por jogo, mas
demonstra que alguns jogos atraem uma grande atenção do público.
Em relação aos autores de comentários, os dados mostram a pre-
sença de 333.624 usuários únicos. Destes, 99,08% são classificados
como amadores e apenas 0,92% são críticos especializados. Dentro
dessa minoria, aproximadamente 23% deles produziram mais de
um comentários. Contudo, essa pequena parcela de profissionais é
responsável por 34,22% de todos os comentários produzidos no site,
indicando um nicho de altíssima produtividade. com alguns usuá-
rios escrevendo quase 10.000 comentários, como visto na CDF da
Figura 2b. Já os amadores, mais de 98% possuem até 10 comentários,
demonstrando que a maioria deles não participam tão ativamente
na plataforma do Metacritic, apenas opinando em jogos de seu in-
teresse. Entretanto, também temos usuários altamente ativos, com
alguns chegando a perto de 100 jogos avaliados.
4.2
Análise de Toxicidade
Além das informações já obtidas, foi também processada a toxi-
cidade de cada comentário, através da ferramenta do Perspective
API3. Esse modelo analisa textos e identifica níveis de toxicidade
numa pontuação que varia de 0 a 1. Baseando-se em outros estudos
[17], nós consideramos que um comentário é determinado tóxico,
se ele está acima do limiar de 0.75. Além disso, mantivemos o idi-
oma original de cada comentário, uma vez que o modelo mostra
bom desempenho no cenário com múltiplos idiomas [14]. Assim,
processamos todos os comentários dos usuários amadores (669.277)
a fim de mensurar a toxicidade dos jogos coletados.
Nossa primeira análise focou nas principais plataformas no qual
os jogos são publicados, PlayStation, Nintendo, PC e Xbox (Fig. 3a. É
possível observar que, as curvas apresentadas são notavelmente si-
milares, indicando que a plataforma não ser um fator determinando
na toxicidade. Mesmo que existam rivalidades nas comunidades
fãs de cada plataforma e jogos exclusivos, tais características não
refletem, de forma geral, em como os jogos são recebidos no Me-
tacritic. Outra característica interessante presente nos dados é o
gênero de cada jogo avaliado. A Figura 3b contabiliza a quantidade
de comentários tóxicos (> 0.75) por gênero. Observa-se que “Es-
portes” possui relativamente uma taxa de toxicidade muito maior
que as demais categorias (1.1%), muito superior a “Simulação”, que
vem em segundo lugar com 0.5%. Essa alta pode ser causada pela
natureza competitiva desse gênero, em que frustração dos jogado-
res podem se refletir em críticas mais contundentes ao jogo em
questão. Em contrapartida, gêneros como, “Estratégia”, “Aventura” e
“RPG” apresentam toxicidade menor, próximo de 0.2%. Essas mídias
tendem a ser mais focados em resolução de problemas, narrativa ou
3https://perspectiveapi.com/
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
0.0
0.2
0.4
0.6
0.8
Toxicidade Média
0.0
0.2
0.4
0.6
0.8
1.0
Probabilidade Cumulativa
PlayStation
Nintendo
PC
Xbox
(a) CDF de Toxicidade por Plataforma
Esportes
Simulação
Corrida
Ação
Outros
Tiro
Puzzle
Estratégia
Aventura
RPG
0.0%
0.2%
0.4%
0.6%
0.8%
1.0%
Comentários Tóxicos (%)
(b) Toxicidade Relativa por Gênero
pt
de
nl
zh
pl
ko
ru
cs
it
en
ar
es
ja
fr
hi-Latn
hi
sv
0%
1%
2%
3%
4%
5%
6%
7%
Comentários Tóxicos (%)
(c) Toxicidade Relativa por Idioma
Figura 3: Análises de toxicidade e idioma dos comentários dos jogos do Metacritic.
cooperação, podendo assim desmotivar sentimentos negativos. Por
fim, o gráfico 3c mostra a distribuição percentual relativa de comen-
tários tóxicos por diferentes idiomas. Notavelmente, português têm
a maior taxa de toxicidade, atingindo quase 8%. Essa diferença pode
ser um reflexo das normas de comunicação do idioma, onde uma
linguagem considerada tóxica por um povo, pode ser considerada
normal para outro.
CARACTERIZANDO REVIEW BOMBING
Nossa primeira abordagem para compreender a natureza dos ata-
ques de review bombing nos dados foi usar das features extraídas
para agrupar os jogos e encontrar possíveis padrões. Para isso,
realizamos diversos experimentos com diferentes algoritmos de
clusterização. Este processo foi desenvolvido em várias etapas, com
o objetivo de separar mais claramente as mídias que sofreram ata-
ques de review bomb daquelas que não foram afetadas.
Dada a definição do review bombing que envolve as revisões e
comentários publicados pelos usuários na plataforma, nesta etapa
foi realizada uma filtragem dos jogos do dataset com apenas aqueles
com pelo menos 50 comentários, possibilitando assim, uma análise
mais aprofundada e com maior explicabilidade do conteúdo dos
comentários em cada jogo, levando ao dataset final de 1645 jogos
distintos. Esse valor foi alcançado baseado nos resultados apontados
na Figura 2a, em que a média de comentários por jogos é de 47.
Com o conjunto de dados final em mãos, a próxima etapa foi defi-
nir quais features, dentre aquelas coletadas e derivadas descritas na
Seção 3, serão utilizadas no algoritmo de clusterização, visando uma
separação mais clara entre os jogos que sofreram ataque daqueles
que não sofreram.
Tendo em vista limitar ainda mais esse conjunto, aprimorando
assim ainda mais a precisão dos clusters e reduzindo processamento
desnecessário, optou-se por utilizar o algoritmo do Mutual Informa-
tion (MI) [15] do sklearn para identificar quais parâmetros são mais
relevantes para conseguir discernir um alvo, no caso em questão, jo-
gos que sofreram review bomb. Ao aplicá-lo, foi possível selecionar
os atributos que apresentam maior relevância e capacidade explica-
tiva no contexto em questão. Assim, foram escolhidos os seguintes
parâmetros, que serão utilizados na clusterização: “totalComments”
o número total de comentários feitos em um jogo, “meanToxicity”
UMAP Dimensão 1
UMAP Dimensão 2
Cluster 0
Cluster 1
Alvos
Figura 4: Clusterização de jogos.
a toxicidade média dos comentários, “totalAlike” o número to-
tal de comentários semelhantes (avaliados utilizando a biblioteca
fuzzywuzzy4 com um threshold de 0.8), “userRtotal” o conjunto
total de notas atribuídas por usuários amadores‘ “totalCurse” e o
número total de comentários que contêm palavrões (considerando
como palavrão qualquer comentário que contenha **** dentro dele).
No qual valores de toxicidade, similaridade e palavrões são expres-
sos em percentuais.
Para os algoritmos de clusterização, foram considerados o K-
means, HDBSCAN, DBSCAN, Gaussian Mixture. Foram realizados
testes de desempenho em todos, avaliando a capacidade de cada
um em separar os jogos normais daqueles que sofrefram RB. Como
resultado, calculamos a métrica de F1-score, considerando o cluster
com a maior proporção de jogos que sofreram ataques, como o de
review bomb, e o outro, como sendo livre de ataque. Para casos com
mais de 2 clusters, foi escolhida a configuração que resultaria no
maior F1. Após a realização dos testes, foi observado que, o k-means
com 𝑘= 2 apresentou os resultados mais satisfatórios.
Além disso, também foi aplicado um método de redução de di-
mensionalidade com o UMAP, para melhorar a visualização dos
clusters. Os resultados são apresentados na Figura 4. Nela, todos
os itens pertencentes ao cluster 1 (representado pela cor vermelho)
4Available at: https://pypi.org/project/fuzzywuzzy/
Quando as Avaliações Viram Bombas: Explorando a Dinâmica do Review Bombing nos Jogos no Metacritic
WebMedia’2024, Juiz de Fora, Brazil
Predição\Real
Sim
Não
Total
Sim
Não
Total
Tabela 1: Matriz de confusão da clusterização
foram classificados como alvos de ataques, enquanto os itens do
cluster 0 (representado pela cor verde) foram classificados como
livres de ataques.
5.1
Matriz de Confusão
Na matriz de confusão, exibida na Tabela 1, observa-se um desem-
penho medido pelo F1-score de 0.5263. Este resultado, apesar de
modesto à primeira vista, merece uma discussão detalhada sobre
as peculiaridades de cada mídia e revela características e desafios
interessantes do problema analisado.
Focando inicialmente nos falsos negativos, ou seja, os jogos que
sofreram Review bombing, mas que não foram identificados pelo mo-
delo de detecção. Alguns exemplos notáveis incluem Bastion e Toy
Soldiers, cujos comentários negativos foram removidos pelo próprio
site do Metacritic [24], dificultando a identificação desses casos pelo
algoritmo. Além disso, existem situações como a de Borderlands
3 [33], no qual a onda de comentários ocorreu na plataforma da
Steam, diferente do Metacritic, levando a uma classificação incor-
reta do jogo. Examinando também os falsos positivos, isto é, os
jogos que foram classificados como alvos de Review Bombing, em-
bora não estivessem presentes na nossa rotulação inicial. Como é o
exemplo de FIFA 20 [6] e Baldur’s Gate 3, que não entrou na nossa
rotulação inicial, mas recentemente também foi alvo de ataques
de Review Bombing [3], porém, como esses ataques tiveram uma
menor repercussão ou foram mais recentes, acabaram não incluídos
na rotulação original.
Portanto, vemos que a moderação ativa do Metacritic e variedade
de outras plataformas de avaliação podem apresentar um obstáculo
grande a análise do review bombing, uma vez que elas podem ocul-
tar as evidências dos ataques que esses jogos sofreram. Entretanto,
observando tanto a figura quanto os resultados, vemos que grande
parte dos jogos no cluster de Review bombing Compartilham carac-
terísticas semelhantes que sugerem que esse fenômeno pode ser
identificado e, portanto, combatido dentro das plataformas.
5.2
Atributos do Cluster Review Bombing
Seguindo os resultados da clusterização, podemos observar os cen-
troides de cada cluster na Figura 5 para entender melhor as ca-
racterísticas individuais dos jogos normais daqueles que sofreram
ataque.
Primeiramente, ao examinar o número total de comentários
(totalComments), tanto os valores absolutos dos centroides quanto
a distribuição mostrada no boxplot indicam uma diferença substan-
cial. O número médio de comentários para os jogos afetados é quase
19 vezes maior em comparação com os jogos não afetados. Suge-
rindo assim, um aumento massivo de comentários em jogos que
sofreram review bomb. Outro atributo relevante é o userRtotal),
que indica a quantidade total de notas atribuídas a um jogo. A quan-
tidade de notas que um jogo que sofreu ataque recebe também é
significativamente maior, quase 17 vezes mais. Reforçando ainda
mais a ideia de que jogos atacados, recebem um aumento massivo
no número de comentários.
Abordando também a média de toxicidade (meanToxicity), e
o total de comentários contendo palavrão (totalCurse), tem-se
uma pequena diferença no valor de um centroide para o outro,
embora os Review Bomb sejam um pouco mais elevado. Ao comparar
ambos, é possível notar uma variação muito menor dos quartis de
um cluster para outro, demonstrando que os jogos atacados são
mais consistentes. Um fator que pode explicar a semelhança entre
os dois grupos é o sistema de moderação do Metacritic que apaga
comentários muito tóxicos ou não-apropriados. Ainda assim, ambos
atributos se mostraram relevantes na diferenciação dos jogos.
A métrica totalAlike, que representa quantos comentários são
similares entre si, traz um debate interessante. A grande diferença
nos valores sugere que os comentários dos ataques são geralmente
compostos por um argumento único ou repetitivo dentro da comu-
nidade, impulsionando a onda de comentários negativos, enquanto
jogos normais possuem comentários mais individualizados. Um
exemplo notável é o jogo Madden 21, em que os fãs reclamaram
da falta de mudanças significativas de uma versão para outra, re-
sultando em uma avalanche de comentários com textos semelhan-
tes [33].
A definição de um review bomb como: “a inundação de um site
com avaliações negativas (seja de um produto ou negócio)“ [20]
se alinha com os resultados encontrados. Conforme discutido an-
teriormente, uma mídia que sofreu um ataque tende a apresentar
um aumento massivo no número de comentários, predominante-
mente negativas. No entanto, é possível expandir essa definição
para incluir também a similaridade entre os comentários e uma
leve elevação na toxicidade. Essas outras características refletem
o fato do Review bombing ser também uma campanha coordenada
e de insatisfação, gerando esse efeito negativo e homogêneo nos
comentários.
ANÁLISE DE CONTEÚDO DOS
COMENTÁRIOS
Nesta seção voltamos nossas análises para o conteúdo dos co-
mentários dos jogos que sofreram review bombing no Metacritic.
Utilizando-se de nuvens de palavra, buscamos observar tendências
gerais e tópicos pertinentes que servem de motivação para esses ata-
ques. Também foi realizada uma análise de discurso de ódio presente
nos comentários, relacionando com uma lista de palavras-chave
criada a partir do contexto de ódio presente nos jogos.
6.1
Word Clouds
As Word Clouds são ferramentas analíticas úteis que permitem uma
visualização abrangente de tópicos presentes em grandes volumes
de texto. No contexto deste estudo, foram geradas nuvens de pala-
vras a partir de todos os jogos que sofreram review bomb, com o
objetivo de identificar críticas recorrentes e tópicos emergentes. A
análise dessas nuvens de palavras revelou a existência de três tópi-
cos comuns, que se repetiram em diversos jogos, fornecendo uma
visão clara das principais áreas de insatisfação dos usuários que
praticam review bombing. O primeiro tópico identificado envolve
críticas diretas às empresas responsáveis pelo desenvolvimento e
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
Outros ReviewBomb
totalComments
Outros ReviewBomb
meanToxicity
0.126
0.158
Outros ReviewBomb
totalAlike
0.193
8.301
Outros ReviewBomb
userRtotal
16285
Outros ReviewBomb
totalCurse
0.096
0.103
Figura 5: Comparação de atributos de jogos com Review Bombing e demais jogos, sendo o verde o centroide do cluster de jogos
normais e o vermelho o cluster dos jogos que sofreram review bombing.
lançamento dos jogos. Nesses casos, jogadores frequentemente ex-
pressam insatisfação com decisões corporativas que, segundo eles,
afetam negativamente a qualidade dos jogos. Dois exemplos emble-
máticos dessa situação são os caso de Grand Theft Auto: The Trilogy
– The Definitive Edition [25] e Diablo IV [26]. Conforme visto nas Fi-
guras 6c e 6f, os nomes das empresas (Rockstar e Blizzard) aparecem
em destaque nas nuvens de palavras, junto a palavras como "bug",
"money", "release", "content"e "patch". Observando os comentários
e buscando por notícias relacionadas aos jogos, podemos ver que
ambas as empresas foram alvos de críticas que as acusam de ter
lançado os jogos inacabados ou com muitos bugs [7, 8, 21, 23, 30].
O próximo tópico comum das críticas são o conjunto história
e jogabilidade, como visto nas Figuras 6a e 6b. Esses pontos são
abordados principalmente nas mídias que sofreram review bombing
por motivos políticos, como é o caso dos jogos The Last of Us 2
e Horizon Forbidden West. Os ataques aos jogos se deram princi-
palmente pela orientação sexual das personagens principais [27].
Como demonstrando anteriormente por [19], os usuários que parti-
cipam de ataques ligados a questões sociais tendem a demonstrar
seu ódio indiretamente para escapar da moderação de conteúdo,
pois comentários com discurso de ódio explícito são detectados
com mais facilidade. Desse modo, focam suas críticas na história e
jogabilidade de maneira geral.
O terceiro tópico envolve o julgamento sobre jogos percebidos
pelo público como cópias pouco inovadoras de versões anteriores,
um problema particularmente comum em mídias de esporte, como a
franquia Madden. Na Figura 6d, podemos ver que "franchise"e "mo-
ney"aparecem em destaque, relacionando o lançamento de novos
títulos da franquia com o intuito de apenas ganhar dinheiro. Como
já citado, este jogo de futebol americano, lançado anualmente, foi
alvo de críticas dos usuários que reclamam da falta de mudanças
significativas no título de um ano para o outro.
Por fim, existem as críticas diversas, que tratam de contextos
específicos. Um desses casos é o jogo Call of Duty: Modern Warfare,
que enfrentou uma enxurrada de comentários negativos devido à
representação controversa dos russos [10], o que é constatado na
Figura 6e, em que "Russian"aparece em destaque. O jogo foi criticado
por insinuar que os russos estavam envolvidos no "Highway of
Death", um ataque ocorrido durante a Guerra do Golfo, realizado
por tropas aliadas aos Estados Unidos no Iraque, denunciado pelo
uso excessivo de força [11].
6.2
Discurso de Ódio
Identificar discurso de ódio é uma tarefa complexa, especialmente
em plataformas como o Metacritic, que possuem sistemas de mode-
ração que já removem diretamente conteúdos mais ofensivos. No
entanto, usuários muitas vezes conseguem contornar esses sistemas,
utilizando linguagem que sugere, de forma indireta, discurso de
ódio.
Por esse motivo, foi compilada uma lista extensiva de palavras-
chave que, no contexto da plataforma, são geralmente utilizadas
em enunciados com discurso de ódio. A lista começou a ser elabo-
rada com a palavra "woke", que é majoritariamente utilizada em
contextos de crítica à justiça social e racial [1]. A partir dessa pa-
lavra foi realizada uma análise manual de todos os comentários
que continham ela, buscando outras palavras que a acompanham
normalmente. Encontramos, por exemplo, as siglas SSJ (social jus-
tice warrior) e LGBTQIA+ (comumente utilizada de forma negativa
no site). Também foram adicionadas à lista, pelos autores, algumas
outras palavras consideradas explicitamente ofensivas.
Foram identificadas 22 palavras-chave em inglês. Considerando
que 87.6% do conteúdo do site está nesse idioma, decidimos aplicar
essa lista a todos os idiomas que representam mais de 1% do site.
Isso inclui Espanhol com 4.5%, Russo com 2.8%, e Português com
2.3%. Logo, todas as palavras-chave foram traduzidas para cada um
desses idiomas, cabendo adaptá-las devido às nuances dos idiomas
e gírias, que variam conforme contexto linguístico. Cabe ressaltar
que, devido à falta de conhecimento dos autores sobre russo, foi
consultado um fórum do site Reddit, especializado no idioma, onde
a tradução foi feita por terceiros.
Dessa maneira, foi possível elaborar a Tabela 2, que mostra as
5 palavras-chave mais utilizadas e os cinco jogos que tem mais
palavras-chave, sendo esse percentual relativo ao total de comen-
tários que possuem. É possível perceber que os jogos com a maior
quantidade de palavras-chave são justamente os jogos que sofreram
review bombing por questões sociais, como o gênero e sexualidade
das personagens. Como observado anteriormente nas nuvens de
palavras, os usuários tendem a esconder seu discurso de ódio, uti-
lizando críticas gerais a história e aos personagens para justificar
notas baixas e porque não gostaram do jogo.
Ao organizar o conjunto de dados em duas categorias, comentá-
rios com as palavras-chave e comentários sem as palavras-chave,
foi possível notar uma diferença estatisticamente significativa na
toxicidade. O conjunto com as palavras-chave tem uma média de
Quando as Avaliações Viram Bombas: Explorando a Dinâmica do Review Bombing nos Jogos no Metacritic
WebMedia’2024, Juiz de Fora, Brazil
(a) The Last of Us Part II
(b) Horizon Forbidden West
(c) Grand Theft Auto: The Trilogy - The Defi-
nitive Edition
(d) Madden NFL 21
(e) Call of Duty: Modern Warfare
(f) Diablo IV
Figura 6: Comparação entre outros jogos
Palavra
Total
Jogo
Total
woke
The Last of Us Part II
agenda
Mortal Kombat 11
sjw
Ghost of Tsushima
lgbt
Horizon Forbidden West DLC
lesbian
Battlefield V
Tabela 2: Jogos e Palavras-Chave
toxicidade de 0.291±0.209, enquanto o conjunto sem as palavras
apresenta uma média de 0.118±0.182. Esses resultados reforçam a
eficácia do método.
6.3
Discussão
Chegando ao final do estudo, foi finalmente possível responder às
perguntas de pesquisa.
RQ1: Quais são as características de um jogo que sofreu review
bombing online?
Com o auxílio dos centroides, foi possível determinar que um
jogo que sofreu review bombing tem as seguintes características: Um
número de comentários e reviews muito maior que a média. Uma
grande similaridade entre os comentários, indicando um reclamação
em comum entre os usuários. E por fim, uma toxicidade média e um
número de palavras de baixo calão levemente maior que o normal.
RQ2: Como se comportam os usuários da plataforma e qual é o
nível de toxicidade nas suas interações?
Em geral, os usuários comuns do Metacritic não são muito ativos,
com a grande maioria tendo menos de 10 comentários. No entanto,
ao observarmos os críticos, percebemos uma realidade diferente,
onde eles tendem a ser muito mais ativos na plataforma. Quanto
à toxicidade, o site consegue moderar bem, apresentando poucos
casos extremos. Contudo, ainda há espaço para melhorias, pois é
possível observar discrepâncias no gênero de esportes e no idioma
português, que apresentam muito mais toxicidade, conforme visto
nas Figuras 3b e 3c. RQ3: Quais as principais motivações dos
comentários desses ataques?
As motivações dos ataques variam de jogo para jogo, mas é pos-
sível separar em 3 tópicos diferentes. (1) Críticas diretas à empresa
e suas decisões relativas ao desenvolvimento e vendas, como lançar
um jogo inacabado ou adicionar microtransações abusivas. (2) His-
tória e Jogabilidade, tópico geralmente ligados ao discurso de ódio
contra minorias, onde os usuários disfarçam seu ódio em críticas
gerais à história, personagens e jogabilidade. (3) Franquias repetiti-
vas, em que as empresas desenvolvedoras lançam jogos anualmente
sem muitas novidades e melhorias técnicas, porém cobrando preços
caros, o que ocorre principalmente em franquias de esporte. Por fim,
existem as outras críticas, que abordam temas específicos, que não
compartilham um tópico em comum. Abaixo, é possível observar
algumas dessas reclamações.
• “What a disgusting cash grab. It makes me so angry that rockstar
removed the original games from the stores.”
• “they ruined this game. focused on political correctness. lgbt agenda.
not worth the money. story is very weak.”
• “The same game as Madden 20. The face of the franchise is trash.”
CONCLUSÃO
A motivação deste trabalho é analisar, caracterizar e pensar metodologias
para identificar o fenômeno de review bomb em plataformas de avaliação
de mídias. Para tal, realizamos uma vasta coleta do Metacritic. A partir da
limpeza e caracterização desses dados, conseguimos resultados relevantes a
respeito do comportamento dos usuários e também percebemos que, me-
diante uma estratégia de aferição de toxicidade aliada à clusterização, é
possível aferir quais jogos sofreram review bombing. Dentre nossos resulta-
dos, foi possível caracterizar o ataque de Review bombing como sendo uma
grande onda de comentários altamente tóxicos e similares uns aos outros,
refletindo a característica de campanha coordenada de insatisfação que eles
representam. Também identificamos três categorias diferentes de tópicos
que aparecem em review bombings no Metacritic: reclamações sobre as em-
presas desenvolvedoras e suas decisões empresarias, reclamações quanto a
história e personagens, que geralmente estão permeadas por discurso de
ódio e, por fim, reclamações sobre jogos de franquias muito semelhantes a
seus antecessores.
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
Embora não encontrada diferenças significativas entre as plataformas
PlayStation, Nintendo, PC e Xbox, vimos que o gênero do jogo pode ser um
atributo relevante na caracterização dos ataques, com a categoria “Esportes”
sendo o tipo com comentários mais tóxicos. Além disso, vimos que os co-
mentários no Metacritic feitos em português exibem níveis de toxicidade
mais elevados em comparação aos demais. Também notamos que discurso
de ódio é bem presente dentro das comunidades, e é direcionado principal-
mente à comunidade LGBTQIA+, e se disfarça como críticas genéricas sobre
a história, personagens e jogabilidade. Com nossos resultados, vimos que é
possível identificar ataques aos jogos no Metacritic, e até mesmo suas moti-
vações. Isso pode servir como uma ferramenta para ajudar na moderação
dessas plataformas, reduzindo o impacto que as review bombings geram. Em
trabalhos futuros, serão introduzidas novas métricas, como análises tempo-
rais e de sentimento, para identificar ataques, com o objetivo de conseguir
detectar esses ataques enquanto eles ocorrem e alertar as plataformas.

--- FIM DO ARQUIVO: 30125.txt ---

--- INÍCIO DO ARQUIVO: 30130.txt ---
Um framework de rastreamento corporal para reabilitação
neuromotora com suporte a aplicativos multimídia
Elvis Ribeiro
elvishribeiro@gmail.com
Universidade Federal de São João
del-Rei (UFSJ)
São João del-Rei, MG, Brasil
Alexandre Brandão
alexandre.brandao@puc-
campinas.edu.br
Universidade Pontifícia Católica de
Campinas (PUC-Campinas)
Campinas, SP, Brasil
Marcelo Guimarães
marcelo.paiva@unifesp.br
Universidade Federal de São Paulo
(UNIFESP)
Osasco, SP, Brasil
Leonardo Rocha
lcrocha@ufsj.edu.br
Universidade Federal de São João
del-Rei (UFSJ)
São João del-Rei, MG, Brasil
José Remo Brega
remo.brega@unesp.br
Universidade Estadual Paulista
(UNESP)
Bauru, SP, Brasil
Diego Dias
diego.dias@ufes.br
Universidade Federal do Espírito
Santo (UFES)
Vitória, ES, Brasil
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
físicas na população infantil. Como mostrado por Guimaraes
et al. [5], o cansaço físico é um fator limitante ao tempo de
interação do usuário com tecnologias virtuais baseada em
gestos, evidenciando que um maior grau de fortalecimento
da musculatura do corpo poderia colaborar com o aumento
de tempo de interação dos usuários com as iminentes tecnolo-
gias suportadas por gestos corporais, tais como o controle de
televisores inteligentes por meio de movimentos com as mãos,
sendo assim um movimento natural ao cotidiano do usuário.
As aplicações de multimídia vêm desempenhando um papel
importante nos dias atuais e estão presente nas mais diversas
áreas, incluindo como ferramenta no auxílio de reabilitação
[1, 3, 15]. A interação é parte essencial do engajamento do
usuário com o ambiente virtual, podendo ser alcançada por
meio de controles, toques ou até mesmo gestos. A solução
apresentada no trabalho utiliza-se da interação natural do
usuário (NUI), sendo estes captados por meio de sensores
inerciais embarcados. A interação entre o usuário e o sistema
deve ser, idealmente, o mais natural possível. Em sistemas
de realidade virtual (RV), por exemplo, em que a imersão
impossibilita o uso de teclados, mouses e telas sensíveis ao
toque, a interação é realizada por meio de soluções específi-
cas como joysticks, gestos e voz. Em casos onde o usuário é
incapaz de operar um joystick, ou quando se deseja estimular
a movimentação corporal, a interação por meios de gestos
corporais se mostra uma excelente escolha.
Tecnologias de rastreamento corporal vêm sendo utilizadas
largamente na indústria de jogos [4]. Dispositivos de rastrea-
mento óticos, tal como o Kinect da Microsoft ou soluções
Mocap1, utilizam visão computacional para rastrear o corpo
humano [14]. Porém, o uso do Kinect apresenta limitações
como liberdade, precisão e oclusão, o que limita o movimento
do usuário ao campo de visão do dispositivo e impossibilita a
captura de movimentos do usuário em posição lateral (se uti-
lizado apenas um dispositivo). Tais problemas são mitigados
quando empregadas as soluções Mocap, visto que utilizam
múltiplas câmeras de captura e diversos marcadores anexa-
dos ao corpo do usuário, porém o preço elevado é um fator
obstante na escolha de uma solução boa e razoavelmente
barata. Neste trabalho, utilizamos o biomechanical sensor
node (BSN) como dispositivo de interação.
O BSN [2] é um dispositivo de rastreamento corporal ino-
vador desenvolvido por Brandão e sua equipe. Ele utiliza uma
rede de sensores colocados estrategicamente no corpo para
capturar movimentos e posturas em tempo real, permitindo
que o usuário possa controlar ambientes virtuais por meio da
NUI. Esses dados são então processados e utilizados para criar
uma representação digital precisa do usuário. O BSN tem
aplicações potenciais em uma variedade de campos, incluindo
reabilitação física, esportes e entretenimento interativo. Sua
precisão e facilidade de uso o tornam uma ferramenta valiosa
para profissionais e pesquisadores dessas áreas.
A principal justificativa desse trabalho está relacionada a
dificuldade em se simular a interação do usuário dentro de
1Rastreamento corporal através de marcadores presos ao corpo do
usuário e diversas câmeras de alta definição circundando o ambiente.
um ambiente virtual de maneira intuitiva e imersiva. Outro
benefício, no que tange o uso de ambientes virtuais, é a pos-
sibilidade do usuário realizar o treinamento em um ambiente
controlado e seguro, visto que o ambiente virtual não gera
perigo real, quando comparado a um ambiente real de treina-
mento. Portanto, pesquisas voltadas a meios de interação
de ambientes virtuais são importantes ao desenvolvimento e
difusão de novas aplicações.
Apresentamos neste trabalho a definição do framework de
rastreamento corporal, que possibilita a criação de aplicações
de NUI utilizando o dispositivo BSN, assim como a criação e
armazenamento de sessões de fisioterapia, contexto apresen-
tado como exemplo para a criação de aplicações voltadas à
reabilitação neuromotora. Outras aplicações exemplo também
são apresentadas.
METODOLOGIA
Desenvolver um framework envolve uma série de etapas
metodológicas. Primeiro, é necessário identificar e entender
claramente o problema ou a necessidade que o framework se
propõe a resolver. Em seguida, é importante realizar uma
análise aprofundada do domínio do problema para identificar
padrões comuns e abstrações. Com base nessa análise, o próx-
imo passo é projetar a arquitetura do framework, definindo
suas principais classes e interfaces, bem como suas interações.
A implementação do framework deve ser feita de forma mod-
ular e extensível, permitindo que os desenvolvedores person-
alizem e estendam sua funcionalidade conforme necessário.
Por fim, é crucial fornecer documentação detalhada e ex-
emplos de uso para ajudar os desenvolvedores a entender e
utilizar o framework efetivamente. É importante notar que o
desenvolvimento de um framework é um processo iterativo
que requer refinamento e evolução contínuos com base no
feedback dos usuários e nas mudanças nas necessidades do
domínio.
O primeiro passo foi levantar quais requisitos comuns
as aplicações beneficiadas pelo uso do framework possuem.
Foram levantados alguns requisitos funcionais, tais como:
conexão de múltiplos BSNs – a possibilidade de sincronizar
vários BSNs para controlar diferentes partes do corpo hu-
mano; interface de fácil utilização – uma interface intuitiva,
que permita a descoberta e conexão dos dispositivos BSN; e
configuração remota – a possibilidade de realizar a configu-
ração a partir de outro dispositivo (smartphone ou desktop).
Adotamos a arquitetura Modelo-Visão-Controle (MVC) e
seguimos o paradigma orientado a eventos. O MVC é um
padrão de design amplamente utilizado na programação de
software que separa os componentes de um sistema em três
partes distintas. Dessa forma, o asset fica completamente
encapsulado, deixando toda a parte de configuração dos BSN
transparente para o programador/usuário.
2.1
Modelo
O Modelo é o módulo responsável por fazer a gerência
dos dispositivos BSN, a coleta de dados e a rotação dos
Um framework de rastreamento corporal para reabilitação neuromotora com suporte a aplicativos multimídia
WebMedia’2024, Juiz de Fora, Brazil
objetos virtuais. É composto pela API2 de BLE Bluetooth-
HardwareInterface, pela classe BSNHardwareInterface. Ela
não armazena os dados do BSN, mas sim uma lista de uma
nova classe, a classe BSNDevice, que abstrai um dispositivo
BSN e a RotatableObject, que é a classe que de fato está
disponível para ser usada pelo programador.
A classe BSNHardwareInterface é responsável apenas por
realizar a comunicação com o BluetoothHardwareInterface e
por armazenar uma lista de BSNDevices. Após receber o co-
mando FindBSN, por meio da ConfigurationAPI, é iniciado o
processo de descoberta. Sempre que um beacon3 é encontrado,
a BSNHardwareInterface recebe uma resposta contendo o
nome e o endereço MAC do dispositivo, em seguida adiciona-
o à lista de BSNs e envia uma resposta à ConfigurationAPI,
informando que o dispositivo foi encontrado. Esse processo
é assíncrono e sempre é executado ao encontrar um novo
beacon, toda a comunicação sendo feita por meio de callbacks.
O procedimento de conexão de um BSN é análogo ao
de descoberta. A ConfigurationAPI envia o comando de
conexão, passando como parâmetro o endereço do dispositivo
que deseja conectar. Esse comando de conexão é repassado
até o dispositivo BLE, que eventualmente responde o sucesso
de conexão informando todos os serviços e características
disponíveis. Nesse momento o objeto BSNDevice cadastra
seus delegates para receberem os dados sempre que forem
atualizados no BSN.
A classe BSNDevice representa um BSN na aplicação,
contendo seu nome, endereço e um campo booleano que
indica se está conectada. Ela conta também com um objeto
da classe RotatableObject, responsável por manipular objetos
3D no Unity 4, possuindo métodos que de fato lidam com os
dados do BSN. Portanto, a BSNDevice não lida com os dados
propriamente dito, apenas os repassa à RotatableObject.
Sendo assim, é possível intercambiar o objeto controlado por
um BSN em tempo de execução.
A classe RotatableObject oferece os métodos que recebem
dados que são chamados pela BSNDevice a cada notify5 do
BSN, a aceleração linear, dados brutos e vetor de gravidade,
sendo eles LinearAccHandler(Vector3 linAcc), RawDataHan-
dler(Vector3 accelerometer, Vector3 gyroscope, Vector3 com-
pass) e GravityVectorHandler(Vector3 gravityVector), respec-
tivamente. Esses métodos não são implementados e devem
ser implementados pelo usuário programador. O método
ResetOffset() também está disponível e deve ser chamado
sempre que se desejar compensar a rotação do BSN.
A fim de sanar a diferença de orientação entre o dispositivo
e o ambiente virtual, a classe RotatableObject possui duas
funcionalidades: a troca de um eixo com outro e a inversão
do sentido de um eixo. Ambas funcionalidades são úteis para
2Application Programming Interface
3Sinal do transmissor BLE do BSN.
4O Unity é uma plataforma de desenvolvimento em tempo real, líder
no mercado mundial, que permite a criação de jogos e simulações
2D e 3D para diversas plataformas (PC, consoles, mobile, VR e AR),
utilizando um editor visual e programação através de scripting, ofere-
cendo ferramentas profissionais capazes de atender aos requisitos de
qualquer jogo.
5Envio de dados de um dispositivo BLE.
corrigir a orientação do eixo dependendo da posição em que
o BSN é anexado ao corpo do usuário.
Surge uma outra questão quando queremos que a posição
inicial do BSN seja diferente da posição padrão – T-pose 6.
Sempre que inicializado, o BSN precisa ser mantido em re-
pouso na horizontal por aproximadamente 30 segundos antes
de começar a utilizá-lo, a fim de calibrar os sensores. No caso
em que o objeto controlado é a perna de um avatar, o BSN
é anexado verticalmente na coxa do usuário, gerando uma
rotação imprevisível na coxa quando a rotação deveria ser
zero. Tendo isso em vista, é necessário a aplicação de um
offset sobre a rotação do BSN, de tal modo que:
𝑄𝑜𝑏𝑗𝑒𝑡𝑜= 𝑄𝑜𝑓𝑓𝑠𝑒𝑡* 𝑄𝐵𝑆𝑁.
(1)
O 𝑄𝑜𝑓𝑓𝑠𝑒𝑡deve ser calculado sempre que se deseja compen-
sar a rotação, ou seja, definir a rotação atual como rotação
zero. O cálculo é dado a partir da Equação 1, multiplicando o
Inverso da 𝑄𝑟𝑜𝑡𝑎çã𝑜𝑑𝑜𝐵𝑆𝑁em ambos os lados da equação.
𝑄𝑜𝑏𝑗𝑒𝑡𝑜* 𝑄𝐵𝑆𝑁
−1 = 𝑄𝑜𝑓𝑓𝑠𝑒𝑡* 𝑄𝐵𝑆𝑁* 𝑄𝐵𝑆𝑁
−1
(2)
𝑄𝑜𝑓𝑓𝑠𝑒𝑡= 𝑄𝑜𝑏𝑗𝑒𝑡𝑜* 𝑄𝐵𝑆𝑁
−1
(3)
Ao ser adicionada a um GameObject no Unity, a classe
RotatableObject oferece campos de configuração no Inspec-
tor. Outros dois campos são criados. O campo Simplified
Name é um apelido que é dado ao objeto que é mostrado
na interface de configuração; já o campo Precision Points
delimita o número de casas decimais que são utilizadas na
rotação. Seu valor padrão é dois, obtido após testes empíricos
onde foi possível observar que mais de duas casas decimais é
majoritariamente ruído.
2.2
Visão
O módulo Visão é composto por uma interface criada no
Unity com auxílio do asset de ícones e animações Modern
UI Pack [11]. Este módulo possui duas classes: UIController,
uma classe que gerencia a interface; e NetConfigurationServer,
que realiza a comunicação de rede.
Como a proposta foi criar um asset de integração em
que fosse possível funcionar em novos projetos e também
projetos legados, a interface foi projetada para ser o menos
invasiva possível. A interface possui um botão cuja função é
ativar e desativar a interface de configuração, que se encontra
inicialmente desativada (Figura 1a).
A tela de configuração é composta por um botão de iniciar
busca, um painel onde são listados os BSNs encontrados e
o botão que compensa (atribui o offset) a rotação de todos
os BSNs. O botão de busca permanece desabilitado até que
se encontre um servidor de configuração (apresentado da
Seção 2.3). Após iniciada a busca, cada BSN encontrado é
adicionado no painel. O item que representa o BSN encon-
trado conta com o nome e o endereço MAC do dispositivo,
como também um menu suspenso para escolha de qual objeto
deseja-se controlar e um botão de conexão.
6Na animação por computador, a T-pose é uma pose padrão para o
esqueleto de um modelo 3D antes de ser animado.
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
(a) Menu de configuração desati-
vado.
(b) Buscando um servidor na
rede.
(c) Interface pronta para busca.
(d) Interface com a busca ini-
ciada. Na imagem há os três
estados do BSN: Desconectado,
Conectando e Conectado.
Figure 1: Telas da interface do BSNAsset
A classe UIController gerencia a entrada do usuário e pro-
jeta na interface de usuário as informações recebidas pelo
Controller. Ela trabalha em conjunto com a NetConfigu-
rationClient, que se comunica em rede com a NetConfigu-
rationServer. Inicialmente, a NetConfigurationServer envia
mensagens de broadcast na rede com o token da aplicação em
busca de um servidor, que após receber é enviada uma req-
uisição de conexão (passos 2 e 3 do diagrama de sequência).
Em seguida o cliente solicita a lista de RotatableObjects e os
armazena em uma lista (passo 4). A partir desse momento
toda interação é disparada pelo usuário final por meio da
interface (passos 5, 6 e 7).
2.3
Controle
O Controle é a camada responsável por receber comandos
e aplicá-los ao Modelo, como também retornar à Visão o
estado atual do Modelo. É composto por uma Interface de
Programação de Aplicação (API) de configuração que dispõe
de métodos necessários para uma sessão de uso do asset.
Em conjunto com a ConfigurationAPI, a classe NetConfigu-
rationServer atua como uma classe que aguarda comandos da
rede, permitindo a configuração remota dos BSNs, escutando
sob um socket UDP gerenciado pela biblioteca Ruffles [12].
Inicialmente, a ConfigurationAPI aguarda por broadcasts na
rede na porta 5556, contendo um token. Se este token recebido
pelo broadcast for o esperado, este é enviado para o disposi-
tivo de origem e imediatamente aguarda pela requisição de
conexão.
Após conectado, a ConfigurationAPI aguarda por men-
sagens do cliente, respeitando o protocolo apresentado na
Tabela 1. Os dados são separados do comando usando o
caractere "/" como separador.
Mensagem
Dado Recebido
Servidor
Broadcast
Token
de
identifi-
cação da aplicação
Connection
-
StartDiscovering
-
Connect
Endereço MAC
GetObjNames
-
SetObj
Endereço
MAC
e
nome do objeto
Cliente
AllBsn
JSON contendo um
resumo de todos os
BSNs conectados
ObjNames
Lista dos nomes dos
RotatableObjects
disponíveis
Table 1: Mensagens que o servidor e cliente de configuração
aguardam
RESULTADOS
Os resultados deste trabalho consistem em implantações
diretas do BsnAsset como alternativa de interação em pro-
jetos de RV, como o Mazze e o Simulador de Pênalti, duas
aplicações que permitem demonstrar o funcionamento da
solução tanto para membros superiores, quanto para mem-
bros inferiores. É apresentada também uma solução quer
permite a criação e gravação de sessões (no contexto de
neuroreabilitação/fisioterapia).
3.1
Mazze
O Mazze é um jogo composto de um labirinto gerado aleato-
riamente (obtido em [16]), dentro do qual são posicionadas
Um framework de rastreamento corporal para reabilitação neuromotora com suporte a aplicativos multimídia
WebMedia’2024, Juiz de Fora, Brazil
moedas douradas e uma esfera (Figura 2). O objetivo do
jogo é coletar todas as moedas no menor tempo possível,
deslocando a esfera pelo labirinto.
Durante o jogo, o usuário rotaciona o BSN para controlar o
labirinto, que também é rotacionado, reproduzindo de forma
fidedigna o movimento realizado pelo usuário, movendo a
esfera pelos corredores. O BSN deve ser posicionado horizon-
talmente, podendo estar situado no dorso ou palma da mão.
Como é utilizada apenas a rotação do dispositivo (ângulos
de Euler), o BSNAsset foi utilizado apenas da forma plug-
and-play, pois apenas necessita indicar que o labirinto deve
ser controlado pelo dispositivo.
Figure 2: Estado inicial do jogo Mazze
O Mazze pode ser utilizado em processos de reabilitação
motora das mãos. Assim, o paciente estará se exercitando à
medida que joga
3.2
Simulador de Pênalti
O Simulador de Pênalti possibilitou o uso de todos os
recursos disponibilizados pelo BSNAsset. Nesta aplicação,
são utilizados os dados brutos do BSN, disponíveis via API
do asset. O jogo consiste em um campo de futebol virtual
com um goleiro e o usuário, como pode ser observado na
Figura 3.
Durante a simulação o usuário controla a perna e canela do
avatar com um par de BSNs colocados na parte anterior da
coxa (porção medial) e perna (porção distal), respectivamente.
A movimentação dos membros é feita utilizando o modo plug-
and-play do asset, que apenas indica no Unity que estes
objetos devem ser controlados pelo BSN. Por sua vez, a força
do chute é obtida a partir da API do asset, que fornece os
dados do acelerômetro do dispositivo.
No momento em que o pé do avatar entra em contato
com a bola, a força do chute é calculada multiplicando a
aceleração obtida do acelerômetro do BSN por um valor
pré-definido (representando a massa). Em seguida, a força é
aplicada na bola como um impulso na mesma direção e em
sentido contrário à normal da colisão.
A defesa do goleiro é feita calculando-se a posição em que a
bola chegará no gol usando equações de movimento balístico.
Em seguida, é disparada a animação em que o goleiro mais
Figure 3: Cenário do Simulador de Pênalti
se aproxima da bola. Entretanto, como as animações foram
criadas de forma determinística, os pontos que o goleiro
consegue defender são limitados. Desta maneira, o profissional
da saúde pode usar o simulador para execução de exercícios
referentes às pernas de forma lúdica.
3.3
Recorder e Recorder Controller
O ReBase [17] foi criado para ser uma base de dados não-
relacional utilizada para armazenar sessões de reabilitação
neuromotora contendo informações da sessão e os movimen-
tos executados pelo paciente durante o exercício. Em [17] é
apresentado uma estrutura de armazenamento como também
é fornecido uma API para gravação e recuperação desses
dados.
Visando consumir essa API de gravação utilizando dados
de rotação de um avatar em 3D, foram criados dois assets,
sendo eles: Recorder, e Recorder Controller. Os assets são
distribuidos em forma de pacotes que podem ser utilizados
de forma desacoplada em diferentes dispositivos, desde que
estejam sob a mesma rede LAN, ou no mesmo dispositivo,
como apresentado em 3.4.
A vantagem de se utilizar o Recorder Controller em um
dispositivo separado é permitir que aplicações em RV sejam
utilizadas pelo paciente através do uso de Óculos VR, uma
vez que o controle de gravação é feito remotamente.
3.3.1
Recorder. O Recorder é uma ferramenta criada para
reconhecer, interpolar e salvar os dados de rotação de um
avatar 3D, podendo ser utilizada independente do método
de rastreamento corporal, desde que o avatar onde a movi-
mentação for aplicada esteja dentro do padrão definido pelo
Recorder.
O fluxo de execução do Recorder inicia quando o comando
de iniciar a gravação da sessão de movimentos é recebido pelo
módulo de comunicação em rede. Em seguida os dados são
armazenados com um sample rate igual à taxa de quadros
(ou Frames per second (FPS)) atual da aplicação.
Uma vez que o ReBase não oferece possibilidade de gravar
o timestamp juntamente com a amostra, é necessário fixar um
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
sample rate fixo, de modo que a recuperação dos dados grava-
dos mantenha a velocidade real do movimento. Além disso,
sabemos que o FPS de uma aplicação depende diretamente
do hardware que ela está sendo executada, impossibilitando
o uso desses dados não confiáveis.
Sendo assim, ao fim da gravação de um movimento, as
amostras devem passar por um processo chamado de resam-
pling. Esse processo garante que independente do tamanho
do array de entrada, a saída terá o tamanho especificado,
sendo necessário utilizar interpolação linear para garantir
semelhança entre o array de entrada e o de saída.
3.4
Mobility Tests
Em sua dissertação, Otoboni [13] realizou uma análise
comparativa entre o sistema de rastreamento corporal ótico
VICON e a solução utilizando BSN. Seus testes se basearam
na execução da tarefa Sit-to-Stand que consiste em um exercí-
cio onde o paciente levanta e senta em uma cadeira sem apoio
das mãos. Para esse comparativo, obteve-se os ângulos go-
niométricos do Quadril (ângulo formado no eixo látero-lateral
pelo abdômen e coxa) e do Joelho (ângulo formado no eixo
látero-lateral pela coxa e perna) obtidos por um conjunto
de 3 BSNs posicionadas no abdômen, coxa e perna (plano
frontal) juntamente com as respectivas rotações obtidas pelo
VICON.
Sendo assim, surgiu-se a demanda de uma aplicação para
captura desses dados com os seguintes requisitos:
∙Aplicação simples, sem RV;
∙possibilidade de conexão de 3 BSNs simultaneamente;
e
∙dados devem ser gravados em um arquivo no formato
CSV (comma-separated Values) contendo as informações
de rotação brutas (Acelerômetro e Giroscópio) e os val-
ores dos ângulos goniométricos do quadril e do joelho.
Buscando atender aos requisitos da demanda, foi criado
o aplicativo BSN Mobility Tests, utilizando todos os assets
criados anteriormente: BSNAsset, Recorder, Recorder Con-
troller, todos funcionando em um único aparelho, uma vez
que não temos necessidade do paciente interagir com a apli-
cação. Além dos assets citados, um gravador de arquivos
CSV foi criado de modo que exportasse os dados da sessão
simultaneamente com a gravação no ReBase.
Finalmente, Otoboni [13] apresenta uma correlação regu-
lar/forte entre as rotações em ambas as soluções, reafirmando
a validade do uso de BSNs na área da saúde.
3.5
Body Tracking
A partir da demanda do BSN Mobility Tests surgiu a
necessidade da criação de uma aplicação que pudesse re-
alizar o rastreamento corporal completo utilizando BSNs.
Proporcionando gravação de dados de acelerômetro, giroscó-
pio e rotação 3D, além de possibilitar a gravação de dados
de rotação de cada articulação do corpo humano (ângulos
goniométricos).
A aplicação permite o usuário utilizar livremente as ar-
ticulações do corpo humano, limitando o uso a grupos de
articulações: superior direito – braço direito, antebraço dire-
ito; superior esquerdo – braço esquerdo, antebraço esquerdo;
inferior direito – coxa direita, perna direita; e inferior es-
querdo – coxa esquerda, perna esquerda.
Os BSNs devem ser posicionadas no corpo humano de
acordo com a Figura 4. Nos braços, os BSNs devem ser
posicionados com o led voltado para cima e com a face do
dispositivo voltada para fora do corpo. Nas pernas, as BSNs
devem ser posicionadas com o led voltado para cima e com a
face do dispositivo voltada para frente do corpo.
(a) Posiciona-
mento
das
BSNs
nos
braços.
(b) Posiciona-
mento
das
BSNs
nas
pernas.
Figure 4: Posicionamento das BSNs no corpo humano.
3.5.1
Gráficos. A seguir, apresentaremos os gráficos gera-
dos a partir dos dados coletados pelo BSN Body Tracking.
Os gráficos foram gerados utilizando o software Excel, e os
dados utilizados foram coletados de um usuário realizando
movimentos de flexão e extensão dos membros superiores e
inferiores.
3.5.2
Rosca direta. A rosca direta é um exercício de muscu-
lação que trabalha os músculos do bíceps braquial. O movi-
mento consiste em flexionar o cotovelo, aproximando a mão
do ombro. O movimento é realizado com os braços estendidos
ao lado do corpo, com os cotovelos flexionados e os punhos
em supinação.
A coleta de dados foi realizada com o usuário realizando
um total de 10 repetições do exercício com o braço direito
(Figura 5a) e 10 repetições do exercício com o braço esquerdo
(Figura 5b).
Nos gráficos da Figura 5 é possível observar que os ân-
gulos goniométricos do cotovelo diminui conforme o usuário
realiza a flexão do cotovelo, e aumenta conforme o usuário
realiza a extensão do cotovelo, variando entre 180º e 90º,
aproximadamente.
Os gráficos da Figura 6 representam os dados de acel-
erômetro coletados durante a execução do exercício. É pos-
sível observar que os dados de acelerômetro do braço direito
e do braço esquerdo são semelhantes, assim como os dados de
Um framework de rastreamento corporal para reabilitação neuromotora com suporte a aplicativos multimídia
WebMedia’2024, Juiz de Fora, Brazil
(a) Rosca direta com o braço direito.
(b) Rosca direta com o braço esquerdo.
Figure 5: Gráficos goniométricos da rosca direta.
acelerômetro do antebraço direito e do antebraço esquerdo.
Os acelerômetros de ambos antebraços apresentam valores
mais altos que os acelerômetros dos braços, pois os antebraços
executaram um movimento de rotação durante a execução
do exercício, enquanto os braços permaneceram estáticos.
3.5.3
Marcha estacionária. A marcha estacionária é um exer-
cício que consiste em simular a marcha sem sair do lugar. O
movimento é realizado suspendendo o joelho até que fique
alinhado com o quadril, e em seguida voltando a posição
inicial.
A coleta de dados foi realizada com o usuário realizando
um total de 10 repetições do exercício com a perna direita
7a e 10 repetições do exercício com a perna esquerda 7b.
Nos gráficos da Figura 7 é possível observar que os ângulos
goniométricos do joelho e do quadril diminuem conforme
o usuário realiza a flexão do joelho, e aumenta conforme o
usuário realiza a extensão do joelho, variando entre 180º e
90º, aproximadamente.
Os gráficos da Figura 8 representam os dados de acel-
erômetro coletados durante a execução do exercício. É pos-
sível observar que os dados de acelerômetro da perna direita
e da perna esquerda são semelhantes, assim como os dados
de acelerômetro da canela direita e da canela esquerda. Os
acelerômetros de ambas as pernas fazem uma troca de eixo,
representando uma rotação do dispositivo. Em contrapartida,
os acelerômetros das canelas não apresentam rotação, porém
(a) Acelerômetro do braço direito
(b) Acelerômetro do braço esquerdo
(c) Acelerômetro do antebraço direito
(d) Acelerômetro do antebraço esquerdo
Figure 6: Gráficos de acelerômetro da rosca direta
apresentaram variações nos eixos Y e Z, causados pelo movi-
mento de sobe e desce da canela.
WebMedia’2024, Juiz de Fora, Brazil
Ribeiro et al.
(a) Marcha estacionária com a perna direita
(b) Marcha estacionária com a perna esquerda
Figure 7: Gráficos da marcha estacionária
CONCLUSÕES E TRABALHOS FUTUROS
Com o crescimento da RV presente nas casas e consultórios
médicos, ela se mostra um excelente incentivo para o exercício
físico e ajuda na reabilitação de pacientes com dificuldades
neuromotoras causadas por várias patologias (como o aci-
dente vascular cerebral). A inevitabilidade de criar inúmeras
aplicações para uso específico se torna uma grande possi-
bilidade. Entretanto, embora os óculos de RV facilitem e
tornem a imersão mais acessível, um método de interação
natural ainda é visto como um desafio no desenvolvimento
de aplicações de RV.
A necessidade de criar um método de interação natural
que seja suficientemente genérico para funcionar em várias
aplicações de RV e que ofereça espaço para a inserção de
particularidades de cada projeto tem governado este trabalho
e incentivado a criação do BSNAsset.
Propusemos e desenvolvemos um asset que nos permite
realizar a conexão de múltiplos dispositivos, rastrear o corpo
humano e mover um avatar virtual. A solução desenvolvida
é fácil de importar e usar em qualquer projeto da Unity.
Como mostramos, é possível gerar aplicações de rastreamento
corporal completo, interagindo com membros superiores e
inferiores.
AGRADECIMENTOS
Os autores agradecem o financiamento em parte pela Co-
ordenação de Aperfeiçoamento de Pessoal de Nível Superior -
(a) Acelerômetro da perna direita
(b) Acelerômetro da perna esquerda
(c) Acelerômetro da canela direita
(d) Acelerômetro da canela esquerda
Figure 8: Gráficos de acelerômetro da marcha estacionária
Brasil (CAPES) - Código de Financiamento 001, Conselho
Nacional de Desenvolvimento Científico e Tecnológico (CNPq)
e a Fundação de Amparo à Pesquisa do Estado de Minas
Um framework de rastreamento corporal para reabilitação neuromotora com suporte a aplicativos multimídia
WebMedia’2024, Juiz de Fora, Brazil
Gerais (FAPEMIG). Também agradecemos à Fundação de
Amparo à Pesquisa e Inovação do Espírito Santo (FAPES) pe-
los recursos disponibilizados através dos projetos PROAPEM
(368/2022 - P: 2022-NGKM5) e PDPG (129/2021 - P: 2021-
GL60J), fundamentais para a realização deste trabalho.

--- FIM DO ARQUIVO: 30130.txt ---

--- INÍCIO DO ARQUIVO: 30131.txt ---
Um Framework para Análise Bidimensional de Disseminação de
Informações em Plataformas de Mídias Sociais
Geovana S. Oliveira1, Otávio Venâncio3, Vinícius Vieira4, Jussara Almeida3
Ana P. C. Silva3, Ronan Ferreira2, Carlos H. G. Ferreira1
geovana.so@aluno.ufop.edu.br,otavio.venancio@dcc.ufmg.br,vinicius@ufsj.edu.br,jussara@dcc.ufmg.br
ana.coutosilva@dcc.ufmg.br,ronan.ferreira@ufop.edu.br,chgferreira@ufop.edu.br
1Departamento de Computação e Sistemas, Universidade Federal de Ouro Preto, Brasil
2Departamento de Ciências Exatas e Aplicadas, Universidade Federal de Ouro Preto, Brasil
3Departamento de Ciência da Computação, Universidade Federal de Minas Gerais, Brasil
4Departamento de Ciência da Computação, Universidade Federal de São João Del Rey, Brasil
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
Especificamente, busca-se caracterizar perfis de disseminação de
informação, explorando uma lacuna deixada por outros trabalhos
na literatura e avaliando o fenômeno de propagação de conteúdo de
forma bidimensional. A premissa principal explorada aqui é que os
usuários que utilizam esses ambientes podem ser classificados com
base no volume do conteúdo com os quais co-interagem e na veloci-
dade com a qual isso é feito. A partir dessa classificação, usuários
de uma mesma classe podem ser caracterizados de forma global
(distribuição de conectividade) e local (formação de comunidades).
Considerando a forma como se organizam, acredita-se que seja
possível revelar padrões de disseminação de informação distintos.
Para isso, consideramos as seguintes etapas: inicialmente, mode-
lamos redes de co-interações, uma considerado o volume e outra
considerando a velocidade dessas interações. Em seguida, aplicamos
técnicas de extração de backbone para identificar as arestas mais
salientes nas duas redes, simultaneamente. Após, classificamos as
arestas de acordo com o julgamento dos métodos de backbone em
diferentes perfis. Essa abordagem nos permite categorizar as arestas
que ligam dois dados usuários em quatro perfis de disseminação
de informações: (1) usuários que disseminam um baixo volume de
informações e de forma lenta; (2) usuários que disseminam um alto
volume de informações, mas de forma lenta; (3) usuários que dis-
seminam um baixo volume, porém de forma rápida; e (4) usuários
que disseminam um alto volume de informações e de forma rápida.
Por fim, caracterizamos a topologia observada em sub-redes for-
madas por usuários desses quatro perfis de interações, investigando
a estrutura de comunidades delas.
Aplicamos nosso framework em dois estudos de caso nas platafor-
mas Twitter/X e Telegram durante períodos que contêm fortes ev-
idências de coordenação [1, 15, 42]. Nossos principais resultados
mostram que o framework proporciona um aprofundamento da
compreensão do fenômeno de disseminação de informações a partir
de contas de usuários em plataformas de mídias sociais. Observamos
diferentes perfis de disseminação de informações, caracterizados
tanto pelo volume quanto pela rapidez das interações. Destacamos
que no Twitter/X as comunidades são mais coesas e com alto vol-
ume de retweets, enquanto no Telegram as interações se distribuem
em comunidades menores, com um volume menor, mas com maior
rapidez de disseminação durante eventos críticos.
O restante do trabalho está organizado da seguinte forma: a
Seção 2 apresenta os trabalhos relacionados. A Seção 3 detalha a
metodologia, enquanto a Seção 4 descreve os estudos de casos. Em
seguida, a Seção 5 apresenta os resultados e, por fim, a Seção 6 traz
as conclusões e direções futuras.
TRABALHOS RELACIONADOS
Vários estudos anteriores empregaram modelos de rede para detec-
tar e analisar a disseminação de informações em plataformas de
mídias sociais, como Twitter/X, Telegram, WhatsApp, Instagram,
etc [3, 7, 10, 16, 18, 19, 35, 39, 40, 43]. Ainda, um grande número de
trabalhos destaca a importância de se identificar e remover arestas
que possam refletir relações esporádicas e aleatórias que podem
representar ruído ao modelo de redes. Com essa remoção, o modelo
concentra-se apenas em arestas relevantes para o fenômeno de
disseminação de informações [9, 12–14, 27–31, 42]. Dessa forma, as
arestas salientes revelam o que é chamado de backbone: uma estru-
tura subjacente à rede inicialmente modelada que busca fornecer
uma visão mais clara e organizada para identificar padrões topológi-
cos que auxiliem a compreensão da propagação de conteúdo.
Por exemplo, Pacheco et al. [30] investigaram campanhas de
desinformação contra a Defesa Civil Síria no Twitter/X, explorando
um modelo de rede que conecta usuários que postaram tweets sim-
ilares. Os autores empregaram a extração de backbone para re-
mover arestas, cujos pesos estão abaixo de um determinado limiar
(limiarização). Em outro trabalho, os mesmos autores propuseram
identificar ações coordenadas nas mídias sociais, notadamente no
Twitter/X, em diferentes cenários, também empregando a extração
de backbone por meio de limiarização [31]. Keller et al. [19] explo-
raram a rede de retweets para estudar a coordenação de anúncios
durante a eleição presidencial na Coreia do Sul também explorando
limiarização.
Linhares et al. [21] e da Rosa et al. [9] investigaram comunidades
de usuários no Twitter/X que foram, potencialmente, usadas, para
disseminação coordenada de conteúdos relacionados a alegação de
fraude durante a eleição dos EUA de 2020, usando métodos como
Disparity Filter e Tripartite Backbone Extraction (TriBE) [12, 17, 34].
Na mesma direção, Araujo et al. [1] estudaram a promoção coor-
denada de campanhas políticas antecipadas no Twitter/X durante
o período pré-eleitoral brasileiro de 2022, utilizando uma rede de
co-retweets e o Disparity Filter + Neighborhood Overlapping. No-
bre et al. [28] investigaram a rede de co-compartilhamento de mí-
dia, construída conectando usuários que compartilharam o mesmo
conteúdo, também com o uso do Disparity Filter para estudar co-
munidades. Também usando o Disparity Filter e Disparity Filter
+ Neighborhood Overlapping para revelar arestas salientes e com
evidências de coordenação no Telegram, Venâncio et al. [42] investi-
garam a disseminação de informações políticas durante as eleições
brasileiras de 2022 e os tumultos de 8 de janeiro.
Embora esses esforços explorem a extração de backbone para
compreender a disseminação de informações de forma estruturada
na rede, um aspecto desconsiderado por esses estudos é a distinção
das formas como a disseminação pode ocorrer. Especificamente,
os esforços supramencionados focam em entender a disseminação
apenas de um ponto de vista, do volume. Dessa forma, há uma
lacuna na literatura quanto a estratégias para identificar diferentes
perfis de disseminação de informações por esses grupos de usuários
potencialmente coordenados e compreender o sincronismo con-
siderando duas dimensões: o volume e a velocidade da informação
disseminada, o que é explorado neste trabalho.
FRAMEWORK PROPOSTO
O framework proposto neste trabalho consiste em várias etapas,
conforme ilustrado na Figura 1. Inicialmente, modelamos duas re-
des, uma que representa o volume de co-interações e outra que
representa o intervalo de tempo entre essas co-interações. Em
seguida, aplicamos métodos de extração de backbone para iden-
tificar as arestas mais relevantes em cada modelo de rede. Então,
classificamos as arestas em perfis distintos de disseminação de infor-
mações, com base em sua relevância, usando métodos de extração de
backbone. Finalmente, caracterizamos e analisamos a topologia das
redes formada por essas classes incluindo a análise de comunidades.
Um Framework para Análise Bidimensional de Disseminação de Informações em Plataformas de Mídias Sociais
WebMedia’2024, Juiz de Fora, Brazil
Extração do
backbone
Dados do
estudo de
caso
Modelagem da
rede de co-
interações
Modelagem da
rede de tempo
médio de co-
interações
Análise de
padrões de
disseminação
Classificação
Extração do
backbone
Figura 1: Visão geral do framework proposto.
3.1
Modelagem das Redes
A metodologia proposta neste trabalho parte da definição de dois
modelos de redes: um que representa o volume de co-interações
e outro que representa o intervalo tempo entre essas interações.
A primeira rede de co-interações é representada por um grafo
𝐺volume = (𝑉, 𝐸volume), tal que cada elemento 𝑣𝑖∈𝑉representa um
usuário, e cada elemento 𝑒𝑖𝑗∈𝐸volume é uma aresta que conecta
um par de usuários. O peso da aresta 𝑤volume(𝑒𝑖𝑗) representa o
número de co-interações entre os usuários 𝑣𝑖e 𝑣𝑗.
Já na rede 𝐺tempo = (𝑉, 𝐸tempo), as arestas 𝑒𝑖𝑗∈𝐸tempo ar-
mazenam informação sobre o tempo decorrido entre as interações
dos usuários representados por 𝑣𝑖e 𝑣𝑗. Embora outras funções pos-
sam ser utilizadas para agregar os tempos de todas as co-interações
de 𝑣𝑖e 𝑣𝑗, neste trabalho, o peso 𝑤tempo(𝑒𝑖𝑗) de cada aresta 𝑒𝑖𝑗
refere-se ao tempo médio das interações dos usuários representa-
dos por 𝑣𝑖e 𝑣𝑗. Assim, 𝑤tempo(𝑒𝑖𝑗) =
𝑤volume(𝑒𝑖𝑗)
Í𝑤volume(𝑒𝑖𝑗)
𝑝=1
𝑡𝑝
𝑖𝑗
,
em que 𝑝é o tempo decorrido na 𝑝-ésima interação entre 𝑣𝑖e 𝑣𝑗.
Os algoritmos utilizados neste trabalho (tanto para extração de
backbone, como para identificação de comunidades) consideram
a noção de que o peso de uma aresta 𝑒𝑖𝑗deve ser diretamente
proporcional à importância da relação entre 𝑣𝑖e 𝑣𝑗, o que é contra-
intuitivo com a definição da função de peso 𝑤tempo apresentada
anteriormente. Um tempo curto entre as ações de𝑣𝑖e𝑣𝑗de enviarem
uma mesma mensagem em uma rede social, por exemplo, deveria
sugerir uma forte relação entre 𝑣𝑖e 𝑣𝑗, e o contrário é definido por
𝑤tempo. Para que isso seja alcançado, os pesos são normalizados
de acordo com a Equação 1, garantindo que relações mais fortes
sejam representadas por valores de peso mais altos. Assim, o peso
normalizado 𝑤′
tempo(𝑒𝑖𝑗) é dado por:
𝑤′
tempo(𝑒𝑖𝑗) = max(𝑊tempo) −𝑤tempo(𝑒𝑖𝑗) + 1
(1)
em que max(𝑊tempo) é o maior peso encontrado na rede 𝐺tempo. O
termo +1 evita que haja pesos nulos de forma indevida. Para sim-
plificar, o peso das arestas 𝑤tempo irá referir-se à definição de peso
normalizado apresentada na Equação 1 no restante deste trabalho.
3.2
Extração dos Backbones
A segunda etapa do framework consiste em extrair os backbones
de ambas redes modeladas na etapa anterior. Diversos métodos de
extração de backbone propostos na literatura são aplicados para
modelar padrões de disseminação, tipicamente explorando a dis-
crepância e heterogeneidade das redes. Exemplos incluem Threshold
[30], Disparity Filter [34], Disparity Filter + Neighborhood Overlap
[21] e Polya Urn [22]. Em resumo, esses métodos são usados para ex-
plorar a heterogeneidade da rede, identificando quão salientes são as
arestas com pesos significativamente maiores, com base em padrões
individuais (locais) ou de rede (globais), como representativas de
interações persistentes e repetitivas. Com exceção do Thresholding,
que é um método puramente estrutural e opera de forma global,
excluindo arestas com base em um limiar da distribuição dos pesos
das arestas da rede, os demais são métodos probabilísticos que con-
stroem modelos nulos para cada nó e são capazes de capturar, do
ponto de vista local, o quão saliente uma aresta é de acordo com o
peso. Uma revisão do uso destes métodos para este contexto está
disponível na literatura [17].
Nosso principal argumento nessa etapa reside na hipótese de que
algumas arestas representarão co-interações fortes e consistentes
durante o período observado, enquanto outras serão esporádicas,
podendo ainda terem sido observadas em um intervalo de tempo
excepcionalmente curto ou um intervalo grande. Então, vamos anal-
isar o fenômeno sob a perspectiva do volume de co-interações e do
intervalo de tempo em que ocorrem. Para isso, considere novamente
os seguintes cenários em uma plataforma de rede social: (1) uma
mensagem é disseminada massivamente, compartilhada por muitos
usuários pouco ativos na plataforma, formando assim arestas de
co-interações com pesos pequenos; e (2) um outro conjunto de
mensagens é disseminado de forma consistente e repetitiva por um
outro conjunto de usuários, nesse caso formando arestas de pesos
maiores. De maneira análoga, podemos considerar o aspecto tempo-
ral da disseminação dessas mensagens, buscando compreender se
isso ocorre de maneira muito rápida ou não. Ambas as perspectivas,
– pelo volume de co-interações e pelo tempo em que ocorrem – já
foram mostradas como importantes no processo de disseminação
de informações de forma coordenada na literatura. Existem fortes
evidências da alocação de contas temporárias, pagas ou não, para
disseminarem tweets específicos de forma rápida [4, 11, 20, 41] e da
atuação de grupos de usuários disseminando informações de forma
sistemática e consistente [21, 29, 42] em diversas plataformas, como
o Twitter/X. Porém, a incorporação conjugada dessas duas dimen-
sões à análise do fenômeno de disseminação de informação não
é utilizada em outros trabalhos na literatura e pode ser apontada
como uma novidade trazida pelo framework proposto.
Neste trabalho, propomos o uso do método de Polya Urn Fil-
ter [22], devido à sua flexibilidade e capacidade de considerar a
importância local das arestas na identificação do backbone. No en-
tanto, vários métodos de extração de backbone para identificação de
atividades relacionadas à coordenação estão na literatura e podem
empregados [17]. Em resumo, Polya Urn assume que os pesos das
arestas surgem do processo agregado das preferências individuais
dos nós para interagir entre si ao longo do tempo. Ele também
pressupõe que as interações entre os nós são mantidas e reforçadas,
de modo que quanto maior o número de interações entre dois nós,
maior a probabilidade de eles interagirem novamente. Um modelo
de referência é construído para cada aresta, capaz de capturar o
reforço das interações existentes, examinando o grau e a força (a
soma dos pesos de todas as arestas incidentes ao nó) de cada nó
conectado a essa aresta. Esse mecanismo de reforço pode ser regu-
lado e estimado por meio de um processo de ajuste fino. Arestas
salientes são aquelas cujos pesos desviam de forma significativa do
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
modelo de referência com valores maiores, de acordo com um nível
de confiança definido [22]. Dessa forma, o Polya Urn é aplicado ao
grafo 𝐺volume, gerando o grafo 𝐵volume que captura os nós e arestas
com padrões de co-interações mais altos do que o esperado.
De maneira análoga, o método de backbone também deve ser
aplicado ao grafo𝐺tempo. É importante apontar que os pesos𝑤tempo
são normalizados de acordo com a Equação 1, para que o método de
backbone funcione como foi projetado e capture arestas com pesos
mais altos do que o esperado [17, 22]. Assim, a aplicação do método
Polya Urn ao grafo 𝐺tempo, gera o backbone 𝐵tempo, que captura
as arestas com tempos médios de disseminação que fogem ao es-
perado. Ao extrair os backbones das duas redes 𝐺volume e 𝐺tempo,
podemos classificar as arestas mantidas como representativas de
interações fortes e consistentes, enquanto as arestas removidas
representam interações esporádicas e menos significativas. Dessa
forma, é possível extrair da rede os nós e arestas mais significativos
tanto em volume de informação quanto em tempo de disseminação,
classificados conforme explicado na próxima seção.
3.3
Classificação das Arestas
Ambos os modelos de redes considerados pela metodologia proposta
(representados por 𝐺volume e 𝐺tempo) correspondem ao mesmo con-
junto de arestas, associando cada par de vértices que representam
usuários que co-interagiram nas redes sociais. É possível, então,
definir quatro classes de arestas, com base na sua presença ou ausên-
cia em 𝐵volume e 𝐵tempo, após a identificação dos backbones a partir
de 𝐺volume e 𝐺tempo, respectivamente. Formalmente, cada classe de
arestas é definida como:
• Classe 1 (baixo volume e baixa velocidade):
𝐶1 =

𝐸volume −𝐸𝐵
volume

∩

𝐸tempo −𝐸𝐵
tempo

(2)
Essas arestas não são mantidas em nenhum dos backbones.
• Classe 2 (alto volume e baixa velocidade):
𝐶2 = 𝐸𝐵
volume ∩

𝐸tempo −𝐸𝐵
tempo

(3)
Essas arestas são mantidas em 𝐵volume mas não em 𝐵tempo.
• Classe 3 (baixo volume e alta velocidade):
𝐶3 =

𝐸volume −𝐸𝐵
volume

∩𝐸𝐵
tempo
(4)
Essas arestas são mantidas em 𝐵tempo mas não em 𝐵volume.
• Classe 4 (alto volume e alta velocidade):
𝐶4 = 𝐸𝐵
volume ∩𝐸𝐵
tempo
(5)
Essas arestas são mantidas em ambos os backbones.
Como ilustração, no contexto do Twitter/X, as arestas classifi-
cadas podem ser interpretadas da seguinte forma: Classe 1 indica
que dois usuários não retuitaram um alto volume de tweets e quando
isso acontece, é feito em um grande intervalo de tempo médio, sug-
erindo baixo volume e velocidade de disseminação. Classe 2 indica
que dois usuários retuitaram muitos tweets, mas com um grande
intervalo de tempo entre eles, sugerindo um volume alto, porém
pouco veloz de disseminação. Classe 3 indica que dois usuários
retuitaram poucos tweets e com um intervalo muito curto de tempo
entre eles, sugerindo alta velocidade de disseminação, mas com
baixo volume. Classe 4 indica que dois usuários retuitaram muitos
tweets em um curto intervalo de tempo, sugerindo uma forte coorde-
nação entre eles. Em resumo, a abordagem proposta aqui difere das
metodologias aplicadas na literatura por permitir a diferenciação
de perfis e estruturas além de uma visão unidimensional.
3.4
Análise de Padrões de Disseminação
Considerando a classificação das arestas descrita na seção anterior,
quatro sub-redes são identificadas, cada uma relacionada a uma das
classes identificadas com base no volume e na velocidade das inter-
ações entre os vértices. Assim, podemos definir perfis de dissemi-
nação tomando como ponto de partida a análise dos padrões obser-
vados nas diferentes classes, investigando a organização topológica
que emerge de cada sub-rede, assim como semelhanças e diferenças
entre elas. Em particular, como se dá a organização dos nós dessas
sub-redes em estruturas de comunidades, isto é, subconjuntos de
nós com conexões mais densas entre si do que com o restante da
rede. A investigação da organização de nós em comunidades tem
sido aplicada com sucesso no domínio de plataformas de mídias
sociais para a identificação de ideologias específicas, caracterização
de campanhas políticas antecipadas, detecção de desinformação ou
notícias falsas [1, 14, 28, 29]. Embora diferentes métodos possam
ser empregados para identificação de comunidades de nós em redes
com as características dos modelos aqui propostos [36], neste tra-
balho é utilizado o algoritmo de Louvain [5], amplamente utilizado
em outros trabalhos na literatura.
O algoritmo de Louvain visa maximizar a modularidade, que
mede a qualidade da divisão de uma rede em comunidades. Modu-
laridade avalia o quão densamente conectados os nós estão dentro
das comunidades, em comparação com uma rede aleatória com a
mesma distribuição de conexões. O valor da modularidade varia
de -0,5 a 1, onde valores acima de 0,3 indicam comunidades bem
definidas. O algoritmo é heurístico e funciona de forma aglomer-
ativa. Ele começa atribuindo cada nó da rede a uma comunidade
própria. Em seguida, o algoritmo move nós entre comunidades
para aumentar a modularidade. Quando não é possível melhorar
mais a modularidade, as comunidades formadas são tratadas como
novos nós, e o processo recomeça. O resultado final é a estrutura
de comunidades com a maior modularidade encontrada.
Neste trabalho, limitamos o escopo dos experimentos à análise
topológica da estrutura das redes. Entretanto, encorajamos uma
compreensão mais profunda do fenômeno de disseminação de infor-
mação por meio da análise dos conteúdos das mensagens enviadas
em cada um dos grupos, que pode evidenciar padrões de comporta-
mento, estratégias de disseminação de desinformação, ou esforços
coordenados para promover determinadas informações.
ESTUDOS DE CASO
Nesta seção, descrevemos os dois estudos de casos explorados à luz
do framework descrito na seção anterior.
4.1
Twitter/X
Nosso primeiro estudo de caso consiste na análise de disseminação
de informações no Twitter, recentemente redesenhado como X1. O
Twitter/X tem sido amplamente usado para disseminação de infor-
mações, especialmente em eventos políticos [9, 21, 30, 32, 33]. Aqui,
concentramos nossa análise em torno de eventos relacionados às
1www.x.com
Um Framework para Análise Bidimensional de Disseminação de Informações em Plataformas de Mídias Sociais
WebMedia’2024, Juiz de Fora, Brazil
eleições brasileiras de 2022, em uma coleção de dados disponibiliza-
dos em [15]. Essa coleção é composta por dados coletados entre 1o
de outubro de 2022 e 10 de fevereiro de 2023, período que abrangeu
eventos-chave das eleições gerais brasileiras de 2022, como os dois
turnos de votação, apuração dos resultados, pesquisas eleitorais,
debates e o ataque aos prédios da Câmara dos Deputados, Senado
e do Superior Tribunal Federal em 8 de janeiro de 2023. Durante
esse período, já se observava a disseminação de informações de
conteúdos antidemocráticos no Twitter2.
Para analisar as interações dos usuários, consideramos os mode-
los de redes para duas janelas de tempo diárias 𝑇= {01/11/2022,
08/01/2023}, que correspondem aos dois dias de maior atividade
observada. Dessa forma, temos os grafos 𝐺01/11/2022
volume
e 𝐺01/11/2022
tempo
,
correspondentes aos usuários com atividade na data de 01/11/2022 e
suas respectivas co-interações nessa data, e os grafos 𝐺08/01/2023
volume
e
𝐺08/01/2023
tempo
, correspondentes aos usuários com atividade na data de
01/11/2023 e suas respectivas co-interações nessa janela de tempo.
Os pesos 𝑤(𝑒𝑖𝑗) das ligações entre os usuários 𝑣𝑖e 𝑣𝑗são calculados
de acordo com o modelo definido na Seção 3.1. Os parâmetros do
método Polya Urn para a extração dos backbones neste estudo de
caso foram definidos conforme as diretrizes de [17, 22], utilizando o
parâmetro de reforço 𝑎= 0.5 para 𝐺volume e para 𝐺tempo, e 𝛼= 0.05
para 𝐺volume e 𝛼= 0.1 para 𝐺tempo.
4.2
Telegram
Nosso segundo estudo de caso diz respeito à disseminação de in-
formações em grupos de Telegram. A plataforma conecta usuários
em conversas ponta a ponta e também em grupos, que têm se
mostrado eficazes para a grande disseminação de informações
[6, 10, 26, 38, 39]. Neste estudo de caso, utilizamos mensagens de
grupos politicamente orientados do Telegram no Brasil, publicadas
entre 1ode setembro de 2022 e 31 de janeiro de 2023, incluindo o
período das eleições presidenciais e os tumultos de janeiro de 2023,
em uma coleção de dados disponibilizada em [42]. Para analisar a
disseminação dos usuários para disseminar conteúdo nos grupos do
Telegram, utilizamos os modelos de rede originalmente usados pelos
autores, que conecta usuários que compartilharam o mesmo con-
teúdo textual entre os grupos observados. Além disso, analisamos
três janelas de tempo de 15 dias, sendo 7 antes e 7 depois em torno
dos seguintes eventos para análise: 𝑇= {01/10/2022(1o𝑇𝑢𝑟𝑛𝑜),
30/10/2022(2o𝑇𝑢𝑟𝑛𝑜), 08/01/2023(𝑇𝑢𝑚𝑢𝑙𝑡𝑜𝑠)}, representativas dos
dois turnos da eleição e o tumulto de 8 de janeiro de 2023.
Consideramos, então, os modelos de redes correspondentes às
janelas de tempo:𝐺1oTurno
volume e𝐺1oTurno
tempo ,𝐺2oTurno
volume e𝐺2oTurno
tempo , e𝐺Tumultos
volume
e 𝐺Tumultos
tempo
, correspondentes aos usuários que postaram ao menos
uma mensagem em um dos grupos do Telegram durante a janela
especificada. Cada aresta 𝑒𝑖𝑗indica que os usuários 𝑣𝑖e 𝑣𝑗com-
partilharam a mesma mensagem (conteúdo textual idêntico) pelo
menos uma vez dentro da mesma janela de tempo. Os parâmetros do
método Polya Urn para a extração dos backbones foram novamente
definidos conforme as diretrizes de [17, 22], utilizando o parâmetro
de reforço 𝑎= 0.5 e 𝛼= 0.05 para 𝐺volume e 𝛼= 0.1 para 𝐺tempo.
2https://portal.stf.jus.br/noticias/verNoticiaDetalhe.asp?idConteudo=508935&tip=
UN,https://www.bbc.com/portuguese/brasil-63990040
Tabela 1: Topologia das redes do Twitter/X e Telegram.
Rede
Data
𝑛
𝑚
𝑑
ˆ𝑘
𝐶𝑀𝐴
𝐶𝐶
# 𝐶𝑜𝑚.
𝑄
Twitter/X
08/01/2023
28,247
86,478,838
0.216
6123,045
0.689
0.232
Twitter/X
01/11/2022
27,920
72,777,139
0.186
5213,262
0.700
0.487
Telegram
1oTurno
1,625
81,236
0.062
99,983
0.641
0.117
Telegram
2oTurno
3,444
161,383
0.027
93,718
0.621
0.123
Telegram
Tumultos
4,772
71,046
0.006
29,776
0.549
0.349
RESULTADOS
Esta seção apresenta nossos resultados. Primeiro, mostramos a
análise topológica das redes estudadas. Em seguida, analisamos os
padrões de disseminação de acordo com o framework proposto.
5.1
Análise Topológica das Redes
A Tabela 1 apresenta as características estruturais em cada rede
originalmente modelada, isto é, sem a etapa de extração de backbone,
tanto do Twitter/X como do Telegram. 𝑛é o número de nós em cada
rede, 𝑚é o número de arestas, 𝑑é a densidade das redes, ˆ𝑘é o grau
médio dos nós, 𝐶𝑀𝐴é o coeficiente médio de agrupamento dos nós,
𝐶𝐶é o número de componentes conectadas, # 𝐶𝑜𝑚. é o número de
comunidades encontradas e 𝑄é a modularidade da partição.
Ao comparar as redes do Twitter/X e Telegram, observamos
características distintas em suas topologias, que é resultado da
própria diferença entre os modelos para a construção das redes.
Nas redes do Twitter/X, as arestas entre vértices são criadas quando
seus respectivos usuários retweetam um mesmo conteúdo, o que
acontece com uma frequência bastante elevada, fazendo com que
o número de arestas (e, consequentemente, a densidade e o grau
médio) seja bastante elevado. Nas redes do Telegram, uma aresta só
é criada quando um par de usuários publica mensagens de conteúdo
idêntico e, ainda assim, observa-se uma alta quantidade de arestas
(mesmo que muito menor que o observado nas redes do Twitter/X).
Essa alta densidade observada nas redes em ambos os estudos de
caso (e especialmente no Twitter/X) representa um forte indicativo
da necessidade da aplicação de alguma estratégia para filtragem
das relações mais importantes para uma melhor compreensão do
fenômeno de disseminação de informação, o que neste trabalho é
realizado através da aplicação do método para extração de back-
bone. A presença de um alto número de arestas e, potencialmente,
uma grande presença de relações ruidosas, prejudica, inclusive, a
identificação da organização dos vértices em comunidades, já que
observa-se valores de modularidade que refletem grupos pouco
claros. Mesmo nas redes do Telegram, que possuem suas compo-
nentes bastante fragmentadas, nota-se valores de modularidade
baixos, correspondentes à divisão dos vértices em grupos no máx-
imo fracamente evidentes, como no caso da janela de tempo dos
Tumultos.
5.2
Extração dos Padrões de Disseminação
5.2.1
Twitter. Movendo para a extração dos perfis de dissemi-
nação e caracterização das redes, as características topológicas de
rede, bem como da estrutura de comunidades para cada classe das
redes do Twitter/X, são apresentadas na Tabela 2.
De imediato, nota-se que há, para as classes nas duas janelas de
tempo, um equilíbrio no número de vértices, que não é observado
para as arestas das redes. A Figura 2 ilustra a distribuição de vértices
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
Tabela 2: Topologia das redes do Twitter/X por classe.
Classe
Data
𝑛
𝑚
𝑑
ˆ𝑘
𝐶𝑀𝐴
𝐶𝐶
# 𝐶𝑜𝑚.
𝑄
Classe 1
01/11/2022
27,911
47,173,429
0.121
3380,275
0.467
0.494
Classe 2
01/11/2022
20,363
1,373,833
0.0066
134,934
0.586
0.434
Classe 3
01/11/2022
27,899
23,980,819
0.061
1719,116
0.356
0.471
Classe 4
01/11/2022
18,453
249,058
0.0014
26,993
0.271
0.623
Classe 1
08/01/2023
28,230
41,729,887
0.104
2956,421
0.300
0.257
Classe 2
08/01/2023
27,401
4,940,636
0.013
360,617
0.397
0.311
Classe 3
08/01/2023
28,236
37,192,895
0.093
2634,430
0.389
0.323
Classe 4
08/01/2023
27,578
2,615,420
0.0068
189,674
0.350
0.342
Classe da aresta
0.0
0.2
0.4
0.6
0.8
1.0
Fração
Variável
Arestas
Nós
(a) Janela de tempo: 01/11/2022
Classe da aresta
0.0
0.2
0.4
0.6
0.8
1.0
Fração
Variável
Arestas
Nós
(b) Janela de tempo: 08/01/2023
Figura 2: Fração de nós e arestas por classe nas redes do Twitter/X.
e arestas entre as classes e nos ajuda a ter uma melhor compreensão
das características topológicas das redes modeladas para o Twit-
ter/X. Ademais, existe um desequilíbrio no número de arestas das
redes de diferentes classes. Enquanto a rede para a Classe 1, que
representa relações que não são salientes nem para a dimensão vol-
ume e nem para a dimensão tempo, apresenta um elevado número
de arestas, a rede para a Classe 4, que representa relações que são
excepcionalmente relevantes em ambas as dimensões, apresenta
um número bastante inferior de arestas, especialmente na janela ref-
erente à data de 01/11/2022. Isso se reflete em algumas informações
exibidas na Tabela 2, como os graus médios (e, consequentemente as
densidades), que são substancialmente mais altas para a Classe 1 e
mais baixas para a Classe 4, mais uma vez revelando a importância
da identificação das arestas mais relevantes para a compreensão do
fenômeno de disseminação de informação. É interessante também
notar uma diferença entre as características das redes da Classe 2,
que preserva arestas salientes relacionadas ao volume e da Classe
3, que preserva arestas salientes relacionadas ao tempo. Esses resul-
tados mostram que a dimensão volume apresenta relações muito
mais díspares do que a dimensão tempo, fazendo com que as redes
da Classe 2 sejam substancialmente menos densas que as redes
da Classe 3.
A organização dos vértices em comunidades nas redes com difer-
entes classes de arestas, mais uma vez, reforça a importância da apli-
cação do backbone de forma bidimensional. Isso fica claro quando
observamos os valores de modularidade da Classe 4 em compara-
ção com as outras classes em ambas janelas, mas em especial na rela-
cionada à data de 01/11/2022. Isso mostra que quando são filtradas as
arestas ruidosas, provenientes de relações possivelmente esporádi-
cas ou aleatórias, considerando, simultaneamente, as dimensões
volume e tempo, uma estrutura de comunidades de usuários pode
ser mais claramente observada.
Com o propósito de permitir uma visualização mais aprofundada
da forma com que a informação é propagada nas redes com arestas
Tabela 3: Topologia das redes do Telegram por classe.
Classe
Data
𝑛
𝑚
𝑑
ˆ𝑘
𝐶𝑀𝐴
𝐶𝐶
# 𝐶𝑜𝑚.
𝑄
Classe 1
1oTurno
1,406
54,467
0.055
77.478
0.449
0.126
Classe 2
1oTurno
0.025
1.321
0.056
0.883
Classe 3
1oTurno
1,208
26,446
0.036
43.785
0.419
0.460
Classe 4
1oTurno
0.040
4.800
0.205
0.379
Classe 1
2oTurno
3,150
107,218
0.022
68.075
0.488
0.159
Classe 2
2oTurno
0.018
1.677
0.063
0.844
Classe 3
2oTurno
2,141
53,743
0.023
50.204
0.443
0.462
Classe 4
2oTurno
0.037
5.022
0.208
0.414
Classe 1
Tumultos
4,763
70,313
0.006
29.525
0.528
0.351
Classe 2
Tumultos
0.013
3.362
0.173
0.549
Classe 3
Tumultos
0.009
2.304
0.092
0.789
Classe 4
Tumultos
0.300
1.200
0.000
0.444
de diferentes classes, as Figuras 3 apresentam as funções de dis-
tribuição cumulativas (CDFs) do número de retweets e do tempo
médio de retweets de acordo com cada classe. O eixo-x indica, em es-
cala logarítmica, os valores da variável aleatória em questão (seja #
retweets ou tempo médio), enquanto o eixo-y indica a probabilidade
acumulada dessa variável.
É possível observar nas Figuras 3(a) e 3(b), que mostram as cur-
vas de distribuição do volume de retweets das arestas por classe,
que as Classes 2 e 4, apresentam distribuições notadamente mais
à direita no eixo-x, indicando que as interações mais relevantes
em relação ao volume, identificadas pelo método de backbone, são
também aquelas que representam um maior número de retweets.
Nota-se na Figura 3(a), por exemplo, que para a Classe 1(Classe
3), aproximadamente 65%(80%) das arestas possuem peso 1. Já para
as Classes 2 e 4, que capturam arestas com pesos representando
um número maior de co-interações, os valores mínimos observados
partem de 3. Uma análise análoga pode ser feita tomando como base
a dimensão tempo, a partir das Figuras 3(c) e 3(d), que exibem as
CDFs do tempo médio entre retweets em minutos para cada classe.
Para melhorar o sentido intuitivo da análise, a normalização do
tempo definida pela Equação 1 (Seção 3) foi desconsiderada neste
experimento. As Classes 3 e 4 apresentam valores de tempo médio
entre retweets consideravelmente menores, evidenciando que essas
arestas mais relevantes em relação à dimensão tempo, capturadas
nestas classes, representam disseminações mais rápidas, sejam em
baixo ou alto volume. Por exemplo, na Figura 3(d), aproximada-
mente 90% das arestas das Classes 3 e 4 foram geradas a partir de
um tempo médio de co-interação, isto é, co-retweets, de aproximada-
mente 1 minuto3, indicando uma probabilidade maior de arestas
com disseminação muito mais rápida do que as Classes 1 e 2, em
que este número representa cerca de 52%.
5.2.2
Telegram. As características topológicas de rede e da es-
trutura de comunidades para cada classe das redes no Telegram
são apresentadas na Tabela 3. Diferentemente das redes observadas
para o estudo de caso do Twitter/X, nas redes do Telegram observa-
se além do desequilíbrio no número de arestas, observa-se também
um grande desequilíbrio no número de vértices entre as classes.
A Figura 4, que ilustra graficamente a distribuição dos vértices e
arestas entre classes, nos ajuda a compreender melhor as carac-
terísticas topológicas das redes. Nas redes do Telegram, a Classe
3A função Ceiling (Teto) foi usada para gerar uma melhor visualização e evitar tempo
médio igual a zero.
Um Framework para Análise Bidimensional de Disseminação de Informações em Plataformas de Mídias Sociais
WebMedia’2024, Juiz de Fora, Brazil
# retweets (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(a) Volume - 01/11/2022
# retweets (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(b) Volume - 08/01/2023
Tempo médio entre retweets (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(c) Velocidade - 01/11/2022
Tempo médio entre retweets (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(d) Velocidade - 08/01/2023
Figura 3: CDF retweets e tempo médio entre retweets para cada classe nas janelas de tempo 01/11/2022 e 08/01/2023.
Classe da aresta
0.0
0.2
0.4
0.6
0.8
1.0
Fração
Variável
Arestas
Nós
(a) Janela de tempo: 1oTurno
Classe da aresta
0.0
0.2
0.4
0.6
0.8
1.0
Fração
Variável
Arestas
Nós
(b) Janela de tempo: 2oTurno
Classe da aresta
0.0
0.2
0.4
0.6
0.8
1.0
Fração
Variável
Arestas
Nós
(c) Janela de tempo: Tumultos
Figura 4: Fração de nós e arestas por classe no Telegram.
1, que corresponde às arestas que não são filtradas pelo método
de backbone em nenhuma das dimensões, concentra um número
consideravelmente maior de nós e arestas, assim como no estudo
de caso do Twitter/X. Porém, há outras características topológicas
que não se repetem neste estudo de caso. As Classe 2 e 4, que
representam relações salientes em relação à dimensão volume, ap-
resentam um número notadamente baixo de arestas e também de
vértices, revelando alguns aspectos interessantes sobre o fenômeno
de disseminação de informação nessa rede social. Em primeiro lugar,
há um número baixo de arestas que se destacam por representarem
o compartilhamento de um grande volume do mesmo conteúdo,
mostrando que esse tipo de comportamento é bastante concentrado
em poucos pares de usuários. Esse fato está fortemente relacionado
não só ao fenômeno de disseminação de informação, mas à forma
como a rede foi modelada no Telegram, que define como uma aresta
o compartilhamento de mensagens com conteúdo textual idêntico,
o que é resultado de uma ação menos intuitiva do que um retweet
no Twitter/X, por exemplo. Além disso, o resultado para as Classe
2 e 4 mostra também que os vértices que estão associados a essas
classes concentram relações apenas com essa característica e, nesse
sentido, são menos diversos do que aqueles do Twitter/X. Dessa
forma, as redes dessas classes apresentam, além do baixo número de
vértices e arestas, uma alta fragmentação, o que pode ser observado
pelo alto número de componentes conexas observado na Tabela 3.
A fragmentação observada nas redes das diferentes classes no
Telegram tem efeito direto na estrutura de comunidades identificada.
Em muitos casos observa-se valores de modularidade superiores
a 0.8, como na Classe 2 nas janelas de 1o Turno e 2o Turno. En-
tretanto, deve-se considerar esse resultado com cautela dentro da
análise do fenômeno de disseminação de informação, já que a clara
definição da estrutura de comunidade pode ser decorrente da di-
visão dos vértices em componentes distintos. Assim, mesmo que
uma informação seja propagada dentro das comunidades, cada com-
ponente potencialmente envolverá um baixo número de vértices,
dificultando o espalhamento de uma mensagem, o que é reforçado
pela barreira imposta pelas diferentes componentes. Isso mostra que
considerar arestas no backbone identificado apenas na dimensão vol-
ume pode não ser uma estratégia adequada para a compreensão do
fenômeno de espalhamento de informação no Telegram. Por outro
lado, as redes da Classe 3, que consideram arestas identificadas
pelo backbone na dimensão tempo, parecem mostrar comunidades
melhor organizadas, mesmo com valores de modularidade menores.
Essas redes são notadamente menos fragmentadas, o que pode ser
visto pelo seu número de componentes conectadas, e podem ser
estruturadas em comunidades que permitem uma melhor investi-
gação sobre a disseminação de informação. Quando comparamos
os resultados observados para o Telegram com aqueles observados
para o Twitter/X, temos uma noção da complexidade do fenômeno
de disseminação de informação em suas diferentes dimensões e
particularidades quanto à plataforma em que acontece. Isso, mais
uma vez, reforça a ideia central argumentada neste trabalho sobre a
importância do estudo de relações relevantes em redes sociais, con-
siderando não apenas uma, mas mais dimensões simultaneamente.
Para permitir a investigação sobre a forma como ocorre a propa-
gação nas diferentes classes de arestas, a Figura 3 apresenta as
distribuições CDF do número de mensagens e o tempo médio de
compartilhamento, em minutos, por classe e por janela de tempo.
Analisamento o volume de informações por meio da distribuição
do número de mensagens (Figuras 5(a) e 5(b)) é possível observar
que as Classes 2 e 4 apresentam valores muito maiores indicando
arestas com alto volume de disseminação, mostrando que as arestas
mais relevantes, presentes no backbone tendem a também repre-
sentar relações com alta co-interação. Por outro lado, as arestas
das classes Classes 1 e 3 revelam um volume de disseminação
bastante baixo, em que em todos os cenários aproximadamente
pelo menos 95% das arestas possuem peso igual a 1. Isso revela a
forte presença de co-interações esporádicas, porém rápidas. Em
termos da velocidade com que a disseminação ocorre na plataforma,
WebMedia’2024, Juiz de Fora, Brazil
Oliveira et al.
# de mensagens (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe
Classe 1
Classe 2
Classe 3
Classe 4
(a) Volume - 1oTurno
# de mensagens (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe
Classe 1
Classe 2
Classe 3
Classe 4
(b) Volume - 2oTurno
# de mensagens (x)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe
Classe 1
Classe 2
Classe 3
Classe 4
(c) Volume - Tumultos
10000
15000
20000
Tempo médio (min)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(d) Velocidade - 1oTurno
10000
15000
20000
Tempo médio (min)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(e) Velocidade - 2oTurno
10000
15000
20000
Tempo médio (min)
0.0
0.2
0.4
0.6
0.8
1.0
P(X<=x)
Classe da aresta
Classe 1
Classe 2
Classe 3
Classe 4
(f) Velocidade - Tumultos
Figura 5: Função de distribuição acumulada do número de mensagens e tempo médio para cada classe nas redes do Telegram.
as CDFs das Figuras 5(d), 5(e) e 5(f) mostram padrões de dissemi-
nação diferentes daqueles observados no Twitter/X. As arestas das
Classes 3 e 4 apresentam, novamente, distribuições de tempo mé-
dio de compartilhamento de mensagens menores do que as demais,
principalmente nas Figuras 5(d) e 5(e).
Entretanto, alguns valores da Classe 2 chamam a atenção, apre-
sentando tempos médios tão pequenos quanto os das Classes 3 e
4. Relembre que o método de extração de backbone Polya Urn (Seção
3) é baseado em um modelo que assume que as arestas salientes
de um nó são aquelas cujos pesos destoam significativamente das
demais em relação às arestas incidentes ao próprio nó, isto é, uma
análise local. Este resultado mostra que, na rede de disseminação
do Telegram, algumas arestas da Classe 2 possuem pesos tão
baixos quanto os das Classes 3 e 4, mas ainda assim, não são
capturadas pelo backbone e, portanto, não são classificadas como
esses perfis de disseminação. Isso ocorre devido ao fato de essas
arestas pertencerem a pares de nós que possuem, em seus conjuntos
exclusivos de arestas, distribuições de pesos de tempo médio de
compartilhamento majoritariamente baixos em relação à mesma
distribuição de toda a rede. Nestes casos, os valores ainda mais
baixos dessas distribuições de pesos das arestas fazem com que
algumas arestas sejam classificadas como sendo das Classes 3
ou 4, enquanto os demais, ligeiramente maiores em relação ao nó,
mas bem menores para o padrão geral da rede, sejam classificados
como sendo da Classe 2. Em resumo, esses pares de nós possuem
conjuntos de arestas que apresentam grande potencial de dissemi-
nação em termos de velocidade se comparados aos demais nós na
rede. Mesmo as arestas que não são capturadas pelas Classes 3 e 4
ainda têm tempos de disseminação menores comparados às demais.
Dessa forma, nossos resultados revelam a existência de nós na rede
com valores de disseminação predominantemente mais rápidos do
que aqueles observados na rede como um todo. Por fim, ressalta-se
que, durante os tumultos, a distribuição dos pesos baseados em
tempo médio (Figura 5(f)) apresenta, para todas as classes, tempos
ainda menores de forma geral.
CONCLUSÕES E TRABALHOS E FUTUROS
Este estudo abordou a identificação e análise de padrões de dissem-
inação de informações em redes sociais propondo um framework
que combina volume e velocidade das interações para caracteri-
zar diferentes perfis de disseminação de informações. Utilizando
dois estudos de caso - Twitter/X e Telegram - analisamos períodos
com evidências de coordenação e características topológicas dis-
tintas. Nossos principais resultados mostraram que o framework
proporcionou uma visão mais detalhada dos padrões de coorde-
nação entre contas de usuários. Identificamos diferentes classes
de arestas com base no volume e na velocidade de disseminação,
revelando padrões distintos de coordenação e comportamentos sin-
cronizados nas plataformas analisadas. Mostramos também que,
além do alto volume, arestas com outros perfis de disseminação
formaram comunidades estruturadas, representativas de estratégias
particulares de disseminação e coordenação.
Como trabalhos futuros, propomos estender a análise para outros
cenários e eventos, focando na análise de conteúdo para entender
como diferentes tipos favorecem cada perfil de disseminação. In-
vestigaremos como narrativas, temas e formatos de mensagens
contribuem para a velocidade e o volume de disseminação em cada
classe de arestas.
AGRADECIMENTOS
Este estudo recebeu suporte financeiro da Universidade Federal de
Ouro Preto por meio do Programa de Bolsas de Iniciação Científica e
Tecnológica, e do Conselho Nacional de Desenvolvimento Científico
e Tecnológico (CNPq).
Um Framework para Análise Bidimensional de Disseminação de Informações em Plataformas de Mídias Sociais
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30131.txt ---

--- INÍCIO DO ARQUIVO: 30132.txt ---
Uma Abordagem em Etapa de Processamento para Redução do
Viés de Popularidade
Rodrigo Ferrari de Souza
Universidade de São Paulo
São Carlos-SP, Brasil
rodrigofsouza@usp.br
Marcelo Garcia Manzato
Universidade de São Paulo
São Carlos-SP, Brasil
mmanzato@icmc.usp.br
WebMedia’2024, Juiz de Fora, Brazil
Rodrigo Ferrari de Souza and Marcelo Garcia Manzato
Figura 1: Representação de três possíveis cenários de geração de recomendações.
• Em processamento: A calibração nesta etapa envolve a mo-
dificação ou introdução de novos algoritmos com o objetivo
de reduzir vieses no modelo durante o treinamento.
Dessa forma, a estratégia estudada neste trabalho foi a estratégia
de calibração em etapa de processamento. Esse tipo de estratégia
visa modificar algoritmos existentes ou introduzir novos algoritmos
que resultem em classificações e recomendações justas, por exemplo,
removendo preconceitos e discriminação durante o processo de
treinamento do modelo. Normalmente, tais métodos visam aprender
um modelo sem vieses, ao mesmo tempo que consideram a justiça,
incorporando mudanças na função objetivo de um algoritmo por
um termo de justiça ou impondo restrições de justiça [19].
A abordagem BPR (Bayesian Personalized Ranking for Implicit Fe-
edback) [20] é uma técnica LTR (Learning to Rank) do tipo pairwise
que procura posicionar itens relevantes no topo da lista de reco-
mendação. Para isso, são feitas comparações entre pares de itens
– um conhecido e outro desconhecido pelo usuário – de modo a
maximizar a diferença de suas respectivas representações. Apesar
de não lidar com injustiça e vieses, o BPR possui uma flexibili-
dade em sua construção que permite utilização com outros modelos
de recomendação, e também extensões para que outras condições
(como vieses e injustiça) sejam impostas durante seu treinamento
[3]. Porém, conforme relatado na seção de trabalhos relacionados,
há uma deficiência de trabalhos em etapa de processamento que
sejam capazes de lidar com diferentes aspectos de justiça e vieses.
Assim, a proposta deste trabalho é combinar uma forma de ca-
libração personalizada baseada na popularidade dos itens com o
método BPR [20] em etapa de processamento. O objetivo dessa
combinação é trazer um sistema eficiente que traga recomendações
coerentes com as preferências dos usuários, reduza o viés de popu-
laridade e se aproveite do mecanismo de otimização de ranking das
recomendações de acordo com a relevância dos itens.
A estrutura deste trabalho é a seguinte: na Seção 2, discutimos
trabalhos relacionados e comparamos as abordagens existentes com
o nosso trabalho. A Seção 3 descreve a estrutura para nossas pro-
posta de calibração. A Seção 4 detalha a metodologia de avaliação
do sistema proposto. A Seção 5 discute os resultados obtidos. Final-
mente, na Seção 6, concluímos nosso estudo, apresentando algumas
direções futuras para pesquisas.
TRABALHOS RELACIONADOS
A literatura recente apresenta diversas propostas destinadas a cali-
brar recomendações para alinhá-las de forma mais consistente com
os perfis dos usuários. Conforme [19], é possível aplicar a calibração
em três diferentes etapas, que serão detalhadas a seguir.
2.1
Etapa de Pré-Processamento
Essa etapa consiste em alterar os dados a serem utilizados pelo
sistema antes das recomendações serem geradas. Possui vantagens
como: ajuda a melhorar a qualidade dos dados removendo as incon-
sistências antes de serem utilizados e pode melhorar a eficiência
do tempo de processamento e adaptar os dados às necessidades do
algoritmo. Apesar disso, esses ajustes podem aumentar o tempo de
preparação do sistema e levar a perda de informações importantes
dos dados a serem utilizados.
O trabalho [14] apresenta uma abordagem de omissão de atri-
butos para tentar remover o viés nos dados. Além disso, o mesmo
trabalho mostra uma estratégia de alteração dos rótulos dos itens
para remover os vieses. Por outro lado, essas técnicas podem ser in-
suficientes para garantir justiça nas recomendações, além de existir
a possibilidade de tais métodos reduzirem a acurácia do sistema.
Uma outra estratégia é apresentada no trabalho [22], onde é
proposto um algoritmo baseado em seleção de amostras para um
treinamento justo e robusto. Para tanto, é formulado um problema
de otimização combinatória para a seleção imparcial de amostras na
presença de problemas nos dados de treinamento. Um dos principais
riscos dessa seleção de amostras é a introdução de injustiças, caso o
procedimento de amostragem não seja cuidadosamente projetado,
já que se a seleção for tendenciosa para determinados grupos de
dados, o modelo treinado também poderá apresentar vieses. Um
outro problema é o custo computacional dessa seleção de amostras
e a complexidade para realizar essa seleção dependendo do contexto
dos dados.
2.2
Etapa de Pós-Processamento
A etapa de pós-processamento consiste em calibrar o sistema após o
modelo ter gerado as recomendações e tem as seguintes vantagens:
é independente do algoritmo de recomendação, pois pode ser feita
uma reclassificação da lista gerada por qualquer outro modelo; tam-
bém é mais eficiente, porque não afeta o tempo de processamento
do algoritmo que gera as recomendações. Todavia, essa etapa pode
reduzir a precisão do sistema já que altera a lista calibrada gerada
pelo modelo de recomendação.
Trabalhos como [24] e [9] fazem ajustes com base nos interesses
dos usuários nos gêneros de itens para alcançar um sistema mais
consistente. Apesar do foco na consistência, esses trabalhos não
abordam a questão do viés de popularidade.
Uma Abordagem em Etapa de Processamento para Redução do Viés de Popularidade
WebMedia’2024, Juiz de Fora, Brazil
Para abordar o viés de popularidade, alguns trabalhos implemen-
tam estratégias de calibração nessa etapa, como o artigo [23] que
apresenta uma proposta de calibração baseada em chaveamento.
Esta abordagem opta pela calibração com base na popularidade dos
itens ou nos gêneros dos itens. Por outro lado, conforme menci-
onado anteriormente, a precisão do sistema pode ser afetada, já
que há uma reordenação na lista de itens sugeridos após a etapa de
recomendação.
2.3
Etapa de Processamento
Na etapa de processamento, a calibração ocorre junto com o trei-
namento e geração das recomendações, e pode levar a um melhor
desempenho do algoritmo em termos de precisão. Além disso, tam-
bém permite aplicar diferentes técnicas de mitigação da injustiça
durante o treinamento do algoritmo. Entretanto, esses ajustes po-
dem aumentar a complexidade e o tempo de treinamento do sistema.
As abordagens existentes na literatura normalmente usam apren-
dizado de máquina para construir modelos de classificação. Em geral,
esses modelos categorizam listas não vistas de forma semelhante à
classificação dos dados de treinamento. O objetivo geral é ter um
modelo que minimize uma função de perda que capture a distância
entre o que foi aprendido e a classificação de entrada [19].
O trabalho [30] segue essa ideia adicionando termos de regula-
rização, que expressam medidas de injustiça que o modelo deve
minimizar além da minimização da função de perda original. Outra
abordagem apresentada é vista em [16], que utiliza a estratégia de
rede neural artificial denominada variational autoencoders (VAE).
Nessa abordagem a filtragem colaborativa é feita juntamente com
parâmetros de regularização que melhoram a representação dos
dados implementados pelo modelo.
O trabalho [3] propõe uma abordagem para reduzir o viés de
popularidade em sistemas de recomendação. O método apresen-
tado conecta as perspectivas do usuário e do item para minimizar a
correlação entre a popularidade de um item e sua relevância para
um usuário específico. Isso é feito por meio de um modelo probabi-
lístico que aprende os fatores latentes dos usuários e dos itens. O
algoritmo atualiza os fatores do usuário com base na diferença entre
a avaliação do usuário para dois itens (popular e menos popular) e
a probabilidade prevista de o usuário preferir o item menos popular.
Devido à similaridade com a proposta deste trabalho, o método foi
usado como um dos baselines desta pesquisa.
Existem estratégias alternativas para gerar recomendações mais
consistentes. A proposta [12] emprega grafos para mitigar injustiças
no sistema, considerando o gênero do usuário. O trabalho [6] imple-
menta uma abordagem pareada considerando a justiça entre grupos
de itens, ajustando a lista de recomendações durante o treinamento.
A abordagem [17] sugere a utilização de grafos e redes neurais para
atribuir pesos aos itens recomendados, equilibrando-os de acordo
com as preferências do usuário.
Embora essas abordagens produzam resultados interessantes,
os estudos não abordam o viés de popularidade de forma a trazer
recomendações que atendam os interesses dos usuários por esse
aspecto, havendo uma lacuna na área com relação a sistemas que
tragam recomendações coerentes e que lidem com o viés de popu-
laridade em etapa de processamento. Além disso, o trabalho [11]
destaca a importância dos vieses e como eles afetam os sistemas,
o que faz ser necessário selecionar métodos adequados para lidar
com o viés presente no sistema.
Dessa forma, a estratégia deste trabalho é combinar uma forma
de calibração personalizada baseada na popularidade dos itens com
o método BPR [20] em etapa de processamento. O objetivo dessa
combinação é trazer um sistema eficiente que traga recomendações
coerentes, reduza o viés de popularidade e seja capaz de fornecer
recomendações relevantes de acordo com o perfil de cada usuário.
As próximas seções descrevem a implementação dessa combinação
e os resultados dessa abordagem.
ESTRATÉGIA DE CALIBRAÇÃO
Supondo que há um conjunto de itens 𝐼= {𝑖1,𝑖2, ...,𝑖|𝐼|}, um con-
junto de usuários 𝑈= {𝑢1,𝑢2, ...,𝑢|𝑈|} e um conjunto de itens
candidatos para cada usuário 𝐶𝐼𝑢= {𝑖1,𝑖2, ...,𝑖𝑁}, onde 𝑁é o nú-
mero de itens sugeridos pelo sistema de recomendação. Além disso,
existem as informações dos usuários sobre as preferências de popu-
laridade. A tarefa é explorar essas preferências para gerar uma lista
de recomendações que aumente a justiça em relação a popularidade
dos itens.
Para tanto, propõe-se uma abordagem de calibração em etapa
de processamento. Na prática, o método utiliza medidas de diver-
gência na etapa de geração de recomendações para realizar uma
calibração de acordo com diferentes níveis de popularidade de inte-
resse do usuário. Como resultado, os usuários recebem uma lista
de recomendações próxima ao seu perfil de interesse em termos
de popularidade. Essa calibração é incorporada ao BPR. A Figura 2
apresenta a estrutura de calibração de popularidade, cujos detalhes
são descritos a seguir.
Figura 2: Estrutura de calibração proposta. A calibração por
popularidade é aplicada de forma combinada ao treinamento
do BPR, resultando em uma lista calibrada de recomendações
de acordo com as preferências do usuário sobre popularidade
e gêneros.
3.1
Divisão de Popularidade
A calibração da lista de recomendações com base na popularidade
dos itens já consumidos pelo usuário é feita por meio de uma divisão
de popularidade para agrupar os itens com base na quantidade
WebMedia’2024, Juiz de Fora, Brazil
Rodrigo Ferrari de Souza and Marcelo Garcia Manzato
Figura 3: Curva representando a divisão dos itens em grupos
de popularidade.
de interações. A divisão de popularidade, introduzida em [2], é
baseada no conceito de cauda longa dos sistemas de recomendação,
conforme pode ser visualizado na Figura 3. A curva foi dividida
em três partes. O Head (H), com itens representando 20% do total
de interações. A Tail (T) com itens que somam menos de 20%
das interações, e o grupo Mid (M), que contém itens que não são
nem Head (H) nem Tail (T). Vale ressaltar que esta divisão por
percentual foi escolhida com base no princípio de Pareto [18].
3.2
Calibração
A calibração por popularidade foi uma adaptação da fórmula pro-
posta por [24]. Seu trabalho pressupõe que os itens podem ter mais
de um gênero, o que não é válido no contexto de popularidade,
onde um item possui apenas um nível de popularidade. Então, ao
invés disso, foram calculadas as somas dos pesos de cada tipo de
popularidade sobre a soma de todos os pesos.
Assim, 𝑥(𝑡|𝑢) é definido como a distribuição alvo baseada na
popularidade dos itens com os quais o usuário interagiu no pas-
sado. Na Equação 1 os pesos 𝑟𝑢𝑖são definidos como a classificação
explícita ou implícita que o usuário 𝑢deu ao item 𝑖:
𝑥(𝑡|𝑢) =
Í
𝑖∈𝐼𝑢𝑟𝑢𝑖· 𝑥(𝑡|𝑖)
Í
𝑖∈𝐼𝑢𝑟𝑢𝑖
(1)
onde 𝐼𝑢é o conjunto de itens interagidos pelo usuário 𝑢, e 𝑥(𝑡|𝑖) é
definido como 1 se o item 𝑖estiver na categoria de popularidade 𝑡.
Então, para lidar com a distribuição de lista recomendada, a Equação
2 define 𝑦(𝑡|𝑢) como:
𝑦(𝑡|𝑢) =
Í
𝑖∈𝑅∗𝑢𝑤𝑝(𝑢,𝑖) · 𝑥(𝑡|𝑖)
Í
𝑖∈𝑅∗𝑢𝑤𝑝(𝑢,𝑖)
(2)
Neste caso, usamos os pesos 𝑤𝑝(𝑢,𝑖) como a posição de classifi-
cação do item 𝑖na lista reordenada recomendada 𝑅∗𝑢para o usuário
𝑢.
Várias métricas avaliam a imparcialidade em sistemas de reco-
mendação [26]. Porém, nesse caso, utiliza-se a medida de diver-
gência Kullback-Leibler pelas mesmas razões apontadas por [24] e
exploradas por [9]. O Kullback-Leibler quantifica a desigualdade
no intervalo [0, ∞], onde 0 significa que ambas as distribuições são
quase iguais e valores mais altos indicam injustiça.
Adicionalmente, é adotada a regularização proposta por [24],
que definiu 𝛼= 0.01 como uma variável de regularização para
evitar divisão por zero quando 𝑦(𝑡|𝑢) vai para zero. Embora exis-
tam outras métricas de divergência, como Hellinger e Person Qui-
Square, propostas por [4] e exploradas por [9], foi utilizada apenas
a Kullback-Leibler devido à sua simplicidade:
𝐷𝐾𝐿(𝑥∥𝑦) =
∑︁
𝑡
𝑥(𝑡|𝑢) · 𝑙𝑜𝑔
𝑥(𝑡|𝑢)
(1 −𝛼) · 𝑦(𝑡|𝑢) + 𝛼· 𝑥(𝑡|𝑢)
(3)
A divergência de Kullback-Leibler é uma medida que quantifica a
diferença entre duas distribuições de probabilidade, neste caso, entre
a distribuição observada 𝑥(𝑡|𝑢) e a distribuição de referência 𝑦(𝑡|𝑢).
No contexto da calibração por popularidade, 𝑥(𝑡|𝑢) representa a
distribuição empírica dos itens observados pelo usuário𝑢, enquanto
𝑦(𝑡|𝑢) representa uma distribuição de referência desejada, que é
baseada na popularidade dos itens na base de dados.
3.3
O Método BPR
O BPR [20] é uma abordagem eficaz para recomendação de itens
em sistemas de recomendação baseados em feedback implícito. Ao
modelar as preferências dos usuários por meio de características
latentes e otimizar a função de perda, o modelo é capaz de aprender
efetivamente as preferências dos usuários e gerar recomendações
personalizadas, levando em consideração a ordem de preferência
dos itens.
Mantendo a notação utilizada na seção anterior, as letras de inde-
xação especial distinguem usuários e itens: um usuário é indicado
como𝑢e um item é referido como𝑖, 𝑗;𝑟𝑢𝑖refere-se ao feedback explí-
cito ou implícito de um usuário 𝑢para um item 𝑖. No primeiro caso,
é um número inteiro fornecido pelo usuário indicando o quanto
ele gostou do conteúdo; no segundo caso, é apenas um booleano
mostrando se o usuário consumiu ou visitou o conteúdo ou não. A
predição do sistema sobre a preferência do usuário 𝑢para o item 𝑖
é representada por ˆ𝑟𝑢𝑖, que é um valor de ponto flutuante estimado
pelo algoritmo de recomendação. O conjunto de pares (𝑢,𝑖) para
os quais 𝑟𝑢𝑖é conhecido é representado por 𝐾= {(𝑢,𝑖)|𝑟𝑢𝑖}.
Em um modelo de fatorização tradicional, cada usuário 𝑢é asso-
ciado a um vetor de fatores 𝑝𝑢∈R𝑓e cada item 𝑖com um vetor de
fatores 𝑞𝑖∈R𝑓. Uma regra de previsão seria:
ˆ𝑟𝑢𝑖= 𝑝𝑇
𝑢𝑞𝑖
(1)
(4)
Conjuntos adicionais são 𝑁(𝑢), que indica o conjunto de itens
para os quais o usuário 𝑢forneceu um feedback implícito, e 𝑁(𝑢),
que indica o conjunto de itens desconhecidos para o usuário 𝑢. Uma
característica importante desse tipo de feedback é que apenas as
observações positivas são conhecidas; os pares usuário-item não
observados são interpretados como feedback negativo.
O trabalho [20] discute um problema que surge quando um mo-
delo de recomendação de itens é treinado apenas com esses dados
Uma Abordagem em Etapa de Processamento para Redução do Viés de Popularidade
WebMedia’2024, Juiz de Fora, Brazil
Figura 4: O quadro à esquerda representa os dados observados.
A abordagem cria uma relação par de itens específica para o
usuário 𝑖≻𝑢𝑗entre dois itens. No lado direito da tabela, o
sinal de mais indica que o usuário 𝑢está mais interessado no
item 𝑖do que no item 𝑗; o sinal de menos indica que o usuário
prefere o item 𝑗ao 𝑖; o ponto de interrogação indica que não
se pode inferir nenhuma conclusão entre os itens.
positivos/negativos. Como as entradas observadas são positivas e
as restantes são negativas, o modelo será ajustado para fornecer
apenas pontuações positivas para os itens observados. Os elementos
restantes, incluindo aqueles que podem ser de interesse para o usuá-
rio, serão classificados pelo modelo como pontuações negativas e
a classificação não poderá ser otimizada, pois as previsões estarão
em torno de zero.
Os autores propuseram um método genérico para aprender o
comportamento do usuário para classificação personalizada [20].
Em vez de treinar o modelo usando apenas os pares usuário-item,
eles também consideraram a ordem relativa entre um par de itens, de
acordo com as preferências do usuário. Se um item 𝑖foi visualizado
pelo usuário 𝑢e 𝑗não (𝑖∈𝑁(𝑢) e 𝑗∈𝑁(𝑢)), então 𝑖é preferido
a 𝑗. A Figura 4 mostra um exemplo do método. Quando 𝑖e 𝑗são
desconhecidos para o usuário, ou equivalentemente, ambos são
conhecidos, nenhuma conclusão sobre sua importância relativa
para o usuário pode ser inferida.
Para estimar se um usuário prefere um item a outro, [20] propu-
seram uma análise Bayesiana usando uma função de probabilidade
𝑝𝑟𝑜𝑏(𝑖≻𝑢𝑗|𝑢, Θ) e a probabilidade anterior para o parâmetro do
modelo 𝑝𝑟𝑜𝑏(Θ). O critério final de otimização, BPR-Opt, é definido
como:
𝐵𝑃𝑅-𝑂𝑝𝑡B
∑︁
(𝑢,𝑖,𝑗)∈𝑆𝐾
ln𝜎(ˆ𝑠𝑢𝑖𝑗) −ΛΘ∥Θ∥2
onde ˆ𝑠𝑢𝑖𝑗B ˆ𝑟𝑢𝑖−ˆ𝑟𝑢𝑗e 𝑆𝐾é o conjunto de triplas (𝑢,𝑖, 𝑗) onde 𝑖
está em 𝑁(𝑢) e 𝑗não está. O símbolo Θ representa os parâmetros
do modelo, ΛΘ é o conjunto de constantes de regularização, e 𝜎é a
função logística definida como 𝜎(𝑥) =
1+𝑒−𝑥.
Os autores também propuseram uma variação na técnica de
descida de gradiente estocástico, denominada LearnBPR, que amos-
tra aleatoriamente de 𝑆𝐾para ajustar Θ. O Algoritmo 1 mostra
uma visão geral do método de aprendizagem, onde 𝛼é a taxa de
aprendizado.
No presente estudo, definimos a abordagem BPR para considerar
a regra de predição ˆ𝑟𝑢𝑖do modelo de fatorização simples definido
na Equação 4. Portanto, aplicar a Equação 4 em ˆ𝑠𝑢𝑖𝑗resulta em Θ =
Algorithm 1: Aprendizado via LearnBPR.
Input: 𝐷𝐾
Output: Parâmetros ajustados Θ
1 Inicializar Θ com valores aleatórios
2 for cont = 1,...,#Iterações do
obtenha (𝑢,𝑖, 𝑗) a partir de 𝑆𝐾
ˆ𝑠𝑢𝑖𝑗←ˆ𝑟𝑢𝑖−ˆ𝑟𝑢𝑗
Θ ←Θ + 𝛼

𝑒−ˆ𝑠𝑢𝑖𝑗
1+𝑒−ˆ𝑠𝑢𝑖𝑗. 𝜕
𝜕Θ ˆ𝑠𝑢𝑖𝑗−ΛΘΘ

6 end
{𝑝𝑢,𝑞𝑖,𝑞𝑗}, que devem ser aprendidos. Calculamos as derivadas
parciais em relação a ˆ𝑠𝑢𝑖𝑗:
𝜕
𝜕Θ ˆ𝑠𝑢𝑖𝑗=


𝑞𝑖−𝑞𝑗
quando Θ = 𝑝𝑢
𝑝𝑢
quando Θ = 𝑞𝑖
−𝑝𝑢
quando Θ = 𝑞𝑗
caso contrário
Esses gradientes são então usados para atualizar os fatores de
usuário e item em direção ao mínimo da função de perda, iterativa-
mente, até que a convergência seja alcançada ou um número fixo
de iterações seja concluído. Desse modo, o SGD permite ajustar
os fatores de usuário e item de forma a maximizar a diferença en-
tre as pontuações dos itens positivos e negativos, resultando em
recomendações mais precisas e personalizadas.
3.4
BPR com Calibração por Popularidade
Com o objetivo de combinar um modelo em etapa de processa-
mento com uma forma de calibração de popularidade para trazer
recomendações que reduzam o viés de popularidade no sistema, a
estratégia adotada foi alterar o algoritmo de aprendizado LearnBPR
(Algoritmo 1). Assim, pode-se combinar o BPR com a calibração
por popularidade, acrescentando no algoritmo a divergência de
Kullback-Leibler implementada na calibração de popularidade.
Essa combinação pode possibilitar uma maior justiça nas reco-
mendações em termos de popularidade, já que esse aspecto seria
levado em conta na função de perda do BPR. A alteração é realizada
na linha 5 do Algoritmo 1 somente quando Θ = 𝑝𝑢:
𝑝𝑢←𝑝𝑢+ 𝛼

𝑒−ˆ𝑠𝑢𝑖𝑗
1 + 𝑒−ˆ𝑠𝑢𝑖𝑗.(𝑞𝑖−𝑞𝑗)
+ 𝜆

1 −𝐷𝐾𝐿(𝑥∥𝑦)
𝐷𝐾𝐿𝑣𝑜𝑖𝑑

−Λ𝑝𝑢𝑝𝑢

(5)
onde 𝜆é utilizado como coeficiente do impacto que a divergência
terá no sistema, e 𝐷𝐾𝐿𝑣𝑜𝑖𝑑é definido como:
𝐷𝐾𝐿𝑣𝑜𝑖𝑑=
∑︁
𝑡
𝑥(𝑡|𝑢) · 𝑙𝑜𝑔𝑥(𝑡|𝑢)
𝛼· 𝑥(𝑡|𝑢)
(6)
A razão para dividir a divergência 𝐷𝐾𝐿(𝑥,𝑦) por 𝐷𝐾𝐿𝑣𝑜𝑖𝑑é nor-
malizar o valor da divergência, deixando o valor ajustado para uma
escala específica. Essa normalização pode ser útil para realizar a
calibração por popularidade entre diferentes usuários ou grupos,
WebMedia’2024, Juiz de Fora, Brazil
Rodrigo Ferrari de Souza and Marcelo Garcia Manzato
independentemente do número total de itens ou da escala de popu-
laridade na base de dados, possibilitando a aplicação em diferentes
contextos.
Ao considerar não apenas as preferências individuais dos usuá-
rios, mas também a popularidade relativa dos itens, a abordagem
modificada pode levar a recomendações mais relevantes e persona-
lizadas. Isso pode resultar em uma melhor experiência do usuário e
maior satisfação com o sistema de recomendação.
AVALIAÇÃO
A execução do experimento foi realizada três vezes em dois conjun-
tos de dados do domínio de filmes para garantir a confiabilidade dos
resultados. A repetição dos testes ajuda a mitigar o impacto de vari-
ações aleatórias, assegurando que a média dos valores obtidos seja
representativa do desempenho do modelo. O t-test de Student foi
escolhido para análise por ser amplamente utilizado na comparação
de médias entre dois grupos, especialmente com amostras pequenas
ou moderadas [15], onde as variáveis seguem uma distribuição apro-
ximadamente normal. Como as bases de dados utilizadas possuem
dados contínuos e distribuídos de forma aproximada à normal, este
teste é adequado para a análise estatística. A Tabela 1 resume as
informações dos conjuntos de dados utilizados.
• Yahoo Movies1: Este conjunto de dados é uma classificação
de filmes do usuário, onde o usuário atribui notas de um a
cinco aos filmes que assistiu. Na etapa de pré-processamento,
foram removidos apenas filmes sem gênero nos metadados.
Em vez de binarizar a classificação como feito por [24], foi
utilizado o feedback explícito como o peso 𝑟𝑢𝑖na Equação 1.
• MovieLens-20M2: Neste conjunto de dados, semelhante
a [24] e em contraste com o conjunto de dados do Yahoo
Movies, foi feita a binarização das classificações retendo as
interações onde a classificação era superior a 4. Além disso,
devido a limitações de hardware, o tamanho do conjunto
de dados foi reduzido, removendo filmes com menos de dez
interações e usuários com menos de 180 filmes.
Tabela 1: Estatísticas dos conjuntos de dados após realização
do pré-processamento.
Conjunto de dados
# Usuários
# Interações
# Itens
Yahoo Movies
7,642
211,231
11,916
MovieLens 20M
12,603
3,984,599
10,417
O experimento foi executado três vezes em cada conjunto de
dados para obter a média dos valores gerados pelas métricas e
garantir a estabilidade dos resultados. Os conjuntos de dados de
teste e treinamento foram escolhidos dividindo aleatoriamente o
conjunto de dados em 70/30% de interações, seguindo respectiva-
mente [2, 9]. O desempenho da abordagem foi comparado com os
seguintes trabalhos do estado da arte:
(1) BPR: Proposta em [20], é um algoritmo de recomendação
projetado para lidar com dados de feedback implícito, onde
as interações entre usuários e itens são representadas como
1https://webscope.sandbox.yahoo.com/
2https://grouplens.org/datasets/movielens/20m/
preferências binárias. Para os três conjuntos de dados, foi
aplicado o batch = 1024.
(2) PairWise: Proposto por [3], este método atua como uma
etapa de processamento para redução do impacto do viés de
popularidade. Para o conjunto de dados do Yahoo Movies,
foram aplicados 𝑒𝑝𝑜𝑐ℎ= 100, 𝑏𝑎𝑡𝑐ℎ= 1024 e escolhido o
melhor 𝛼variando no intervalo [0, 1]. Para o conjunto de
dados MovieLens, foram utilizados 𝑏𝑎𝑡𝑐ℎ= 2048 e 𝑒𝑝𝑜𝑐ℎ=
20. A implementação seguiu aquela feita pelos autores3.
Métricas. Em nossos experimentos, avaliamos os efeitos da ca-
libração em termos de precisão, justiça e viés de popularidade,
conforme detalhado a seguir:
(1) Precisão e Qualidade: usamos as métricas Mean Reciprocal
Rank (MRR) e Mean Average Precision (MAP) para medir
a qualidade da classificação do item na lista reclassificada.
MAP e MRR variam no intervalo [0, 1] onde valores mais
altos são melhores.
(2) Justiça: utilizamos uma métrica proposta por [9], denomi-
nada Mean Rank Miscalibration (MRMC), que cobre o in-
tervalo [0, 1], onde valores mais baixos são melhores.
Inicialmente, ela foi usada para calcular a justiça em gêne-
ros na lista de recomendações, mas neste trabalho ela foi
adaptada para calcular o erro de calibração de popularidade.
Embora nossa proposta visa reduzir a injustiça em termos
de popularidade, nós também medimos a justiça de gêneros
neste trabalho. Para isso, usamos a média harmônica F1 entre
MRMC de gêneros e popularidade, onde valores mais altos
são melhores:
𝐹1 = 2 (1 −𝑀𝑅𝑀𝐶𝐺𝑒𝑛𝑒𝑟𝑜) ∗(1 −𝑀𝑅𝑀𝐶𝑃𝑜𝑝)
(1 −𝑀𝑅𝑀𝐶𝐺𝑒𝑛𝑒𝑟𝑜) + (1 −𝑀𝑅𝑀𝐶𝑃𝑜𝑝)
(7)
(3) Viés de popularidade: usamos as métricas de cobertura
de cauda longa (LTC) [2] e popularidade média do grupo
(ΔGAP) [2] para medir o viés de popularidade. A métrica LTC
indica a fração de itens que os usuários recebem nas listas de
recomendação e varia no intervalo [0, 1], onde 0 significa que
todos os itens recomendados são os mais populares e 1 signi-
fica que todos os itens recomendados a um usuário estão nas
categorias menos populares. Assim, quanto mais próximo
de 1, mais diversificado será o conteúdo recomendado
[2]. O ΔGAP varia no intervalo [−1, 1], onde valores negati-
vos significam que as recomendações são menos populares
do que o esperado segundo as preferências dos usuários, e
valores positivos significam que as recomendações são mais
populares do que o esperado. Também adotamos três divi-
sões de grupos de usuários, com base em [2] para o ΔGAP:
BlockBuster (BB) cujo consumo dos usuários é de pelo
menos 50% dos itens mais populares, Nicho (N) onde o con-
sumo dos usuários é de pelo menos 50% dos itens de menor
popularidade e Diverso (D) cujas preferências dos usuários
divergem dos outros dois grupos. Finalmente, como os valo-
res ótimos de Δ𝐺𝐴𝑃devem ser próximos de zero, propomos
neste artigo a utilização do Root Mean Squared Error (RMSE)
entre os três grupos de usuários, onde valores mais baixos
são melhores:
3https://github.com/biasinrecsys/wsdm2021
Uma Abordagem em Etapa de Processamento para Redução do Viés de Popularidade
WebMedia’2024, Juiz de Fora, Brazil
𝑅𝑀𝑆𝐸=
√︃
Δ𝐺𝐴𝑃2
𝐵𝐵+ Δ𝐺𝐴𝑃2
𝑁+ Δ𝐺𝐴𝑃2
𝐷
(8)
RESULTADOS
5.1
Yahoo Movies
A Tabela 2 apresenta os resultados obtidos para o conjunto de
dados Yahoo Movies. Analisando apenas a precisão dos modelos
pela métrica MAP, notamos que a abordagem PairWise [3] atingiu
o maior valor de MAP. No entanto, esta conquista significa que
os itens não são muito diversos entre si, como mostram os seus
resultados relativos a LTC, F1 e RMSE.
Em relação à justiça dos gêneros através do MRMC de gêneros,
a Tabela 2 indica que a proposta de calibração combinada com o BPR
produziu o melhor resultado, indicando que foi capaz de fornecer
itens mais próximos do perfil em termos de gênero. O mesmo foi
verificado em relação à justiça de popularidade, com a proposta
tendo o melhor resultado do MRMC Pop.
Em termos de cobertura de cauda longa, a tabela indica que
o modelo mais eficaz para recomendar itens diversos foi o BPR. O
PairWise com pontuações mais altas no MAP obteve valores mais
baixos para o LTC. Em relação à métrica F1, é possível observar que
a proposta conseguiu alcançar o melhor resultado, indicando que a
abordagem de calibração foi capaz de calibrar recomendações de
acordo com gêneros e popularidade. Este aspecto é ainda validado
ao analisar a métrica RMSE, onde a mesma abordagem obteve
menor erro com a calibração, indicando que ela aborda os pontos
de justiça mencionados e reduz o viés de popularidade do sistema.
Os resultados relatados na Tabela 2 mostram que a abordagem
de calibração foi capaz de equilibrar recomendações de acordo com
gêneros e popularidade, em oposição aos outros trabalhos, que são
mais adequados para um único aspecto, como precisão, gêneros
ou popularidade. Além disso, os resultados mostram a importância
de adotar métricas além da precisão na análise de algoritmos de
recomendação. Reconhece-se a alta precisão do PairWise, conforme
indicado pela métrica MAP. No entanto, os usuários que preferem
itens de nicho, diversos e impopulares são afetados por recomenda-
ções injustas e tendenciosas produzidas por essas abordagens.
5.2
MovieLens 20M
A Tabela 2 apresenta os resultados obtidos para o conjunto de dados
MovieLens 20M. Analisando a precisão, assim como na base de
dados anterior, o PairWise [3] superou as outras abordagens. No
entanto, os resultados também indicam que estas abordagens devol-
vem recomendações injustas em termos de gênero e popularidade,
e carecem de diversidade.
Em relação à justiça dos gêneros e à justiça de popularidade,
a abordagem de calibração proposta obteve os melhores resultados,
fato confirmado pela métrica F1. Em relação à cobertura de cauda
longa, o BPR obteve o melhor resultado entre todas as aborda-
gens. Além disso, o PairWise [3] alcançou um valor baixo para essa
métrica, apesar de ter uma alta precisão.
Com relação ao F1, pode-se observar que a proposta obteve os
melhores valores, destacando seu alto desempenho em termos de
justiça nos gêneros e popularidade. Ademais, a proposta também
obteve o melhor resultado em RMSE, indicando que o sistema
reduziu com sucesso o viés de popularidade para diferentes grupos
de usuários.
A Tabela 2 reporta resultados semelhantes aos do conjunto de
dados Yahoo Movies, indicando que a proposta melhorou a justiça
dos gêneros e a popularidade em ambos os conjuntos de dados. Em-
bora a abordagem de calibração proposta não tenha alcançado alta
precisão, obteve o menor erro de calibração de gênero e de popula-
ridade, o que significa que o modelo fornece recomendações que
respeitam o perfil do usuário tanto no gênero quanto no consumo
de popularidade.
CONCLUSÃO
O objetivo do BPR é aprender representações latentes para usuários
e itens que capturem suas preferências individuais. O procedimento
de aprendizado do BPR envolve a otimização de uma função de
perda que visa maximizar a ordenação correta dos pares de itens
positivos e negativos para cada usuário. Isso é feito por meio de
gradiente descendente estocástico, onde os gradientes da função de
perda são calculados para atualizar os vetores latentes dos usuários
e dos itens.
A modificação proposta, que combina o BPR com a calibração
por popularidade, visa melhorar a justiça nas recomendações, con-
siderando não apenas as preferências individuais dos usuários, mas
também a popularidade relativa dos itens. Isso é alcançado incor-
porando a divergência de Kullback-Leibler na função de perda do
BPR, levando a recomendações mais relevantes e personalizadas. Os
experimentos realizados em dois conjuntos de dados mostram que a
abordagem modificada obtém resultados comparáveis ou melhores
em relação aos métodos do estado da arte, tanto em métricas de
classificação quanto em métricas de popularidade e justiça.
No entanto, é importante ressaltar que a abordagem proposta
ainda pode ser aprimorada em vários aspectos. Por exemplo, a esco-
lha dos parâmetros do modelo, como o tamanho do lote e o número
de épocas, pode afetar significativamente o desempenho do sistema.
Além disso, a implementação de técnicas adicionais de regulariza-
ção ou otimização pode ajudar a evitar o sobreajuste e melhorar a
convergência do modelo. Há também a possibilidade de combinar
técnicas de redução do viés de popularidade com os sistemas con-
versacionais [28]. Futuras pesquisas podem explorar essas direções
para desenvolver ainda mais a abordagem proposta e melhorar sua
eficácia em uma variedade de cenários de recomendação.
AGRADECIMENTOS
Os autores gostariam de agradecer o apoio financeiro da FAPESP,
processo número 2022/07016-9, e CNPq.

--- FIM DO ARQUIVO: 30132.txt ---

--- INÍCIO DO ARQUIVO: 30133.txt ---
Uma Investigação sobre Técnicas de Data Augmentation
Aplicadas a Tradução Automática Português-LIBRAS
Marcos André Bezerra da Silva
Universidade Federal da Paraíba
João Pessoa/PB, Brasil
marcos.andre@lavid.ufpb.br
Manuella Aschoff C. B. Lima
Universidade Federal da Paraíba
João Pessoa/PB, Brasil
manuella.lima@lavid.ufpb.br
Diego Ramon Bezerra da Silva
Universidade Federal da Paraíba
João Pessoa/PB, Brasil
diego.silva@lavid.ufpb.br
Daniel Faustino L. de Souza
Universidade Federal da Paraíba
João Pessoa/PB, Brasil
daniel@dcx.ufpb.br
Rostand Edson O. Costa
Universidade Federal da Paraíba
João Pessoa/PB, Brasil
rostand@lavid.ufpb.br
Tiago Maritan U. de Araújo
Universidade Federal da Paraíba
João Pessoa/PB, Brasil
tiagomaritan@lavid.ufpb.br
WebMedia’2024, Juiz de Fora, Brazil
Silva et al.
transformer e os resultados foram avaliados com base na métrica
BLEU.
TRABALHOS RELACIONADOS
A tradução automática PB-LIBRAS envolve a tradução entre línguas
de modalidades diferentes (oral-auditiva e visual-espacial) e que
possuem estruturas linguísticas não paralelas entre si. Além disso,
as LS geralmente têm poucos recursos, ou seja, existem poucos
bancos de dados extensos para LS e, quando existem, são limitados
a algumas LS específicas [5, 10, 15]. Assim, a escassez de dados é
um dos principais desafios para a tradução automática para língua
de sinais (Sign Language Translation - SLT). Algumas estratégias
para lidar com o problema de tradução automática com poucos re-
cursos incluem aumento de dados, aprendizagem por transferência,
retrotradução e abordagens híbridas [8]. No tocante à técnica de
aumento de dados, foco deste trabalho, destacam-se os trabalhos
de Moryossef et al. 2021, Fadaee et al. 2017, Sanchez-Cartagena et
al. 2021, Maimaiti et al. 2021, Jang et al. 2022 e Wang, Yang 2022.
Moryossef et al. 2021 se concentraram na tarefa de tradução de
texto dentro do SLT e introduziram duas estratégias de aumento de
dados baseadas em regras. Eles apresentaram regras abrangentes e
específicas do idioma para criar pares texto-glosa pseudo-paralelos.
Esses pares foram posteriormente empregados no processo de re-
trotradução, melhorando o desempenho geral do modelo.
O trabalho desenvolvido por Fadaee et al. 2017 utilizou uma
estratégia de substituição de palavras alinhadas para aumentar
a frequência de palavras raras no conjunto de treinamento. As
sentenças geradas foram posteriormente filtradas por um modelo de
linguagem que foi treinado com o objetivo de avaliar se as sentenças
sintéticas são fluentes, ou seja, estão corretas gramaticalmente e
fazem sentido semântico. Foi observado um ganho de 2,5 pontos
na métrica BLEU na tradução de inglês para alemão.
Já Sánchez-Cartagena et al. 2021 aplicaram duas técnicas para
aumento de dados em sua pesquisa. Uma delas é a substituição
de palavras alinhadas semelhante à apresentada por Fadaee et al.
2017, porém sem se preocupar com a fluência das sentenças geradas,
portanto, não sendo necessário um modelo de linguagem adicional.
De maneira auxiliar, também foi utilizada a tarefa de reversão de
tokens, tendo os resultados avaliados na tradução entre inglês e
alemão, hebreu e vietnamita, resultando em um ganho médio de
1,6 BLEU.
Maimaiti et al. 2021 em sua pesquisa propuseram a substituição
de sinônimos com part-of-speech tagging. Esse trabalho aplicou essa
técnica para tradução dos idiomas azerbaijão, hindi, uzbeque, turco,
alemão e chinês para inglês. Como resultado, foram observados
ganhos de BLEU entre 1,16 e 2,39 pontos.
Na pesquisa realizada por Jang et al. 2022, foram utilizadas técni-
cas de aumento de dados para língua de sinais coreana (Gloss-level
Korean Sign Language). A proposta utilizou retrotradução, subs-
tituição por sinônimos restrita às classes de substantivos, nomes
próprios e pronomes, além de substituição de palavras utilizando
um modelo de linguagem coreano. Foram observados ganhos de
BLEU em 10, 12 e 16 pontos, respectivamente.
Por fim, destaca-se a pesquisa de Wang, Yang 2022 que traba-
lharam em modelos de tradução entre os idiomas inglês, chinês e
tailandês. Neste trabalho foram propostos aumento de dados por
substituição de palavras alinhadas e substituição por sinônimos.
Observaram ganhos de 1 BLEU ao utilizar substituição de palavras
alinhadas na tradução de chinês para inglês e de 4 BLEU na tradução
de inglês para chinês. Já a substituição por sinônimos na tradução
de chinês para tailandês resultou em um ganho de 2,7 BLEU.
FUNDAMENTAÇÃO TEÓRICA
3.1
Tradução Automática Neural
Tradução Automática Neural (Neural Machine Translation - NMT) é
a aplicação de redes neurais para a tarefa de tradução de sentenças
de uma língua de origem para uma língua de destino. Em geral, é
utilizada uma rede sequence-to-sequence, onde o encoder constrói
uma representação da sentença no idioma de origem e o decoder
utiliza essa representação e as palavras geradas anteriormente pela
própria rede para gerar a sentença traduzida no idioma alvo [3].
Em contraste à Tradução Automática Baseada em Regras (Rule
Based Machine Translation - RBMT), que transforma a sentença de
origem através de algoritmos de substituição por regras, derivadas
do conhecimento de linguistas, a NMT é baseada em dados. Sendo
assim, para que a NMT seja possível, é necessária a construção
de um corpus bilíngue. As redes neurais treinadas para resolver
o problema de tradução são capazes de aprender as regras de tra-
dução diretamente dos dados, eliminando assim a necessidade da
construção de algoritmos explícitos com regras de tradução. Espe-
cificamente no contexto da LIBRAS, modelos NMT oferecem uma
capacidade de desambiguação de palavras que têm grafia igual no
PB, porém significado e sinal diferentes na LIBRAS. Essa capaci-
dade de desambiguação não seria alcançada utilizando apenas uma
abordagem de RBMT, já que é necessário que o modelo de tradução
compreenda o contexto em que o termo ambíguo está inserido para
decidir a desambiguação correta [17].
3.2
Data Augmentation
A qualidade de um modelo de Tradução Automática depende prin-
cipalmente da existência de um corpus bilíngue de alta qualidade e
extensão significativa. Este corpus serve como base de treinamento
para que o modelo de tradução neural aprenda os padrões linguísti-
cos de ambos idiomas de origem e destino [17]. A LIBRAS, assim
como outras LS, pode ser classificada como língua com poucos
recursos (low-resource language), tendo em vista a baixíssima quan-
tidade de dados disponíveis para o treinamento de componentes de
PLN [2]. Para as línguas com poucos recursos, não há dados autênti-
cos traduzidos por humanos suficientes disponíveis para treinar um
modelo de NMT e obter resultados de alta qualidade. Desta forma,
a geração de dados sintéticos é uma estratégia interessante para
complementar o corpus criado pelos linguistas.
Com um corpus pequeno, o universo de sentenças a serem tradu-
zidas pelo modelo (quando o tradutor estiver sendo utilizado pelo
usuário) será muito maior que os exemplos vistos no treinamento.
O objetivo de algoritmos de aumento de dados é, a partir dos dados
autênticos, expandir e diversificar o corpus de treinamento com
dados sintéticos para que, idealmente, se aproxime da distribuição
de dados de todo o universo de pares de sentenças e traduções váli-
das, de modo que a quantidade e diversidade desses novos dados
beneficiem o modelo de tradução [19].
Uma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS
WebMedia’2024, Juiz de Fora, Brazil
Algoritmos de aumento de dados (data augmentation) geram da-
dos adicionais, sintéticos, a partir dos dados autênticos, através de
modificações sobre as sentenças autênticas. Esses dados adicionais
são então incorporados ao corpus de treinamento original. Esta
é uma tarefa desafiadora no ramo de PLN e tradução automática,
onde qualquer modificação na sentença pode ter impacto no seu
significado. Por isso, é importante que as traduções na língua de
destino mantenham equivalência com o significado na língua de
origem. É uma alternativa mais barata na falta de exemplos curados
por linguistas e é extremamente importante em línguas com poucos
recursos. Porém, por ser um procedimento automático, as sentenças
sintéticas geradas pelos algoritmos de aumento de dados tendem
a ser de menor qualidade em relação às sentenças autênticas pro-
duzidas por humanos. Deve-se, portanto, utilizar uma quantidade
razoável de dados sintéticos [14].
Existem diversas técnicas para data augmentation e, neste traba-
lho, abordaremos as técnicas de retrotradução, reversão e substitui-
ção de palavras, as quais serão detalhadas a seguir.
3.2.1
Retrotradução. A técnica de retrotradução (tradução reversa
ou back-translation) é amplamente utilizada para aumento de dados,
pois tende a gerar sentenças de boa qualidade. Nessa abordagem,
como pode ser observado na Figura 1, é utilizado um outro modelo
de tradução e a sentença é traduzida de um idioma para outro e
depois de volta para o idioma original2. Assim, a partir de um par
autêntico de uma sentença na língua de origem e sua respectiva
tradução, é possível gerar novas sentenças na língua de origem que
tenham o mesmo significado da tradução da sentença original. Isso
é feito ao executar o algoritmo nas sentenças do idioma de origem,
resultando em novas sentenças sintéticas no mesmo idioma. Essas
novas sentenças, resultantes da tradução reversa, são adicionadas
ao corpus original no lado do idioma de origem. As traduções corres-
pondentes, na língua de destino, associadas a essas novas sentenças
sintéticas, serão idênticas às traduções das sentenças autênticas
originais devido à tendência de preservação do significado [14].
Figura 1: Exemplo de retrotradução.
3.2.2
Reversão. A reversão dos tokens da sentença na língua de
destino, como exemplificado na Tabela 1, é uma tarefa auxiliar e não
convencional. No entanto, faz com que o modelo de tradução utilize
mais informações da representação do encoder para prever palavras
que geralmente aparecem no final da frase, onde a influência do
encoder tende a diminuir. Dado que as sentenças geradas não são
fluentes, é recomendada a adição de um token especial no início de
cada par de sentenças sintéticas [16].
2Por exemplo, traduzir uma sentença do português para o inglês e, em seguida, de
volta para o português.
3.2.3
Substituição de Palavras. Técnicas baseadas em substituição
de palavras envolvem gerar novas sentenças sintéticas escolhendo
uma palavra alvo na sentença autêntica para ser substituída ale-
atoriamente por outra palavra. Isso pode ser aplicado tanto nas
sentenças da língua de origem quanto nas da língua de destino,
e as substituições podem ser realizadas independentemente entre
si. Alternativamente, pode-se respeitar o alinhamento entre as pa-
lavras e realizar a substituição aos pares, como exemplificado na
Tabela 1: ao substituir uma palavra na sentença de origem, tam-
bém é substituída a palavra correspondente na sentença de destino.
Mesmo que a substituição seja aleatória, a introdução de ruído nas
sentenças de destino ajuda o modelo a aprender a prever a próxima
palavra corretamente, mesmo após a geração de uma palavra que
não corresponde exatamente ao padrão ouro3 da tradução humana
[19] [3].
Substituir palavras por sinônimos é uma abordagem que gera
sentenças sintéticas com significado mais próximo da sentença au-
têntica. Ao escolher uma palavra para ser substituída, é consultada
uma tabela de paráfrases que contém sinônimos que são candidatos
a substituir a palavra original. Através da representação vetorial
da palavra original e das palavras candidatas, é calculada a simi-
laridade cosseno a fim selecionar a palavra candidata com maior
similaridade para substituir a palavra original. Também é possível
utilizar marcação de partes do discurso (part-of-speech tagging) para
limitar as opções de substituição dentro da mesma classe grama-
tical, como exemplificado na Figura 2. Ao identificar cada classe
gramatical presente em uma sentença por meio de part-of-speech
tagging, torna-se viável realizar substituições de palavras mantendo
a coerência gramatical do texto. Essas restrições também evitam
erros que podem ocorrer com o uso da retrotradução, que confia
totalmente na saída do modelo de tradução [11].
Tabela 1: Exemplos de reversão e substituição alinhada.
Transformação
Idioma
Sentença
Nenhuma (sen-
tença original)
Origem
Tivemos uma calorosa recepção.
Destino
TER CALOROSO&ANIMADO
RECEPÇÃO [PONTO]
Reversão
Destino
[PONTO]
RECEPÇÃO
ANI-
MADO&CALOROSO TER
Substituição ali-
nhada
Origem
Tivemos uma calorosa rio.
Destino
TER CALOROSO&ANIMADO
RIO [PONTO]
METODOLOGIA
4.1
Técnicas Selecionadas
Com o objetivo de identificar quais métodos de aumento de dados
trazem o melhor ganho de desempenho ao tradutor, foram selecio-
nadas as seguintes técnicas: (a) retrotradução, por ser amplamente
utilizada em NMT, sendo uma referência sólida para comparação,
(b) a substituição alinhada e a tarefa auxiliar de reversão como des-
crito por Sánchez-Cartagena et al. 2021 e exemplificado na Tabela
3Tradução realizada por humanos e tomada como referência.
WebMedia’2024, Juiz de Fora, Brazil
Silva et al.
Figura 2: Exemplos de substituição por sinônimos com part-
of-speech tagging. [11]
1, dado que é uma evolução sobre métodos de substituição aleatória
anteriores, sendo selecionada por sua capacidade de melhorar a
diversidade dos dados sem a necessidade de modelos de linguagem
adicionais e (c) a substituição por sinônimos com part-of-speech
tagging como proposto por Maimaiti et al. 2021 e observado na
Figura 2, por ser uma alternativa à retrotradução que também tende
a gerar sentenças fluentes.
Para a retrotradução, são utilizados dois modelos de tradução
automática disponíveis no framework Hugging Face Transformers: o
modelo Helsinki-NLP/opus-mt-ROMANCE-en, que traduz do PB para
o inglês, e o modelo Helsinki-NLP/opus-mt-en-ROMANCE, que traduz
do inglês de volta para o PB. Durante a etapa de retrotradução, as
sentenças em PB do corpus autêntico são traduzidas para o inglês
pelo primeiro modelo e, em seguida, traduzidas de volta para o
PB pelo segundo modelo. Após remover as duplicatas, o corpus
expandido apresenta um aumento de cerca de 50% no número total
de sentenças.
A substituição de palavras alinhadas, como apresentada por
Sánchez-Cartagena et al. 2021, foi realizada utilizando o sistema
de tradução automática estatística MOSES4 que utiliza a biblioteca
GIZA para o alinhamento de palavras. O MOSES produz um léxico
que contém entradas de palavras na língua de origem, juntamente
com a probabilidade de que a palavra correspondente na língua
de destino seja uma tradução apropriada. Para a substituição, foi
determinado substituir uma palavra por sentença que é sorteada
aleatoriamente, devendo o par de palavras alinhadas ter uma proba-
bilidade maior que 0.7 no léxico. Em seguida, este par de palavras
é substituído por outro par de palavras, também de probabilidade
maior que 0.7, proporcionando uma variação semântica na sentença.
O método de reversão consiste em inverter a lista dos tokens da
string da sentença original, separados pelo caractere de espaço. Para
mitigar o impacto de sentenças potencialmente não fluentes, foi
adicionado um token especial precedendo a sentença de origem em
cada método. Essa medida tem como objetivo diminuir a influência
dessas sentenças no resultado final do tradutor.
4O MOSES é um sistema de tradução automática que opera através do alinhamento
um para um de todos os tokens da língua de origem para a língua de destino.
A implementação da substituição por sinônimos baseada em
part-of-speech tagging, conforme descrito por Maimaiti et al. 2021,
utiliza uma combinação de técnicas e recursos de PLN, incluindo
modelos de part-of-speech tagging, a base de dados WordNet para
identificar sinônimos candidatos e word embeddings para calcular a
similaridade entre os sinônimos candidatos e escolher o sinônimo
com maior similaridade cosseno. Para realizar a marcação de partes
do discurso, foi utilizado o modelo POS_tagger_brill.pkl, disponível
no repositório do GitHub inoueMashuu/POS-tagger-portuguese-nltk
para o framework de PLN nltk. Esse modelo é responsável por
atribuir classes gramaticais às palavras em PB para a identificação
das palavras alvo que serão substituídas por sinônimos. A base de
dados WordNet 5, acessada através da biblioteca nltk, foi empregada
como fonte de sinônimos para as palavras identificadas pelo part-of-
speech tagging. Por fim, para calcular a similaridade entre as palavras
alvo e os sinônimos disponíveis no WordNet, foram utilizadas as
embeddings skip-gram de 100 dimensões do Repositório de Word
Embeddings do NILC [6]. As word embeddings são representações
vetoriais das palavras que capturam suas relações semânticas com
base em seu contexto de ocorrência. Essas representações vetoriais
permitem calcular a similaridade cosseno entre palavras, necessário
para identificar o sinônimo mais adequado para a substituição.
4.2
Ambiente de Treinamento
O treinamento foi realizado utilizando o framework Fairseq6. Optou-
se por uma versão reduzida de um modelo que adota a arquitetura
transformer, proposta por Vaswani et al. (2017). No Fairseq, o modelo
transformer reduzido transformer_iwslt_de_en foi pré-treinado na
tradução de alemão para inglês. A escolha desse modelo permite
um treinamento mais rápido, uma vez que fazer o ajuste fino (fine-
tuning) de um modelo de linguagem já treinado, mesmo que em um
par de idiomas diferentes, tende a ser mais eficiente do que treinar
um modelo do zero [15].
O tradutor do VLibras possui uma arquitetura híbrida baseada
em RBMT e NMT. A sentença em PB é pré-processada por um
componente RBMT. A saída desse pré-processamento alimenta o
modelo transformer, que é treinado com o objetivo de aproximar
a glosa gerada pelo componente RBMT para a glosa gerada pelos
intérpretes humanos [2].
O corpus do VLibras possui pares de sentenças em PB como
língua de origem e uma representação intermediária da LIBRAS,
denominada glosa, para as sentenças no idioma de destino. O corpus
mencionado consiste em aproximadamente 65.000 exemplos de
pares de sentenças [2], cobrindo uma ampla variedade de tópicos
e contextos linguísticos. As glosas são criadas por intérpretes e
linguistas especializados na LS e cada sinal da glosa possui uma
animação associada. O uso de glosas intermediárias como uma
forma de representação linguística da LS permite que os algoritmos
de PLN trabalhem de forma mais eficaz com a LIBRAS [9].
5O WordNet é uma vasta base de dados lexical que organiza palavras em conjuntos
de sinônimos, conhecidos como synsets, e fornece informações sobre suas relações
semânticas e gramaticais. Essa base de dados permite a consulta de sinônimos restritos a
classes gramaticais específicas, como sujeitos, adjetivos, advérbios e verbos, permitindo
que a substituição gere sentenças de maior qualidade.
6Desenvolvido pelo Facebook, projetado com foco no treinamento de redes sequence-
to-sequence, que são adequadas para tarefas de tradução automática.
Uma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS
WebMedia’2024, Juiz de Fora, Brazil
No pipeline de tradução do VLibras, já existem cinco métodos de
aumento de dados que geram sentenças fluentes e foram construídos
com o conhecimento de linguistas da LIBRAS.
• Lugares: tem como objetivo introduzir variação nas senten-
ças ao identificar nomes de lugares como cidades, estados
ou países, e substituí-los por outras localidades disponíveis
em tabelas de substituição auxiliares.
• Negação: trabalha na identificação de sinais na frase que
podem ser negados e gera novas sentenças realizando essa
substituição. Essa técnica é essencial para aprimorar a ca-
pacidade do sistema de tradução em compreender e gerar
corretamente sentenças negativas, um aspecto importante
da gramática da LIBRAS.
• Intensidade: foca na identificação de advérbios na frase
que podem ser substituídos para gerar novas frases com
diferentes níveis de intensidade. Por exemplo, a substituição
de "muito" por "pouco" ou vice-versa.
• Famosos: visa diversificar o conteúdo das sentenças ao iden-
tificar sinais referentes a pessoas famosas e substituí-los por
outros sinais representando outros famosos. Essa técnica
melhora a capacidade de desambiguação do sistema ao lidar
com pessoas conhecidas.
• Direcionalidade: visa capturar uma nuance gramatical es-
pecífica da LS, levando em consideração o emissor e receptor
da ação do verbo, assemelhando-se a concordância número-
pessoal. O método de Direcionalidade identifica esses sinais
na sentença e realiza substituições apropriadas com outros
verbos direcionais equivalentes.
Todas essas técnicas de aumento de dados são aplicadas sobre as
sentenças do corpus autêntico construído por linguistas especializa-
dos em LIBRAS. As sentenças sintéticas geradas por essas técnicas
são então anexadas ao corpus autêntico, formando o corpus padrão
utilizado no treinamento.
4.3
Treinamento
Os pares de sentenças do corpus são processados por cada mé-
todo de aumento de dados já implementado no pipeline do VLibras,
conforme descrito na seção 4.2. O corpus aumentado gerado pela
saída do pipeline do VLibras compõe o conjunto de dados de treina-
mento que é o padrão (baseline) para os experimentos realizados,
possuindo cerca de 106 mil pares de sentenças.
Em seguida, cada método de aumento de dados proposto e imple-
mentado neste trabalho é aplicado sobre o conjunto de sentenças
do conjunto padrão. Nesta etapa, quando há combinação de dife-
rentes métodos, a entrada de cada método de aumento de dados é
restrita às sentenças do conjunto padrão, exceto para o método de
retrotradução, devido a restrições de recursos computacionais. No
caso da retrotradução, as sentenças sintéticas geradas são anexadas
às sentenças autênticas antes da execução dos métodos de aumento
de dados padrão do pipeline a fim de otimizar recursos e diminuir o
custo com o uso de GPU.
A semente de geração de números aleatórios é fixada em todos
os experimentos realizados durante o treinamento do modelo, per-
mitindo a reprodutibilidade dos resultados e uma comparação justa
entre diferentes treinamentos.
Por fim, o desempenho do modelo é avaliado através de um
conjunto de avaliação cuidadosamente selecionado por linguistas,
contendo 50 sentenças para cada grupo relevante no domínio da
LIBRAS. Esses grupos incluem sentenças básicas, com números
cardinais, com palavras homônimas, relacionadas à direcionalidade,
a pessoas famosas, à intensidade, a lugares e à negação, permitindo
uma avaliação do desempenho do modelo em diferentes contextos
linguísticos.
4.4
Métricas de Interesse
Avaliar a qualidade das traduções geradas por modelos de NMT é
uma tarefa desafiadora devido à natureza complexa e subjetiva da
linguagem. Diferentes traduções podem ser consideradas aceitáveis
para uma mesma sentença de origem, dependendo de uma variedade
de fatores como contexto, estilo e preferências individuais.
Uma das métricas mais amplamente utilizadas para avaliar a
qualidade da tradução automática é o BLEU (Bilingual Evaluation
Understudy) [13]. O BLEU é uma métrica que varia de 0 a 100 e
busca automatizar e replicar como um humano julgaria a qualidade
da tradução. Essa métrica é baseada na comparação dos n-gramas
da sentença gerada com as traduções de referência disponíveis, a
fim de calcular a similaridade entre a tradução gerada pelo modelo
e a tradução de referência.
O BLEU4, por exemplo, é calculado com base em n-gramas de
quatro palavras: Isso significa que o modelo é avaliado com base
na precisão dos n-gramas de quatro palavras em suas traduções,
em comparação com as traduções de referência. Uma das princi-
pais vantagens do BLEU é sua rapidez de cálculo, o que o torna
uma métrica eficiente para avaliação automática. No entanto, é
importante ressaltar que o BLEU apresenta algumas limitações. Por
exemplo, não são consideradas as similaridades semânticas entre
as traduções, o que pode levar a pontuações imprecisas em alguns
casos.
Neste trabalho, a análise dos resultados é realizada com base no
desempenho do modelo de tradução seguindo a métrica BLEU4.
RESULTADOS E DISCUSSÕES
Foram realizadas duas rodadas de experimentos com o objetivo
de avaliar o impacto de diferentes métodos de aumento de dados
no desempenho do modelo de tradução. Na primeira rodada, cada
método foi aplicado em sequência sobre o conjunto padrão, sem
limitar a quantidade de sentenças sintéticas geradas. Na segunda
rodada, o tamanho do conjunto de treinamento foi restrito a 155
mil sentenças a fim de equilibrar a proporção entre sentenças au-
tênticas e sintéticas. A Tabela 2 mostra o desempenho do modelo
utilizando exclusivamente o corpus autêntico com 65 mil pares de
sentenças, sem a aplicação de estratégias de aumento de dados. Em
contraste com o corpus padrão, que inclui os métodos de aumento
de dados do pipeline de tradução do VLibras mencionados na seção
4.2, totalizando 106 mil pares de sentenças.
Todos os métodos de aumento de dados apresentaram melhorias
em relação ao conjunto padrão, conforme observado na Tabela 3. A
retrotradução trouxe um ganho geral médio significativo (+15,82
BLEU), mesmo com o menor acréscimo de dados (totalizando 155 mil
sentenças). Tanto a reversão com substituição de palavras alinhadas,
quanto a substituição por sinônimos baseada em part-of-speech
WebMedia’2024, Juiz de Fora, Brazil
Silva et al.
Tabela 2: Resultado em BLEU do desempenho do modelo
utilizando apenas o corpus autêntico, sem aumento de dados.
Categoria
Pontuação BLEU
Básicas
17,35
Cardinais
6,22
Contexto
4,71
Direcionalidade
0,00
Famosos
0,00
Intensidade
0,00
Lugares
10,79
Negação
0,00
Romanos
3,92
tagging demonstraram resultados próximos (50,73 e 50,32 pontos
de BLEU, respectivamente). Observa-se que o aumento de dados traz
consideráveis melhorias, analizando sob a perspectiva da métrica
BLEU, para todos os grupos de validação.
Tabela 3: Resultado em BLEU do desempenho do modelo de
tradução utilizando as técnicas de aumento de dados selecio-
nadas.
Padrão
(106 mil)
RT
(155 mil)
RV
+ SA
(313 mil)
SS
(346 mil)
Básicas
42,05
51,66
47,89
52,67
Cardinais
50,18
64,78
72,86
70,52
Contexto
24,37
43,92
47,67
38,86
Direcionais
0,00
18,57
30,28
29,06
Famosos
27,34
45,07
43,23
40,70
Intensidade
38,47
49,38
42,5
36,55
Lugares
44,04
56,68
51,85
55,04
Negação
44,04
50,84
57,14
62,42
Romanos
25,86
57,85
63,15
67,12
Média
32,93
48,75
50,73
50,32
Legenda: RT (Retrotradução); RV (Reversão); SA (Substituição
alinhada); SS (Substituição por sinônimos)
Ao adicionar a retrotradução em conjunto com a combinação
de reversão com substituição alinhada, conforme a Tabela 4, foi
observada uma pequena piora não significativa no resultado geral.
Por outro lado, a combinação de retrotradução com a substituição
por sinônimos resultou em um ganho expressivo de desempenho
(+19,25 BLEU em relação ao conjunto padrão), possivelmente de-
vido ao aumento substancial na quantidade de sentenças resultante
dessa combinação (totalizando 540 mil sentenças). No entanto, ao in-
cluir mais métodos de aumento de dados, mesmo que isso aumente
ainda mais a quantidade de sentenças no conjunto de treinamento,
não foi observada uma melhoria significativa no desempenho do
modelo. Isso sugere que há uma limitação na melhoria do desempe-
nho proporcionada pela quantidade de sentenças sintéticas geradas,
especialmente em função da quantidade de dados autênticos dis-
poníveis. Esses resultados evidenciam a importância de encontrar
Figura 3: Média em BLEU do desempenho das técnicas de
aumento de dados sem restrição no tamanho do conjunto de
treinamento.
um equilíbrio adequado entre a quantidade de dados autênticos e
sintéticos no conjunto de treinamento.
Na segunda fase dos experimentos, conforme exibido na Tabela 5,
onde o tamanho do conjunto de treinamento foi limitado, o método
de retrotradução obteve os melhores resultados. Esse resultado não
é surpreendente, uma vez que a retrotradução é um dos métodos
mais estabelecidos e amplamente utilizados para aumento de dados
em PLN.
Ao considerar um cenário com aumento de 25% nos dados para
cada método empregado, foram testadas diversas combinações de
métodos aos pares, conforme visto na Tabela 6. Notadamente, a
combinação de retrotradução e substituição por sinônimos com
part-for-speech tagging demonstrou o melhor desempenho quando
há a limitação do tamanho do conjunto de treinamento, apresen-
tando ganho de 16,5 BLEU. Essa combinação já havia apresentado
bons resultados quando não há limitação no tamanho do conjunto
de treinamento, com 19,25 pontos de BLEU sobre o padrão (ver
Tabela 4), isso mostra que as duas técnicas produzem resultados
bons quando usadas em conjunto. Uma possível explicação para o
bom desempenho dessa combinação é o aumento da diversidade do
vocabulário proporcionado pela retrotradução. Ao traduzir as sen-
tenças de volta para o idioma original, o conjunto de treinamento
passa a contar com um vocabulário mais amplo, o que, por sua vez,
aumenta as opções de substituição por sinônimos.
Devido à limitação na quantidade de sentenças, a eficácia do
método de reversão foi reduzida quando combinado com outras
técnicas. Essa é uma desvantagem que o experimento traz para
esse método, porque ele é sugerido como uma tarefa destinada a
Uma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS
WebMedia’2024, Juiz de Fora, Brazil
Tabela 4: Desempenho em BLEU de combinações de técnicas de aumento de dados sem restrição na quantidade de sentenças.
Padrão
(106 mil)
RT + RV + SA
(467 mil)
RT + SS
(540 mil)
RT + RV + SS
(695 mil)
Todos
(851 mil)
Básicas
42,05
48,55
48,14
50,76
53,53
Cardinais
50,18
61,54
67,29
66,79
66,49
Contexto
24,37
45,68
43,73
39,36
49,25
Direcionalidade
0,00
27,66
33,60
36,95
32,66
Famosos
27,34
43,74
48,09
44,83
42,85
Intensidade
38,47
45,37
39,61
42,29
42,29
Lugares
44,04
54,45
48,10
52,42
47,12
Negação
44,04
53,84
65,94
59,17
59,34
Romanos
25,86
74,17
75,19
76,07
75,21
Média
32,93
50,55
52,18
52,07
52,08
Legenda: RT (Retrotradução); RV (Reversão); SA (Substituição alinhada); SS (Substituição por sinônimos)
Tabela 5: Resultado em BLEU de cada técnica utilizada isola-
damente, com restrição de 155 mil sentenças no conjunto de
treinamento.
Padrão
RT
SS
RV
SA
Básicas
42,05
51,66
48,74
43,07
48,51
Cardinais
50,18
64,78
56,29
60,33
60,69
Contexto
24,37
43,92
35,00
32,80
37,85
Direcionalidade
0,00
18,57
22,34
Famosos
27,34
45,07
47,41
34,36
45,23
Intensidade
38,47
49,38
38,54
38,65
43,14
Lugares
44,04
56,68
51,5
47,87
47,73
Negação
44,04
50,84
58,87
42,95
53,6
Romanos
25,86
57,85
40,72
31,07
57,25
Média
32,93
48,75
41,89
36,78
46,26
Legenda: RT (Retrotradução); RV (Reversão); SA(Substituição
alinhada); SS (Substituição por sinônimos)
fortalecer o encoder de forma independente de outras técnicas de
aumento de dados. Além das combinações de pares de métodos,
também foram exploradas outras configurações envolvendo três
técnicas distintas.
Por fim, foram testadas mais algumas configurações, conforme
mostrado na Tabela 7. Uma combinação de 25% de retrotradução,
25% de substituição por sinônimos e 25% de substituição alinhada
e uma configuração com 1/3 de contribuição por cada método.
Observa-se, no entanto, que essas combinações não resultaram
em melhorias significativas em relação aos resultados obtidos an-
teriormente. Sendo assim, mesmo com a combinação de diversas
técnicas de aumento de dados, a quantidade de sentenças ainda é
um fator limitante para alcançar melhorias de desempenho.
Figura 4: Média em BLEU do desempenho das técnicas de
aumento de dados com limitação de 155 mil sentenças no
conjunto de treinamento.
WebMedia’2024, Juiz de Fora, Brazil
Silva et al.
Tabela 6: Resultado em BLEU do treinamento do modelo de tradução com contribuição de 25 mil sentenças por método.
Padrão
RV + SA
RV + SS
RT + SS
RT + RV
RT + SA
Básicas
42,05
47,29
40,84
48,37
47,86
46,25
Cardinais
50,18
66,69
57,17
66,33
67,18
64,80
Contexto
24,37
43,86
33,08
38,86
38,56
43,28
Direcionalidade
0,00
18,61
17,12
28,12
21,91
25,01
Famosos
27,34
52,12
41,58
46,67
42,07
46,46
Intensidade
38,47
40,21
49,63
43,62
44,40
42,06
Lugares
44,04
48,65
47,97
50,84
52,17
52,65
Negação
44,04
57,69
48,55
60,05
46,69
50,59
Romanos
25,86
49,51
34,07
62,09
48,99
60,08
Média
32,93
47,18
41,11
49,43
45,53
47,90
Legenda: RT (Retrotradução); RV (Reversão); SA (Substituição alinhada); SS (Substituição por sinônimos)
Tabela 7: Resultado em BLEU para combinações envolvendo
retrotradução, substituição alinhada e substituição por sinô-
nimos.
Padrão
3 RT +
3 SA + 1
3 SS
2 RT +
4 SA + 1
4 SS
Básicas
42,05
54,24
52,80
Cardinais
50,18
63,44
63,78
Contexto
24,37
43,20
44,02
Direcionalidade
0,00
22,35
16,92
Famosos
27,34
49,94
50,71
Intensidade
38,47
42,44
33,28
Lugares
44,04
51,37
51,24
Negação
44,04
39,94
52,71
Romanos
25,86
60,28
62,68
Média
32,93
47,46
47,57
Legenda: RT (Retrotradução); SA (Substituição alinhada);
SS (Substituição por sinônimos)
CONCLUSÃO
A geração de dados sintéticos para aprimorar modelos de NMT em
cenários de poucos recursos (low-resource language), especialmente
para LS como a LIBRAS, é de suma importância para a acessibilidade
e inclusão de pessoas surdas. Dessa forma, este trabalho explorou
diferentes métodos de aumento de dados a fim de identificar quais
abordagens podem melhorar o desempenho do tradutor, com base
em testes computacionais amparados pela métrica BLEU.
Observou-se que a técnica amplamente utilizada de retrotradu-
ção também é eficaz na tradução de PB para LIBRAS, trazendo um
ganho de 15,82 pontos de BLEU em relação ao conjunto padrão.
A combinação de retrotradução e substituição por sinônimos com
part-of-speech tagging trouxe os melhores resultados em ambos os
cenários: sem restrição no tamanho do conjunto de treinamento
(+19,25 BLEU sobre o padrão) e também quando o conjunto de
treinamento foi limitado a 155 mil sentenças (+16,5 BLEU sobre
o padrão). Essas técnicas podem ser utilizadas para aumentar a
quantidade de exemplos em conjuntos de sentenças que estejam
sub-representados no corpus. Destaca-se também, diante dos resul-
tados alcançados, a importância de manter um equilíbrio entre a
quantidade de dados sintéticos gerados em relação à quantidade de
dados autênticos no corpus original.
Por fim, enxerga-se que a investigação de técnicas mais custosas,
porém potencialmente mais eficazes, pode abrir novas possibilida-
des para aprimorar ainda mais a qualidade da tradução automática.
Modelos de linguagem podem produzir texto fluente em uma va-
riedade de contextos linguísticos, portanto, utilizar esses modelos
para gerar dados sintéticos é uma sugestão para trabalhos futuros.
AGRADECIMENTOS
Os autores agradecem à Secretaria Nacional dos Direitos da Pessoa
com Deficiência do Ministério dos Direitos Humanos e da Cidadania
pelo apoio financeiro para a realização desta pesquisa.

--- FIM DO ARQUIVO: 30133.txt ---

--- INÍCIO DO ARQUIVO: 30134.txt ---
WebMedia’2024, Juiz de Fora, Brazil
Rocha et al.
interagir de forma assíncrona e em tempo real com ambientes in-
certos e dinâmicos. A escolha do framework adequado para auxiliar
no desenvolvimento de sistemas para robôs depende dos requisitos
específicos do projeto como a complexidade dos comportamentos
robóticos desejados e a integração de sensores e atuadores.
O projeto do framework OROCOS [3] busca desenvolver um
software mais aberto, que seja adaptável e que possa atender a to-
das as necessidades intrínsecas ao desenvolvimento de software de
controle para robôs. Além disso, ele busca atender às necessidades
de quatro categorias de usuários: usuários finais, desenvolvedores
de aplicações, desenvolvedores de componentes e desenvolvedores
de frameworks. Dao [8] propôs um framework denominado ORO-
MACS para a construção de um sistema de controle multiagente
hierarquicamente estruturado que tem como base o OROCOS.
O ROS (Robot Operating System) [17] não é um sistema opera-
cional no sentido tradicional de gerenciamento e agendamento de
processos. Segundo Martin et al. [14], o ROS tornou-se, de fato, o
framework padrão para o desenvolvimento de software em robó-
tica. Ele é uma estrutura flexível de código aberto para a escrita de
software para robôs que fornece uma coleção de mecanismos de co-
municação, ferramentas, bibliotecas e regras que visam simplificar
a tarefa de criação de software para várias plataformas robóticas
[1].
Szücs et al. [22] propuseram um software de treinamento baseado
em um robô humanoide amplamente utilizado na literatura, o robô
NAO, da empresa Aldebaran Robotics. O software foi construído
utilizando-se o framework NAOqi. O framework busca atender às
necessidades de programação como paralelismo, sincronização, ge-
renciamento de eventos e gerenciamento de recursos. Além disso a
Choregraphe Suite [21], uma IDE fornecida pela empresa SoftBank
Robotics para programação dos robôs NAO e Pepper, se integra
perfeitamente ao framework NAOqi.
O framework YARP (Yet Another Robot Platform) [15] foi escrito
por e para pesquisadores em robótica humanoide que se deparam
com uma pilha complicada de hardware para controlar e com uma
pilha igualmente complicada de software. Ele oferece suporte a
múltiplos protocolos de comunicação e flexibilidade para integrar
componentes heterogêneos.
Ritschel [19] e Ritschel et al. [18] apresentam uma abordagem
para implementar os movimentos do robô Reeti de maneira paralela
e independente. A proposta utiliza o framework URBI (Universal
Robot Body Interface) [2] que é baseado em uma arquitetura cli-
ente/servidor de maneira que o servidor roda no robô e é acessado,
normalmente, via TCP/IP. O framework inclui uma linguagem de
script utilizada pelo cliente que é capaz de controlar as articulações
do robô ou acessar seus sensores, câmera, alto-falantes etc.
Os trabalhos supracitados trazem contribuições importantes para
a robótica, porém, apresentam limitações. Apesar da abrangência e
do poder do ROS [17] sua curva de aprendizagem pode ser íngreme
para iniciantes. Canela et al. [4] conduziram um estudo abordando
os desafios encontrados no aprendizado do ROS. O estudo apre-
senta, na forma de um mapa mental, vários problemas relacionados
a diversos fatores, entre eles: problemas de concorrência (problemas
de memória compartilhada em funções callbacks e perda de men-
sagens), problemas gerados pelo impacto na frequência no envio
de mensagens (publish-subscribe) e a complexidade na definição de
novos formatos de mensagem pois esse processo requer a alteração
de código em vários lugares em arquivos diferentes. Além disso,
a sua gestão de dependências entre pacotes pode ser complicada
e a compatibilidade entre versões pode causar problemas. O fra-
mework OROCOS [3] é complexo de instalar e configurar devido
suas múltiplas dependências e requisitos de sistema. Ele exige do
usuário conhecimento aprofundado de controle em tempo real e
programação de componentes. Os frameworks NAOqi [22] e a IDE
Choregraphe Suite [21] foram projetados exclusivamente para os
robôs NAO e Pepper sendo ferramentas de software proprietário
e de custo elevado. O framework YARP [15] apresenta um certo
grau de complexidade em sua configuração inicial e na integração
de diferentes módulos. A sua flexibilidade, dando suporte a múl-
tiplos protocolos de comunicação, pode causar overhead gerando
um impacto negativo em aplicações de tempo real. Por possuir
uma linguagem de script própria, a utilização do URBI [2] exige
que o usuário aprenda urbiscript e isso pode ser uma barreira para
usuários familiarizados com outras linguagens.
Este trabalho apresenta uma proposta de framework para desen-
volvimento de sistemas de software de controle para plataformas
de robótica social. O framework é de fácil instalação e configuração
com apenas uma dependência, a instalação e configuração de um
broker MQTT. Isso reduz significativamente o tempo e o esforço ne-
cessários para que um usuário iniciante na área de desenvolvimento
de software para robôs possa começar a desenvolver seu sistema
de controle. Além disso a utilização de uma arquitetura orientada
a objetos combinada com a comunicação baseada em MQTT faci-
lita a integração de novos módulos e dispositivos permitindo uma
adaptação rápida às mudanças e a novos requisitos de projeto.
FRAMEWORK PROPOSTO
Segundo Pressman e Maxim [16], um Framework é uma "miniar-
quitetura" reutilizável que serve como base para e a partir da qual
outros padrões de projeto podem ser aplicados. Ele pode ser visto
como uma estrutura (um arquétipo) de um sistema que é instan-
ciado e especializado para gerar uma família de aplicações [10],
sendo empregado para representar um problema de domínio espe-
cífico, podendo servir como uma solução potencial para o problema
em questão. Neste trabalho, foi adotada uma abordagem de es-
pecificação orientada a objetos para facilitar o entendimento da
estruturação do framework. Essa estrutura é genérica e deve ser
especializada para cada cenário específico. A Figura 1 apresenta os
componentes do framework com suas classes e interfaces.
As interfaces apresentadas na Figura 1 definem algumas ope-
rações que devem ser implementadas pelas classes (módulos de
controle) do sistema do robô. A interface IRobotModule, por exem-
plo, é responsável por definir comportamentos e métodos que são
comuns a quaisquer módulos do sistema robótico que se deseja
desenvolver, sejam eles, um módulo de controle de algum hard-
ware, uma funcionalidade específica do robô ou um módulo de
controle geral do sistema. Há também duas interfaces (IBlocking
e IUnBlocking) que modelam um comportamento de bloqueio e
desbloqueio para a implementação de módulos que executam suas
tarefas de maneira síncrona com outros módulos. A implementa-
ção ou não dessas interfaces estará diretamente ligada a função
Uma Proposta de Framework para Sistemas de Controle para Plataformas de Robótica Social
WebMedia’2024, Juiz de Fora, Brazil
Figura 1: Componentes do framework proposto.
específica do módulo do robô sendo desenvolvido. Esse comporta-
mento será exemplificado e detalhado através de um diagrama de
sequência na Seção 4.2.
Além do arcabouço básico para a criação de módulos de controle
para cada funcionalidade específica de um robô como o controle
de atuadores, a leitura de sensores ou a requisição de serviços
(locais ou na nuvem), o framework provê uma base para a criação
de tipos de módulos de controle mais gerais. Um exemplo desse tipo
seria um módulo Script Player que pode ser utilizado para executar
comandos provenientes de alguma linguagem de script. Além disso,
o framework disponibiliza uma classe abstrata RobotMemory que
modela um sistema básico de memória definindo comportamentos
intrínsecos a esse tipo de elemento como inicialização, criação,
definição e alteração de valores de variáveis na memória do robô.
O estilo arquitetural adotado nesta proposta é baseado no para-
digma publish-subscribe. A utilização desse padrão permite que os
módulos executem e se comuniquem de maneira assíncrona e distri-
buída. Sendo assim, cada módulo instanciado a partir do framework
deve possuir uma instância de um cliente MQTT a fim de que seja
possível a comunicação entre os módulos e o broker MQTT. Um cli-
ente MQTT se torna disponível a um módulo a partir do momento
que esse módulo implementa a classe abstrata RobotModule que
contém uma instância da classe concreta BrokerConnection.
A próxima seção descreve com detalhes a instanciação do fra-
mework proposto em dois cenários concretos: os sistemas de con-
trole dos robôs EVA e FRED.
CENÁRIOS DE INSTANCIAÇÃO DO
FRAMEWORK
Nesta seção, dois cenários de utilização do framework proposto
são apresentados, a fim de exemplificar o seu uso. O primeiro ce-
nário demonstra a instanciação das classes básicas utilizadas pata
a criação do sistema de controle do robô social EVA, com todas a
suas funcionalidades, e um software capaz de executar scripts na
sua linguagem de programação EvaML [7]. O segundo cenário usa
como base a plataforma de robótica open-source FRED. O cenário
serve para exemplificar como a mesma estrutura utilizada para o
Figura 2: Plataforma EVA de robótica social de código aberto.
EVA pode ser utilizada para a construção de um sistema de controle
para uma outra plataforma que possui uma arquitetura básica de
hardware bem mais simples que a do robô EVA.
4.1
Funcionalidades do Robô EVA
O EVA, que pode ser visto na Figura 2, tem sido usado para orientar
intervenções terapêuticas para pessoas com demência e auxiliar
os cuidadores na abordagem de comportamentos perturbadores. O
robô EVA é uma plataforma de robótica social de open-source proje-
tada para apoiar pesquisas na área de interação Humano-Robô [6].
Ele possui diversas funcionalidades e elementos de comunicação
verbal e não verbal [5]. Além disso, o EVA possui uma linguagem
de programação declarativa baseada em XML, chamada EvaML
[7], e um software de controle, o EvaSIM, [13] que pode executar
scripts EvaML controlando o robô físico. Cada uma dessas funcio-
nalidades foi implementada utilizando o framework proposto neste
trabalho. A seguir será apresentada uma breve descrição de suas
funcionalidades.
Expressões do Olhar. O EVA pode expressar a seguintes emoções
através do seu display: neutra, raiva, nojo, medo, felicidade, tristeza
e surpresa.
Comunicação Verbal. O EVA pode falar utilizando recursos da
nuvem como o serviço de Text-To-Speech (TTS) do IBM Watson. Ele
também pode transformar voz em texto utilizando o serviço na
nuvem da API de Speech-To-Text (STT) do Google.
Movimentação da Cabeça e dos Braços. O EVA é capaz de mover
a sua cabeça com dois graus de liberdade: para cima e para baixo,
para esquerda e para a direita. Além da movimentação da cabeça,
o EVA pode mover seus braços para cima e para baixo, podendo
também, executar o movimento de shaking sacudindo os braços em
torno de suas posições correntes.
Animações com os LEDs RGB. O EVA pode executar diversas
animações com o conjunto de LEDs RGB em seu tórax. Essas anima-
ções utilizam várias cores pré-determinadas e tem como o objetivo,
através de comunicação não verbal, expressar emoções.
Controle da Lâmpada Inteligente. O robô possui a capacidade de
controlar efeitos sensoriais de luz através do controle uma lâmpada
inteligente.
Player de Áudio. O EVA pode tocar vários tipos de arquivos
de áudio como músicas, efeitos sonoros e os áudios gerados pelo
WebMedia’2024, Juiz de Fora, Brazil
Rocha et al.
Figura 3: Arquitetura de software do robô EVA implementada
a partir do framework proposto.
serviço de TTS do IBM Watson, para isso, ele utiliza uma caixa de
som externa.
Visão Computacional. O EVA possui uma câmera acoplada à sua
cabeça e a partir das imagens capturadas por ela, o sistema do
robô implementa três funcionalidades: o reconhecimento da face
do usuário, o reconhecimento de expressões faciais e a leitura de
QR Codes.
4.2
Instanciação do Software do EVA
Como foi dito anteriormente, os módulos de controle definidos a
partir do framework utilizam o paradigma publish-subscribe de troca
de mensagens assíncronas. Cada módulo de controle do robô, que
implementa cada uma de suas funcionalidades, é controlado através
de mensagens que são definidas pelo desenvolvedor do sistema.
Essas mensagens também são utilizadas como um mecanismo de
sincronização entre os módulos, além de permitir o compartilha-
mento de informações relacionadas aos seus serviços. Os valores
provenientes da leitura de sensores como um microfone (no caso
específico de um módulo que transforma áudio em texto) ou de
algum módulo capaz de classificar a emoção contida em um texto
podem ser passados de um módulo para o outro através da troca de
mensagens publicadas em tópicos específicos destinados a este fim.
O software de controle do EVA é composto por dez módulos.
Nove deles, executam dentro de uma placa Raspberry PI 4, que
fica embutida dentro do corpo impresso em 3D do robô. O décimo
módulo roda em uma máquina separada. Os nove módulos são
responsáveis por controlar cada uma das funcionalidades descritas
na Seção 4.1. Todos os módulos foram implementados tendo como
base o framework proposto neste trabalho. A Figura 3 permite que se
tenha uma visão geral desses módulos e o esquema de comunicação
de cada um deles com o broker MQTT.
Com base no comportamento de cada módulo do sistema do
EVA, é possível dividi-los em três grupos distintos: I) os módulos
que realizam tarefas de maneira assíncrona, sem fazer uso de al-
gum mecanismo de sincronização com outros módulos, ou seja,
eles recebem um comando de ativação e realizam suas tarefas sem
Tabela 1: Tópicos e mensagens para a implementação do
sistema de controle do EVA.
Grupo
Módulo
Tópico
(Assinado)
Mensagem
Display
evaEmotion
"HAPPY", "FEAR", "SAD" etc.
Light
light
"BLACK|OFF", "BLUE |ON" etc.
RGB LEDs
leds
"HAPPY", "ANGRY" etc.
Motion
motion/head
"UP", "DOWN", "LEFT" etc.
motion/arm/left
"POS0", "POS1", SHAKE1 etc
motion/arm/right
"POS0", "POS1", SHAKE1 etc
STT
listen
Empty text
TER
textEmotion
"Text"
Computer
Vision
userEmotion
Empty text
userID
Empty text
qrRead
Empty text
TTS
talk
"Voice|Text"
Audio
sound
"file_name|BLOCKING"
speech
"file_name"
Script Player
"Ver Tabela 2"
"Ver Tabela 2"
sinalizar o seu término; II) os módulos que realizam tarefas síncro-
nas e que retornam algum tipo de resposta (em formato de texto)
sinalizando seu término através de uma mensagem de desbloqueio;
III) os módulos que realizam tarefas de maneira síncrona, assim
como os módulos do grupo II, porém, sem retornar qualquer tipo
de resposta em formato textual. Como os processos de instanciação
dos módulos pertencentes a cada grupo são similares entre si, esses
processos serão descritos e discutidos a partir de cada um dos três
grupos. A Tabela 1 apresenta cada grupo e seus módulos.
4.2.1
Módulos - Grupo I. O primeiro grupo é formado pelos
módulos Motion, responsável pelo controle da movimentação da
cabeça e dos braços do robô, pelo módulo RGB LEDs, que executa
animações com os LEDs RGB no tórax do EVA, pelo módulo Light,
que controla a Lâmpada Inteligente, e pelo módulo Display, que
mostra expressões do olhar do robô através de um display de 5.5".
Todos os módulos desse grupo iniciam a execução de suas tarefas a
partir do recebimento de um comando de ativação, proveniente de
outro módulo. Os comandos de ativação são mensagens especificas
publicadas no broker em tópicos determinados pelo desenvolvedor
do sistema. Após sua execução, um módulo do grupo I não retorna
qualquer informação como resposta e nem utiliza qualquer meca-
nismo que possa sinalizar o término de sua tarefa para o módulo
que o controla. Um módulo de controle do robô como o módulo
Script Player, que será visto mais adiante, pode enviar um comando
para o módulo do Display fazendo com que o EVA mostre uma
expressão de alegria. O módulo de controle atinge seu objetivo
publicando a mensagem "HAPPY" no tópico evaEmotion, previa-
mente assinado pelo módulo do Display. A Figura 4 apresenta todo
o processo descrito junto com os elementos envolvidos.
A Figura 5 apresenta a instanciação do módulo Display. Basica-
mente, para implementar um módulo utilize o framework proposto,
é necessário que o módulo seja desenvolvido seguindo os três pas-
sos:
(1) Estender a classe abstrata RobotModule. Ao fazer isso, o
módulo passa a ter a capacidade de definir um tópico base
que será usado como o tópico raiz do sistema do robô. Além
Uma Proposta de Framework para Sistemas de Controle para Plataformas de Robótica Social
WebMedia’2024, Juiz de Fora, Brazil
Figura 4: Controle das expressões do olhar do robô através
da publicação da mensagem "HAPPY" no tópico evaEmotion.
disso, ele se torna capaz de instanciar um objeto de conexão
com o broker MQTT.
(2) Instanciar o objeto de conexão com o broker obtendo um ob-
jeto do tipo mqtt_client. Isso permite que o módulo seja capaz
de "assinar" e/ou "publicar" nos tópicos de seu interesse.
(3) Implementar a interface IRobotModule. No contrato da in-
terface ela determina que todo módulo que pretende realizá-
la deve implementar os métodos: onConnect(), que é o local
onde o módulo deve assinar os tópicos de seu interesse; o
método onMessage(), que é onde o módulo recebe e trata
as mensagens publicadas nos tópicos por ele assinados; o
método sendMessage(), que deve implementar o envio (a pu-
blicação) de mensagens, caso o módulo retorne algum valor
e, por último, o método init(), que deve ser implementado
a fim de instanciar o objeto de conexão com o broker, ob-
tendo o cliente mqtt e anexando a ele as funções callback
supracitadas: onConnect() e onMessage().
Os demais módulos do grupo I, por apresentarem o mesmo pa-
drão de comportamento, seguem o mesmo processo de instanciação
do módulo Display.
4.2.2
Módulos - Grupo II. Os módulos que fazem parte deste
grupo, são: o módulo STT (Speech-To-Text), que retorna um texto
a partir do áudio da fala do usuário, o módulo CV (Visão Compu-
tacional), que executa três funcionalidades baseadas nas imagens
capturadas através da PiCamera, retornando suas respostas em for-
mato de texto, e o módulo TER (Text-Emotion-Recognition), que
classifica um texto retornando a emoção contida nele. A resposta
retornada pelo módulo TER também é em formato de texto. A ins-
tanciação do módulo CV pode ser vista na Figura 6.
O módulo de visão computacional do EVA executa três tarefas
que têm como base as imagens capturadas pela PiCamera, que fica
acoplada sobre sua cabeça. O driver da câmera não permite que
ela seja compartilhada por mais de um processo simultaneamente,
fazendo com que as funcionalidades de identificação da face do
usuário, de reconhecimento de expressões faciais e de leitura de QR
Code, que fazem uso da câmera, tenham que ser implementadas em
um só módulo, ou seja, no módulo que se conecta à câmera.
Um módulo pertencente ao grupo II, assim com os módulos do
primeiro grupo, deve possuir as características básicas necessárias
Figura 5: Instanciação do módulo Display.
Figura 6: Instanciação do módulo CV (Visão Computacional).
para a implementação de uma funcionalidade no robô utilizando
o framework proposto. Para isso, o módulo CV segue os mesmos
passos de implementação dos módulos do grupo I. Os módulos do
grupo II podem executar tarefas de maneira sincronizada com os
outros módulos, sinalizando o término de sua atividade através
de um mecanismo (uma mensagem) de desbloqueio. Para isso, o
framework proposto disponibiliza uma interface chamada IUnBloc-
king que deve ser implementada pelo módulo. Esse método deve
ser o responsável pelo envio (publicação) da mensagem que sina-
liza o término da execução do módulo sendo implementado. Todos
WebMedia’2024, Juiz de Fora, Brazil
Rocha et al.
Tabela 2: Tópicos e mensagens especiais utilizados no sistema
de controle do EVA.
Propósito
Tópico
Descrição
Log de eventos
log
Os módulos do robô EVA utilizam este tópico
para fornecer informações sobre seus estados
e serviços.
Passagem de va-
lores de variá-
veis
var
Este tópico é usado na passagem de valores
de variáveis entre módulos. É utilizado pelos
módulos CV, STT, TER e Script Player.
Sincronização
entre módulos
state
Através deste tópico, os módulos podem fun-
cionar de maneira síncrona, trocando mensa-
gens de bloqueio e desbloqueio.
os módulos do EVA que implementam esse método enviam suas
mensagens de sincronismo para um tópico especial, chamado state.
No sistema de controle do EVA, todo módulo que controla ou é
controlado por outro módulo utiliza a publicação mensagens nesse
tópico como mecanismo de sincronização.
Além da utilização do mecanismo de sincronismo, com a im-
plementação da interface IUnBlocking, um módulo do grupo II
sempre retorna algum tipo de informação que é proveniente do
seu serviço e essa informação é sempre no formato de texto (uma
string). No sistema do EVA, os módulos que precisam passar suas
informações para os outros módulos utilizam o método sendMes-
sage(). Esse método é usado para enviar/publicar a informação a
ser retornada em forma de mensagem através do tópico especial
var. Sendo assim, qualquer módulo interessado em receber o valor
de alguma variável deve assinar esse tópico. A Tabela 2 mostra, de
uma maneira mais detalhada, a lista dos tópicos especiais utilizados
no sistema do EVA.
4.2.3
Módulos - Grupo III. Os módulos do EVA que pertencem
a este grupo são capazes de sincronizar sua execução com outros
módulos. Eles implementam a interface IUnBlocking e são capazes
de sinalizar o término de suas tarefas através do envio/publicação
de mensagens no tópico especial, state. Porém, eles não retornam
qualquer tipo de informação textual. Como pode ser visto na Tabela
1, os módulos deste grupo, são: o módulo TTS, o módulo Audio e o
módule Script Player.
O módulo TTS é responsável por transformar um texto (uma
string) em um arquivo de áudio. Esse arquivo contém o áudio do
texto falado por uma das vozes disponibilizadas pelo serviço na
nuvem do IBM Watson. O módulo é ativado a partir da recepção
de uma mensagem publicada no tópico talk. Como pode ser visto
na Tabela 1, a mensagem deve conter a especificação da voz a ser
usada no processo de TTS, concatenada com o texto que deve ser
transformado. Como pode ser visto na Figura 3, os arquivos de áudio
gerados pelo IBM Watson são armazenados em uma pasta local
chamada TTS Cache Files. Após o processo de geração do arquivo de
áudio, o módulo TTS envia um comando de ativação para o módulo
Audio, que deverá tocar o arquivo de áudio gerado pelo Watson e foi
armazenado em cache. Esse comando é uma mensagem publicada
no tópico speech, que é assinado pelo módulo Audio e contém o
nome do arquivo de áudio com a fala.
O módulo Audio é responsável por tocar todo o tipo de áudio
usado pelo robô, seja ele, um arquivo de música, um efeito sonoro
ou um arquivo de áudio gerado pelo módulo TTS. Como pode ser
visto na Tabela 1, o módulo assina dois tópicos, o tópico sound e o
tópico speech, sendo ativado através do recebimento de mensagens
publicadas nesses tópicos. O primeiro deles, o sound, recebe no texto
da mensagem, o nome do arquivo de áudio a ser reproduzido, con-
catenado com uma informação booleana ("TRUE" ou "FALSE") que
indica se o módulo deve executar de maneira síncrona ou assíncrona
com o módulo de controle. Caso o valor passado seja verdadeiro,
o módulo enviará um sinal de desbloqueio ao finalizar a execução
do áudio. O segundo tópico assinado pelo módulo Audio é o speech.
As mensagens recebidas nesse tópico ativam o módulo e contém
apenas o nome do arquivo de áudio da fala, que foi previamente
armazenado na pasta local TTS Cache Files. Todos os arquivos de
áudio de fala são tocados de maneira síncrona, sinalizando o seu
término através do tópico especial, state.
O módulo Script Player (SP) possui todas as características dos
outros dois módulos do grupo III, porém, ele possui algumas ca-
racterísticas únicas. O módulo SP é um módulo de controle, sendo
capaz de ler scripts de interação escritos na linguagem EvaML [7].
A EvaML é uma linguagem de programação open-source destinada
ao desenvolvimento de scripts de interação para plataformas de
robótica social. O módulo SP pode ler o conteúdo de um arquivo
XML que contém o script EvaML e executar os comandos que repre-
sentam as funcionalidades do robô, como: <evaEmotion>, <light>,
<motion>, <talk> etc. Ao ler cada comando do script com seus
respectivos atributos o módulo SP envia os sinais de ativação aos res-
pectivos módulos fazendo com que o robô responda aos comandos
sendo enviados. A fim de poder executar comandos de maneira sin-
cronizada o módulo SP implementa uma outra interface fornecida
pelo framework, a interface IBlocking. Através da implementação
do método block() o módulo SP pode bloquear seu fluxo de exe-
cução, ativar um módulo que funcione de maneira síncrona como
o módulo STT e, após a execução do módulo, que sinaliza o seu
término enviando uma mensagem para o tópico especial state, o mó-
dulo SP é desbloqueado, seguindo seu fluxo executando o próximo
comando.
Para facilitar a implementação dos módulos que precisam imple-
mentar algum tipo de memória para o robô, o framework disponibi-
liza uma classe abstrata que oferece um modelo simples de memória
com alguns métodos concretos implementados. Esse módulo pode
ter seus métodos sobrecarregados a fim de prover a manipulação
de outros tipos de dados, além de strings. A Figura 7 mostra a inter-
face IBlocking, a classe abstrata RobotMemory e o processo de
instanciação dos três módulos do grupo III.
A Figura 8 apresenta um diagrama de sequência que demonstra
como um comando de TTS é executado pelos módulos SP (a partir
do seu envio), TTS (o processo de transformação de texto-para-
fala) e Audio (execução do arquivo de áudio da fala). Através do
acompanhamento da sequência apresentada no diagrama, é possível
compreender com mais clareza o processo de sincronização entre
módulos.
Ao executar um script EvaML e encontrar um comando do tipo
<talk> o módulo SP publica uma mensagem no tópico talk. Como
especificado na Tabela 1, a mensagem deve conter o timbre de voz
que será utilizado pelo serviço do IBM Watson e o texto a ser trans-
formado para fala. Em seguida, o módulo SP chama o seu método
block(), bloqueando seu fluxo de execução. O módulo permanecerá
Uma Proposta de Framework para Sistemas de Controle para Plataformas de Robótica Social
WebMedia’2024, Juiz de Fora, Brazil
Figura 7: Instanciação dos módulos TTS (Text-To-Speech), Audio e Script Player.
bloqueado até que uma mensagem de desbloqueio seja publicada no
tópico especial, state. Com o recebimento da mensagem, o módulo
TTS é ativado, chamando logo em seguida, sua função interna que
envia uma requisição para o serviço na nuvem do IBM Watson. Após
a conversão do texto para fala, o módulo TTS salva o arquivo na
pasta TTS Cache Files. O módulo TTS publica uma mensagem no tó-
pico speech (do módulo Audio) contendo o nome do arquivo gerado
e encerra sua execução. A mensagem é recebida pelo módulo Audio,
que chama suas funções internas em sequência para ler o arquivo na
pasta local e tocá-lo. Ao finalizar sua tarefa o módulo Audio envia
um sinal de desbloqueio através da publicação no tópico especial,
state. A mensagem é recebida pelo módulo SP fazendo com que
ele seja desbloqueado, seguindo seu fluxo executando o próximo
comando do script.
4.3
Funcionalidades do robô FRED
O robô FRED [9] (Friendly Robot for EDucation and Healthcare) é
outra plataforma de robótica social open-source. Seu sistema de
controle, assim como o do EVA, implementa suas capacidades de
comunicação verbal e não verbal. O FRED pode falar (TTS), pode
reconhecer a fala do usuário (STT), pode expressar emoções através
do olhar, pode executar animações com os LEDs em seu tórax, pode
tocar arquivos de áudio utilizando uma caixa de som externa, pode
controlar efeitos sensoriais de luz usando uma lâmpada inteligente,
pode classificar a emoção de um texto, pode fazer várias poses e se
mover utilizando suas pernas, e tem os mesmos recursos de visão
computacional do EVA. Sendo o FRED uma proposta de plataforma
robótica social open-source de baixo custo, seus componentes de
hardware são poucos e com poder de processamento restrito. Apesar
disso, o FRED pode ser bastante expressivo através da combinação
dos seus recursos de comunicação verbal e não verbal. Diferente
Figura 8: Diagrama de sequência para a execução do recurso
de fala do robô EVA.
do EVA, que utiliza como base para o seu sistema de software um
Raspberry PI 4, o FRED utiliza apenas uma placa Arduino UNO
(que controla todo o seu hardware interno) e uma placa NodeMCU
(rodando o firmware LUA). Para representar seus olhos e boca, o
robô utiliza três matrizes de LED 8x8. Para o movimento de suas
pernas são utilizados 4 servomotores SG-90. Todo o seu corpo é
impresso em 3D e pode ser customizado. Ele mede cerca de 20cm
de altura e pesa em torno de 200g. Seu custo total, incluindo o
WebMedia’2024, Juiz de Fora, Brazil
Rocha et al.
Figura 9: O robô FRED (a) e sua versão feminina Frida (b).
Figura 10: Arquitetura de hardware e software do FRED.
filamento PLA, fios, conectores e placas, fica em torno de US$ 70,00.
Uma imagem do FRED e de sua versão feminina customizada, Frida,
pode se vista na Figura 9.
4.4
Instanciação do Software do FRED
O sistema de controle do FRED foi implementado utilizando-se o
framework proposto e conta com módulos similares ao do sistema
do EVA. Esses módulos pertencem aos mesmos grupos I, II e III
definidos anteriormente. A Figura 10 apresenta uma visão geral da
arquitetura de hardware e software implementada para o FRED.
Assim como no sistema de controle do EVA, o módulo Script
Player roda em um dispositivo externo ao robô como por exemplo
um laptop. Porém, devido às capacidades de processamento restri-
tas dos componentes de hardware do FRED, outros seis módulos
rodam no mesmo dispositivo externo, são eles: TER, TTS, STT, Light,
CV e Audio. As instanciações desses módulos em nada diferem dos
mesmos módulos usados no software do EVA. Os mecanismos de
sincronização entre os módulos e de transferência de valores de
variáveis são os mesmos do sistema do EVA, utilizando os mesmos
tópicos especiais, state e var, respectivamente. Os tópicos definidos
para a ativação dos módulos são basicamente os mesmos apresen-
tados na Tabela 1, com pequenas variações para funcionalidades
especificas do FRED. Ele não pode mover sua cabeça e nem seus
braços, mas pode se mover e fazer poses utilizando suas pernas.
Portanto, os tópicos do módulo Motion, são: pose e move.
Como pode ser visto na Figura 10, no sistema do FRED, o broker
roda num dispositivo externo (laptop). Os três módulos do FRED
que não rodam no dispositivo externo, são: o Motion, o Display e o
RGB LEDs. Eles são implementados numa placa NodeMCU, rodando
um firmware LUA, permitindo ao FRED se tornar um cliente MQTT.
A placa NodeMCU, além de permitir a conexão com broker, permite
a conexão e o controle da placa Arduino através de uma conexão
com a porta serial.
A instanciação dos três módulos segue o mesmo padrão descrito
para os módulos do Grupo I, na Seção 4.2. Os módulos desse grupo
não executam de maneira síncrona com outros módulos, isto é, não
sinalizam o final das suas tarefas. Além disso, eles não retornam
qualquer tipo de informação em forma de texto. Um vídeo dos
robôs funcionado com o sistema de software desenvolvido com o
framework proposto pode ser visto neste link.
CONCLUSÃO
Este trabalho propôs um framework para desenvolvimento de siste-
mas de controle para plataformas de robótica social. O framework
proposto adotou uma abordagem de especificação orientada a obje-
tos com o intuito de facilitar o entendimento de sua estruturação. O
framework utiliza um paradigma publish-subscribe para troca men-
sagens assíncronas entre os módulos desenvolvidos tendo como
dependência apenas a instalação de um broker MQTT. Foram apre-
sentados os elementos que compõem o framework, suas interfaces e
suas classes concretas e abstratas. A utilização do framework foi va-
lidada através de dois cenários concretos utilizando as plataformas
de robótica social EVA e FRED.
Uma das limitações do framework proposto pode estar na uti-
lização do paradigma de troca de mensagens assíncronas publish-
subscribe pois a comunicação entre os módulos baseada na troca de
mensagens usando MQTT pode introduzir alguma latência, o que
pode ser inadequado para aplicações de tempo real muito rigorosas.
Outro ponto que merece atenção é o uso seguro do MQTT para
evitar que potenciais invasores publiquem mensagens no broker,
atrapalhando o funcionamento dos robôs. A solução segura será
desenvolvida futuramente.
Como trabalhos futuros, pretende-se realizar estudos de desempe-
nho comparativos entre o framework proposto e outros frameworks
populares como ROS, NAOqi, YARP e OROCOS. Pretende-se tam-
bém demonstrar a aplicação prática do framework proposto em
diferentes contextos como educação e saúde.
AGRADECIMENTOS
Os autores agradecem o apoio recebido do Google Research, CA-
PES, CAPES PRINT, CNPq, FINEP, INCT-MACC, ICNT-ICoNIoT e
FAPERJ.
Uma Proposta de Framework para Sistemas de Controle para Plataformas de Robótica Social
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30134.txt ---

--- INÍCIO DO ARQUIVO: 30135.txt ---
Únicos, mas não incomparáveis: abordagens para identificação de
similaridades em respostas emocionais de diferentes indivíduos
ao mesmo estímulo audiovisual
Guilherme O. Aguiar
Juan P. D. Esteves
Instituto Federal de Mato Grosso
Pontes e Lacerda, Brasil
guilherme.aguiar@estudante.ifmt.edu.br
dantas.esteves@estudante.ifmt.edu.br
Cleon X. Pereira Júnior
Thamer H. Nascimento
Instituto Federal Goiano
Iporá, Brasil
cleon.junior@ifgoiano.edu.br
thamer.nascimento@ifgoiano.edu.br
Renan V. Aranha
Universidade Federal de Mato Grosso
Cuiabá, Brasil
renan.aranha@ufmt.br
WebMedia’2024, Juiz de Fora, Brazil
Aguiar, et al.
identificar quais pessoas reagem de maneira semelhante aos mes-
mos estímulos e quais reagem de forma diferente. Além disso, é
fundamental desenvolver métodos para comparar essas reações
emocionais.
Se, por um lado, o processo de identificação de similaridades
não é inédito e está embutido em diversas soluções que envolvem a
utilização de algoritmos de Aprendizado de Máquina, é também fato
que o processo nem sempre é tratado com transparência e enfoque
na explicabilidade. Assim, visando contribuir com este cenário, a
partir de práticas identificadas na literatura, este estudo apresenta
duas abordagens para a comparação de respostas emocionais de
diferentes indivíduos. Ao analisar as reações emocionais de 39 in-
divíduos a um estímulo audiovisual sob as duas abordagens, são
realçadas as particularidades de cada abordagem, bem como dis-
cutidas as contribuições decorrentes de uma análise conjunta, que
integra as duas abordagens propostas.
CONTEXTO TEÓRICO E CONTRIBUIÇÕES
DESTE ESTUDO
Estudos que discutem reações emocionais de indivíduos a estímulos
audiovisuais não são inéditos na literatura e podem ser classificados
quanto ao objetivo da investigação. Uma revisão exploratória da
literatura revela uma maior quantidade de estudos que visam à
detecção de engajamento ou interesse em conteúdo audiovisual
a partir das respostas emocionais dos usuários. Como exemplo dessa
prática, a recém-publicada pesquisa de Oakes, Peschel e Barraclough
[15] investiga a viabilidade de se utilizar dados de expressões faciais,
registrados de forma não invasiva, para avaliar o engajamento do
público em apresentações artísticas. Diferentemente de estudos que
analisam os dados de cada usuário separadamente, esta pesquisa
adota como premissa que a similaridade das respostas emocionais
do público é um indicativo de engajamento com a apresentação. Esta
premissa emerge de estudos anteriores, que observaram reações
similares de usuários enquanto estavam engajados ao ouvir músicas.
Ainda nesse contexto, uma pesquisa conduzida por de Sá et al.
[7] analisou as respostas emocionais de 10 usuários ao trailer de
um filme do gênero horror que, à época do estudo, ainda não havia
sido lançado. O intuito estava em predizer a intenção de assistir ao
filme. Foram observados sinais EEG e, com o uso de eye tracking, a
região do vídeo para a qual cada usuário estava olhando. Embora os
resultados indiquem padrões no reconhecimento das emoções, não
há informações quanto à similaridade de respostas emocionais de
diferentes indivíduos. Em trabalho similar, conduzido por Da Silva
et al. [6] também usando as técnicas de EEG eeye tracking, foram
observadas as reações emocionais de usuários a conteúdos audiovi-
suais, visando a favorecer a avaliação da percepção desses usuários
quanto ao conteúdo assistido.
No âmbito da comparação similaridade em respostas emoci-
onais, [2] descrevem uma pesquisa em que dois grupos de pessoas,
um formado por indivíduos com perfil político liberal e outro grupo
formado por indivíduos com perfil conservador, tiveram suas res-
postas emocionais a estímulos visuais registrados com o uso de ele-
tromiografia facial. Indivíduos de ambos os grupos foram expostos
a um conjunto de imagens. Ao final do processo, os pesquisadores
aplicaram o teste t de Student para identificar se havia diferença
significativa entre os dados dos diferentes grupos. Os resultados,
relatados em [1], revelam que não houve diferença estatisticamente
significativa entre os dois grupos de usuários.
A literatura também revela desafios inerentes à análise emo-
cional a partir de estímulos audiovisuais. Embora o uso de filmes
para elicitar emoções em usuários tenha se mostrado uma aborda-
gem efetiva tanto para emoções positivas quanto negativas [11],
há casos em que os impactos emocionais são sutis. Como exemplo,
análises de [4] revelaram que, ao contrário do esperado, os vídeos
utilizados como estímulo não foram capazes de gerar respostas
emocionais equivalentes. Em alguns casos, os índices identificados
para determinadas expressões faciais associadas a emoções foram
muito baixos. Como possível justificativa, os autores explicaram
que os participantes haviam sido orientados a não olhar para a tela
em momentos desconfortáveis (possivelmente em cumprimento
aos procedimentos éticos). Como consequência, o software utili-
zado para o reconhecimento de expressões faciais pode não ter
identificado as expressões adequadamente nesses instantes.
A análise dos trabalhos supracitados revela o amplo conjunto
de oportunidades de contribuições no âmbito da comparação de
repostas emocionais de diferentes indivíduos aos mesmos estímulos
emocionais, evidenciando a relevância da presente investigação já
a partir da categorização de abordagens para comparação de dados
emocionais, que será apresentada na Seção 3. Em complemento, os
resultados da comparação de reações emocionais de 39 indivíduos,
cujo processo metodológico é descrito na Seção 4, contribuem para
a compreensão das características e particularidades de cada abor-
dagem, bem como para o avanço na identificação de similaridades
em repostas emocionais.
ABORDAGENS PARA A COMPARAÇÃO DE
DADOS EMOCIONAIS
As técnicas de reconhecimento de emoções em tempo real (de ca-
ráter fisiológico ou não) coletam uma grande quantidade de dados
durante determinado período. Como consequência, o tratamento
e a interpretação desse grande conjunto de dados pode ser um
processo complexo, condutível de diferentes formas. A partir de
reflexão crítica em relação às práticas adotadas na literatura para a
interpretação de dados relacionados aos estados emocionais, são
propostas a seguir duas abordagens aplicáveis a esse contexto.
3.1
Análise de experiência emocional
A primeira abordagem, neste estudo denominada de “Análise de
Experiência Emocional” (AEE), considera as emoções elicitadas por
um indivíduo durante um determinado período. Nesta perspectiva, o
enfoque está em compreender quais foram as respostas emocionais
dos usuários aos estímulos, bem como a frequência ou intensidade
em que essas emoções foram detectadas.
A Figura 1 ilustra de forma simplificada como a análise de expe-
riência emocional se caracteriza. Nesta Figura, há a representação
visual de uma linha do tempo, com marcações que indicam di-
ferentes trechos de um vídeo. Abaixo, há duas trilhas, cada uma
representando um usuário diferente. Na trilha de cada usuário, na
região equivalente a cada trecho de vídeo, existe a representação
visual da emoção que o usuário vivenciou durante aquele momento.
A Figura revela que dois indivíduos, denotados pelas alcunhas de
Únicos, mas não incomparáveis: abordagens para identificação de similaridades em respostas emocionais
WebMedia’2024, Juiz de Fora, Brazil
Figura 1: Representação do processo de interpretação dos dados segundo a abordagem AEE, em que são comparadas as reações
emocionais de cada usuário a diferentes trechos de um mesmo estímulo audiovisual.
Figura 2: Representação do processo de interpretação dos dados segundo a abordagem ATE.
"Pessoa A"e "Pessoa B", tiveram expressões faciais associadas a emo-
ções reconhecidas em dois trechos do vídeo. Enquanto a Pessoa A
estava com a expressão facial associada à raiva no primeiro trecho,
a emoção identificada no segundo trecho era a de alegria. O pro-
cesso inverso é observado para a Pessoa B, que inicia o vídeo com
a expressão facial associada à alegria e o conclui com a expressão
facial associada à raiva.
Ao aplicar-se uma análise de experiência emocional, as experi-
ências dos usuários A e B serão consideradas equivalentes, pois, no
processo de análise, as duas emoções foram observadas em igual
intensidade. Nota-se, portanto, que nesta abordagem a ordem de
manifestação das emoções não é relevante, mas sim a frequência ou
intensidade com que se manifestaram. Seguindo esta abordagem,
a conclusão indicaria que o vídeo gerou reações emocionais de
alegria e raiva em ambos os usuários. Do conceito à implementa-
ção, podem ser aplicadas, neste contexto, as seguintes técnicas: i)
cálculo de média das emoções; ii) quantificação da expressão facial
mais intensa em um determinado período; iii) cálculo de distância
euclidiana; e iv) algoritmos de clusterização, como o kNN. Como
exemplo de pesquisas que adotam tal prática, têm-se os estudos de
González-Rodríguez et al. [12] e Boğa et al. [4].
3.2
Análise de trajetória emocional
Diferentemente da abordagem anterior, que desconsidera a tem-
poralidade no comportamento emocional, a análise de trajetória
emocional dedica-se a observar se, ao longo de um determinado
período, a intensidade com que uma emoção foi identificada cres-
ceu ou decresceu ao longo do tempo. Nessa abordagem, valoriza-se
o impacto dos acontecimentos do vídeo no estado emocional dos
usuários. Novamente recorrendo à Figura 1 para a exemplificação
dessa abordagem, pode-se entender que, para a Pessoa A, o primeiro
trecho do vídeo elicitava a emoção de raiva, enquanto o segundo
trecho elicitava a emoção de alegria. Em contraponto, uma situação
inversa pode ser observada para a Pessoa B. Nota-se, neste ponto,
WebMedia’2024, Juiz de Fora, Brazil
Aguiar, et al.
uma questão importante: o mesmo conjunto de dados pode gerar di-
ferentes interpretações a partir da abordagem de análise. Enquanto
a abordagem anterior possibilitava concluir que ambos os usuários
tiveram experiências equivalentes, a atual abordagem realça que
estes usuários tiveram experiências completamente distintas, como
evidência a Figura 2.
Em termos práticos, a análise de trajetória emocional pode ser
observada a partir da correlação. Neste contexto, um desafio está na
definição da janela de tempo em que será analisada a similaridade
entre dois usuários. Em [15], por exemplo, optou-se por analisar a
correlação dos estados emocionais de diferentes usuários em uma
janela de tempo de um minuto.
MATERIAIS E MÉTODOS
Visando a comparar e validar abordagens para a identificação de
similaridades em respostas emocionais de diferentes usuários, serão
aplicadas as abordagens AEE e ATE, apresentadas na seção anterior.
Para nortear o percurso metodológico deste estudo, foram definidas
as questões de pesquisa apresentadas a seguir:
• QP1: Há experiências emocionais similares de usuários que
assistiram aos mesmos estímulos audiovisuais?
• QP2: Para um mesmo estímulo emocional, há usuários com
trajetórias emocionais similares?
• QP3: Há casos de usuários que compartilham tanto experi-
ências quanto trajetórias emocionais?
Para responder à QP1, serão analisadas as similaridades de expe-
riências emocionais de diferentes usuários aos mesmos estímulos
audiovisuais, conforme abordagem AEE, descrita na Seção 3.1. Para
a redução da amostragem, será gerado um vetor de características
para cada indivíduo (𝑉𝑖), contendo a média da intensidade de cada
expressão facial associada à emoção. Foi estabelecida a seguinte
ordem: neutra (𝜇1), alegria (𝜇2), tristeza (𝜇3), raiva (𝜇4), medo (𝜇5),
nojo (𝜇6) e surpresa (𝜇7).
𝑉𝑖= [𝜇1, ..., 𝜇𝑛]
(1)
Em seguida, os dados serão submetidos ao algoritmo kNN (k-
Nearest Neighbors), com parâmetro 𝑛= 39, para possibilitar a com-
paração de todos os pares de indivíduos. Assim, para cada par, será
calculada a distância entre esses indivíduos. Para responder à QP2,
que trata da trajetória emocional (abordagem ATE, descrita na Se-
ção 3.2), será calculada a correlação entre as trajetórias emocionais
dos usuários. Considerando a distribuição dos dados, o método de
cálculo de correlação utilizado será a correlação de Spearman. Uma
vez que a correlação compara a variação entre duas variáveis, será
calculada a correlação considerando a expressão facial neutra. Para
a discussão, serão consideradas apenas correlações estatisticamente
significativas (𝑝< 0.05) e que possuam correlação, no mínimo,
moderada (𝜌𝑠>= 0.5). Finalmente, para a discussão da QP3, o
índice de correlação (𝜌𝑠) e o índice de similaridade do kNN serão
analisados em conjunto para os pares de usuários.
4.1
Seleção do conjunto de dados
Embora existam diversos conjuntos de dados de expressões faciais
associadas a emoções, conjuntos de dados que contenham vídeos
ou fotografias de pessoas reagindo ao mesmo conjunto de estímulos
audiovisuais, com sincronização temporal, são escassos. Após uma
pesquisa exploratória, foram identificados dois conjuntos de dados
que continham tal característica: o AM-FED, mantido pela Affectiva,
e o Emognition Wearable Dataset 2020 [18]. Ambos os conjuntos
de dados são gratuitos para uso em pesquisas científicas, sendo
este outro critério de seleção. Com relação aos aspectos éticos, a
Resolução CNS 674/2022 habilita o desenvolvimento de pesquisas
que envolvam conjuntos de dados já coletados. Nesse sentido, as
pessoas autoras submeteram solicitação de acesso aos produtores
do conjunto de dados Emognition e também para o AM-FED. Até o
momento de elaboração deste trabalho, houve apenas resposta por
parte dos produtores do dataset Emognition.
Emognition. O conjunto de dados Emognition reúne dados de 39
1 participantes (18 do sexo masculino e 21 do feminino, com idade
de 21 ± 2 anos ). Cada voluntário assistiu a dez vídeos. Deles, um
foi considerado neutro, enquanto os outros nove evocavam uma
emoção específica. O protocolo utilizado para a coleta de dados
envolveu os seguintes procedimentos: ao início da coleta, durante
cinco minutos, os voluntários assistiram a um vídeo contendo linhas
e pontos dispostos em uma tela preta. Em seguida, responderam a
um questionário de autoavaliação. Então, para cada um dos dez ví-
deos utilizados como estímulo, adotou-se a realização das seguintes
etapas: i) 2 minutos de vídeo contendo linhas e pontos dispostos
em uma tela preta; ii) vídeo com a intencionalidade de se evocar
uma emoção específica (com duração 1 a 2 minutos); e iii) respon-
der ao questionário de autoavaliação. Além dos vídeos gravados
com os voluntários, o conjunto de dados é composto por dados de
reconhecimento de expressões faciais, analisadas com o uso do soft-
ware Quantum Sense, que reconhece expressões faciais associadas
às seguintes emoções: raiva, nojo, alegria, tristeza e surpresa, além
da face neutra. Como os registros disponibilizados no dataset não
estavam associados ao frame do vídeo, visando à compatibilidade
das análises, os vídeos foram processados novamente pelas pessoas
autoras deste trabalho. No âmbito deste estudo, por limitação de
escopo, foram consideradas as reações emocionais de usuários ao
estímulo associado à emoção de nojo.
4.2
Processamento dos dados
Os vídeos que integram o conjunto de dados Emognition estavam
organizados com a identificação do usuário e do estímulo utilizado
naquela coleta. Ao início da fase de processamento, para cada ar-
quivo de vídeo, foram extraídas imagens estáticas, uma para cada
segundo de duração do vídeo. Assim, um vídeo com duração de 120
segundos resultou na geração de 120 imagens estáticas. Tal processo
foi realizado visando-se a reduzir a dimensionalidade dos dados.
As imagens foram, portanto, submetidas ao processamento pela bi-
blioteca Face-api.js, cuja eficácia no reconhecimento de expressões
faciais associadas a emoções foi avaliada em estudo anterior. Para
cada imagem, a biblioteca retorna um vetor com a intensidade de
cada uma das sete expressões faciais: neutra, alegria, medo, nojo,
raiva, tristeza e surpresa. É importante mencionar que, diferente-
mente de algumas soluções que analisam a ocorrência de expressões
faciais associadas a emoções de forma independente, a Face-api.js
considera a ocorrência excludente. Se em determinada imagem for
detectada a expressão facial neutra com intensidade de 80%, os
1O número de 39 participantes considera apenas indivíduos para os quais há registros
de imagens disponíveis.
Únicos, mas não incomparáveis: abordagens para identificação de similaridades em respostas emocionais
WebMedia’2024, Juiz de Fora, Brazil
índices relacionados às demais expressões faciais devem somar 20%.
Portanto, a expressão facial neutra é um indicativo de evocação
emocional em um determinado instante. Para cada usuário, em cada
estímulo, foi gerado um arquivo no formato CSV (comma-separated
values) contendo o vetor de intensidades emocionais em determi-
nada imagem. Também foi anotado o número da imagem, visando
a favorecer a comparação das respostas emocionais de diferentes
usuários.
RESULTADOS
5.1
Detecção de experiências emocionais
similares
Para identificar similaridades em experiências emocionais de di-
ferentes indivíduos (QP1), foi calculada, para cada participante, a
média de intensidade de cada expressão facial. Obteve-se, portanto,
um vetor de sete características para cada indivíduo. A Figura 3 apre-
senta, nesse contexto, um gráfico de dispersão dos dados emocionais
desses indivíduos2. Para a geração da figura, dada a necessidade de
redução de dimensionalidade, de sete para duas, foi aplicado um
algoritmo PCA (Principal Component Analysis). A Figura 3 eviden-
cia, a partir da proximidade dos pontos, que há uma quantidade
considerável de indivíduos que apresentaram experiências emoci-
onais similares. Em contraponto, há indivíduos com experiências
emocionais distintas.
-0.5
-0.25
0.25
0.5
0.75
-0.6
-0.4
-0.2
0.2
0.4
0.6
Figura 3: Dispersão dos indivíduos quanto às expressões faci-
ais coletadas durante o estímulo audiovisual.
Enriquecendo o processo de interpretação dos dados, foram sele-
cionados de forma arbitrária, a partir da dispersão apresentada na
Figura 3, dois pares de indivíduos para uma análise mais detalhada:
2Visando à reprodutibilidade, os indivíduos estão identificados com o código utilizado
pelos autores do conjunto de dados Emognition.
Indivíduo
𝜇1
𝜇2
𝜇3
𝜇4
𝜇5
𝜇6
𝜇7
0.89
0.09
0.02
0.00
0.00
0.00
0.00
0.92
0.08
0.00
0.00
0.00
0.00
0.00
0.85
0.00
0.07
0.00
0.01
0.00
0.06
0.07
0.82
0.10
0.00
0.00
0.01
0.00
Tabela 1: Comparação dos vetores de características de duas
duplas de indivíduos.
um par graficamente próximo (indivíduos 36 e 64) e outro par grafi-
camente distante (indivíduos 28 e 63). Os dados, apresentados na
Tabela 1, evidenciam a similaridade dos vetores de características
dos indivíduos 36 e 64, assim como a diferença entre os vetores de
características dos indivíduos 28 e 63. Enquanto o primeiro par teve
alta incidência de expressão facial neutra, com a ocorrência também
da expressão facial associada à alegria em intensidade similar, o
segundo par apresentou experiências distintas. O indivíduo 28 teve
maior incidência de expressão facial neutra, enquanto o indivíduo
63 teve maior incidência de expressões faciais associadas à emoção
de alegria.
Em seguida, também aplicou-se o algoritmo kNN, que calcula
a distância entre indivíduos que compõem um par. Quanto mais
próximo de zero é o índice, maior é a similaridade identificada entre
dois usuários. Em decorrência do elevado número de combinações
(1482, tendo 𝑛= 39), tem-se na Tabela 2 a apresentação dos cinco pa-
res com maior similaridade, seguidos pelos cinco pares com menor
similaridade.
Indivíduo
Vizinho
Distância
0.00
0.00
0.01
0.01
0.01
1.24
1.24
1.24
1.23
1.23
Tabela 2: Resultados provenientes do algoritmo kNN.
Potencializando a análise e favorecendo uma compreensão de
todo o cenário, a Figura 4 apresenta um gráfico de distribuição dos
índices de distância calculados para cada par de usuários com o algo-
ritmo KNN. Como pode-se observar, há uma expressiva quantidade
de ocorrências com alta similaridade entre os indivíduos.
5.2
Identificação de trajetórias emocionais
similares
Considerando a QP2, que trata da identificação de similaridades de
trajetórias emocionais de diferentes indivíduos aos mesmos estímu-
los audiovisuais, calculou-se, para cada par de usuários, a correlação
WebMedia’2024, Juiz de Fora, Brazil
Aguiar, et al.
Frequência
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figura 4: Distribuição do índice de similaridade calculado
pelo algoritmo KNN.
de Spearman. A operação foi realizada para os índices relacionados
à expressão facial neutra, considerando os motivos expostos anteri-
ormente na Seção 4. Os resultados, descritos parcialmente na Tabela
3, revelam que foram encontradas 210 correlações estatisticamente
significativas (𝑝< 0.05). Destas, 67 correlações são negativas e 143
positivas. Para auxiliar o processo de compreensão dos dados e de
validação dessa abordagem, optou-se pela utilização de recursos
gráficos.
Indivíduo 1
Indivíduo 2
𝜌𝑠
𝑝
0.97
0.00
0.97
0.00
0.96
0.00
0.84
0.0
0.82
0.00
0.24
0.04
0.24
0.04
0.24
0.04
0.24
0.04
0.25
0.03
Tabela 3: Índices de correlação entre pares de usuários.
As Figuras 5, 6 e 7 apresentam gráficos de linhas que compa-
ram, longitudinalmente, a variação da intensidade da expressão
facial neutra de dois usuários. No caso da Figura 5, tem-se uma
comparação entre os usuários identificados pelos números 38 e 48.
Embora a intensidade da expressão facial neutra de ambos os usuá-
rios tenha apresentado variação ao longo do tempo, há correlação
positiva moderada e significativa (𝜌𝑠= 0.54, 𝑝= 0.0), evidenciada
pela similaridade no comportamento longitudinal das linhas. Ao
analisar-se a Figura 6, que ilustra a comparação para os usuários
43 e 47 no mesmo intervalo de tempo contemplado pela Figura 5,
evidencia-se inicialmente que o comportamento facial do segundo
par de indivíduos apresentou pouca variação quando comparado ao
par anterior. Estes usuários não tiveram ativação emocional notável
em suas expressões faciais em considerável parte do vídeo, mas
apresentaram forte correlação (𝜌𝑠= 0.97, 𝑝= 0.0).
Ademais, embora a questão de pesquisa que fomenta essa dis-
cussão trate da identificação de trajetórias similares, a abordagem
também é capaz de revelar trajetórias emocionais opostas, a partir
da análise de correlações negativas. A Figura 7 apresenta uma si-
tuação em que houve moderada correlação negativa (𝜌𝑠= −0.65,
𝑝= 0.0) entre os usuários 43 e 63.
5.3
Comparação entre as abordagens
Em vistas a discutir a QP3, que trata da ocorrência de experiências
e trajetórias emocionais similares, foi realizado o confronto dos
resultados do algoritmo kNN com a correlação de Spearman. A
Tabela 4 apresenta os dez primeiros registros após a ordenação por
correlação e similaridade.
Indivíduo
Vizinho
𝜌𝑠
𝑝
kNN
0.97
0.0
0.01
0.97
0.0
0.03
0.96
0.0
0.01
0.84
0.0
0.06
0.82
0.0
0.07
-0.76
0.00
8.07
-0.72
0.00
8.04
-0.65
0.00
7.99
-0.61
0.00
7.01
-0.57
0.00
6.10
Tabela 4: Índices de correlação e distância entre pares de
indivíduos, contemplando as abordagens AEE e ATE.
Para exemplificação dos resultados, serão discutidos de forma
detalhada dois pares de indivíduos: 43-50 e 47-63. O primeiro par,
formado pelos indivíduos identificados pelos números 43 e 50, é
caracterizado por grande similaridade nas duas abordagens de iden-
tificação de similaridade emocional. Uma consulta aos dados de
expressões faciais revela que, para ambos os indivíduos, o estímulo
audiovisual não provocou considerável manifestação de expressões
faciais associadas a emoções detectadas com o uso do software,
como revela a Tabela 5. A análise é corroborada pela Figura 8, que
evidencia comportamento fortemente similar.
Indivíduo
𝜇1
𝜇2
𝜇3
𝜇4
𝜇5
𝜇6
𝜇7
0.99
0.01
0.98
0.02
0.97
0.03
0.07
0.82
0.1
0.01
Tabela 5: Comparação dos vetores de características de duas
duplas de indivíduos a partir da análise conjunta de AEE e
ATE.
Em contraponto, a comparação entre as duas abordagens também
favorece a identificação de pares de indivíduos em que há grandes
distinções, tanto na abordagem AEE quanto na abordagem ATE,
caso do par formado pelos indivíduos 47 e 63. Como indicado na
Tabela 5, nota-se que enquanto para o indivíduo 47 predominou-se
a expressão facial neutra, para o indivíduo 63 observou-se maior
Únicos, mas não incomparáveis: abordagens para identificação de similaridades em respostas emocionais
WebMedia’2024, Juiz de Fora, Brazil
Intensidade
Figura 5: Comparação gráfica da trajetória emocional dos indivíduos 38 e 48, com correlação moderada (𝜌𝑠= 0.54, 𝑝= 0.0)
Intensidade
Figura 6: Comparação gráfica da trajetória emocional dos indivíduos 43 e 47, com correlação forte (𝜌𝑠= 0.97, 𝑝= 0.0)
Intensidade
Figura 7: Comparação gráfica da trajetória emocional dos indivíduos 47 e 63, com correlação moderada e inversa (𝜌𝑠= −0.65,
𝑝= 0.0)
incidência da expressão facial associada à alegria. Similarmente, a
Figura 7 evidencia a comparação da ATE para esses dois indivíduos,
revelando a distinção entre suas reações ao longo do vídeo, como
já discutido anteriormente.
DISCUSSÕES
6.1
Contribuições
Os resultados apresentados na Seção 5 permitem concluir que as
abordagens AEE e ATE são, de fato, eficazes para a comparação
de similaridade de respostas emocionais evocadas por diferentes
indivíduos a partir de um mesmo estímulo audiovisual, tanto em
relação às experiências emocionais quanto em relação às trajetórias.
Os resultados também validam as particularidades das abordagens,
evidenciando que elas oferecem diferentes perspectivas de compre-
ensão das respostas emocionais dos indivíduos e, portanto, devem
ser escolhidas sob um cauteloso processo de análise dos pesquisa-
dores quanto ao objetivo do estudo. Enquanto a AEE oferece uma
perspectiva quanto às emoções vivenciadas, com suas respectivas
intensidades, a ATE evidencia as alterações emocionais ocorridas ao
longo do estímulo. Tais abordagens podem, inclusive, ser combina-
das para a identificação de total similaridade entre dois indivíduos.
Podem, também, revelar comportamentos complemente opostos,
denotados por um índice representativo de grande distância (AEE)
WebMedia’2024, Juiz de Fora, Brazil
Aguiar, et al.
Intensidade
Figura 8: Comparação gráfica da trajetória emocional dos indivíduos 43 e 50, com correlação forte e positiva. (𝜌𝑠= 0.97, 𝑝= 0.0)
e correlação inversa (ATE). A classificação de práticas relatadas na
literatura em duas abordagens, seguida pela sua validação, oferece
não apenas novas perspectivas para a análise emocional no âmbito
de sistemas interativos, mas também abre caminho para a prospec-
ção de um conjunto de aplicações práticas. Nesse sentido, pode-se
mencionar desde a avaliação de conteúdo em plataformas de strea-
ming até adaptação de materiais de ensino às respostas emocionais
dos alunos.
6.2
Desafios e oportunidades
O desenvolvimento de uma investigação que visa a comparar repos-
tas emocionais de diferentes indivíduos é desafiadora sob diversos
aspectos. Dentre eles, está na coleta sincronizável das respostas
emocionais. Uma alternativa nesse cenário envolve a construção
de datasets em um ambiente controlado, como ocorreu na base de
dados utilizada neste estudo. Todavia, a utilização de vídeos para
estimular emoções nos voluntários nem sempre é satisfatória, como
realçado na literatura e observado ao longo dos resultados. Como
discutido na seção de resultados, foram identificados vários casos
em que os voluntários permaneceram com a expressão facial neutra
durante o tempo em que assistiam ao vídeo utilizado como estímulo.
Apesar dessa limitação, destaca-se a efetividade das abordagens
avaliadas neste estudo, que conseguiram identificar indivíduos que
tiveram tal comportamento similar.
Ademais, no âmbito deste estudo, foram considerados estímulos
audiovisuais, caracterizados pelo mesmo tempo de duração. Em
aplicações interativas, como jogos eletrônicos, o processo de inte-
ração é marcado pelo protagonismo do usuário, que tem o poder
de decisão quanto aos eventos e ações da aplicação. Assim, pode
não haver sincronicidade nos estímulos apresentados. Neste cená-
rio, a comparação de similaridades de respostas emocionais dos
usuários tende a ser ainda mais desafiadora, já que o percurso do
usuário na aplicação pode não ser equiparável a outro. Como alter-
nativa, pode-se optar pela comparação de respostas emocionais em
intervalos de tempo menores, delimitados pelo acontecimento de
determinado evento, como a exibição de um diálogo ou a conclusão
de um objetivo. Técnicas como Multi-Dimensional Dynamic Time
Warping (MDDTW), por exemplo, podem ser aplicadas para essa
comparação.
Finalmente, visando a reprodutibilidade da análise, os dados pro-
cessados durante o desenvolvimento deste estudo estão disponíveis
para acesso público a partir da seguinte URL https://shorturl.at/
mJV8y.
CONSIDERAÇÕES FINAIS
Compreender as emoções humanas e o modo como diferentes in-
divíduos reagem aos mesmos estímulos é um desafio altamente
complexo, mesmo diante de avanços em áreas como neurologia,
psicologia e computação. Ao mesmo tempo que a compreensão dos
estados afetivos humanos é um campo com oportunidades a serem
exploradas, há também oportunidades para enriquecer a experiên-
cia de uso de sistemas computacionais a partir da adaptação desses
sistemas considerando as emoções vivenciadas pelos usuários du-
rante o uso de determinada aplicação. Este processo, entretanto,
envolve o agrupamento de usuários com características similares,
que recebem tratamento equivalente.
Comparando os resultados provenientes de duas abordagens
para a identificação de similaridades em respostas emocionais, este
estudo provoca o estado da arte ao discutir as particularidades de
cada abordagem, bem como ao sugerir uma profunda reflexão crítica
quanto à abordagem de comparação de respostas emocionais em
estudos futuros.
É evidente, portanto, que os desafios relacionados a esse tema
não são esgotados no presente trabalho. Deste modo, investigações
futuras devem expandir o escopo de análise, contemplando respos-
tas emocionais de indivíduos a um amplo conjunto de estímulos
audiovisuais. Há, ainda, possibilidades quanto à expansão da aná-
lise com a inclusão de fatores demográficos, como idade, gênero e
nacionalidade, além de características individuais, como traços de
personalidade. Espera-se que a condução de novas investigações
nesse sentido enriqueça o arcabouço conceitual da área de Compu-
tação Afetiva, favorecendo o desenvolvimento de aplicações que
disponham de adaptações afetivas com efetividade.
AGRADECIMENTOS
Os autores expressam gratidão ao Freepik pelas imagens utilizadas
na criação de algumas figuras deste artigo.

--- FIM DO ARQUIVO: 30135.txt ---

--- INÍCIO DO ARQUIVO: 30139.txt ---
Caracterizando Polarização em Redes Sociais: Um Estudo de Caso
das Discussões no Reddit sobre as Eleições Brasileiras de 2018 e
Gustavo F. Cunha
Universidade Federal de Minas Gerais (UFMG)
Belo Horizonte, Minas Gerais, Brasil
gustavocunha@dcc.ufmg.br
Ana Paula Couto da Silva
Universidade Federal de Minas Gerais (UFMG)
Belo Horizonte, Minas Gerais, Brasil
ana.coutosilva@dcc.ufmg.br
WebMedia’2024, Juiz de Fora, Brazil
Gustavo F. Cunha and Ana Paula Couto da Silva
ao conteúdo compartilhado e verificaram que tópicos mais polariza-
dos possuem uma variância maior nos sentimentos dos conteúdos
quando comparados com tópicos de menor polarização.
Em [6], os autores apresentaram um modelo quantitativo baseado
no comportamento do usuário para avaliar o nível de polarização
em diferentes tópicos de discussões no Reddit em língua inglesa.
Cada discussão foi modelada como uma árvore de discussão de dois
lados, na qual a raiz é um comentário inicial de uma discussão e os
demais vértices são os comentários desta discussão. As arestas da
árvore foram modeladas com base na cadeia de respostas do debate
e cada comentário é submetido a uma análise de sentimentos que
retorna um valor no intervalo [−4, 4]. A discussão é dividida em dois
lados de posicionamento, gerando uma árvore bipartida. Por fim,
foi proposta uma medida de polarização que passa por avaliações
empíricas, cujos resultados indicaram que o modelo é capaz de
capturar os diferentes níveis de polarização em diferentes assuntos.
Em direção complementar, o trabalho apresentado em [10] utili-
zou um conjunto de modelos sintéticos de grafos para representar
conexões em mídias sociais. Nos grafos gerados, cada usuário (vér-
tice) possui uma opinião em relação a um tópico em discussão. Esta
opinião varia no intervalo [−1, 1], sendo −1 um posicionamento
extremamente contrário e 1 um posicionamento a favor do debate
estabelecido entre os vizinhos no grafo (que representam os re-
lacionamentos na rede social). Os usuários são divididos em dois
grupos: (i) os que possuem posicionamento negativo e (ii) os que
possuem posicionamento positivo. A partir da definição da métrica
balance, que é a razão da cardinalidade destes grupos, os autores
caracterizam diferentes cenários onde a discussão entre os usuários
é mais aberta a ser realizada por usuários de opiniões diversas e os
casos onde os usuários se organizam de forma polarizada.
Considerando o contexto brasileiro, em [18] foi examinado o
papel do hiperpartidarismo e da polarização no Twitter durante as
eleições presidenciais de 2018, mostrando que há uma forte conexão
entre polarização, hiperpartidarismo e desinformação. Em [7], os
autores quantificaram a polarização política do público no contexto
da pandemia de COVID-19. Já em [11], os pesquisadores explora-
ram, com dados do Instagram, se fatores psicológicos poderiam
contribuir para o apelo de uma figura polarizadora.
Os resultados apresentados neste trabalho se diferenciam dos
demais nos aspectos a seguir. Primeiramente, combinamos e adapta-
mos os modelos de interação apresentados em [6, 10, 12]. Resumida-
mente, para a detecção de posicionamento, substituímos a análise de
sentimentos, que segundo alguns trabalhos não é a melhor técnica
a ser utilizada [4], pelo uso de Large Language Models (LLMs), mais
precisamente o ChatGPT5, que se mostrou adequado para a tarefa
no contexto da língua inglesa [24, 25]. Para língua portuguesa, o
trabalho em [20], utilizou-se desta mesma LLM para uma tarefa
diferente. Uma vez que a maioria dos trabalhos no contexto brasi-
leiro utilizam dados do Instagram e Twitter (X), com enfoque nas
eleições presidenciais de 2018, nossa contribuição é analisar as inte-
rações no Reddit, comparando os cenários de possível polarização
nas eleições de 2018 e 2022.
METODOLOGIA
A seguir descrevemos a metodologia utilizada neste trabalho.
5https://platform.openai.com/
Tabela 1: Estatísticas dos subreddits selecionados.
Subreddit
#Postagens
#Comentários
#Usuários
r/brasil
5.895
139.651
16.273
r/BrasildoB
3.357
1.207
r/brasilivre
3.855
61.288
6.346
Total
10.578
204.296
23.826
3.1
Conjunto de Dados
O Reddit é uma mídia social on-line multilíngue, fundada em 2005,
organizada em subcomunidades por áreas de interesse (subreddits).
Nesta rede, os usuários discutem diferentes assuntos, através de
interações do tipo postagem-comentários, chamadas de threads.
Nosso conjunto de dados consiste em atividades de usuários (posta-
gens e comentários) que ocorreram entre os meses de setembro e
novembro de 2018 e 2022, período que engloba pré e pós-eleições.
A seleção dos subreddits mais relevantes, em total de publicações
para o contexto analisado, foi realizada utilizando a pesquisa por re-
levância na API Python do Reddit6. A partir dos subreddits de maior
relevância, foram coletadas 95.933 postagens datadas de setembro
a novembro de 2018 e 2022 . Um subconjunto destas postagens
foi selecionado, com a presença pelo menos de uma das palavras-
chave relacionadas ao contexto das eleições, tendo em vista os
dois candidatos mais bem votados, tais como eleições, política, Lula,
Bolsonaro, Haddad, esquerda, direita, PT, PSL, PL, resultando em
20.515 postagens. Por fim, retiramos também as postagens com
conteúdo deletado/removido, escritas em outra língua e/ou consti-
tuídas somente por links. Finalmente, os comentários realizados nas
10.578 postagens restantes foram coletados. A Tabela 1 apresenta
a principais características dos subreddits selecionados.
3.2
Modelo Matemático
As discussões realizadas pelos usuários foram modeladas através
de árvores, adaptando a proposta de [6]. Cada postagem inicial,
denominada 𝑝𝑗, com 0 ≤𝑗≤𝑃é considerada um contexto de
discussão e pode receber 𝑅comentários como resposta.
Cada resposta (comentário) 𝑟𝑖, com 0 ≤𝑖≤𝑅, associada à uma
postagem 𝑝𝑗, gera uma árvore 𝑇= (𝑉, 𝐸)7, onde 𝑉é o conjunto
da cadeia de comentários que sucedem 𝑟𝑖(com 𝑟𝑖inclusive) e 𝐸
é o conjunto de arestas direcionadas (𝑣𝑘, 𝑣𝑙) que conectam um
comentário 𝑣𝑘(origem) à sua resposta 𝑣𝑙(destino). A cada uma das
arestas direcionadas, associamos um peso 𝜎𝑘𝑙∈[−1, 1], calculado a
partir da técnica de detecção de posicionamento descrita na Seção
3.3. Resumidamente, se 𝑣𝑙é o comentário resposta à 𝑣𝑘, 𝜎𝑘𝑙assume
valores negativos se 𝑣𝑙possui um posicionamento contra ao de 𝑣𝑘e
valores positivos, se o posicionamento é a favor ao de 𝑣𝑘. Os casos
com 𝜎𝑘𝑙= 0 são os de posicionamento neutro.
Com os pesos definidos, o processo final é a bipartição de cada
árvore em dois subgrupos distintos de posicionamento 𝐴e 𝐵, con-
forme os demais trabalhos da literatura, analisando a polarização
em torno de dois polos de pensamento. A raiz 𝑟𝑖é atribuída a um
dos lados, que convencionamos, sem perda de generalidade, ser o
lado 𝐴. Para os vértices 𝑣𝑘de nível 1, e para 𝜎𝑖,𝑘> 0, 𝑣𝑘é assinalado
ao lado 𝐴. Para 𝜎𝑖,𝑘< 0, 𝑣𝑘é adicionado ao lado 𝐵. Para o caso
6https://praw.readthedocs.io/
7Para facilitar a leitura da notação, iremos desconsiderar o índice que identifica cada
árvore associada a cada resposta.
Caracterizando Polarização em Redes Sociais: Um Estudo de Caso das Discussões no Reddit sobre as Eleições Brasileiras de 2018 e 2022
WebMedia’2024, Juiz de Fora, Brazil
de 𝜎𝑖,𝑘= 0, seguindo trabalhos da literatura [3, 6], 𝑣𝑘é atribuído
também ao lado 𝐵. Este processo se repete a cada novo nível da
árvore, considerando pares pai-filho de comentários.
3.3
Análise da Polarização
Um dos pontos mais importantes para a análise da polarização em
discussões on-line é definir o posicionamento de um usuário com
base na sua resposta a um comentário. Ou seja, o objetivo é medir
o quanto uma resposta concorda ou discorda de um comentário. A
partir dos posicionamentos de todas as respostas, construímos uma
árvore bipartida, com dois subgrupos distintos de posicionamento.
Conforme apresentado em [4], o uso de técnicas de análise de
sentimentos não é a melhor escolha para a detecção de posiciona-
mento.8 Assim, em consonância com os trabalhos [24, 25], aplica-
mos o modelo GPT-4 [2] para realizar esta tarefa, a partir do uso
da API da Open AI. 9 O score (valor) da resposta que representa o
seu posicionamento em relação ao comentário inicial varia entre
[−1, 1], com −1 para os casos extremamente discordantes e 1 para
os casos extremamente concordantes. Este valor é denominado
Stance Score.10 A Figura 1 apresenta a árvore bipartida a partir da
definição dos posicionamentos. As arestas são coloridas em três co-
res: vermelho, amarelo e verde, que representam, respectivamente,
peso negativo, peso igual à zero e peso positivo. Já a coloração dos
vértices representa os dois lados da discussão, de acordo com a
bipartição discutida na Seção 3.2 Após a definição do posiciona-
mento para um pequeno conjunto inicial de árvores, realizamos
uma validação manual dos resultados, que mostraram-se adequados
para a execução da tarefa proposta.
Levando em conta que o modelo GPT-4 requer o pagamento de
requisições à sua API e que este trabalho é um estudo inicial da
metodologia proposta, realizamos uma amostragem dos dados a
serem anotados pela ferramenta, mantendo a representatividade de
cada comunidade neste subconjunto. A Tabela 2 apresenta o total
de dados analisados nesta fase.
Com a definição das árvores bipartidas de discussão, o próximo
passo é verificar a existência e medir intensidade da polarização
nas discussões. Para tal, calculamos as seguintes métricas:
• Desvio-Padrão e Média do Stance Score. Seguindo [12],
discursos polarizados possuem maior variância de Stance
Score das arestas da árvore de discussão. No nosso estudo,
adaptamos o ponto de corte de desvio-padrão proposto em
[12]. Assim, árvores com indícios de polarização são as que
possuem desvio-padrão maior que 0.4375.
• Balance Score e Balance Side. Estas métricas avaliam o
quão balanceados estão os dois lados (bipartições) da dis-
cussão. A primeira, proposta em [10], calcula a razão entre
o mínimo e o máximo do total dos Stance Scores positivos
e negativos, ou seja, os pesos das arestas positivas e nega-
tivas das árvores. Na segunda, proposta neste trabalho, a
cardinalidade dos conjuntos gerados pela bipartição é o total
8Realizamos um teste do uso da análise de sentimentos, a partir da biblioteca VADER[5].
No entanto, após uma análise manual, comentários claramente discordantes eram
colocados em um mesmo lado da discussão, por serem negativos. O mesmo ocorriam
para casos para sentimentos positivos.
9https://platform.openai.com/
10Optamos por utilizar os nomes das métricas em inglês, para seguir os trabalhos da
literatura.
Figura 1: Exemplo de uma árvore bipartida de discussão.
de comentários em cada lado da discussão. As medidas têm
intervalo de variação [0, 1], sendo 0 indicativo da predomi-
nância total de apenas um lado da discussão, indicando pouca
ou nenhuma polarização e 1 evidenciando uma discussão
polarizada. A Figura 1, por exemplo, apresenta uma árvore
de discussão em que o Balance Score é igual a 0.5 (6 arestas
vermelhas e 3 verdes) e o Balance Side é igual a 0.66 (60%
azul claro e 40% azul escuro).
• Controversy Level. Esta métrica, proposta neste trabalho,
é calculada a partir do Desvio-Padrão do Stance Score das
arestas de saída de cada comentário. A média é calculada
considerando todos os comentários da árvore em análise.
Caso o comentário tenha uma resposta, o Controversy Level
é igual ao módulo do Stance Score desta resposta. No caso
de uma folha, o Stance Score é o módulo referente à resposta
dada ao comentário de nível superior. A métrica varia no
intervalo [0, 1], onde valores maiores apontam discussões
controversas, com a presença de polarização.
ESTUDO DE CASO: DEBATE NO REDDIT
A seguir apresentamos os resultados preliminares da aplicação
da metodologia proposta para quantificação da polarização nas
discussões realizadas no Reddit durante as eleições brasileiras de
2018 e 2022.
A primeira análise foi um estudo de correlação entre as métricas
utilizadas para verificar e quantificar a intensidade de polarização.
Devido a restrição de espaço, descrevemos brevemente os resultados
obtidos, omitindo as matrizes de correlação calculadas. As métricas
Balance Score e Desvio-Padrão do Stance Score apresentam uma corre-
lação positiva elevada nos dois períodos, acima de 0.7, corroborando
[12], que afirma que tópicos controversos possuem variância mais
elevada com relação a tópicos não controversos. Adicionalmente,
destaca-se a correlação altamente negativa entre o Balance Side e o
valor médio do Stance Score, evidenciando que, quando a discussão
atinge teor de maior discordância (Stance Score negativo), os lados
opostos no debate tendem a estar mais balanceados, indicando a
presença de polarização na discussão.
Na segunda análise, calculamos a distribuição de probabilidades
das diferentes métricas consideradas. Devido a restrição de espaço,
descrevemos brevemente os resultados obtidos, omitindo os gráficos.
A média do Stance Score possui maiores densidades de valores mais
próximos a -1 em 2022 do que em 2018. Adicionalmente, a média do
Controversy level possui distribuição de maior densidade em valores
mais elevados do que os valores encontrados em 2018. A maior
densidade de árvores com Balance Side no intervalo [0.4, 0.6] em
WebMedia’2024, Juiz de Fora, Brazil
Gustavo F. Cunha and Ana Paula Couto da Silva
Tabela 2: Total de submissões, comentários e árvores utilizadas na análise de polarização nos anos de 2018 (18) e 2022 (22).
Subreddit
#Postagens (18)
#Postagens (22)
#Comentários (18)
#Comentários (22)
#Árvores (18)
#Árvores (22)
r/brasil
2.292
2.447
r/BrasildoB
r/brasilivre
Total
2.672
3.089
2018 aponta para maior presença de debates com caráter moderado,
onde os comentários estão envolvidos em threads com pontos de
vista diversos.
Nossa última análise foca nas opiniões individuais, com o obje-
tivo de analisar a formação de echo chambers e de maiores evidências
de polarização. Para tal, apresentamos na Figura 2 o mapa de calor
da distribuição de densidade do Stance Score de cada usuário (consi-
derando o posicionamento do comentário-pai, no eixo x) e da média
do Stance Score dos comentários realizados pelos nós filhos. Nota-se
uma concentração de densidade mais significativa em torno de zero
no eixo y em 2018, evidenciando que os indivíduos envolvem-se em
debates com outros de mesmo posicionamento, mas também estão
abertos a participar de discussões em assuntos dos quais possuem
visões discordantes. Adicionalmente, a densidade no primeiro qua-
drante, representando concentração de discussões em assuntos de
alta concordância entre os pares, é maior em 2018 do que em 2022.
Porém, no primeiro ano, a densidade de pontos neste quadrante não
tem uma separação bem demarcada com a densidade mais central
(em torno do zero), como ocorre em 2022. Neste caso, podemos
inferir que as discussões no segundo ano são mais "herméticas"do
que as realizadas no primeiro, evidenciando uma formação mais
forte de echo chambers, onde existe uma amplificação de discussões
que reforçam uma opinião em particular. Mais ainda, o gráfico de
2022 apresenta uma maior densidade no terceiro quadrante, onde
os posicionamentos entre os comentários pai e filhos são opostos,
apontando uma maior polarização em torno de discussões políticas
em mídias sociais.
Adicionalmente, realizamos uma análise qualitativa das árvores
presentes nos quadrantes 1 e 3. Árvores no primeiro quadrante, ou
seja, relacionadas à formação de echo chambers, são árvores com
discussão quase ou totalmente unilateral, com Balance Side baixo.
Mesmo quando debatem assuntos polêmicos, os comentários reali-
zados compartilham o mesmo ponto de vista. Também verificamos
árvores com vértices no terceiro quadrante que, por outro lado,
têm caráter de discussão acalorado, com Balance Side elevado, se
tratando normalmente de assuntos polêmicos como, por exemplo,
"Sigilo de 100 anos", "Carona de Lula até a COP27", "Isolamento Político
de Bolsonaro às vésperas das Eleições 2022", com vários ramos do tipo
resposta-réplica. Por fim, todas as análises realizadas apresentam
resultados complementares, fornecendo evidências de aumento de
polarização e formação de echo chambers nas discussões realizadas
nas eleições de 2022, considerando a amostra de dados obtidas a
partir do Reddit.
CONCLUSÕES
Neste trabalho apresentamos uma metodologia para caracterização
de discussões polarizadas em mídias sociais, baseada na modelagem
matemática de árvores de discussão, stance detection utilizando
(a) Eleições de 2018.
(b) Eleições de 2022.
Figura 2: Densidade de distribuição de Stance Score.
o GPT-4 e cálculo de métricas matemáticas para aferir o nível de
polarização nas threads.
Considerando como caso de estudo a comparação entre as dis-
cussões realizadas no Reddit, referentes às eleições de 2018 e 2022,
nossos resultados mostraram uma tendência de aumento da polari-
zação e diminuição do caráter de diversidade dos debates realizados
por estes usuários entre os períodos analisados, corroborando dis-
cussões abordadas em trabalhos de diferentes campos de pesquisa.
Assim, os resultados iniciais apresentados neste artigo se mostra-
ram promissores para a aplicação da metodologia de quantificação
de polarização em um maior conjunto de dados como também em
diferentes contextos e mídias sociais.
Agradecimentos: Este trabalho foi parcialmente financiado pela
FAPEMIG, CAPES e CNPq.
Caracterizando Polarização em Redes Sociais: Um Estudo de Caso das Discussões no Reddit sobre as Eleições Brasileiras de 2018 e 2022
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30139.txt ---

--- INÍCIO DO ARQUIVO: 30141.txt ---
Detecção de Fake News via Sinais Implícitos de Crowds: Uma
Abordagem para Mitigar o Cold-Start no Cálculo da Reputação
Viviane Antonia Corrêa Thomé
vthome@ime.eb.br
Instituto Militar de Engenharia
Rio de Janeiro, Brasil
Paulo Márcio Souza Freire
paulo.freire@dde.faetec.rj.gov.br
Fundação de Apoio à Escola Técnica
Rio de Janeiro, Brasil
Ronaldo Ribeiro Goldschmidt
ronaldo.rgold@ime.eb.br
Instituto Militar de Engenharia
Rio de Janeiro, Brasil
WebMedia’2024, Juiz de Fora, Brazil
Viviane Thomé et al.
dos textos associados às notícias. A inspiração para formulação do
SCS se apoia no trabalho de Schwarz e Jalbert [9], onde os autores
descrevem que as pessoas tendem a compartilhar notícias que lhes
pareçam familiares, dada a similaridade de opiniões. Para tanto, as
notícias precisam ser compatíveis com suas crenças preexistentes.
Os experimentos preliminares com o SCS foram realizados sobre
um dataset com notícias escritas em língua portuguesa (pt-br). Os
resultados obtidos pelo método proposto apresentaram desempe-
nhos superiores aos do método original HCS-I, provendo, portanto,
os primeiros indícios de validade da hipótese formulada.
Este artigo está organizado como segue: a Seção 2 apresenta os
trabalhos do estado da arte no combate automático às Fake News
mais fortemente ligados à pesquisa ora descrita; a Seção 3 recorda
resumidamente o método HCS-I, utilizado como base para o desen-
volvimento do SCS; a Seção 4 apresenta o método SCS propriamente
dito; os resultados dos experimentos realizados são expostos e de-
batidos na Seção 5. Por fim, na Seção 6 estão as considerações finais
do estudo realizado, destacando as contribuições da pesquisa e as
iniciativas de trabalhos futuros.
RELATED WORK
Este estudo considera trabalhos que propuseram métodos de de-
tecção de fake news baseados em crowd signals. Nesses estudos, as
notícias são classificadas como f ou 𝑓, por meio da combinação das
opiniões (i.e, signals) de usuários (i.e, crowd), onde essas opiniões
são ponderadas com base na reputação do usuário. A reputação de
cada usuário é obtida a partir da sua capacidade histórica em opinar
(i.e., acertos e erros em opiniões anteriores).
O método Detective [14] tem por objetivo mitigar a disseminação
de fake news, visando interromper a sua propagação. Para tanto, o
Detective, por meio de uma funcionalidade experimental presente
na rede social Facebook, coletava as opiniões explícitas dos usuários
sobre uma dada notícia. Com o objetivo de classificar a notícia como
f ou 𝑓, uma inferência bayesiana ponderava essas opiniões com
base nas respectivas reputações dos usuários membros do crowd.
O trabalho HCS [13], diferente de Detective, apresenta uma abor-
dagem que utiliza as opiniões implícitas dos usuários membros do
crowd, na tarefa de detecção de fake news. Na HCS há dois métodos
disponíveis, o HCS-I (implicit) e HCS-F (full). O HCS-I coleta as opi-
niões implícitas dos usuários, a partir da divulgação da notícia por
esses usuários. O HCS-F, além das opiniões implícitas dos usuários
divulgadores, considera a opinião fornecida por outros métodos
de detecção de fake news existentes na literatura. Ambos, assim
como o Detective, utilizam a inferência bayesiana para ponderação
das opiniões com base nas reputações dos membros do crowd. Os
métodos da HCS demonstraram desempenho similar ao Detective,
apesar de não dependerem de uma funcionalidade no meio digital
para coletar a opinião explícita do usuário e, nem tão pouco, da boa
vontade do usuário em opinar.
O TA-TCS [3], assim como o HCS-I, considera as opiniões im-
plícitas dos usuários divulgadores e pondera essas opiniões com
base na reputação desses usuários. Entretando, com o objetivo de
obter uma detecção antecipada, utiliza a propagação temporal das
notícias. O TA-TCS se diferencia da HCS por incluir uma nova
etapa de particionamento dos intervalos de divulgação da notícia. O
TA-TCS demonstrou assertividade similar ao HCS-I, além de obter
a detecção antecipada.
Embora esses métodos baseados em crowd signals tenham demons-
trado resultados promissores na detecção de fake news, observa-se
que parte dos usuários não possuem dados históricos de divulgação
de notícias para os cálculos de suas reputações. Essa carência carac-
teriza o problema de cold-start no cálculo de suas reputações. Isso
ocorre porque os métodos baseados em crowd signals dependem
do histórico de opiniões dos usuários sobre notícias passadas para
o cálculo de suas reputações. A Seção 3 apresenta um resumo do
funcionamento do método HCS-I utilizado como base neste estudo.
REVISÃO DO MÉTODO HCS-I
O HCS-I é um método baseado em crowd signals que busca detectar
se uma dada notícia é f ou 𝑓, combinando as opiniões implícitas
dos usuários divulgadores dessa notícia. O HCS-I considera que o
fato de um usuário divulgar uma notícia é um sinal implícito de
que, na opinião desse usuário, a notícia é 𝑓[13]. Tal princípio se
inspira na citação do filósofo Habermas [7], segundo a qual toda
ação comunicativa traz consigo uma inevitável pretensão à verdade.
É importante destacar que cada opinião implícita é ponderada utili-
zando a reputação do respectivo usuário. Essa reputação é calculada
com base nos acertos e erros que esse usuário obteve ao opinar, de
forma implícita, sobre notícias do passado e que, portanto, sabe-se
quais são f e quais são 𝑓. Com base nessa ponderação, o HCS-I
pode ser beneficiado, inclusive, na ocorrência de opiniões erradas
e/ou maliciosas. Para detectar fake news em um MDDN, o método
HCS-I executa as três etapas representadas na Figura 1 para cada
notícia 𝑛𝐷∈𝑁𝐷, onde 𝑁𝐷representa o conjunto de notícias a
serem analisadas. A seguir apresentam-se os detalhes de cada etapa.
Figura 1: Visão Geral das Etapas do Método HCS-I.
Composição do Crowd: o objetivo é construir o Crowd 𝐶, iden-
tificando no conjunto de usuários, do meio digital, aqueles que
divulgaram a notícia 𝑛𝐷a ser analisada. Dessa forma, ao final desta
etapa, cada 𝑐𝑖∈𝐶é um usuário divulgador de notícia 𝑛𝐷.
Cálculo da Reputação: responsável por medir a reputação de
cada membro 𝑐𝑖∈𝐶. A reputação de cada 𝑐𝑖é expressa pela pro-
babilidade de acertar e, consequentemente, de errar as opiniões já
fornecidas por 𝑐𝑖sobre as notícias f ou 𝑓divulgadas no passado.
Assim, a aferição da reputação dos membros de 𝐶depende da dispo-
nibilidade de um conjunto de notícias 𝑁𝐿, já rotuladas como f ou 𝑓.
Para isto, cada notícia 𝑛𝐿∈𝑁𝐿possui seu respectivo rótulo 𝑌∗(𝑛𝐿),
cujo valor 𝑦∗(𝑛𝐿) ∈{𝑓, 𝑓}. O cálculo da reputação de cada membro
𝑐𝑖é realizado com base na sua opinião sobre as notícias rotuladas
existentes em 𝑁𝐿. Dessa forma, é necessário calcular para cada
membro 𝑐𝑖sua probabilidade de acertar (e errar) sua opinião sobre
notícias fake e acertar (e errar) sua opinião sobre notícias not fake.
Para tanto, nesta etapa, o método HCS-I constrói, para cada 𝑐𝑖, uma
Detecção de Fake News via Sinais Implícitos de Crowds: Uma Abordagem para Mitigar o Cold-Start no Cálculo da Reputação
WebMedia’2024, Juiz de Fora, Brazil
matriz de opinião 𝑂𝑐𝑖como representada na Figura 2 (a). Cada com-
ponente 𝑛𝑟𝑠de 𝑂𝑐𝑖(𝑟,𝑠∈{𝑓, 𝑓}) indica a quantidade de notícias
sinalizadas como r por 𝑐𝑖, dado que os rótulos reais dessas notícias
são 𝑠. Se 𝑐𝑖é um usuário divulgador da notícia 𝑛𝐷, HCS-I utiliza as
notícias divulgadas por 𝑐𝑖, armazenadas em 𝑁𝐿, para preencher a
primeira linha de 𝑂𝑐𝑖. Inicialmente 𝑛𝑓𝑓= 0 e 𝑛𝑓𝑓= 0. Uma vez que
𝑐𝑖tenha decidido divulgar uma notícia 𝑛𝐿, esse comportamento é
assumido como um sinal implícito de que 𝑐𝑖considera 𝑛𝐿como not
fake. Isto é, 𝑌𝑐𝑖(𝑛𝐿) = 𝑓. Assim, para cada 𝑛𝐿∈𝑁𝐿divulgado por
𝑐𝑖, se Y*(𝑛𝐿) = 𝑓, então 𝑛𝑓𝑓= 𝑛𝑓𝑓+ 1, senão 𝑛𝑓𝑓= 𝑛𝑓𝑓+ 1.
Para preencher a segunda linha de 𝑂𝑐𝑖seria necessário recuperar
todas as notícias que 𝑐𝑖visualizou, mas optou por não divulgá-
las, por considerá-las fake. Como tal informação não se encontra
disponível, o método HCS-I busca inferir a capacidade de 𝑐𝑖em
identificar notícias fake por meio de dois critérios a seguir:
(1) Deve-se preservar a capacidade de 𝑐𝑖em acertar ou errar na
sinalização. Desta forma, 𝑛𝑓𝑓/(𝑛𝑓𝑓+ 𝑛𝑓𝑓) deve ser equiva-
lente a 𝑛𝑓𝑓/(𝑛𝑓𝑓+ 𝑛𝑓𝑓).
(2) O método HCS-I deve calcular o número de exemplos sina-
lizados nas duas classes, preservando a proporcionalidade
da primeira linha de 𝑂𝑐𝑖. Assim, 𝑛𝑓𝑓deve ser dado por
(𝑛𝑓𝑓/𝑛𝑓) × 𝑛𝑓, onde 𝑛𝑓e 𝑛𝑓representam, respectivamente,
o total de notícias fake e not fake em 𝑁𝐿.
A Figura 2 (b) apresenta exemplo de preenchimento parcial da
matriz na primeira linha. A Figura 2 (c) apresenta o preenchimento
completo da segunda linha. Considerando 𝑛𝑓= 30 e 𝑛𝑓= 60, a pri-
meira linha representa a situação em que um dado usuário divulgou
15 notícias, sendo 3 𝑓e 12 𝑓. A primeira linha é preenchida con-
forme os rótulos reais das notícias. A segunda linha é preenchida
de forma proporcional de acordo com a capacidade apurada sobre
o usuário de acertar e de errar. Portanto, para completar a matriz
na primeira coluna da segunda linha, tem-se 𝑛𝑓𝑓= (𝑛𝑓𝑓/𝑛𝑓) × 𝑛𝑓
ou 𝑛𝑓𝑓= 3/30 × 60 = 6. Para a segunda coluna da segunda linha
tem-se 𝑛𝑓𝑓= (𝑛𝑓𝑓× 𝑛𝑓𝑓)/𝑛𝑓𝑓ou 𝑛𝑓𝑓= (12 × 6)/3 = 24.
Figura 2: Matriz de Opinião
Com base na versão completa de 𝑂𝑐𝑖, o método HCS-I pode
realizar as inferências das probabilidades 𝜃𝑐𝑖,𝑓e 𝜃𝑐𝑖,𝑓para cada
usuário 𝑐𝑖membro do Crowd, onde: 𝜃𝑐𝑖,𝑓= 𝑛𝑓𝑓/(𝑛𝑓𝑓+ 𝑛𝑓𝑓) e
𝜃𝑐𝑖,𝑓= 𝑛𝑓𝑓/(𝑛𝑓𝑓+ 𝑛𝑓𝑓). Os elementos 𝜃𝑐𝑖,𝑓e 𝜃𝑐𝑖,𝑓indicam a
probabilidade de 𝑐𝑖em sinalizar uma notícia 𝑛𝐷como 𝑓, dado que
a notícia é de fato 𝑓(𝑃(𝑌𝑐𝑖(𝑛) = 𝑓|𝑌∗(𝑛) = 𝑓)). Ou, da mesma
forma, que seja sinalizada como f, dado que a notícia é de fato f
(𝑃(𝑌𝑐𝑖(𝑛) = 𝑓|𝑌∗(𝑛) = 𝑓)). Esse processo de inferência resulta na
reputação de cada usuário 𝑐𝑖, apresentada na matriz de reputação
𝑅𝑐𝑖abaixo. Com essas probabilidades calculadas, o HCS-I é capaz
de armazenar em 𝐶𝑅cada 𝑐𝑖∈𝐶com a sua respectiva reputação
𝑅𝑐𝑖(i.e. 𝐶𝑅= {𝑐𝑖𝑅/𝑐𝑖𝑅= (𝑐𝑖, 𝑅𝑐𝑖) e 𝑐𝑖∈𝐶}). Importante destacar
que quando 𝑐𝑖não possui histórico de divulgação de notícias (i.e.,
𝑐𝑖não divulgou qualquer notícia em 𝑁𝐿), 𝜃𝑐𝑖,𝑓= 𝜃𝑐𝑖,𝑓= 50%.
𝑅𝑐𝑖=
"
𝜃𝑐𝑖,𝑓
1 −𝜃𝑐𝑖,𝑓
1 −𝜃𝑐𝑖,𝑓
𝜃𝑐𝑖,𝑓
#
Classificação das Notícias: Nesta etapa, o HCS-I utiliza as
reputações 𝑅𝑐𝑖de todos os membros do crowd retornados por 𝐶𝑅
para calcular a probabilidade de 𝑛𝐷ser 𝑓ou 𝑓. Seguindo uma
abordagem bayesiana, o método utiliza as Equações 1 e 2, onde 𝜔e,
respectivamente, 1 - 𝜔, representam a probabilidade da notícia ser 𝑓
ou 𝑓. A classe correspondente à maior dentre as duas probabilidades
é, portanto, o resultado gerado pelo HCS-I.
𝑃(𝑌∗(𝑛) = 𝑓) = 𝜔.
Ö
𝑐𝑖∈𝐶𝑅
(1 −𝜃𝑐𝑖,𝑓)
(1)
𝑃(𝑌∗(𝑛) = 𝑓) = (1 −𝜔).
Ö
𝑐𝑖∈𝐶𝑅
𝜃𝑐𝑖,𝑓
(2)
MÉTODO PROPOSTO SCS
O método SCS é uma adaptação do método HCS-I. Ele busca fazer
inferências sobre os comportamentos dos usuários participantes de
𝐶(membros do crowd) que não divulgaram notícias no passado (i.e.,
não divulgaram qualquer notícia em 𝑁𝐿). Por meio de similaridades
entre as notícias a serem analisadas (𝑛𝐷∈𝑁𝐷) e as notícias do
passado cujos rótulos já são conhecidos (𝑛𝐿∈𝑁𝐿), procura-se
refinar o cálculo das reputações dos usuários 𝑐∈𝐶. Para tanto, um
LLM deve ser utilizado para gerar as representações vetoriais dos
textos vinculados às notícias (exs: títulos, conteúdos, etc).
O método SCS segue a mesma dinâmica de execução do método
HCS-I, com as mesmas etapas: Composição do Crowd, Cálculo da
Reputação e Classificação da Notícia. No entanto, diferentemente
do HCS-I, o SCS não atribui 50% à probabilidade dos usuários sem
histórico de divulgação opinarem que uma notícia seja 𝑓. Em vez
disso, o SCS executa quatro passos adicionais ao final da Etapa
Cálculo da Reputação do HCS-I, para tornar possíveis as inferências
de opiniões desses usuários. Tais passos estão ilustrados na Figura
3. Cada passo encontra-se descrito a seguir.
Figura 3: Passos Adicionais à Etapa Cálculo da Reputação.
Representação Vetorial de Notícias: Nesta etapa, um modelo
de LLM pré-treinado, escolhido pelo analista responsável pelos
experimentos, faz a extração das características semânticas de cada
notícia e a representa vetorialmente por meio dessas características.
Dessa forma, cada notícia a ser analisada 𝑛𝐷∈𝑁𝐷e cada 𝑛𝐿∈𝑁𝐿,
que tenha sido divulgada por um membro participante do crowd
𝐶, passará a contar com uma representação vetorial. Ao final, o
conjunto 𝑉𝐷deverá conter as representações vetoriais das notícias
a serem analisadas e o conjunto 𝑉𝐿os vetores que representam
notícias previamente rotuladas.
Identificação Usuários sem Divulgação: o objetivo desta
etapa é identificar cada 𝑐𝑖que não possui histórico de divulgação de
notícias entre as notícias em 𝑁𝐿. Neste momento, 𝐶é dividido em
WebMedia’2024, Juiz de Fora, Brazil
Viviane Thomé et al.
dois subconjuntos: 𝐶𝐿e 𝐶𝐿contendo, respectivamente, membros
do crowd que possuem e que não possuem histórico de divulgação
de notícias, entre as notícias em 𝑁𝐿.
Inferências de Divulgação: para cada 𝑐𝑖∈𝐶𝐿, seleciona-se
cada notícia a ser analisada 𝑛𝐷∈𝑁𝐷que tenha sido divulgada por
𝑐𝑖e a sua representação vetorial 𝑣𝐷∈𝑉𝐷. Para cada 𝑣𝐷, é feito
um cálculo de similaridade, comparando 𝑣𝐷com a representação
vetorial 𝑣𝐿de cada 𝑛𝐿∈𝑁𝐿(i.e., cada notícia divulgada no passado).
Este cálculo deve ser implementado por meio de uma função de
similaridade 𝑓𝑆(·, ·), gerando uma lista ordenada decrescente dos
valores 𝑓𝑆(𝑣𝐷, 𝑣𝐿). Um critério de corte é aplicado sobre esta lista
para selecionar as Top-k notícias cujas representações vetoriais 𝑣𝐿
sejam mais similares à representação vetorial 𝑣𝐷da notícia 𝑛𝐷.
Tal lista resultante do corte é armazenada em 𝐿𝑖𝑛𝑓
𝑐𝑖
e representa as
inferências sobre notícias do passado que sejam mais similares à
notícia divulgada no presente por 𝑐𝑖e que deverá ser analisada pelo
método de detecção de fake news. Tal conjunto será considerado
pelo SCS de forma a mitigar o problema do cold-start.
Refinamento Cálculo da Reputação: para cada membro do
crowd 𝑐𝑖∈𝐶, o preenchimento da 1ª linha de 𝑂𝑐𝑖é realizado da
seguinte forma: (i) se 𝑐𝑖∈𝐶𝐿, o cálculo considera cada 𝑛𝐿∈𝑁𝐿
divulgado por 𝑐𝑖; (ii) se, por outro lado, 𝑐𝑖∈𝐶𝐿, o cálculo utiliza
cada inferência de notícia 𝑛𝐿∈𝐿𝑖𝑛𝑓
𝑐𝑖. Em ambos os casos, esta etapa
utiliza as mesmas equações para o preenchimento das matrizes 𝑂𝑐𝑖
e 𝑅𝑐𝑖, conforme apresentado na Seção 3.
Para exemplificar, considere o cenário com 30 notícias 𝑓, 60 𝑓
e um usuário 𝑐𝑥∈𝐶que divulgou a notícia 𝑛𝐷e não tenha divul-
gações prévias. Considere também que, durante o passo Inferência
de Divulgação, tenham sido identificadas 10 notícias 𝑛𝐿similares a
𝑛𝐷. Ao analisar cada notícia similar, foi constatado que dentre as
Top-k (supondo 𝑘= 5), 4 são 𝑓e 1 é 𝑓. Para preencher a 1ª linha de
𝑂𝑐𝑥é preciso considerar as opiniões implícitas inferidas: 𝑛𝑓𝑓= 1
e 𝑛𝑓𝑓= 4. Para a 2ª linha deve-se proceder com o cálculo propor-
cional: 𝑛𝑓𝑓= 8 e 𝑛𝑓𝑓= 2. Importante notar que, neste mesmo
exemplo, as componentes da matriz 𝑂𝑐𝑥receberiam valor zero.
EXPERIMENTOS E RESULTADOS
Para a análise do método proposto foram realizados experimentos
preliminares com o dataset FakeNewsSet [4]. O FakeNewsSet contém
textos de notícias escritas em português e extraídas do Twitter™.
É composto por 600 notícias, nas quais 300 são fake e 300 são not
fake. Possui 16.024 usuários e 27.059 divulgações.
Nos testes realizados foi aplicada a técnica de validação cruzada
com 10 folds. Em cada iteração, 90% das notícias foram alocadas no
conjunto que contém o histórico das notícias 𝑁𝐿(para cálculo da
reputação) e, os outros 10% das notícias foram alocadas no conjunto
𝑁𝐷(para detecção). Para a extração das representações vetoriais
dos textos vinculados às notícias (título e conteúdo) foi utilizado
o LLM pré-treinado BERT, um dos modelos do estado da arte em
aplicações envolvendo linguagem natural [5, 12].
No passo Inferências de Divulgação, foi utilizado o parâmetro 𝑘
do Top-k para indicar a quantidade de notícias similares inferidas
a serem consideradas nas análises. Nos experimentos, Top-k foi
configurado com 𝑘= 5, um valor ímpar fixado a fim de impedir a
ocorrência de empates nos cálculos das probabilidades entre f e 𝑓.
Tabela 1: Resultados dos experimentos.
Método
Acurácia
(𝜇± 𝜎)
Precisão
(𝜇± 𝜎)
Recall
(𝜇± 𝜎)
F1
(𝜇± 𝜎)
HCS-I
0.957±0.024
0.932±0.049
0.984±0.022
0.957±0.026
SCS𝑡𝑒𝑥𝑡
0.980±0.018
0.975±0.024
0.983±0.024
0.979±0.026
SCS𝑡𝑖𝑡𝑙𝑒
0.980±0.016
0.996±0.011
0.962±0.032
0.979±0.019
Para avaliação das similaridades foram utilizados textos relaciona-
dos às notícias como os conteúdos (text) e os títulos (title).
A Tabela 1 mostra os resultados dos experimentos comparando
os desempenhos obtidos pelos métodos HCS-I e SCS, nos 10 folds da
validação cruzada. As métricas escolhidas para a avaliação dos ex-
perimentos foram a Acurácia, Precisão, Recall e F-1. O SCS demons-
trou melhores resultados que o HCS-I em quase todas as métricas,
com exceção do Recall. Tal diferença é explicada pelo fato do novo
método ter conseguido aumentar a sua capacidade em identificar
notícias 𝑓com maior precisão, impactando a métrica Recall, visto
que há um trade-off entre estas duas métricas.
Para análise estatística dos resultados, foi aplicado o teste Wil-
coxon Signed Ranks para a métrica Acurácia. A hipótese nula (𝐻0)
considera que não existe diferença entre as médias dos métodos
SCS e HCS-I na classificação das notícias. A Tabela 2 evidencia que
a 𝐻0 foi rejeitada para as versões text e title do SCS, onde p-value
< 0.05. Tais fatos são as primeiras evidências de superioridade do
SCS em relação ao HCS-I, em sintonia com a hipótese levantada
no início do trabalho de que, ao ajustar um método de detecção de
fake news baseado em crowd signals para mitigar o problema do
cold-start, tal método poderia melhorar os resultados produzidos
por sua versão original. Tal ajuste foi feito buscando melhorar o
cálculo da reputação dos usuários sem histórico de divulgação de
notícias no passado. Para tanto, buscou-se inferir qual seria o com-
portamento desses diante de notícias divulgadas no passado. Após
o ajuste, o percentual de cold-start caiu de 57, 67% para 0, 0%.
Tabela 2: Resultados do Teste de hipótese Wilcoxon SR.
SCS
HCS-I
P-Value
𝐻0
SCS𝑡𝑒𝑥𝑡
0.980
0.957
0.017
Rejeitada
SCS𝑡𝑖𝑡𝑙𝑒
0.980
0.957
0.027
Rejeitada
CONSIDERAÇÕES FINAIS
Este trabalho teve como principal contribuição obter os primeiros
resultados experimentais que apontam para a validade da seguinte
hipótese levantada: os desempenhos de métodos de detecção de
fake news via crowd signals podem ser melhorados se tais métodos
mitigarem o problema de cold-start por meio da inferência das repu-
tações dos usuários baseada no comportamento que esses usuários
teriam diante de notícias passadas. Tais resultados foram obtidos
em um experimento preliminar realizado em um dataset com notí-
cias escritas em pt-br, com o método SCS, outra contribuição desta
pesquisa. Como trabalhos futuros, considera-se a realização de no-
vos experimentos do método SCS com outros modelos LLM e com
outros datasets, com número maior de notícias e, também, escri-
tos em inglês. Adicionalmente, espera-se avaliar o uso de opiniões
explícitas de máquinas (i.e. modelos de aprendizado de máquina)
incorporados aos crowds e de cálculos de reputação de usuários por
temática de notícias, visando melhorar o desempenho do SCS.
Detecção de Fake News via Sinais Implícitos de Crowds: Uma Abordagem para Mitigar o Cold-Start no Cálculo da Reputação
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30141.txt ---

--- INÍCIO DO ARQUIVO: 30143.txt ---
Evidências de disseminação de conteúdo no Telegram durante o
ataque aos órgãos públicos brasileiros em 2023
Otávio R. Venâncio
otavio.venancio@dcc.ufmg.br
Departamento de Ciência da Computação
Universidade Federal de Minas Gerais
Gabriel H. S. Gonçalves
gabrielgoncalces@dcc.ufmg.br
Departamento de Ciência da Computação
Universidade Federal de Minas Gerais
Carlos H. G. Ferreira
chgferreira@ufop.edu.br
Departamento de Computação e Sistemas
Universidade Federal de Ouro Preto
Ana Paula C. da Silva
ana.coutosilva@dcc.ufmg.br
Departamento de Ciência da Computação
Universidade Federal de Minas Gerais
20231.
Neste contexto, este trabalho complementa resultados anteriores
[28], focando no uso do Telegram como ferramenta de coordena-
ção e difusão de conteúdo no período pós-eleição presidencial no
Brasil, especificamente em torno dos eventos de 8 de janeiro de
2023. Compilamos mensagens textuais de aproximadamente 270
grupos com orientação política no Telegram, cobrindo um inter-
valo de 15 dias, de 01/01/2023 a 15/01/2023, em torno dos ataques
em Brasília ocorridos no dia 8 de janeiro. Inspirados por pesqui-
sas precedentes [11, 19–21], buscamos indícios de coordenação na
promoção de conteúdo por meio de uma abordagem metodológica
composta pelos seguintes passos. Inicialmente, modelamos a disse-
minação de informação no período em questão por meio de uma
rede media-centric, conectando usuários que replicaram o mesmo
conteúdo entre os grupos analisados. Posteriormente, recorremos
a técnicas avançadas de extração de backbone [24] para destacar
conexões de rede que demonstram indícios de potencial coordena-
ção (diferenciando-se de usuários que compartilharam conteúdo de
maneira independente). Finalizamos aplicando algoritmos de de-
tecção de comunidades para identificar agrupamentos de usuários
com forte interconexão, evidenciando padrões de coordenação e os
descrevemos em termos de características topológicas, participação
na disseminação de informações e conteúdo compartilhado.
TRABALHOS RELACIONADOS
Nos últimos anos, um conjunto amplo de trabalhos na literatura
estudou evidências de coordenação para impulsionamento de infor-
mações a partir de modelos orientados a redes [4, 7, 12, 19–21].
Considerando o WhatsApp, o trabalho de Kiran et al. analisa os
padrões de mensagens em grupos politicamente orientados [12].
Bursztyn et al. ampliaram o escopo desse estudo e analisaram como
1https://www.nytimes.com/live/2023/01/09/world/brazil-congress-riots-bolsonaro
WebMedia’2024, Juiz de Fora, Brazil
Venâncio et al.
a afiliação política de um grupo influencia a dinâmica da dissemina-
ção de grandes mensagens [4]. Nobre et al. mostraram a existência
de comunidades do WhatsApp ativamente envolvidas na dissemina-
ção coordenada de informações, especialmente durante as eleições
brasileiras de 2018 [20, 21]. Chagas et al. abordaram a crescente
preocupação com a propaganda digital e identificaram grupos te-
máticos e grupos funcionais consistentes com as estratégias de
influência e redes de desinformação de agentes políticos [7].
O aumento significativo do uso do Telegram como meio de co-
municação motivou diferentes estudos de como o processo de disse-
minação de informação ocorre entre os seus grupos. Por exemplo,
Urman et. al. investigaram redes de extrema-direita e suas caracte-
rísticas de comunidade [27], enquanto Nobati et. al. adotaram uma
abordagem diferente e analisaram arestas de rede para identificar
mensagens de spam [9]. Adicionalmente, Slobozhan et al. [25] es-
tudaram o uso de diferentes mídias de comunicação no Telegram
durante os protestos em Belarus em 2020, revelando diferentes
papéis e tipos de conteúdo para cada meio. Os autores de [6] rea-
lizaram uma caracterização e análise das redes de apoiadores do
ex-presidente Jair Bolsonaro no período que englobou as manifesta-
ções de 7 de setembro de 2021 e 7 de setembro de 2022, analisando
as mudanças na estrutura da rede, similaridades e diferenças nas
discussões realizadas. Por fim, Rossini et al. exploraram a complexa
relação entre a participação política via aplicativos de mensagens
privadas e a disseminação descontrolada de desinformação eleito-
ral, enfatizando a maior probabilidade de que membros de grupos
políticos acreditem em desinformação [23].
Em complemento aos trabalhos encontrados na literatura, o ob-
jetivo principal deste artigo é mostrar o papel do Telegram como
impulsionador de informação no período subsequente às eleições
presidenciais de 2022, período este marcado por uma forte tensão
política que culminou nos ataques ocorridos em Brasília em 8 de
janeiro de 2023. Mais precisamente, revelamos o papel de diferentes
comunidades-chave na movimentação dos grupos analisados no
período de interesse.
METODOLOGIA
Esta seção descreve a abordagem metodológica adotada neste es-
tudo, abrangendo desde a coleta e modelagem de dados até a extra-
ção dos backbones das redes e a análise das comunidades e caracte-
rísticas textuais das mensagens disseminadas.
3.1
Coleta dos Dados
As mensagens analisadas neste trabalho foram coletadas a partir de
diversos grupos públicos políticos do Telegram, no período compre-
endido entre 01/01/2023 a 15/01/2023. Para encontrar estes grupos,
utilizamos uma estratégia que envolveu a utilização da API do X
(anteriormente conhecido como Twitter).2 que consistiu em seleci-
onar tweets que continham o termo t.me — indicativo de convites
para o Telegram — acompanhado por palavras-chave representando
regiões do Brasil (Norte, Sudeste, Centro-Oeste, Sul, Nordeste), es-
pectros políticos (Esquerda, Direita, Centro) e todos os candidatos
presidenciais de 2022.
Após a coleta dos grupos, acessamos as mensagens enviadas
nestes com o auxílio da biblioteca Telethon, que opera com base na
2https://developer.twitter.com/en/docs/twitter-api/
Tabela 1: Metadados usados na análise.
Campo
Descrição
message_id
ID da mensagem, único por grupo/canal
channel_id
ID do grupo/canal
retrieved_utc
Data de coleta da mensagem
updated_utc
Data hora da última atualização
message
Conteúdo textual da mensagem
views
Número de visualizações
forwards
Número de compartilhamentos
from_id
ID do autor
post_author
Nome de usuário do autor
message_utc
Data hora de envio da mensagem
API do Telegram.3,4 Adicionalmente, realizamos a exploração das
mensagens dos grupos por mais convites a outros grupos, resultado
em uma cobertura total de 274 grupos e de 524.498 mensagens.
Para a análise de conteúdos representativos do debate político,
excluímos mensagens padrão, tais como saudações curtas (por exem-
plo,’Bom dia’, ’Oi’ e ’Ok’). Aplicamos um critério de exclusão para
mensagens com menos de 30 caracteres e aquelas compostas exclu-
sivamente por caracteres não latinos. Ao final, temos um conjunto
de 226.679 mensagens provenientes de 17.661 usuários (from_ids
únicos), cada um tendo contribuído com pelo menos uma mensa-
gem.
A Tabela 1 detalha os metadados coletados. Importante ressaltar
que, segundo o Telegram.5, a contagem de visualizações pode não
refletir precisão absoluta, uma vez que o contador pode ser reinici-
ado caso o conteúdo seja replicado manualmente e visualizações
pelo mesmo usuário em momentos diferentes podem ser contadas
como distintas.
3.2
Modelagem da Rede
Para investigar a presença de padrões de coordenação entre usuários
na disseminação de conteúdo em grupos do Telegram, aplicamos o
modelo media-centric, utilizado anteriormente na literatura [20, 21].
Este modelo visa evidenciar possíveis atividades de coordenação
entre usuários que compartilham e promovem conteúdos seme-
lhantes em diferentes plataformas online, tais como WhatsApp e
Twitter. Mais especificamente, para a janela de tempo de interesse,
modelamos o grafo 𝐺(𝑉, 𝐸), onde um nó 𝑣∈𝑉corresponde a um
usuário que postou uma mensagem em um dos grupos do Telegram.
Adicionalmente, uma aresta não direcionada (𝑣𝑖, 𝑣𝑗) ∈𝐸é formada
entre pares de usuários que compartilharam a mesma mensagem
(i.e., o mesmo conteúdo textual) pelo menos uma vez.
3.3
Extração dos Backbones
A rede modelada permite identificar usuários com fortes evidências
de disseminação conjunta de um grande volume de mensagens nos
grupos observados. No entanto, o desafio é identificar as arestas
que ocorrem de forma esporádica (ou arestas fracas), que podem
esconder a verdadeira estrutura de rede relacionada ao fenômeno de
interesse, ou seja, evidências de coordenação. Assim, para identificar
e filtrar arestas esporádicas e, ao mesmo tempo, preservar as arestas
que provavelmente sugerem coordenação, utilizamos dois métodos
de extração de backbone [24]. Estes métodos foram previamente
3https://docs.telethon.dev/en/stable/
4https://core.telegram.org/
5https://telegram.org/faq_channels
Evidências de disseminação de conteúdo no Telegram durante o ataque aos órgãos públicos brasileiros em 2023
WebMedia’2024, Juiz de Fora, Brazil
utilizados para o estudo de coordenação de usuários em várias
plataformas [11, 17, 20, 21].
O primeiro método Disparity Filter (DF) identifica e preserva
arestas significativamente mais fortes que outras conexões dos
mesmos nós, focando em usuários que compartilham um número
de mensagens muito maior do que o esperado, indicando possível
coordenação. Uma aresta 𝑣𝑖, 𝑣𝑗é preservada no backbone se seu
peso difere do modelo de referência tanto para 𝑣𝑖quanto para 𝑣𝑗.
O segundo método DF+NB, proposto em [17], combina o DF com
o conceito de Neighborhood Overlap (NB) para superar a limitação
do DF de manter arestas periféricas ou pontes, desconsiderando
padrões de vizinhança. DF+NB foca em arestas que indicam co-
ordenação e conectam usuários com muitos vizinhos em comum,
facilitando a descoberta de comunidades não triviais e mais forte-
mente conectadas.
Considerando os parâmetros dos modelos, ambos os métodos
utilizam um p-value (𝛼= 0, 05) para testar a independência das
arestas entre usuários, sendo este o parâmetro do modelo nulo do
DF, enquanto o DF+NB também requer a definição de um limiar
para a métrica de Neighborhood Overlap, assumindo o percentil 95𝑡ℎ
conforme o artigo original [17], permitindo a análise do backbone
sob diferentes perspectivas de possíveis esforços de coordenação.
3.4
Detecção de Comunidades
A partir da extração dos backbones, visamos identificar como essas
arestas com fortes evidências de coordenação formam comunidades
representativas de usuários que compartilham de maneira persis-
tente o mesmo conteúdo, sugerindo uma possível coordenação para
o impulsionamento em massa deste conteúdo.
Assim, aplicamos o algoritmo Louvain para encontrar as comu-
nidades que formam os backbones. O algoritmo Louvain maximiza a
modularidade das comunidades [2], sendo que este valor representa
a densidade de conexões nas comunidades em comparação com
uma rede aleatória. Os valores de modularidade variam de -0,5 a +1,
com pontuações mais altas (acima de 0,4) indicando estruturas de
comunidade bem definidas [18]. Em outras palavras, valores mais
altos de modularidade indicam redes mais densas e valores entre
0,3 e 0,7 sugerem comunidades bem definidas.
3.5
Análise de Tópicos
Para analisar o conteúdo político disseminado pelas comunidades
de usuários identificadas, utilizamos o framework BERTopic [13],
que extrai tópicos de discussão a partir de grandes conjuntos de
dados textuais, incluindo análises de discussões em plataformas
de mídia social [8, 10]. O BERTopic combina modelos de repre-
sentação vetorial com técnicas de agrupamento para identificar
temas recorrentes nas mensagens. Configuramos o BERTopic para
usar o modelo BERTimbau [26], eficaz na captura da semântica do
português.6 Após gerar as representações vetoriais das mensagens,
o BERTopic reduz a dimensionalidade desses vetores com UMAP
e aplica o algoritmo HDBSCAN para agrupá-los por similaridade
semântica. Os tópicos são definidos aplicando a técnica c-TF-IDF,
que identifica palavras-chave distintivas para cada grupo.
Para a parametrização do BERTopic, adotamos as diretrizes es-
pecificadas na documentação do framework para encontrar um
6https://huggingface.co/neuralmind/bert-base-portuguese-cased
Tabela 2: Dados topológicos das redes analisadas.
Rede
# Nós
# Arestas
Densi.
Comu.
Mod.
Completa
4.692
72.629
0,0066
0,3473
DF
0,0134
0,4695
DF+NB
0,0342
0,8718
Tabela 3: Análise da distribuição de padrões de coordenação.
Rede
# Usu.
(%)
# Mens.
(%)
# Com.
(%)
# Visu.
(%)
# Grupos
(%)
Completa
4.692
33.966
2M
422M
DF
316 (6,73%)
9.385 (27,63%)
985K (38,7%)
203M (48,09%)
132 (68,39%)
DF+NB
64 (1,36%)
2.447 (7,2%)
245K (9,65%)
91M (21,53%)
70 (36,27%)
equilíbrio ótimo entre o número de tópicos e o tamanho do con-
junto de dados [13].7 Como resultado encontramos: (i) número
de vizinhos e componentes para UMAP igual a 3 e; (ii) tamanho
mínimo de tópico igual a 80, que determina o menor conjunto de
mensagens únicas que um determinado tópico pode representar e
garante que apenas tópicos significativos e bem definidos sejam
considerados para análise.
RESULTADOS
Nesta seção, apresentamos e discutimos nossas análises.
4.1
Análise Topológica e de Comunidades
A Tabela 2 apresenta as propriedades topológicas e de disseminação
de conteúdo da rede completa e dos respectivos backbones extraí-
dos a partir das técnicas DF e DF+NB. Nota-se que as técnicas DF
e DF+NB eliminam, respectivamente, 93,63% e 98,63% dos nós e
99,08% e 99,90% das arestas. Apesar disso, os valores da modulari-
dade (Mod.) resultantes da execução do algoritmo Louvain, em cada
uma das topologias reveladas aumenta, especialmente no DF+NB
que chega a 0,87. Em ambos os casos, as comunidades detectadas
nos backbones extraídos são bem mais estruturadas do que a rede
originalmente modelada. Na prática, estes resultados mostram que
os backbones revelam grupos de usuários bem definidos e coesos
que podem, potencialmente, representar esforços coordenados de
disseminação de informação ou mobilização.
A Tabela 3 apresenta as principais estatísticas referentes ao po-
tencial de disseminação de conteúdo dos usuários dos backbones
comparados a rede completa. Notavelmente, no backbone DF+NB,
composto por pouco mais de 1% dos usuários, observa-se que 7,2%
das mensagens únicas foram disseminadas por eles. Tais mensagens
receberam 9,65% dos compartilhamentos totais (# Com.) e 21,53%
das visualizações (# Visu.). Analisamos a porcentagem de grupos (#
Grupos) que receberam mensagens diretas dos usuários dos backbo-
nes, revelando que 68,39% dos grupos receberam informações dos
usuários do DF e 36,27% do DF+NB. Essas estatísticas revelam como
um contingente reduzido de usuários pode não só de disseminar,
mas também de introduzir novas informações nos grupos.
A Figura 1 mostra a topologia do backbone DF, onde as cores
indicam as comunidades e o tamanho dos nós é proporcional ao
número de mensagens disseminadas, destacando que algumas co-
munidades, como a 2, 3 e 5, têm usuários com volumes significa-
tivamente maiores de informações disseminadas. No entanto, o
7https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html
WebMedia’2024, Juiz de Fora, Brazil
Venâncio et al.
Figura 1: Visualização das maiores comunidades do DF e suas
características.
Tabela 4: Descrição dos tópicos e padrões de disseminação.
# Compart.
# Visu.
Palavras Características
1M
209M
militar, intervenção, patriotas, 2023
3M
39M
poder, armadas, anos, ainda
1M
21M
2023, dia, patriotas, atenção, congresso
96K
27M
sigam, canal, guerra, urgente, deus
58K
6M
manifestantes, esplanada, intervenção
104K
10M
posse, urgente, federal
80K
8M
tomar, esplanada, dia, militares
29K
3M
canal, anos, porque, sigam
57K
8M
grupos, todas, aqui, pode, patriotas
58K
6M
paralisação, sendo, STF, sistema
85K
9M
sigam, canal, agir, polícia
63K
10M
atenção, 55, aqui, ainda, sistema
44K
11M
federal, manifestantes, frente
39K
4M
deus, pois, guerra, armadase
138K
15M
polícia, federal, manifestantes
padrão predominante seja de usuários com um número de men-
sagens compartilhadas uniforme.Por fim, notam-se desconexas da
maior componente, compostas majoritariamente por 2 e 3 usuários.
4.2
Análise dos Tópicos Disseminados
A seguir, focamos na análise dos tópicos de discussão a partir das
mensagens disseminadas como descrito na Seção 3. A Tabela 4
apresenta os tópicos mais populares, isto é, que tiveram pelo menos
100 mensagens únicas.
Observa-se que vários tópicos se relacionam com os eventos
políticos subsequentes ao ocorrido em 8 de janeiro, destacando-se o
tópico 1, com o maior volume de mensagens, que enfatiza chamados
para intervenção militar e tentativas de tomada de poder, exemplifi-
cado por mensagens que demandam ações das forças armadas e da
população. Tais mensagens concentram-se especialmente em torno
do dia 8 e nos dias subsequentes, quando ocorre o pico de mensa-
gens como, por exemplo: “As forças armadas estão aguardando nosso
comando! O povo de forma ordeira tem que invadir o congresso! No
recesso de janeiro não tem ninguém! Temos que ocupar agora!”
Já o tópico 5, concentrou mensagens a respeito do dia oito de
janeiro, dia dos ataques compostos por uma série de vandalismos,
manifestações e depredações do patrimônio público em Brasília.
Manifestantes invadiram edifícios do governo federal visando ins-
tigar um golpe militar contra o governo eleito de Luiz Inácio Lula
n.° Comunidade
ID T´opico
−2
Mensagens nos T´opicos[Normalizado]
(a) DF
n.° Comunidade
ID T´opico
−2
Mensagens nos T´opicos[Normalizado]
(b) DF+NB
Figura 2: Percentual das mensagens nos tópicos das quatro
maiores comunidades dos backbones extraídos.
da Silva e restabelecer Jair Bolsonaro como presidente do Brasil8.
Várias mensagens instigavam a organização e ida de apoiadores
para Brasília, “Quem precisar de ônibus para Brasília, só chamar no
WhatsApp OMITIDO, OMITIDO está disponibilizando 3.000 Ônibus.”
Em resumo, os demais tópicos exploram uma variedade de temas,
cada um refletindo diferentes facetas da discussão política na rede.
Finalmente, buscando entender a operação das comunidades nos
backbones sobre a disseminação de tópicos, calculamos a distri-
buição das mensagens das 4 maiores comunidades nos 15 tópicos
principais, exibidos na Figura 2. Cada célula representa o total de
mensagens únicas disseminadas pela comunidade com normaliza-
ção 𝑧−𝑠𝑐𝑜𝑟𝑒por coluna. Podemos observar que no DF, Figura 2(a),
a comunidade 1 abordou vários tópicos, enquanto as comunidades
2, 3 e 4 focaram em tópicos específicos. No DF+NB, Figura 2(b), as
comunidades 1 e 4 foram ativas em muitos tópicos, enquanto as
comunidades 2 e 3 não se desviaram da média. Em resumo, nos-
sos resultados mostram que comunidades específicas do Telegram
podem emergir como influenciadoras centrais em determinadas
discussões, refletindo uma dinâmica complexa de comunicação.
CONCLUSÕES E TRABALHOS FUTUROS
Neste trabalho apresentamos um conjunto de análises iniciais das
mensagens compartilhadas no Telegram pós-eleições de 2022 vi-
sando identificar evidências de coordenação em comunidades que
potencialmente favoreceram a propagação de conteúdos antide-
mocráticos. Concentramos os esforços em três análises principais:
identificação de padrões de coordenação, investigação das comuni-
dades envolvidas e análise do conteúdo disseminado. Os resultados
mostram como uma parcela de usuários do Telegram manifestou
indícios de coordenação durante o período analisado, sendo respon-
sáveis pela ampla disseminação de mensagens no período.
Para futuras investigações, pretendemos: (i) explorar outras ma-
nifestações de coordenação, como a utilização de hashtags e links,
visando um entendimento mais completo dessas dinâmicas; (ii) exa-
minar o papel das mídias alternativas nesse contexto; (iii) analisar o
posicionamento político das mensagens coletadas e (iv) desenvolver
métodos para prever a popularidade e o impacto de determinados
conteúdos com base em suas características textuais.
Agradecimentos: Este trabalho foi parcialmente financiado pela
FAPEMIG, CAPES e CNPq.
8https://jornal.usp.br/artigos/o-que-foi-o-8-de-janeiro/
Evidências de disseminação de conteúdo no Telegram durante o ataque aos órgãos públicos brasileiros em 2023
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30143.txt ---

--- INÍCIO DO ARQUIVO: 30144.txt ---
Integrando Avaliações Textuais de Usuários em Recomendação
baseada em Aprendizado por Reforço
Naan Vasconcelos
naan.vasconcelos@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Davi Reis
davireisjesus@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Thiago Silva
thiagosilva@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Nícollas Silva
ncsilvaa@dcc.ufmg.br
UFMG
Minas Gerais, Brazil
Washington Cunha
washingtoncunha@dcc.ufmg.br
UFMG
Minas Gerais, Brazil
Elisa Tuler
etuler@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ
Minas Gerais, Brazil
5. Apesar dos novos valores
WebMedia’2024, Juiz de Fora, Brazil
Vasconcelos N. et al.
estarem mais de acordo com as preferências dos usuários [11, 17],
provocaram um desequilíbrio no dilema de exploration/exploitation:
uma redução significativa na entropia de ratings impacta no explo-
ration. Esses resultados apontam que para que MAB possam utilizar
RARS com sucesso, suas estratégias de exploration precisam buscar
alternativas à entropia.
FUNDAMENTAÇÃO TEÓRICA
2.1
Multi-Armed Bandits
Multi-Armed Bandits (MAB) é a principal abordagem de Apren-
dizado por Reforço para SsR interativa [15]. Trata-se de um modelo
de decisão sequencial que, continuamente, escolhe uma ação 𝑎entre
um conjunto de ações A, denominadas arms. A cada seleção de
uma ação em um ponto 𝑡no tempo resulta em uma recompensa
𝑅(𝑎𝑡) ∈R. Esses modelos precisam decidir entre: 1) exploitation,
que visa selecionar os arms com as maiores recompensas do pas-
sado; e 2) exploration, que seleciona diferentes arms para obter
mais informações sobre o domínio e tomar decisões futuras mel-
hores. A escolha entre essas duas opções caracterizam o dilema
de exploitation-exploration (i.e. exp-exp) e exige que o modelo seja
capaz de explorar o máximo conhecimento disponível enquanto
também explora o espaço de solução para adquirir ainda mais con-
hecimento sobre o domínio [32]. Em RS, os itens a serem recomen-
dados são modelados como os arms e a recompensa é o feedback do
usuário para essa recomendação (e.g., rating) [19]. Essa resposta dos
usuários é, em geral, coletada e salva continuamente em um con-
junto H (𝑡). Um item é recomendado de acordo com uma regra de
predição 𝜋, definida como uma função que explora as informações
atuais sobre o usuário: 𝑖(𝑡) ≡𝜋(H (𝑡)). Por isso, a estratégia ideal
deve maximizar as recompensas nas 𝑇interações:
𝑖∗(·) = arg max
𝑖(·)
𝑇
∑︁
𝑡=1
E [𝑟𝑢,𝑖(𝑡) |𝑡]
(1)
Para definir a utilidade dos itens para cada usuário, as principais
abordagens utilizam uma formulação matricial probabilística via
PMF (Probabilistic Matrix Factorization), modelando a distribuição
de rewards pelos fatores latentes de usuários e itens, semelhantes aos
métodos model-based [27]. A recompensa esperada é quase sempre
modelada como o produto dos fatores latentes do usuário 𝒑𝒖com
os fatores do item 𝒒𝒊. A função objetivo e dada da seguinte forma:
𝑖∗(·) = arg max
𝑖(·)
𝑇
∑︁
𝑡=1
E [𝑟𝑢,𝑖(𝑡) |𝑡] = arg max
𝑖(·)
𝑇
∑︁
𝑡=1
E [𝒑⊤
𝒖𝒒𝒊(𝒕) |𝑡]
(2)
Os esforços atuais estão concentrados em como otimizar essa
função objetivo, equilibrando o dilema de exp-exp. As abordagens
tradicionais do MAB, como 𝜖-Greedy, UCB e Thompson Sam-
pling, utilizadas no presente estudo, consideram essa função obje-
tivo como regra de predição [26]. A diferença entre esses algoritmos
está na maneira como eles controlam o dilema de exp-exp [2, 20–22].
Enquanto o 𝜖-Greedy explora a regra de predição com probabili-
dade (1 −𝜖), os algoritmos UCB e Thompson Sampling primeiro
medem uma incerteza Σ em torno das informações disponíveis
sobre o usuário e os itens. Thompson Sampling é um algoritmo
probabilístico que extrai os vetores de usuários e itens de uma dis-
tribuição normal definida pelas informações atuais disponíveis. No
entanto, mesmo essa abordagem mede a recomendação com base
na combinação de 𝑝𝑢e 𝑞𝑖.
2.2
Recomendações Review-Aware
Sistemas de Recomendação Review-Aware [23] partem da premissa
que as preferências dos usuários sobre itens consumidos podem
ser melhor capturadas por meio das avaliações textuais (reviews)
fornecidas pelos usuários ao invés dos ratings diretamente assinal-
ados [12, 17]. Essa área vem sendo impulsionada pelos expressivos
avanços em propostas de arquiteturas de redes neurais aplicadas
a PLN [18]. Há um número crescente de SsR que vêm adaptando
essas propostas de PLN, considerando diferentes abordagens de
arquitetura, tais como redes Attention-Based [14], redes convolu-
cionais temporais (TCN) [5], grafos [31], extração de aspectos [6] e
tópicos [24], análise de sentimento [7] e, mais recentemente, apren-
dizagem contrastiva [29]. Recentemente, em [4] realizou-se uma
revisão sistemática da literatura dessas abordagens, comparando-as
experimentalmente.
Um algoritmo de destaque nessa avaliação é o HRDR [16], que
transforma os comentários em representações vetoriais por meio
de embeddings. Em seguida, aplica-se uma camada de convolução
2D para capturar padrões locais e, em seguida, um mecanismo de
atenção para calcular pesos de atenção, focando em aspectos rele-
vantes das representações. As representações ajustadas pelo mecan-
ismo de atenção são combinadas, resultando em representações
finais dos usuários e itens. De forma semelhante, o CARP [13] tam-
bém vetoriza os comentários de usuários e itens que são processados
por camadas CNN para capturar o contexto e extrair características
importantes. O modelo calcula pesos de atenção para diferentes
partes dos comentários usando mecanismos de autoatenção. Essas
representações de usuários e itens capturam as variações e aspec-
tos relevantes dos comentários. Outro algoritmo de destaque é o
CARM [14] que também utiliza uma camada CNN sobre repre-
sentações vetoriais para aprender características importantes dos
comentários. As características convolucionais extraídas são combi-
nadas com os fatores latentes dos usuários e itens, obtidos através
da PMF. Essas características combinadas são então passadas por
uma camada linear (fully connected) e uma função de ativação Tanh,
transformando as características aprendidas em uma representação
mais compacta. Por fim, o MAN [28] utiliza redes convolucionais
temporais sobre as representações vetoriais para capturar padrões
locais nos comentários. Uma função de atenção calcula uma matriz
de atenção entre as representações dos usuários e itens, as quais
são novamente processadas por uma camada convolucional tem-
poral para ajustar os pesos da atenção. Este processo resulta em
características de interação entre usuários e itens.
Uma característica comum às abordagens de MAB é que todas
consideram os rewards de forma explícita, ou seja, por meio das
ratings atribuídas pelos usuários aos itens recomendados. Nesse
trabalho, nossa meta é alterar esses rewards explícitos por valores
implícitos, extraídos dos reviews dos usuários por meio de aborda-
gens acima descritas.
Integrando Avaliações Textuais de Usuários em Recomendação baseada em Aprendizado por Reforço
WebMedia’2024, Juiz de Fora, Brazil
Dataset
MusicalInstruments
MusicalInstrumentsMAN
MusicalInstrumentsCARM
MusicalInstrumentsHRDR
MusicalInstrumentsCARP
Measure
Hits
Hits
Hits
Hits
Hits
T
Linear UCB
0.267
0.408
0.584
0.188
0.321
0.484
0.191
0.314
0.463
0.185
0.272
0.446
0.192
0.272
0.461
e-Greedy
0.033
0.062
0.125
0.026
0.053
0.106
0.025
0.051
0.102
0.025
0.051
0.103
0.025
0.047
0.105
TS
0.122
0.187
0.287
0.102
0.163
0.270
0.116
0.187
0.301
0.114
0.181
0.279
0.004
0.015
0.048
Measure
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
T
Linear UCB
0.068
0.145
0.228
0.052
0.131
0.197
0.056
0.121
0.186
0.051
0.105
0.177
0.057
0.128
0.191
e-Greedy
0.031
0.058
0.106
0.025
0.050
0.092
0.025
0.049
0.092
0.024
0.047
0.089
0.023
0.044
0.090
TS
0.101
0.140
0.191
0.084
0.121
0.174
0.089
0.128
0.188
0.092
0.131
0.179
0.004
0.015
0.044
Table 1: Musical Instruments - Efetividade e diversidade das estratégias MAB aplicadas nas coleções originais e suas versões
modificadas. Valores em negrito correspondem aos melhores valores validados estatisticamente por Wilcoxon com p-value = 0.05
Dataset
Tucson
TucsonMAN
TucsonCARM
TucsonHRDR
TucsonCARP
Measure
Hits
Hits
Hits
Hits
Hits
T
Linear UCB
0.133
0.225
0.430
0.062
0.142
0.212
0.080
0.151
0.230
0.082
0.147
0.233
0.072
0.136
0.208
e-Greedy
0.023
0.056
0.118
0.022
0.032
0.060
0.010
0.020
0.049
0.018
0.036
0.072
0.018
0.032
0.064
TS
0.073
0.128
0.223
0.057
0.090
0.143
0.044
0.078
0.128
0.052
0.091
0.147
0.045
0.076
0.133
Measure
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
UsersCoverage
T
Linear UCB
0.113
0.155
0.265
0.058
0.128
0.170
0.073
0.128
0.167
0.077
0.125
0.171
0.065
0.117
0.156
e-Greedy
0.023
0.054
0.104
0.022
0.032
0.059
0.010
0.020
0.046
0.018
0.034
0.065
0.018
0.031
0.062
TS
0.069
0.113
0.180
0.056
0.087
0.131
0.042
0.071
0.111
0.050
0.084
0.128
0.044
0.072
0.118
Table 2: Tucson - Efetividade e diversidade das estratégias MAB aplicadas nas coleções originais e suas versões modificadas.
Valores em negrito correspondem aos melhores valores validados estatisticamente por Wilcoxon com p-value = 0.05
AVALIAÇÃO EXPERIMENTAL
O objetivo dessa seção é responder a pergunta de pesquisa Qual
o impacto do uso de ratings extraídos de comentários de usuários e
itens em SsR interativos baseados em MAB? Para isso, nossos ex-
perimentos consistem em comparar o desempenho das estratégias
MAB utilizando as bases de dados originais que contém ratings
assinalados de forma explícita, com versões modificadas dessas
mesmas coleções, onde os ratings são calculados a partir dos comen-
tários textuais.Os modelos de MAB utilizados nos experimentos
estão destacados na Seção 2.1: 𝜖-Greedy, UCB e Thompson Sampling.
Tratam-se das estratégias que são base de funcionamento da grande
maioria das estratégias atuais. Para a geração das bases de dados
modificadas, os cálculos dos ratings foram extraídos pelas quatro
estratégias descritas na Seção 2.2: MAN, CARM, HRDR e CARP,
escolhidos por apresentarem bons resultados em uma recente e
vasta avaliação experimental [4].
Coleção
# Usuários
# Itens
Esparsidade
Amazon - Musical Instruments
27.530
10.620
99.92%
Yelp 2021 - Tucson
8.540
8.867
99,99%
Table 3: Visão geral das coleções utilizadas nas avaliações.
Avaliamos a combinação das estratégias de MAB e Review-Aware
em duas coleções de cenários distintos, uma de comércio eletrônico
e outra de pontos de interesse, conforme descritos na Tabela 3. Em
ambas as coleções, todos os itens com rating também possuem um
comentário textual correspondente. Para cada uma delas, foram
geradas outras quatro versões modificadas pelas quatro estratégias
de Review-Aware.
As três estratégias de MAB foram aplicadas sobre as cinco ver-
sões da base (uma original e quatro modificadas). Consideramos
duas métricas distintas de avaliação, uma de precisão e outra de
diversidade: 1) Hits métrica de precisão que corresponde ao número
de recomendações que pertencem ao histórico de cada usuário;
2) Users Coverage, métrica de diversidade que corresponde à por-
centagem de usuários distintos que estão interessados nos itens
recomendados. Para comparar os resultados alcançados com vali-
dação estatística, adotamos o teste de Wilcoxon, visto a distribuição
não normal dos conjuntos de dados de recomendação [4].
3.1
Resultados Experimentais
As Tabelas 1 e 2 apresentam os resultados de nossa avaliação para
as coleções Musical Instruments e Tucson, respectivamente. Os resul-
tados remetem a média de Hits e UserCoverage acumulados após 𝑇
interações dos usuários com o sistema. Tanto em efetividade quanto
em diversidade, os melhores resultados foram alcançados pelas es-
tratégias MAB nas coleções originais, nas quais os ratings foram
assinalados de forma explícita pelos usuários. As diferenças entre os
resultados alcançados nas bases originais e modificadas são ainda
maiores à medida que ocorrem mais iterações. A principal razão é
que as estratégias MAB aprendem menos sobre os usuários quando
consideram bases com ratings calculados por estratégias RARs.
Tratam-se de resultados que contrastam com aqueles reportados na
literatura para outros cenários, por exemplo recomendação offline,
que apontam ratings extraídos a partir dos reviews dos usuários são
capazes de elucidar melhor as preferências dos usuários [12, 17, 28].
WebMedia’2024, Juiz de Fora, Brazil
Vasconcelos N. et al.
(a) Distribuição Acumulativa de Ratings
(b) Popularidade vs. Entropia - Original
(c) Popularidade vs. Entropia - CARM
Figure 1: Caracterização Comparativa entre a coleção original Musical Instruments e suas versões modificadas por RARs.
Para compreender o comportamento das estratégias MAB nas
coleções modificadas, realizamos uma caracterização comparativa
delas com suas versões originais. Por restrição de espaço, os resulta-
dos serão reportados apenas para coleção Musical Instruments, mas
as conclusões são similares na Tucson. Primeiramente, comparamos
a distribuição de ratings, apresentadas na Figura 1(a). Observamos
que enquanto na base original essa distribuição é bem dispersa entre
os valores de 1 a 5, a distribuição está concentrada entre os valores
de 3,5 a 5 nas bases modificadas. Os valores calculados estão coer-
entes com a literatura [12, 17], que aponta que os mesmos refletem
de forma mais fidedigna as preferências dos usuários. Todavia, essa
alteração causou um desequilíbrio em como as estratégias MAB
lidam com o dilema exp-exp.
Assim, em nossa segunda caracterização, comparamos a cor-
relação entre popularidade dos itens, comumente utilizada como
uma estratégia de exploitation, e a entropia entre os itens, comu-
mente utilizada como uma abordagem de exploration, conforme
apresentado nas Figuras 1(b) - base original - e 1(c) - base modifi-
cada pelo algoritmo CARM (resultados semelhantes obtidos pelos
demais algoritmos de RARS). Conforme podemos observar, na base
modificada essa correlação é muito alta quando comparada à base
original, o que significa que as bases modificadas apresentam uma
baixa entropia, impactando diretamente no Exploration.
Exploration geralmente apresenta uma correlação significativa
entre seu comportamento e a entropia dos itens. Em um cenário sem
entropia, onde os rewards são mais previsíveis e não há incerteza,
o comportamento dos algoritmos mudam significativamente. O e-
Greedy, por exemplo, rapidamente converge para a exploitation, com
um valor de 𝜖tendendo a zero. O UCB também reduz o exploration,
pois os intervalos de confiança são muito estreitos devido à certeza
das recompensas. No caso do Thompson Sampling, as distribuições
posteriores se tornam muito concentradas em torno do valor ver-
dadeiro da recompensa de cada opção. Isso resultaria em menos
amostragem de opções não ótimas, pois a incerteza é praticamente
inexistente. Portanto, a alteração da entropia dos itens nas bases
modificadas impacta diretamente o equilíbrio entre exploration e ex-
ploitation, sendo crucial para o design de sistemas de recomendação
baseados em algorimos de aprendizado por reforço eficazes. Tais
análises respondem a pergunta de pesquisa estudada neste trabalho.
CONCLUSÕES E TRABALHOS FUTUROS
Em Sistemas de Recomendação (SsR), Multi-Armed-Bandits (MAB)
são modelos de decisão sequencial que continuamente escolhe entre
um conjunto de itens, aqueles que maximizam o reward esperado
(e.g., a satisfação do usuário), visando equilibrar a seleção entre
itens com as maiores recompensas no passado (exploitation) ou
itens inexplorados (exploration). Esses modelos utilizam como re-
ward valores numéricos explicitamente assinalados pelos usuários
aos itens. Neste trabalho apresentamos um estudo preliminar do
impacto do uso de ratings extraídos de comentários textuais por
meio de recomendação Review-Aware (RARS) em SsR interativos.
Comparamos experimentalmente o desempenho de três estraté-
gias clássicas de MAB utilizando coleções de dados com ratings
assinalados de forma explícita (originais) com suas versões na qual
os ratings eram calculados por meio de quatro estratégias de RARS.
Os melhores resultados foram obtidos nas bases originais, contrar-
iando com resultados reportados na literatura para outros cenários
além de MAB. Nesse sentido, caracterizamos comparativamente as
coleções e constatamos uma redução significativa na entropia de
ratings dos itens nas bases modificadas. Em um cenário de entropia
reduzida, a dinâmica entre exploration e exploitation é drasticamente
simplificada, com os algoritmos rapidamente se concentrando em
exploitation e reduzindo o aprendizado sobre os usuários. Dessa
forma, nosso trabalho abre a possibilidade de novas propostas de
alterações nas estratégias de exploration, que permitam que as abor-
dagens MAB sejam também capazes de usufruir da habilidade de
estratégias RARs em capturar melhor as preferências dos usuários.
Também pretendemos estudar como combinar as duas os ratings
explícitos com a informação obtida por meio das RARs.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Fapemig, FAPESP,
CIIA-Saúde e AWS.
Integrando Avaliações Textuais de Usuários em Recomendação baseada em Aprendizado por Reforço
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30144.txt ---

--- INÍCIO DO ARQUIVO: 30145.txt ---
Interpolação e Previsão de Precipitação por Redes Neurais
Convolucionais para Grafos
Augusto Fonseca
augusto.fonseca@aluno.cefet-rj.br
CEFET/RJ
Ronaldo Goldschimidt
IME/RJ
ronaldo.rgold@ime.eb.br
Eduardo Ogasawara
CEFET/RJ
eogasawara@ieee.org
Mariza Ferro
IC/UFF
mariza@ic.uff.br
Fábio Porto
LNCC
fporto@lncc.br
Eduardo Bezerra
CEFET/RJ
ebezerra@cefet-rj.br
WebMedia’2024, Juiz de Fora, Brazil
Fonseca et al.
Além desta introdução, a Seção 2 aborda a metodologia utilizada
neste trabalho e detalhada as fontes dos dados meteorológicos utili-
zados e os procedimentos de pré-processamento aplicados. A Seção
3 detalha a configuração do experimento e apresenta os resultados
obtidos. Finalmente, a Seção 4 destaca as principais conclusões e
apresenta as direções de trabalhos futuros.
METODOLOGIA
Na Figura 1 é apresentada uma visão geral da técnica de interpola-
ção proposta neste trabalho. Os dados coletados pelos instrumentos
meteorológicos são introduzidos na rede na forma de séries tem-
porais de grafos. As camadas da GCN têm a tarefa de aprender
a interpolar os dados. Os dados interpolados são então passados
como entrada para uma CNN que aprende os padrões espaciais e
temporais processando a tarefa de regressão. Como saída, a rede
gera as previsões da variável meteorológica de interesse para passos
de tempo futuros.
Figura 1: Visão geral da arquitetura. As séries temporais de
grafos são passadas como entrada para a GCN que interpola
os dados. Os dados interpolados são passados para a CNN que
aprende os padrões espacias e temporais dos dados e gera as
predições.
O problema aqui abordado consiste em uma tarefa de aprendi-
zado semi-supervisionado, uma vez que não conhecemos o valor
de precipitação para toda a área de interesse. Cada instrumento
meteorológico coleta diferentes subconjuntos de variáveis. Além
disso, decisões estratégicas ou falhas nos equipamentos podem in-
terromper o monitoramento por horas, ou até mesmo dias. Dado
esse cenário, poderíamos modelar o problema para aprendizado so-
bre grafos heterogêneos e dinâmicos, i.e., grafos em que os vértices
possuem diferentes atributos e que variam sua topologia ao longo
do tempo. No entanto, para fins de simplificação, modelamos o pro-
blema como aprendizado sobre grafos homogêneos e estáticos. Para
viabilizar tal modelagem, aplicamos técnicas sobre o processamento
dos dados abordadas na Seção 2.2.
Formalmente, o problema é definido como segue. Considere que
G(𝑡) =

V, E, X(𝑡)
represente um grafo homogêneo no passo
de tempo 𝑡∈{1, 2, ...,𝑇}. V representa as coordenadas (latitude,
longitude) que é definido pela união de coordenadas de uma grade
regular de aproximadamente 3 km de resolução e as coordenadas
associadas às ETs. E é definido por um limiar de distância entre
as coordenadas empregando a distância Haversine. X ∈R|V|×𝑑
representa a matriz de características (variáveis meteorológicas)
associadas aos vértices. O modelo de interpolação é treinado para
aproximar uma função 𝑓(·) que mapeia uma sequência de variá-
veis meteorológicas observadas para uma grade regular de valores
de precipitação: Z(𝑡) = 𝑓(G(𝑡), G(𝑡−1), . . . , G(𝑡−𝑇−1)), em que 𝑇
representa o tamanho da série temporal de entrada e Z representa
a matriz de valores de precipitação interpolados.
2.1
Dados Meteorológicos
O experimento empregou duas fontes de dados. A primeira é relativa
ao radar meteorológico do INEA instalado na coordenada (lat: -
22.99325, lon: -43.58794). A segunda corresponde a ETs do INMET
e do Sistema Alerta Rio. A Figura 2 apresenta a localização dos
instrumentos. O radar do INEA é do tipo banda S, dupla polarização,
em operação desde 2015 e fornece, dentre outras variáveis, dados de
refletividade1 na atmosfera com resolução espacial de 500 metros. A
cada 5 min o radar escaneia a atmosfera em 360º de azimute, em sete
níveis de altitude, em um raio de abrangência de 250 km. Dentre as
33 ETs do Alerta Rio, dois são estações meteorológicas completas,
cinco são estações meteorológicas parciais, i.e., não observam todas
as variáveis meteorológicas e 26 são pluviômetros, com resolução
temporal de 15 minutos. As variáveis meteorológicas observadas
são pressão atmosférica (mB), temperatura (°C), ponto de orvalho
(°C), umidade relativa do ar (%), direção do vento (°(gr)), velocidade
do vento (m/s) e precipitação (mm). As quatro estações do INMET
são meteorológicas completas, com resolução temporal de uma
hora. Usamos as seguintes categorias de precipitação definidas pela
equipe do sistema Alerta Rio, conforme a quantidade de precipitação
observada em mm/h, a seguir: i) fraca: [0, 5); ii) moderada: [5, 25);
iii) forte: [25, 50) e; iv) extrema: [50,𝑖𝑛𝑓).
Figura 2: Localização dos instrumentos de observação. Pon-
tos em verde representam os pluviômetros do Alerta Rio. Os
ícones de informação nas cores verde, laranja e azul represen-
tam, respectivamente, as estações meteorológicas do Alerta
Rio, as estações meteorológicas do INMET e o radar mete-
orológico do INEA. O ícone de estrela representa a estação
tijuca_muda.
2.2
Pré-processamento dos Dados
Dados de precipitação têm por característica serem significativa-
mente desbalanceados, i.e., muitos dias sem chuva. Além disso, o
1A refletividade é uma das variáveis registradas a partir do eco gerado pelas ondas ele-
tromagnéticas irradiadas pelo radar. Essa variável detecta a presença de hidrometeoros
na atmosfera, tais como água, gelo e neve.
Interpolação e Previsão de Precipitação por Redes Neurais Convolucionais para Grafos
WebMedia’2024, Juiz de Fora, Brazil
volume de dados gerados pelo radar do INEA é de cerca de 1,3 GB
por dia. Por conta disso, foram selecionados somente os dias em
que ocorreu chuva acumulada em uma hora acima de 25 milíme-
tros. Para cada dia selecionado, o dia anterior também foi incluído,
totalizando 224 datas entre 2016 e 2023. Os dados de radar foram
agregados espacialmente para cada vértice em G com base nas ob-
servações em um raio de 1,5 km a partir do vértice, em resolução
temporal horária, limitado às observações até 2 km de altitude de-
vido ao efeito conhecido como cone do silêncio2. No processo de
agregação foram geradas as features média, mínimo, máximo, 1º
quartil, mediana e 3º quartil em relação à refletividade observada
dentro do raio.
Os dados das estações do Alerta Rio e INMET passaram por um
processo de curadoria em que foram tratadas as seguintes questões:
i) duplicatas temporais, i.e., dois ou mais registros para a mesma
tupla (coordenada, horário) com valores distintos observados;
ii) período de operação da estação e quantidade de registros espe-
rada por intervalo de tempo;
iii) dados faltantes;
iv) dados espúrios, i.e., valores fora do intervalo esperado para a
variável e;
v) outras questões de qualidade encontradas durante a análise.
Os dados foram filtrados conforme as datas disponíveis para
os dados de radar. Um ponto a ser ressaltado são os falsos nulos
para a variável direção do vento, quando uma estação meteoroló-
gica registra velocidade 0.0 para a variável velocidade do vento. No
processo de imputação, os falsos nulos receberam valor arbitrário
0.0. As variáveis associadas à direção e velocidade do vento foram
transformadas para componentes cíclicas3. A data/hora também foi
empregada como feature pelo método de transformação usando o
seno e cosseno4. Os dados anteriores a 01/fev/22 foram selecionados
para compor o conjunto de treino e igual ou superior como con-
junto de teste. Os parâmetros de normalização e imputação foram
ajustados somente sobre o conjunto de treino para evitar vazamento
de dados. Foram empregados, respectivamente, a função Min-max
e o algoritmo k-NN Imputer disponíveis na biblioteca scikit-learn5.
Os dados de radar e de ETs foram integrados por junção outer
join. Variáveis não observadas em uma determinada coordenada
receberam valor 0.0. Para distinguir os valores 0.0 verdadeiros, foi
adicionada uma variável binária marcadora de preenchimento para
cada variável meteorológica. Por fim, os dados foram processados
em séries temporais de grafos, em que os vértices representam as
coordenadas e as arestas uma relação entre coordenadas definida
por um limiar de distância de 3,4 km. Cuidados especiais foram
tomados para evitar gerar séries temporais não contíguas, uma vez
que não tínhamos a série histórica completa. Ao final foram gerados
3.644 exemplos de treino ( 78%) e 1.012 exemplos de teste ( 22%).
EXPERIMENTOS E RESULTADOS
Nesta seção, descrevemos os experimentos realizados. Na Seção 3.1
descrevemos aspectos de configuração, e na Seção 3.2 apresentamos
os resultados, junto com uma análise sobre os mesmos.
2Cone do silêncio. Uma porção da área acima e em torno do radar não é observada. O
tamanho da porção varia conforme o ângulo de elevação do radar.
3https://bit.ly/metpy-wind-vector-components
4https://bit.ly/kaggle-encoding-cyclical-features
5https://scikit-learn.org/stable/index.html
3.1
Configuração do Experimento
Neste primeiro experimento buscamos investigar duas questões: i) o
impacto sobre a acurácia quando empregada a feature refletividade e;
ii) a influência dos dados de estações vizinhas em uma determinada
coordenada de interesse. Para tal, escolhemos a estação tijuca_muda
como estação de interesse (EI) por apresentar a maior quantidade
de exemplos em duas das quatro categorias de precipitação, sendo
elas moderada e forte. Em seguida foram identificadas as 11 esta-
ções vizinhas (EV) mais próximas da EI (Apêndice A). A análise
gradativa consistiu em processar 11 conjuntos distintos de datasets
(conjuntos de treino e teste), conforme detalhado na Seção 2.2. Em
cada conjunto, identificados e ordenados como {𝑣1, 𝑣2, ..., 𝑣11}, um
novo vizinho é adicionado ao dataset, do mais próximo para o mais
distante, cumulativamente aos já adicionados no conjunto anterior,
de forma que o conjunto 𝑣1 possui apenas uma EV e o conjunto 𝑣11
possui as 11 EVs. Importante ressaltar que os dados da EI não são
empregados em nenhuma fase do pré-processamento, treinamento
e inferência, simulando portanto uma coordenada sem informação
à qual desejamos inferir por interpolação.
A GCN empregada para treinar o modelo foi a desenvolvida por
Bai et al. [1] e se encontra implementada no framework PyTorch
Geometric Temporal6. Comparando a arquitetura implementada no
framework com a apresentada no artigo, identificamos um provável
erro de implementação nas camadas que operam a convolução tem-
poral. De fato, a nossa versão corrigida obteve melhores resultados
nos testes preliminares. A GCN foi definida com uma camada de
convolução espaço-temporal conforme Bai et al. [1] e duas camadas
fully connected para operar, respectivamente, as embeddings e a
correlação entre os vértices. Como função de custo foi empregada
a Mean Squared Error. A taxa de aprendizado foi configurada para
1 × 10−3 e a rede foi treinada com batchs de tamanho 32. Foram
treinados dois modelos para cada dataset: o modelo gcn_r treinado
com todas as features e o modelo gcn em que a feature refletividade
é removida.
3.2
Resultados e Análise
A Tabela 1 apresenta valores de RMSE por modelo e categoria para
cada dataset. Para eventos de precipitação forte e extrema, os mo-
delos gcn e gcn_r apresentam melhores resultados em todos os
datasets, com erros significativamente menores em diversos casos,
chegando a um erro aproximadamente 82% menor para chuva ex-
trema no conjunto 𝑣6 em relação ao IDW. Outros destaques ocorrem
na categoria extrema para os conjuntos 𝑣5 e 𝑣10 e na categoria forte
para os conjuntos 𝑣3 e 𝑣4 que apresentam erros relativos entre 40%
e 50% abaixo do método IDW. Na categoria moderada o método
IDW obtém melhores resultados em todas os conjuntos, exceto o
conjunto 𝑣1 e na categoria fraca ocorrem seis resultados a favor
do IDW contra cinco das GCN. Apesar disso, a diferença de erro
entre os métodos nas categorias fraca e moderada é relativamente
pequena em quase todos os datasets.
Quanto ao emprego da feature refletividade, a Tabela 1 apresenta
que o modelo treinado com a inclusão dessa feature retorna oito
melhores resultados para a categoria forte e seis para a categoria
extrema. A maioria dos resultados em todas as categorias apresen-
tam resultados próximos quando comparados os modelos gcn e
6https://pytorch-geometric-temporal.readthedocs.io/en/latest/index.html
WebMedia’2024, Juiz de Fora, Brazil
Fonseca et al.
Tabela 1: Resultados da análise gradativa para a estação tijuca_muda (RMSE).
Categoria
Modelo
𝑣1
𝑣2
𝑣3
𝑣4
𝑣5
𝑣6
𝑣7
𝑣8
𝑣9
𝑣10
𝑣11
fraca
gcn
0.773
1.053
0.688
0.655
0.666
0.615
0.538
0.568
0.641
0.677
0.610
gcn_r
0.807
0.930
0.778
0.684
0.644
0.675
0.570
0.623
0.649
0.651
0.630
idw
0.799
0.544
0.521
0.522
0.614
0.608
0.621
0.599
0.626
0.663
0.658
moderada
gcn
4.839
5.512
4.521
4.688
3.807
3.834
3.720
3.941
3.930
4.771
3.733
gcn_r
4.890
5.681
4.593
4.285
3.875
3.881
4.065
3.836
4.133
4.059
3.887
idw
4.989
3.296
3.164
3.061
2.975
3.077
3.169
3.115
3.109
3.144
3.150
forte
gcn
9.843
6.078
6.285
6.862
9.639
10.450
9.756
11.997
11.599
8.304
12.903
gcn_r
9.417
6.606
5.663
6.947
7.196
9.426
8.976
10.617
9.908
11.624
11.134
idw
11.307
10.598
11.385
12.006
11.732
10.351
10.725
11.274
11.533
11.748
12.154
extrema
gcn
18.941
17.570
11.752
13.891
11.255
3.692
11.345
18.451
15.373
5.334
14.368
gcn_r
18.341
25.899
12.979
15.192
6.657
2.624
13.661
14.528
9.837
10.586
12.701
idw
18.400
18.241
16.403
17.477
16.069
14.881
15.166
17.400
18.250
19.052
18.293
gcn_r. No entanto, algumas exceções chamam a atenção. Por exem-
plo, na categoria extrema os resultados para os datasets 𝑣2 e 𝑣10
apresentam erro significativamente maior para o modelo treinado
com refletividade. Por outro lado, para os datasets 𝑣5 e 𝑣9 ocorre o
inverso, em que o modelo treinado com refletividade apresenta erro
significativamente menor.
A Figura 3 apresenta a variação na acurácia dos modelos a me-
dida que adicionamos informações de EVs. Nas categorias fraca
e moderada, o erro tende a diminuir com a adição de EVs, estabi-
lizando a partir da 5𝑎EV adicionada. Na categoria forte, ocorre
um efeito não esperado, em que a partir da 3𝑎EV o erro tende a
crescer. Na categoria extrema, o erro diminui significativamente
até a adição da 6𝑎EV, momento em que o erro começa a aumentar
novamente. Já para o método IDW, o erro tende a permanecer cons-
tante conforme a adição de EVs mais distantes, o que é esperado
dada a formulação matemática do próprio método, que pondera
inversamente as informações de EVs mais distantes.
Figura 3: Impacto da acurácia para o ponto de interesse a
medida que são adicionados dados de vizinhos mais distantes.
CONCLUSÃO
Neste trabalho, nosso objetivo foi empregar GCNs na tarefa de
interpolação e previsão de precipitação. Para tal, treinamos GCNs
de forma semi-supervisionada empregando dados de radar mete-
orológico e ETs. Por ser um trabalho em progresso, inicialmente
avaliamos a acurácia da interpolação estimada por uma GCN em
comparação ao método estatístico IDW. Avaliamos os modelos com
foco em duas questões: i) impacto sobre a acurácia quando empre-
gados dados de radar e; ii) influência dos dados de estações vizinhas
em uma determinada coordenada de interesse.
Foi observado que as GCNs obtêm melhor acurácia em eventos
mais extremos de precipitação e alcançam resultados promissores
em eventos de menor magnitude. Para eventos mais extremos, a
GCN chega a alcançar resultados até 80% superiores em relação
ao método IDW. Além disso, nos experimentos em que o método
IDW obteve melhor resultado, a diferença para o resultado alcan-
çado pela GCN foi relativamente pequena, mostrando o potencial
da abordagem. O emprego dos dados de radar aumentou frequen-
temente a acurácia para eventos mais extremos. De forma geral,
a adição de mais informações vizinhas à coordenada de interesse
aumentou a acurácia da interpolação no caso das GCNs. Também
observou-se que, aumentar em demasia a quantidade de vizinhos
pode diminuir a acurácia. Conforme esperado, o método IDW tende
a estabilizar conforme são adicionados vizinhos mais distantes.
Como trabalhos futuros, pretendemos entender o motivo da di-
minuição da acurácia das abordagens por GCN quando da adição de
certas EVs. Para tal, faremos análises separadas para cada evento em
que a GCN apresentou erro alto. Pretendemos também investigar
o motivo para a inclusão de dados de radar diminuir significativa-
mente a acurácia em casos específicos. Também realizaremos os
mesmos experimentos com outras EI, explorando distribuições es-
paciais distintas de vizinhança. Além disso, avaliaremos o impacto
com outras fontes de dados meteorológicos (e.g. satélite).
ACKNOWLEDGMENTS
Agradecemos à CAPES, ao CNPq, à Faperj (Proc. E-26/210.242/2024)
e Instituto Serrapilheira (N Processo Serra – 2211-41897), pelo finan-
ciamento parcial desta pesquisa; ao LNCC e ao Instituto de IA, por
ceder os recursos computacionais e apoio à missões de pesquisa; e
ao COR, à GeoRio e ao INEA, pelo fornecimento dos dados usados
nos experimentos.
Interpolação e Previsão de Precipitação por Redes Neurais Convolucionais para Grafos
WebMedia’2024, Juiz de Fora, Brazil

--- FIM DO ARQUIVO: 30145.txt ---

--- INÍCIO DO ARQUIVO: 30147.txt ---
Predição Intra-Quadro Baseada em Aprendizado Profundo para
Light Fields Densos
Italo Machado
Bruno Zatt
Daniel Palomino
idmachado@inf.ufpel.edu.br
zatt@inf.ufpel.edu.br
palomino@inf.ufpel.edu.br
Universidade Federal de Pelotas
Pelotas, Rio Grande do Sul
Attilio Fiandrotti
Universidade de Turin
Turin, Itália
attilio.fiandrotti@unito.it
WebMedia’2024, Juiz de Fora, Brazil
Machado et al.
Figura 1: Formas de representar um Light Field
CNN que reproduz a distribuição de pixels do bloco alvo. Defend-
emos que essa abordagem pode ser aplicada de forma eficaz para
preencher os comportamentos de gradiente dos LFs no formato
lenslet, explorando as redundâncias angulares de maneira eficiente.
Para isso, nossa proposta emprega RNCs como preditores intra para
todos os tamanhos de bloco do codificador de vídeo EVC [2, 11],
uma técnica que pode ser generalizada para outros codificadores
de vídeo com estruturas de bloco semelhantes. Apresentamos uma
análise das diferentes arquiteturas, técnicas e hiperparâmetros avali-
ados até chegarmos na melhor solução.
ARQUITETURA PROPOSTA
No decorrer da predição intra, o codificador EVC emprega um con-
texto composto por três blocos vizinhos para estimar o bloco atual
a ser predito. Este trabalho visa realizar esta estimação através
da técnica de preenchimento de imagens [1], na qual um autoen-
coder é utilizado para gerar pixels ausentes com base nos pixels
dos blocos vizinhos. Portanto, conforme ilustrado na Figura 2, a
arquitetura proposta recebe como entrada um contexto de tamanho
HxW, contendo três blocos vizinhos juntamente com um bloco alvo
vazio, e gera um contexto de saída com as mesmas dimensões. Con-
siderando que o EVC divide a imagem em blocos de tamanho 32x32,
16x16 e 8x8, as dimensões HxW podem variar entre 64x64, 32x32 e
16x16. Note que uma arquitetura foi treinada separadamente para
cada tamanho de bloco tendo em vista que as distribuições entre os
blocos vizinhos e alvo mudam consideravelmente de acordo com o
tamanho de bloco.
O codificador da arquitetura é constituído por 5 camadas con-
volucionais com stride 1 intercaladas por 1 uma convolução stride 2.
Todas as camadas do codificador utilizam kernels de tamanho 3x3 e
são seguidas por uma função de ativação PreLU. Note que a última
camada de codificação não é seguida por uma camada convolucional
com stride 2, pois isso reduziria um contexto de entrada de 8x8 a
zero e, consequentemente, necessitaria de uma rede diferente para
blocos de tamanho 8x8.
No lado do decodificador, a arquitetura consiste em quatro ca-
madas. Com exceção da camada final, todas as camadas incluem
uma operação de up-sample com um fator de 2, seguida por uma
camada convolucional com passo 1, refletindo o lado do codificador.
A última camada é uma convolução transposta com kernels de 4x4
e um passo de 2, culminando em uma função Sigmóide que produz
valores de luminância entre 0 e 1. É importante mencionar que a
rede gera um contexto de saída com o mesmo tamanho da entrada,
e a região de interesse é posteriormente cortada, pois experimen-
tos demonstraram que essa estratégia resulta em uma eficiência
aprimorada [1]. A rede possui um total de 3 milhões de parâmetros
ajustáveis e um tamanho estimado de 18,37 MB. Além disso, ao
contrário de abordagens similares, como [1], que empregam con-
voluções mascaradas, optamos por utilizar convoluções regulares
mais simples.
2.1
Preparação do Dataset
O conjunto de dados EPFL [10] foi dividido em dois grupos de
105 imagens para treinamento e 12 imagens para teste, seguindo a
metodologia proposta em [15]. Essas imagens foram extraídas de
arquivos brutos usando o pacote plenopticam [4], resultando em
LFs com resolução de 622x432x13x13 onde as vistas escurecidas
foram descartadas.
Posteriormente, para alinhar o tamanho dos micro images aos
tamanhos de bloco, as 5 vistas mais externas foram descartadas,
gerando um LF com 8x8 vistas. Por fim, os LFs foram reorganiza-
dos no formato de lenslet para melhor explorar as redundâncias
angulares das vistas.
Para avaliar a melhor forma de treinar a rede e evitar overfitting,
foram conduzidos experimentos para treinar as redes usando difer-
entes estratégias para selecionar os exemplos de treino e técnicas
de transformação. Estas incluíram a seleção sequencial de blocos e
a escolha aleatória de blocos de várias áreas das LFs. Além disso,
treinamos a rede sem aplicar transformações e também aplicando
rotações em 90, 180, 270 ou 360 graus e espelhamento horizontal a
cada bloco com uma probabilidade de 50%, resultando em todas as
possíveis variações formando blocos quadrados.
Os experimentos indicaram que a abordagem mais eficaz para
evitar overfitting foi selecionar blocos aleatoriamente e aplicar trans-
formações durante o treinamento. Especificamente, 25% do número
total possível de blocos foram selecionados aleatoriamente para
cada LF, com sobreposição permitida entre blocos. Aumentar o
número de blocos de amostra por LF levou a tempos de treinamento
mais longos sem melhorias nas curvas de aprendizado ou eficiências
na predição. Outro cuidado importante no recorte dos blocos foi de
sempre mantê-los alinhados com o início e fim dos macro pixels, o
que ocorrerá naturalmente durante o processo de codificação em
vista que estes possuem tamanhos múltiplos dos tamanhos de bloco
do codificador.
As redes neurais foram então treinadas usando os dados gerados
por 100 épocas, o otimizador Adam foi utilizado com uma taxa
de aprendizado de 1−4 e uma taxa de decaimento exponencial de
0.2. O otimizador SGD (Stocastic Gradient Descent) e learning rates
de 1−5, 2−5, 2−3, 1−3 também foram avaliados, contudo, estes não
atingiram melhoras nas convergências.
Predição Intra-Quadro Baseada em Aprendizado Profundo para Light Fields Densos
WebMedia’2024, Juiz de Fora, Brazil
Figura 2: Diagrama de Blocos da Arquitetura Proposta
2.2
Integração com o Codificador
O modelo treinado reside em um servidor com memória GPU. Ele
recebe blocos de entrada por meio de conexões UDP e transmite
de volta os tamanhos de bloco preditos. O servidor pode lidar com
tamanhos de contexto de 64x64, 32x32 ou 16x16, oferecendo flexi-
bilidade para vários codecs. Este projeto permite a experimentação
perfeita com diferentes arquiteturas de rede em codecs que com-
partilham estruturas de blocos semelhantes, exigindo poucas ou
nenhuma modificação de código.
Para aproveitar o preditor do lado do servidor, o codec alvo sofre
uma alteração na configuração. Um arquivo de configuração per-
sonalizado substitui um dos modos intra existentes pelo preditor
proposto. Durante o processo de codificação, quando o codec en-
contra o modo designado, ele transmite os três blocos vizinhos ao
redor do bloco de predição atual para o servidor via UDP. O codec
então pausa sua predição regular e aguarda a resposta do servidor
contendo o bloco predito.
A seleção do modo intra para substituição é crucial. Em vista
de que o codificador EVC usa códigos de comprimento variável
para sinalizar os modos de predição, é importante associar o modo
mais provável de ser selecionado ao código de menor comprimento
[3]. Portanto, os preditores propostos para os 3 tamanhos de bloco
foram inseridos no lugar do modo DC (modo 0).
Esta abordagem de substituição oferece várias vantagens. Em
primeiro lugar, gera um bitstream totalmente decodificável. Nen-
huma informação adicional é necessária além dos dados codificados,
supondo que o decodificador tenha acesso ao mesmo modelo de
preditor que o codificador. Em segundo lugar, embora o preditor
pudesse ser introduzido como um modo intra completamente novo,
essa abordagem requer modificações nos esquemas de sinalização
do codificador e do decodificador. O método de substituição evita
essa complexidade adicional.
RESULTADOS
Está seção está dividida em duas subseções, experimentos prelim-
inares e resultados finais. Na primeira abordaremos os experimentos
realizados para encontrarmos a melhor configuração de treino e ar-
quitetura da rede. Já na segunda discutiremos os resultados obtidos
pela arquitetura final proposta.
3.1
Experimentos preliminares
A função das camadas de up-sample na rede servem apenas para
aumentar o tamanho dos mapas de características, o que também
pode ser feito com uma deconvolução com um tamanho de passo
de 2. Avaliamos a utilização desta estratégia, bem como a substitu-
ição da última camada deconvolucional pela up-sample+conv como
as suas camadas anteriores. Os resultados mostraram que ambas
estratégias incorrem em perdas de BD-rate de aproximadamente
1%.
Entre as diferentes arquiteturas experimentadas, avaliamos a
utilização de highway connections e uma arquitetura U-net com skip
connections na tentativa de manter melhor a informação estrutural
entre as camadas da rede. Esta abordagem produziu uma perda
de cerca de 1,5%, apesar de ter a sobrecarga de complexidade das
skip connections. Além disso, as ligações de saltos impossibilitam
a descodificação do espaço latente numa compressão end to end,
uma vez que as skip connections não podem ser transmitidas para o
lado do descodificador. Desta maneira, determinamos que não era
uma estratégia que merecesse uma investigação mais aprofundada.
Por último, a utilização de highway connections proporcionou gan-
hos de menos de 1% para algumas sequências, mantendo a mesma
eficiência em média, o que não justifica a sobrecarga de ligações.
Por fim, avaliamos a utilização de núcleos dilatados nas con-
voluções para explorar melhor as redundâncias angulares entre as
vistas. No entanto, concluímos que o contexto é demasiado pequeno
para ser proveitoso e as nossas experiências produziram uma perda
de BD-rate de aproximadamente 4%.
3.2
Resultados Finais
Depois de todos os experimentos, a rede neural mais eficiente foi
treinada separadamente para os 3 tamanhos de bloco presentes no
EVC: 32x32, 16x16 e 8x8. O impacto de adicionar os preditores para
WebMedia’2024, Juiz de Fora, Brazil
Machado et al.
Tabela 1: BD-rates de todos preditores treinados individual-
mente e em conjunto.
Preditor / Sequência
32x32
16x16
8x8
32x32,16x16,8x8
BD-rate
BD-rate
BD-rate
BD-rate
Ankylosaurus-&-Diplodocus-1
-22.17
-28.17
-24.98
-32.7
Bikes
-27.64
-36.97
-33.94
-41.88
Black-Fence
-6.59
-12.62
-10.32
-10.42
Ceiling-Light
-2.78
-13.15
-19.69
-21.89
Danger-de-Mort
-18.32
-30.38
-31.86
-33.67
Friends-1
-14.48
-22.4
-16.62
-28.13
Houses-&-Lake
-16.63
-22.3
-15.35
-22.15
Reeds
-10.04
-14.24
-11.36
-14.66
Rusty-Fence
-19.19
-30.81
-31.19
-36.06
Slab-&-Lake
-31.52
-40.52
-38.78
-40.16
Swans-2
-35.99
-39.43
-35.01
-46.34
Vespa
-30.47
-34.46
-31.31
-38.33
Média
-19.65
-27.12
-25.03
-30.53
cada um dos tamanhos de bloco e todos em conjunto podem ser
observados na Tabela 1.
Ao inserir apenas o preditor para blocos de tamanho 32x32 se
obteve um ganho médio na eficiência de codificação de -19.65%, já os
preditores de tamanho 16x16 e 8x8 obtiveram eficiências de -27.12%
e -25.03% respectivamente. Em vista que um bloco de tamanho
32x32 é constituído por 16 MIs (4x4 MIs de 8x8 pixels), conclui-se
que o padrão entre as diferentes MIs do bloco podem divergir o
suficientemente para que as redes não consigam prevêlas bem em
todos os casos. Já ao utilizarmos tamanhos de blocos menores de
16x16 que contém 4MIs, obtivemos uma boa capacidade de predição
e generalização. Já o preditor 8x8, embora possa parecer mais fácil
predizer apenas 1 MI a partir de outras 3 MIs por ser um espaço
de busca menor, a pequena vizinhança pode não apresentar in-
formações o suficiente para a rede poder detectar corretamente o
padrão do próximo bloco.
Para entendermos melhor o porque deste comportamento cabe
analisarmos os casos em que cada preditor atua melhor. Inicial-
mente, o preditor de tamanho 32x32 embora já atinja ganhos sig-
nificativos, supera apenas o preditor 8x8 nas sequências Swans 2
e Houses & Lake. Isso se deve ao fato de que ambas sequências
possuem vastas áreas homogêneas que se beneficiam de tamanhos
de blocos maiores. Em vista que blocos maiores providenciam bons
ganhos de codificação e a exploração de áreas homogêneas é de
grande importância no contexto de compressão de vídeos e imagens,
ainda assume-se como importante a utilização de um preditor para
este tamanho de bloco mesmo que com menor performance que
os demais. Ainda, observe que as sequências Black Fence e Ceiling
Light possuem melhoras de apenas -6.59% e -2.78% respectivamente.
A natureza destas imagens possuem uma grande diferença de pro-
fundidade entre objetos, o que causa uma distância maior entre os
pixels de uma MI e, consequentemente, as tornam mais difíceis de
predizer e menos adequadas para um tamanho de bloco maior.
Desta maneira, ao avaliarmos a eficiência do preditor para blocos
de tamanho 16x16 para estas sequências, podemos observar que a
eficiência de codificação foi quase duplicada para a sequência Black
Fence e aumentada em aproximadamente 5 vezes para a Ceiling
Light. Por fim, ao utilizarmos um preditor para apenas os tamanhos
de bloco 8x8 observou-se uma eficiência superior aos demais predi-
tores nas sequências Danger de Mort e Rusty Fence, que por sua vez
são LFs de grades com texturas complexas e constantes alternações
entre objetos próximos (a grade) e distantes (plano de fundo). Este
comportamento torna MIs próximas muito similares mas ao mesmo
tempo MIs mais distantes se tornam muito distintas, fazendo com
que a utilização destas para a predição não seja muito frutífera.
Para obtermos os benefícios de cada um dos 3 tamanhos de bloco
concomitantemente, estes foram inseridos em conjunto no codifi-
cador. Vale ressaltar que, ao estarem sendo utilizados ao mesmo
tempo, os preditores competem entre si, logo, seus ganhos não
são somados, providenciando um aumento de BD-rate de -5,5% e
permitindo que nossa proposta atinja um BD-rate total de -30.53%.
Observe também que, assim como esperado, ao complementarem
uns aos outros, os três preditores quando usados em conjunto pos-
suem melhores eficiências de codificação para todas as sequências.
CONCLUSÃO
Este artigo propôs reinterpretar a predição de imagens Light Field
densas como um problema de preenchimento de imagens e solucioná-
lo utilizando redes neurais convolucionais. Para atingir este obje-
tivo foram realizados diversos experimentos avaliando diferentes
arquiteturas e estratégias de treino.
Os experimentos mostraram que utilizar camadas de up-sample
seguida de convoluções regulares no decoder ao invés de con-
voluções transpostas com stride 2 se mostraram em torno de 1% mais
eficientes. Ao analisarmos diferentes arquiteturas, a arquitetura U-
net se mostrou em torno de 1.5% menos eficiente embora tenha um
adicional de complexidade das skip connections. Já a arquitetura em
Highway não apresentou ganhos o suficientes para justificar o custo
extra das suas conexões. Por fim, selecionar recortes aleatórios do
LF com rotações e um learning rate de 1 ∗10−4 se mostrou a config-
uração mais eficiente para atingir altas eficiências de compressão e
evitar o overfit das redes.
A rede proposta foi treinada e testada em três instâncias, uma
para cada um dos 3 tamanhos de blocos do codificador EVC. Este
codificador foi então utilizado para comprimir as sequências de
teste utilizando como preditor intra as redes separadamente e em
conjunto. O preditor para o tamanho de bloco de 16x16 se mostrou
o mais eficiente atingindo um BD-rate de -27.12%. Já ao utilizar os
preditores propostos para os 3 tamanhos de bloco atingiu-se um
ganho de -30.53% de BD-rate.
Como trabalhos futuros pretende-se avaliar a eficiência dos pred-
itores quando inseridos em codificadores de vídeo mais complexos
como HEVC e VVC. Ainda pode-se aplicar técnicas de convoluções
parciais como em [9, 12] ou técnicas de poda nas redes para diminuir
seus tamanhos e aperfeiçoar suas performances. Outras métricas
de Loss especializadas para codificação de vídeo também podem ser
propostas e avaliadas como, por exemplo, SATD (Soma Absoluta
das Transformadas das Diferenças). Por fim, ainda é possível propor
um esquema de codificação E2E (End to End) para LFs densos.

--- FIM DO ARQUIVO: 30147.txt ---

--- INÍCIO DO ARQUIVO: 30462-829-24910-1-10-20241001.txt ---
Diretrizes de apoio ao processo de construção de sistemas
conversacionais de recomendação acessíveis: um estudo com
usuários idosos
Lucas Araujo
padilha.lucas@usp.br
Universidade de São Paulo
São Carlos, SP, BR
Kamila Rodrigues
kamila.rios@icmc.usp.br
Universidade de São Paulo
São Carlos, SP, BR
Marcelo Manzato
mmanzato@icmc.usp.br
Universidade de São Paulo
São Carlos, SP, BR
CTD’2024, Juiz de Fora/MG, Brazil
Araujo, Rodrigues e Manzato
Figura 1: Usuário em avaliação de usabilidade
utilizando eletrodos para análise de emoções.
Meet para uma entrevista sobre as interações ocorridas. Em seguida,
o segundo estudo utilizou o protótipo de chatbot chamado "Bob",
projetado para identificar e corrigir problemas de desenvolvimento
e discutir resultados qualitativos. Os participantes recebiam o con-
tato do Bob e um código para iniciar o diálogo. Após se apresentar,
o Bob pedia ao usuário para compartilhar sua localização fixa, con-
duzindo a interação por smartphone, e então recomendava um
restaurante, perguntando se a recomendação era útil e poderia ser
aprimorada. No terceiro estudo, um novo teste foi realizado com 5
usuários para avaliar a usabilidade do protótipo Bob, focando na
eficácia do sistema em ajudar com dificuldades de uso, a eficiência
das recomendações em termos de surpresa, relevância e novidade,
e os sentimentos dos usuários. Na Figura 1, um participante está
utilizando o chatbot enquanto suas emoções estão sendo analisadas.
Os participantes, cinco estudantes da área de design e experiên-
cia do usuário da Universidade Federal do Ceará, tiveram suas
emoções avaliadas através de um questionário de autoavaliação
SAM e sensores de eletrocardiograma. Quarto, foram elaboradas
as primeiras diretrizes. No quinto estudo, quatro desenvolvedores
foram convidados a criar chatbots seguindo essas diretrizes, após o
que foi conduzida uma entrevista individual com cada um. Um dos
desenvolvedores estava no mesmo projeto que outro, resultando
em três chatbots distintos. Após as entrevistas, houve uma etapa de
grupo focal, onde os desenvolvedores discutiram as diretrizes, as
dificuldades comuns e propuseram melhorias. As entrevistas foram
gravadas, transcritas e as transcrições analisadas.
RESULTADOS
O experimento inicial, utilizando o método Mágico de Oz, revelou
um conjunto de requisitos funcionais e não funcionais essenciais
para a construção das diretrizes e o desenvolvimento do chatbot Bob.
O uso do protótipo Bob foi fundamental para identificar e corrigir
problemas de desenvolvimento, além de proporcionar uma com-
preensão mais profunda das preferências dos usuários em sistemas
de recomendação. O processo envolveu testes de usabilidade e avali-
ações emocionais, empregando ferramentas como o questionário
Self-Assessment Manikin (SAM) e sensores de eletrocardiograma.
Essas ferramentas ajudaram a compreender quais melhorias de-
veriam ser implementadas no chatbot, e, consequentemente, nas
diretrizes de desenvolvimento. A partir dessas observações, foram
elaboradas diretrizes que orientaram a implementação do chatbot
por desenvolvedores. Os chatbots resultantes foram então avaliados
através de entrevistas individuais e discutidos em grupos focais.
Essas etapas não apenas validaram as diretrizes propostas, mas
também revelaram pontos de melhoria para o aprimoramento dos
sistemas conversacionais de recomendação.
CONSIDERAÇÕES FINAIS
O estudo propôs diretrizes para orientar desenvolvedores sem ex-
periência em acessibilidade. A possibilidade de adaptar e incorporar
essas diretrizes em tais ambientes abre caminho para pesquisas
futuras em sistemas conversacionais. Pesquisas futuras podem ex-
plorar o uso de Large Language Models para melhorar sistemas de
recomendação conversacionais, aprimorando a compreensão da lin-
guagem natural, personalização das recomendações e a integração
de múltiplas modalidades de entrada.
AGRADECIMENTOS
Os autores agradecem o apoio financeiro da Coordenação de Aper-
feiçoamento de Pessoal de Nível Superior – Brasil (CAPES) – Código
de Financiamento 001; da Fundação de Amparo à Pesquisa do Estado
de São Paulo (FAPESP) e do Conselho Nacional de Desenvolvimento
Científico e Tecnológico (CNPq).

--- FIM DO ARQUIVO: 30462-829-24910-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30463-829-24911-1-10-20241001.txt ---
Explorando Formas de Calibração e Redução do Viés de
Popularidade em Sistemas de Recomendação
Rodrigo F. Souza
Universidade de São Paulo
São Carlos-SP, Brasil
rodrigofsouza@usp.br
Marcelo G. Manzato
Universidade de São Paulo
São Carlos-SP, Brasil
mmanzato@icmc.usp.br
CTD’2024, Juiz de Fora/MG, Brazil
Souza & Manzato
Tabela 1: Comparação das abordagens no Movie Lens 20M. Os melhores valores estão em negrito.
Algoritmo
MAP
MRMC Gên.
MRMC Pop.
F1 Score
LTC
Δ𝐺𝐴𝑃𝐵𝐵
Δ𝐺𝐴𝑃𝑁
Δ𝐺𝐴𝑃𝐷
RMSE
BPR Modificado
0.001
0.45
0.33
0.596
0.46
-0.865
-0.060
-0.693
0.370
SVD++ + Calibração personalizada [9]
0.001
0.41
0.69
0.407
0.42
-0.992
-0.970
-0.985
0.568
SVD++ + Calibração dupla [10]
0.001
0.26
0.68
0.447
0.38
-0.990
-0.969
-0.983
0.566
NMF + Calibração personalizada [9]
0.077
0.35
0.11
0.752
0.01
-0.171
0.238
-0.212
0.121
NMF + Calibração dupla [10]
0.081
0.23
0.17
0.799
0.01
-0.379
0.046
-0.152
0.137
VAE + Calibração personalizada [9]
0.079
0.47
0.35
0.584
0.14
0.850
1.553
1.386
0.750
VAE + Calibração dupla [10]
0.055
0.27
0.28
0.725
0.11
0.359
1.636
0.981
0.647
explorar outras abordagens em pós-processamento e mecan-
ismos de nudges.
• Redução do viés de popularidade através de nudges, recomen-
dando itens diferentes do perfil do usuário.
• Calibração em pós-processamento, validada por experimento
com usuários, demonstrando a relevância dos itens calibra-
dos.
• Calibração personalizada, usando uma estratégia de chavea-
mento para recomendações baseadas na popularidade ou no
gênero dos itens.
• Calibração dupla, uma evolução da personalizada, empil-
hando calibração por gênero seguida de popularidade.
A Tabela 1 resume os resultados obtidos na base de dados Movie
Lens 20M com as abordagens propostas neste trabalho. Cada abor-
dagem tem vantagens específicas, compatíveis com as necessidades
do sistema.
A abordagem BPR modificado obteve altos valores para a métrica
LTC, indicada para diversidade. Para precisão, a calibração dupla
combinada com NMF é a melhor. Se o objetivo for justiça em popu-
laridade e gêneros, a calibração dupla com NMF também é recomen-
dada. Para reduzir o viés de popularidade em diferentes grupos, a
calibração personalizada combinada com NMF é mais eficaz.
AGRADECIMENTOS
Os autores agradecem CAPES, CNPq e FAPESP pelo apoio finan-
ceiro.

--- FIM DO ARQUIVO: 30463-829-24911-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30464-829-24912-1-10-20241001.txt ---
Furando a Bolha: Nudges Digitais em Sistemas de Recomendação
Gabrielle Alves
gaah.alves@usp.br
Universidade de São Paulo
São Carlos, SP
Marcelo G. Manzato
Universidade de São Paulo
São Carlos, SP
CTD’2024, Juiz de Fora/MG, Brazil
Alves et al.
Figura 2: Fluxo do experimento
RESULTADOS E DISCUSSÃO
4.1
Análise Qualitativa e Quantitativa
A análise qualitativa e quantitativa demonstrou que os nudges
aumentam a diversidade nas recomendações, sem comprometer a
qualidade do sistema. Utilizando questionários e a escala Likert,
avaliamos a percepção dos usuários sobre satisfação, diversidade e
utilidade. Os resultados mostram que os nudges incentivam maior
interação com conteúdos diversos, reduzindo vieses (𝑝< 0.001).
Controle
Tratamento
Total
Pelo menos um item fora do perfil
407 (74,82 %)
440 (84,62 %)
Nenhum item fora do perfil
137 (25,18 %)
80 (15,38 %)
Total
1.064
Tabela 1: Número de usuários que colocaram ao menos um
item fora do perfil na lista de leitura.
A análise quantitativa confirmou que os nudges aumentam a
diversidade nas interacoesrecomendações. Utilizando o método de
análise de clickthrough de Joachims [5], comparou-se as taxas de
cliques em listas de recomendações diversificadas e não diversifi-
cadas, demonstrando que os nudges incentivaram maior interação
com conteúdos diversos. A análise estatística mostrou que os nudges
reduziram vieses e bolhas de filtro, promovendo uma experiência
de usuário mais equitativa e diversificada.
4.2
Aplicação de um Framework para Avaliação
da Qualidade do Sistema
Para medir a qualidade do sistema em relação à experiência do
usuário e à relevância dos itens recomendados, utilizou-se o frame-
work ResQue [7]. Este framework mede a qualidade dos itens re-
comendados, a usabilidade do sistema, a utilidade, a interface e
a interação do sistema, a satisfação do usuário com o sistema e
a influência desses fatores no comportamento do usuário. A apli-
cação do ResQue permitiu uma avaliação sistemática da qualidade
do sistema de recomendação, identificando áreas de melhoria e
fornecendo insights sobre a percepção dos usuários em relação à
diversidade e satisfação.
4.3
Aplicação de Nudges em Sistemas de
Recomendação
A pesquisa investigou como diferentes mecanismos de nudge po-
dem aumentar as interações dos usuários com conteúdos diver-
sos, sem afetar significativamente a satisfação. Observou-se que
os nudges são eficazes em aumentar a diversidade nas interações
com sistemas de recomendação. Os experimentos demonstraram
que usuários expostos a recomendações com nudges interagem
mais frequentemente com conteúdos diversos em comparação com
recomendações sem nudges. Esses resultados sugerem que é pos-
sível aumentar a diversidade nas recomendações sem sacrificar a
satisfação do usuário.
4.4
Conclusão
A pesquisa demonstrou que a aplicação de nudges em sistemas de
recomendação pode efetivamente mitigar vieses de popularidade
e exposição, promovendo uma experiência de usuário mais equi-
tativa e diversificada. A análise estatística revelou que os nudges
incentivaram interações mais frequentes com conteúdos diversos e
menos populares, contribuindo para a redução das bolhas de filtro
e aumentando a justiça nas recomendações.
4.5
Propostas de Melhorias para Sistemas de
Recomendação
Com base nos resultados, sugerimos as seguintes melhorias para os
sistemas de recomendação:
• Integração de Nudges: Incorporar nudges nas interfaces
de recomendação para incentivar a exploração de conteúdos
diversos.
• Foco na Experiência do Usuário: Refinar a interface e a
interação do sistema com base no feedback dos usuários para
aumentar satisfação e confiança.
• Avaliação Contínua da Qualidade: Utilizar frameworks
como o ResQue para monitorar e avaliar continuamente a
qualidade do sistema, garantindo recomendações relevantes
e diversas.
• Personalização da Diversidade: Desenvolver métodos que
personalizem a diversidade nas recomendações conforme as
preferências dos usuários.
4.6
Limitações da pesquisa
Nudges digitais ampliam a interação com itens fora do perfil, criando
experiências diversificadas. Contudo, a validade externa pode ser
limitada devido ao foco em livros e ao ambiente experimental, o
que pode não refletir totalmente contextos reais.
AGRADECIMENTOS
Os autores agradecem CAPES, CNPq e FAPESP pelo apoio finan-
ceiro.

--- FIM DO ARQUIVO: 30464-829-24912-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30465-829-24913-1-10-20241001.txt ---
Mitigando os Limites das Métricas Atuais de Avaliação de
Estratégias de Modelagem de Tópicos
Antônio Pereira de Souza
Júnior
antonio.pereira@aluno.ufsj.edu.br
Universidade Federal de São João Del
Rei – UFSJ
Minas Gerais, Brasil
Felipe Augusto Resende Viegas
frviegas@dcc.ufmg.br
Universidade Federal de São João Del
Rei – UFSJ
Minas Gerais, Brasil
Leonardo Chaves Dutra da
Rocha
lcrocha@ufsj.edu.br
Universidade Federal de São João Del
Rei – UFSJ
Minas Gerais, Brasil
CTD’2024, Juiz de Fora/MG, Brazil
Pereira A. et al.
cada documento é conhecido (i.e., ACM, 20News e WOS). Compa-
ramos a qualidade dos tópicos gerados por quatro das principais
estratégias de MT (i.e., LDA, NMF, CluWords e BERTopic) com
a estrutura de tópicos prévia de cada coleção. Nossos resultados
mostram que, apesar da importância das métricas de avaliação atu-
ais, estas não conseguiram captar alguns aspectos idiossincráticos
importantes da MT, indicando a necessidade de propor novas mé-
tricas que considerem, por exemplo, a estrutura e organização dos
documentos que compõem os tópicos.
2.2
Objetivo
Mitigar as limitações das métricas de avaliação de MT atuais pro-
pondo/adaptando e avaliando novas métricas.
2.3
Solução
Propomos adaptar métricas comumente utilizadas para avaliar al-
goritmos de clusterização [3], uma vez que existem semelhanças
significativas entre as estratégias de MT e de clusterização, como
sua natureza não-supervisionada e o objetivo de agrupar elemen-
tos semelhantes: Silhouette Score, Calinski-Harabasz e BetaCV. No
entanto, isto implica expandir o espaço de análise por meio da in-
clusão de um novo conjunto de métricas. Assim, propomos também
a consolidação das várias métricas, que consideram tanto a quali-
dade das palavras que compõem os tópicos (tradicionais) como a
estrutura organizacional dos documentos, num resultado unificado
multiperspectiva, utilizando metodologia Multiattribute Utility The-
ory (MAUT) [1] (MAUT). Trata-se de uma metodologia comum em
Teoria de Jogos para tomada de decisão, que visa auxiliar na escolha
de uma alternativa entre várias opções, considerando múltiplos
atributos e preferências conflitantes. A MAUT se fundamenta em
três conceitos principais: atributos, funções de utilidade e pesos. Na
proposta utilizada neste trabalho, os atributos correspondem aos va-
lores de todas as métricas utilizadas (i.e., NPMI, TF-IDF Coherence,
WEP, Silhouette Score, Calinski-Harabasz e Beta CV), a função de
utilidade é a min-max [4] e os pesos, que representam a importância
relativa de cada atributo, foram iguais para todas as métricas. Com
essa adaptação, os tópicos gerados por cada uma das técnicas de
MT podem ser avaliados sob diferente perspectivas.
2.4
Avaliação
A avaliação experimental proposta considerou as mesmas três co-
leções de dados (i.e., ACM, 20News e WOS), primeiramente compa-
rando os tópicos gerados pelas mesmas estratégias de MT (i.e., LDA,
NMF, CluWords e BERTopic), com a estrutura prévia de classes
dos documentos de cada coleção, o ground truth, considerando a
três métricas adaptadas do contexto de clustering (i.e., Silhouette
Score, Calinski-Harabasz e BetaCV). Analisando os resultados ob-
tidos, conforme esperado, a estrutura do ground truth demonstrou
o melhor desempenho nessas métricas, com valores superiores aos
encontrados nas estratégias de MT. Ao contrário das métricas tradi-
cionais de MT que apresentaram resultados contraditórios, quando
os resultados do ground truth eram comparados com os tópicos
gerados pelas técnicas de MT, as métricas de clusterização, sob esse
aspecto, foram bem consistentes, evidenciando sua eficácia na dis-
tinção dos resultados dos algoritmos de MT e do ground truth. Por
outro lado, para ambos os conjuntos de métricas, tradicionais de MT
e de clusterização, não houve um consenso que apontasse qual a
melhor das estratégias de MT para todas as coleções. A partir dessa
constatação, todos os algoritmos de MT foram novamente avaliados
na mesma configuração experimental, dessa vez considerando a
MAUT. Especificamente, nossos resultados permitiram observar os
avanços semânticos gerados pelo uso de word embeddings em algu-
mas estratégias de MT, além da solidez e consistência da construção
de tópicos por meio de estratégias de fatorização de matrizes.
AVANÇO NO ESTADO-DA-ARTE
O principal avanço da dissertação está na identificação das limi-
tações das métricas atuais de avaliação de estratégias de MT que
impactam diretamente na evolução dessas estratégias. Apresen-
tamos uma proposta inicial de novas métricas e também de uma
metodologia de avaliação multiperspectiva por meio da MAUT
que abrem caminho para que futuras pesquisas desenvolvam e re-
finam ainda mais o processo de avaliação de estratégias de MT,
garantindo uma comparação justa, permitindo melhorar a geração
e organização de tópicos em grandes conjuntos de dados textuais.
CONCLUSÃO
Neste trabalho, voltamos nossa atenção para um importante desa-
fio no contexto da MT, que é a avaliação de tópicos gerados pelas
diversas estratégias. As métricas tradicionais capturam a qualidade
dos tópicos avaliando estritamente as palavras que construíram
os tópicos, sintaticamente (ou seja, NPMI, TF-IDF Coherence) ou
semanticamente (ou seja, WEP). Este trabalho demonstrou empiri-
camente, através de experimentos extensivos, que o conjunto atual
de métricas negligência um importante aspecto que geralmente é
esperado por estratégias de MT, a capacidade de organizar semanti-
camente o espaço de documentos em grupos significativos.
Foi proposto então, a utilização de métricas empregadas na ava-
liação de estratégias de clusterização, que quantificam a qualidade
da estrutura e organização dos documentos que compõem os tópi-
cos. Foi adotado também uma abordagem que tem o potencial de
integrar ambos os conjuntos de métricas, MAUT, resultando em
uma avaliação unificada e mais abrangente. Essa estratégia permitiu
avaliar melhor as várias estratégias de MT presentes na literatura.
Como trabalho futuro, pretendemos empregar a metodologia
de avaliação desenvolvida para contrastar os algoritmos de MT
utilizados aqui com a geração de tópicos por meio de modelos Large
Language Model (LLM). Nosso objetivo é estender as análises que
tratam dos tópicos induzidos pelas estratégias de MT. Considerando
outras coleções e sistemas de classificação, bem como estender tam-
bém nossa avaliação para estratégias de tópicos hierárquicas, con-
siderando avaliação de Modelagem de Tópicos Hierárquica (MTH).
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Fapemig e AWS

--- FIM DO ARQUIVO: 30465-829-24913-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30466-829-24914-1-10-20241001.txt ---
Autoria Imersiva em Realidade Virtual para Aplicações
Mulsemídia Interativas
Flávio Miranda de Farias
flavio.farias@midiacom.uff.br
Laboratório MídiaCom, Instituto de Computação
Universidade Federal Fluminense
Niterói, RJ, Brasil
Débora C. Muchaluat-Saade
debora@midiacom.uff.br
Laboratório MídiaCom, Instituto de Computação
Universidade Federal Fluminense
Niterói, RJ, Brasil
CTD’2024, Juiz de Fora/MG, Brazil
Farias et al.
Figure 2: Benchmark UEQ comparativo entre implemen-
tações do AMUSEVR com HMD Oculus e Vive.
positiva da ferramenta. Os participantes destacaram a facilidade de
uso e a capacidade de criar conteúdos imersivos e interativos de
maneira intuitiva, confirmando a eficácia do AMUSEVR como uma
ferramenta de autoria para ambientes de realidade virtual.
Na escala SUS, o AMUSEVR alcançou uma pontuação média
de 82,25, indicando uma excelente usabilidade, visto que qualquer
valor acima de 68 é considerado acima da média. Os participantes
relataram que a interface do AMUSEVR é intuitiva e fácil de usar,
facilitando a criação de conteúdos imersivos sem a necessidade de
conhecimentos técnicos avançados. No UEQ, que avalia diversas
dimensões da experiência do usuário, como atratividade, clareza,
eficiência, precisão, estimulação e novidade, o AMUSEVR recebeu
avaliações positivas em todas as dimensões. Os usuários destacaram
a atratividade visual e a clareza da ferramenta, bem como a eficiência
e precisão na manipulação direta dos objetos no espaço 3D. No
benchmark do UEQ (Figura 2), o AMUSEVR foi classificado como
excelente, sugerindo que a ferramenta não só atende, mas supera as
expectativas dos usuários em termos de qualidade de experiência.
CONCLUSÃO
O ambiente de autoria fornece uma imersão ao autor em realidade
virtual, com liberdade criativa e a disponibilidade de usar uma
ampla variedade de tipos de objetos de mídia e efeitos sensoriais.
Os diversos experimentos realizados de usabilidade e experiência
de usuário com dezenas de participantes apontam a viabilidade da
mesma, pois todos os usuários concluíram suas tarefas de criação em
todas as atividades propostas em todos os três experimentos, no qual
precisavam produziram uma apresentação mulsemídia interativa
360 graus para um aplicativo turístico sem necessidade de auxílio
adicional.
Portanto, através dos experimentos conseguimos responder à
pergunta principal de pesquisa desta tese: "Qual é a experiência
do autor na criação de aplicações multissensoriais interativas em
360 graus usando um ambiente de autoria imersivo?", indicando
grande potencial da abordagem imersiva para facilitar a criação de
projetos multissensoriais interativos com conteúdo 360 graus na
prática, já que a experiência relatada pelos usuários que realizaram
os experimentos foi bastante positiva.
PUBLICAÇÕES
Durante o doutorado, diversos trabalhos e publicações foram real-
izados como visto na Tabela 1, além de dois artigos em processo de
submissão a journals.
Table 1: Tabela com trabalhos já publicados.
Ano
Publicação
2020 Memo-VR: Exercício Cognitivo para Idosos Utilizando Real-
idade Virtual e Interface com as Mãos [3].
2021 AMUSEVR: A Virtual Reality Authoring Environment for
Immersive Mulsemedia Applications [1].
2022 An Immersive Memory Game as a Cognitive Exercise for El-
derly Users [4].
2022 Immersive Authoring of 360 Degree Interactive Applications
[2]
2022 AMUSEVR - Authoring 360° Multimedia and Sensory Ef-
fects in VR, software registrado no INPI - registro número
512022003596-0, data de registro: 24/12/2022.
O trabalho publicado no SBCAS 2020 [3] serviu de base para o
aprendizado e para adquirir experiência em desenvolvimento de
aplicações em Realidade Virtual. Em 2022, este trabalho foi atual-
izado e publicado [4] em workshop do ACM IMX.
Em 2021, foram publicados os resultados preliminares da primeira
implementação e avaliação do AMUSEVR com HMD Vive em [1],
em workshop do ACM IMX.
Em 2022, foi realizada uma publicação no journal IEEE Access
[2], no qual apresentou uma atualização da pesquisa, incluindo
experimentos com mais usuários e a metodologia GQM para avali-
ação.
Em 2022, foi realizado o registro de software da aplicação AMU-
SEVR Oculos com título “AMUSEVR - Authoring 360° MUltimedia
and Sensory Effects in VR” no INPI (Instituto Nacional da Propriedade
Industrial).
Atualmente, estão em processo de submissão dois artigos de
journal, sendo um relacionado ao resultado dos experimentos da
Linguagem MultiSEL para AMUSEVR e o outro mais completo sobre
o AMUSEVR Oculus, apresentando dados sobre efeitos sensoriais e
utilização do MultiSEL.

--- FIM DO ARQUIVO: 30466-829-24914-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30467-829-24915-1-10-20241001.txt ---
Depression Symptoms Identification through Social Media Data
Applying Design Science Research to Develop a Classification Model
Silas Lima Filho
silaslfilho@ppgi.ufrj.br
Instituto de Computação, UFRJ
Cidade Universitária, RJ, Brasil
Monica Ferreira da Silva
mfsilvamail@gmail.com
P. Pós-Graduação Informática, UFRJ
Cidade Universitária, RJ, Brasil
Jonice Oliveira
jonice@ic.ufrj.br
Instituto de Computação, UFRJ
Cidade Universitária, RJ, Brasil
791.
A seguir relatamos parte do estado da arte mapeado na Seção 2,
seguida pela Seção 3 que descreve a abordagem metodológica apli-
cada. Em sequência, a Seção 4 dividida em subseções que descrevem
os resultados dos esforços da pesquisa bem como discussões sobre
tais resultados, seguida da Seção 5 que conclui o artigo.
ESTADO DA ARTE
Para mapear o estado da arte, realizamos uma revisão sistemática
da literatura (RSL) sobre a detecção de sintomas de depressão nas
mídias sociais. A RSL foi realizada em duas etapas: primeiro, nos
repositórios de computação (ACM e IEEE) de 2013 a 2018, total-
izando 47 trabalhos; segundo, nos repositórios de saúde (Pubmed,
APA e Scopus) de 2019 a 2021. O protocolo de RSL foi atualizado
entre as duas etapas, mantendo algumas características em comum.
O aumento das publicações sobre a identificação de usuários de-
pressivos nas mídias sociais, especialmente em 2021, reflete a cres-
cente atenção no campo. Alguns estudos usaram modelos de apren-
dizado profundo, como transformers, enquanto outros utilizaram
abordagens de aprendizado de máquina. Alguns também empre-
garam questionários psicológicos para testar ou filtrar usuários e
validar dados com profissionais de saúde. No entanto, nem todos os
trabalhos abordaram aspectos éticos ou incluíram discussões corre-
latas. Identificamos uma lacuna na utilização de técnicas de apren-
dizado para alavancar métricas do sistema LIWC. Nossa hipótese é
a de melhor classificação pela combinação de modelos individuais
e suas melhores características em um meta-modelo. Uma difer-
ença ao comparar as duas RSLs é em relação ao envolvimento de
profissionais de saúde na construção do modelo. Embora o sistema
LIWC seja amplamente utilizado, apenas um estudo usa métricas
gerais que abstraem características textuais como tom emocional e
autenticidade.
METODOLOGIA DE PESQUISA
Utilizando a Design Science Research (DSR), obtivemos uma con-
tribuição significativa para a área de pesquisa. A DSR é focada na
criação de artefatos para resolver problemas específicos, além de
preservar a relevância na união entre teoria e prática. Nesta tese, as
conjecturas teóricas são fundamentadas em teorias computacionais
CTD’2024, Juiz de Fora/MG, Brazil
Lima Filho et al.
e psicológicas. A teoria da computação cobre redes sociais, mídias
sociais e algoritmos de aprendizado de máquina, enquanto a psi-
cologia fornece definições de saúde mental, transtornos mentais e
distúrbios depressivos. O artefato e as conjecturas teóricas da DSR
devem interagir para validar e orientar sua construção. O estado
da técnica envolve o processo de design do artefato, que, após ser
construído, deve ser validado em um contexto específico.
MÉTODO DE IDENTIFICAÇÃO DE
SINTOMAS DE DEPRESSÃO
4.1
Interação com Comunidade de Saúde
Um estudo com profissionais da saúde foi realizado após a RSL para
entender os métodos de tratamento e identificação do transtorno
depressivo utilizados por psicólogos, além de validar as features
identificadas. Os desafios incluíram a contatação de profissionais
e a explicação do vocabulário de computação (análises, tipos de
dados, features) nas entrevistas e questionários.
O estudo ocorreu em duas etapas. Primeiro, entrevistamos três
psicólogos para entender suas abordagens na identificação de sin-
tomas e determinação do tratamento, além de validar as features
levantadas na RSL. Em seguida, distribuímos um questionário em
várias instituições de saúde para coletar dados quantitativos so-
bre a relevância das diferentes features obtidas em mídias sociais.
Quarenta e nove participantes classificaram as features, que foram
organizadas em quatro grupos principais: discurso, perfil do usuário,
interação e grupos.
Nesse estudo, o grupo de features discursivas foi o mais relevante
para identificar a depressão. As features mais importantes de tal
grupo foram: Autorreferência ao estado físico ou mental e Referência
explícita a alguma doença ou assunto correlato [5].
4.2
Análise de Discurso de Comunidades
Como as features discursivas foram as mais relevantes para os
profissionais e exploradas na RSL, analisamos textos de mídias so-
ciais. Utilizamos a ferramenta Crowdtangle para coletar dados de
grupos público1. A análise linguística foi realizada com o léxico
LIWC. O módulo principal de análise do LIWC quantifica quatro
aspectos: Pensamento Analítico, que mede o uso de palavras in-
dicando padrões formais e lógicos; Influência, que avalia o status
social e confiança na escrita; Autenticidade, que reflete a espon-
taneidade do discurso; e Tom Emocional, onde scores altos indicam
tom positivo e scores baixos, tom negativo.
Analisamos a frequência das palavras nas postagens de 4 comu-
nidades em inglês (835 publicações) e 7 comunidades em português
(1945 publicações), coletadas em setembro de 2022. As palavras
anxiety, depression, people e life foram as mais usadas em ambas as
comunidades, variando apenas na ordem.
Os resultados mostram comportamentos similares nas comu-
nidades em ambos os idiomas. Há uma concentração de scores
baixos em Pensamento Analítico, indicando postagens com baixo
teor formal. Na métrica de Influência, os valores também são baixos,
indicando influência social moderada. Autenticidade possui scores
altos, sugerindo veracidade no vocabulário. A maioria das postagens
tem scores baixos em Tom Emocional, indicando discurso negativo.
1www.crowdtangle.com/resources[acessado em 11-08-2024]
4.3
Modelo de Classificação
Nosso modelo de classificação foi treinado com dados científicos
[4], baseado no modelo de [8], que usa sentimentos de postagens
em mídias sociais como pesos em aprendizado ativo. Analisamos
postagens de três subfóruns: depression, fitness e divorce, rotuladas
como não depressivas (0) e depressivas (1). As postagens foram
avaliadas em Emotional Tone, Analytical Thinking, Clout e Authentic.
Usamos o modelo BERT para vetorizar as postagens com a
biblioteca sentence-transformers [6]. Realizamos o produto de Hada-
mard entre esses vetores e os vetores LIWC para capturar a influên-
cia combinada do conteúdo das postagens e das métricas LIWC.
Após construir os modelos, reaplicamos o modelo para classificar
dados de fóruns sobre depressão do Facebook buscando identi-
ficar padrões heurísticos. Observamos que mensagens depressivas
geralmente tinham baixos escores de influência social , enquanto
predições negativas apresentavam altos escores de pensamento
analítico , como mensagens de boas-vindas e apoio.
De 835 registros, o modelo classificou 23 posts como não de-
pressivos. Em comunidades em português, de 1760 mensagens, 123
foram classificadas como não depressivas, muitas buscando orien-
tação ou discutindo luto e tristeza. Apesar de mais erros devido à
tradução, os resultados foram similares, com mensagens frequente-
mente buscando orientação profissional ou discutindo luto e perda.
CONCLUSÃO
Este artigo apresentou as principais contribuições da pesquisa de
doutorado, destacando a interação com a comunidade de saúde e as
análises de dados de mídias sociais para explorar padrões de saúde
mental. Pesquisas futuras buscarão dados de alta qualidade sobre
populações com depressão ou usuários diagnosticados clinicamente.
A extração de heurísticas de discursos depressivos resultou em um
modelo de classificação para identificar indicadores de saúde mental em ambientes digitais.

--- FIM DO ARQUIVO: 30467-829-24915-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30468-829-24916-1-10-20241001.txt ---
Um Estudo Aprofundado sobre Grupos Semânticos de Palavras -
CluWords - em tarefas de PLN
Felipe Viegas
frviegas@dcc.ufmg.br
UFMG - Minas Gerais - Brasil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ - Minas Gerais - Brasil
Marcos André Gonçalves
mgoncalv@dcc.ufmg.br
UFMG - Minas Gerais - Brasil
CTD’2024, Juiz de Fora/MG, Brazil
Viegas F. et al.
acoplando aos clusters filtros para o tratamento de ruído e para
ponderação dos termos de maneira adequada, de forma a construir
representações de palavras enriquecidas e adaptáveis à tarefa alvo.
2.2
Objetivo
O principal objetivo da tese é fornecer evidências para “comprovar”
a hipótese de que a Cluwords é uma alternativa melhor (mais eficaz,
eficiente e interpretável) para representar textos, especialmente em
conjuntos de dados pequenos, mais “ruidosos” e que sofrem com a
escassez de informações, pois capturam relações semânticas junto
com informações frequentistas, cruciais para tarefas de PLN.
As principais questões de pesquisa, derivadas da hipótese foram:
(i) As CluWords podem ser efetivamente exploradas para avançar
o estado-da-arte em tarefas de PLN e recuperação de informações?
(ii) Mecanismos de filtragem e ponderação específicos para cer-
tas tarefas seriam capazes de efetivamente adaptar as CluWords
a diferentes cenários de PNL?. Questões específicas de pesquisa,
considerando três cenários de aplicação, incluem:
• Modelagem de Tópicos (MT): (i) Podemos explorar as Clu-
Words para melhorar a representação de documentos para
modelagem de tópicos? (ii) As CluWords podem adicionar
mais informações aos modelos hierárquicos de modelagem
de tópicos em níveis mais profundos da hierarquia?
• Análise de Sentimentos (AS): (i) As CluWords podem ser us-
adas para superar problemas de falta de informação em tare-
fas de análise de sentimento? (ii) A polaridade/intensidade
e a classe gramatical (PoS) podem ser usadas para filtrar
palavras das CluWords para análise de sentimento?
2.3
Avaliação
As análises experimentais forneceram evidências para responder
positivamente à primeira questão de pesquisa no contexto da MT.
Por meio de experimentos com 12 conjuntos de dados e oito linhas
de base, confirmou-se que as CluWords constroem tópicos melhores
e enriquecem significativamente as representações de documentos.
Em relação à MT Hierárquica (MTH), a tese apresenta um novo
método não-probabilístico denominado CluHTM, que explora a in-
formação semântica global fornecida pela representação CluWords
e uma aplicação original de uma medida de estabilidade para definir
a “forma” da hierarquia. São apresentadas duas variantes do método
CluHTM, uma que explora embeddings estáticos (f-CluHTM) e
outra que usa dinâmicos (c-CluHTM). Ambas as variantes CluHTM
se destacaram, sendo cerca de duas vezes mais eficazes que as linhas
de base do estado-da-arte. Até o presente momento (2024) nenhuma
abordagem conhecida superou nossos resultados.
A tese ainda propôs, como contribuição adicional, novas métri-
cas para avaliar métodos MTH. As métricas de qualidade propostas
avaliam aspectos relacionados à consistência topológica e à es-
trutura semântica hierárquica que são importantes para métodos
hierárquicos. Esses são aspectos diferentes e complementares daque-
les capturados por métricas tradicionais de MT, como NPMI e Co-
erência. Em outras palavras, as novas métricas de qualidade de
tópicos capturam comportamentos distintos dos tópicos construí-
dos, incluindo duplicidade e consistência semântica. Os resultados
experimentais mostram que novos métodos c-CluHTM e f-CluHTM
apresentam os melhores resultados na construção de uma estrutura
hierárquica quando comparados com o estado-da-arte.
Fazendo a transição para o domínio da Análise de Sentimentos
(AS), em relação às questões de pesquisa RQ2.i e 2.ii, a tese primeira-
mente fornece hipóteses formais apoiadas por fortes evidências
empíricas e experimentais que demonstram o potencial de explo-
ração de CluWords em AS. Além disso, é proposta uma técnica
nova, simples, mas muito eficaz, para expandir léxicos humana-
mente construídos. O método proposto pode usar a representação
geral fornecida por embeddings de palavras e seus relacionamentos
(capturados por simples cálculos de distância) para produzir léxi-
cos de alta cobertura que melhoram significativamente a precisão
dos métodos de AS. Complementarmente apresentamos uma nova
instanciação da CluWords para SA – CluSent – que explora a ex-
pansão semântica e aborda problemas de escassez de informação
e ruído. A representação CluSent é construída por um pipeline
dinâmico de instanciações para construir representações de doc-
umentos adaptadas às características dos conjuntos de dados. A
avaliação experimental revela que o CluSent, por meio de filtragem
baseada em Part-of-Speech e ponderação de sentimento (i.e., polar-
idade), é tão eficaz quanto os melhores métodos Transformers de
última geração para a tarefa de AS.
AVANÇO NO ESTADO-DA-ARTE
A solução proposta para Modelagem de Tópicos (MT) e MT Hi-
erárquica (MTH) são o estado-da-arte, superando estratégias efi-
cazes tais como, BERTopic [2]. Até o presente momento os resul-
tados reportados na tese não foram superados por nenhuma outra
estratégia da literatura. A solução de expansão de léxicos também
superou estratégias não supervisionadas, tais como VADER [3]. Em
AS o CluSent se equiparou com estratégias complexas e caras, tais
como BERT [1], sendo amplamente mais explicável e eficiente.
CONTRIBUIÇÕES
O trabalho resultou na publicação de oito produções diretamente
da tese, sendo 4 em conferências (CIKM - A1; WSDM - A1, ACL -
A1; e WebMedia - A4) e quatro em periódicos (Information Systems
-A2, Scientometrics -A1, Computational Linguistic - A1; e Journal
on Interactive Systems - B1). Outros 16 artigos foram publicados
tendo o doutorando como coautor, em temas relacionados à sua tese
(nove A1, um A2, um A3 e cinco A4). A tese de doutorado recebeu
dois Google Latin America Research Awards (LARA) – oitava e nona
edições.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Google, Fapemig e
AWS

--- FIM DO ARQUIVO: 30468-829-24916-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30469-829-24917-2-10-20241001.txt ---
Paralelização de Tarefas de Codificação de Vídeo VVC utilizando
GPGPUs
Iago Storch
icstorch@inf.ufrgs.br
Universidade Federal do Rio Grande
do Sul (UFRGS)
Porto Alegre, Brasil
Daniel Palomino
dpalomino@inf.ufpel.edu.br
Universidade Federal de Pelotas
(UFPel)
Pelotas, Brasil
Sergio Bampi
bampi@inf.ufrgs.br
Universidade Federal do Rio Grande
do Sul (UFRGS)
Porto Alegre, Brasil
1. Partindo do nível de abstração mais alto
em direção ao mais baixo, as contribuições são divididas em Geren-
ciamento de Carga de Trabalho, Paralelização de Algoritmo
e Co-otimização de Algoritmo e Recursos da GPU.
As contribuições de Gerenciamento de Carga de Trabalho
se concentram em decisões de alto nível que não interferem dire-
tamente na implementação. Isso inclui gerenciar a codificação de
Paralelização de Algoritmo
Co-otimização de Algoritmo e Recursos da GPU
Alto nível de abstração
Baixo nível de abstração
...
...
...
...
...
...
...
...
...
...
Gerenciamento de Carga de Trabalho
...
Figura 1: Metodologia Integrada de Aceleração com GPUs
para Aplicações de Codificação de Vídeo (MIAG-CV).
CTD’2024, Juiz de Fora/MG, Brazil
Iago Storch, Daniel Palomino, and Sergio Bampi
blocos com múltiplas dimensões e posições, mapear as tarefas de
codificação em kernels para fornecer encapsulamento, e gerenciar a
execução e comunicação de kernels para maximizar a utilização dos
recursos. Esta tese de doutorado também propõe uma estratégia
chamada unidades de carga de trabalho para suportar as estruturas
de particionamento flexíveis dos padrões de codificação de vídeo
modernos. Essa estratégia consiste em enumerar todas as possibili-
dades de partição permitidas para uma ferramenta de codificação e,
em seguida, para cada bloco de 128×128 pixels da imagem, organizar
essas partições em uma estrutura semelhante a um mosaico (uma
unidade de carga de trabalho) que mantém blocos com a mesma
dimensão na mesma unidade para manter a regularidade. Cada re-
gião 128×128 gera várias unidades de carga de trabalho para cobrir
todas as possibilidades de particionamento, e todas as unidades
de carga de trabalho são processadas simultaneamente. Essa estra-
tégia maximiza a ocupação enquanto mantém uma estrutura de
processamento regular para se adequar à arquitetura das GPUs.
O nível de Paralelização de Algoritmo está preocupado em
identificar e criar oportunidades de paralelização nos algoritmos de
codificação de vídeo. Isso envolve identificar estágios que podem ser
paralelizados de maneira direta devido à ausência de dependências
de dados e também desenvolver métodos para quebrar dependências
de dados não críticas e permitir a execução paralela de algoritmos
que, de outra forma, seriam sequenciais — nesse caso, assume-se
perdas de eficiência de codificação. Estas contribuições estão foca-
das na concepção geral do algoritmo, e não em sua implementação.
As contribuições no nível de Co-otimização de Algoritmo
e Recursos da GPU consistem em desenvolver implementações
eficientes dos algoritmos paralelos considerando o hardware das
GPUs. Isso inclui gerenciar o layout de dados e a hierarquia de
memória explicitamente para coalescer os acessos à memória e
maximizar o reuso de dados, usar instruções especializadas da GPU e
escalonar as tarefas na GPU para maximizar o throughput. Esta tese
de doutorado propõe uma abstração lógica chamada de conjuntos de
trabalho para auxiliar nesse escalonamento. Cada grupo de threads
do lado do software é responsável por uma unidade de carga de
trabalho com múltiplos blocos. Os conjuntos de trabalho são usados
para subdividir as threads em grupos menores de acordo com a
estrutura de particionamento de cada unidade de carga de trabalho,
facilitando o mapeamento entre threads e amostras em um bloco.
VALIDAÇÃO EXPERIMENTAL
O padrão Versatile Video Coding (VVC) [2] alcança eficiência de
codificação estado-da-arte, mas também impõe uma grande carga
computacional. Portanto, a MIAG-CV é aplicado a duas ferramen-
tas de codificação introduzidas pelo VVC. Além de demonstrar a
aplicação da metodologia, isso também auxilia na redução da carga
computacional dos codificadores VVC. As ferramentas abordadas
são a Estimação de Movimento Afim (Affine ME) e a Predição Intra
Baseada em Matrizes (MIP) [2]. A Affine ME é projetado para ex-
plorar padrões de movimento não translacionais. Em contrapartida,
a MIP explora métodos baseados em aprendizado de máquina para
representar um bloco com base nas amostras vizinhas já codificadas.
Para a Affine ME, as contribuições no gerenciamento de carga
de trabalho incluem a definição das unidades de carga de trabalho e
a exclusão de blocos improváveis. As contribuições na paralelização
do algoritmo incluem o cálculo dos vetores de movimento ignorando
blocos adjacentes, o cálculo de erro de predição e gradiente das
amostras em paralelo, e a construção de sistemas de equações em
paralelo. No nível mais baixo, quatro granularidades de dados são
definidas para processar diferentes estágios de forma eficiente. A
ferramenta MIP também utiliza unidades de carga de trabalho
no nível de gerenciamento de carga, além de um método baseado
em filtros passa-baixa para reproduzir o efeito da quantização e
quebrar as dependências entre blocos adjacentes. As contribuições
na paralelização do algoritmo incluem o cálculo do erro de predição
em paralelo. No nível de implementação, operações especializadas
de produto escalar são usadas para gerar a predição. A abstração
definida pela MIAG-CV permite que algumas contribuições sejam
reutilizadas entre duas ferramentas completamente diferentes.
A validação experimental comparou o tempo de processamento,
o consumo energético e a eficiência de codificação dos kernels de
codificação implementados em GPU com o codificador de referên-
cia rodando em CPU. O codificador de referência do VVC, cha-
mado VTM, foi executado em um processador Intel Core i9 7900X
com 64GB de RAM. Os kernels de GPU foram implementados em
OpenCL e executados em três dispositivos: NVIDIA GTX 1080, NVI-
DIA Titan V e Radeon RX 6900XT. Os utilitários RAPL, nvidia-smi
e rocm-smi foram utilizados para monitorar o consumo de energia.
Os experimentos com Affine ME mostraram que a codificação
baseada em GPU foi de 19 a 267 vezes mais rápida que o codificador
de referência, enquanto consumiu entre 2,10% e 22,42% da energia
consumida pela CPU. Os resultados variam com base no conteúdo
do vídeo e no dispositivo GPU, mas a solução proposta é tanto
mais rápida quanto mais eficiente em termos de energia em todos
os casos. As modificações do algoritmo para expor o paralelismo
requerem um bitrate adicional de 0,017% ∼1,784% para alcançar a
mesma qualidade visual, de acordo com a métrica BD-BR.
Para a MIP, os kernels de GPU aceleram a codificação entre 10
e 136 vezes, enquanto consomem entre 0,75% e 21,45% da energia
consumida pela CPU. Novamente, a solução baseada em GPU é mais
rápida e mais eficiente em termos de energia. Por fim, a solução
com GPUs exige um bitrate adicional entre 0,105% e 0,736% para
alcançar a qualidade visual do codificador VTM.
CONCLUSÃO
Esta tese de doutorado propôs a MIAG-CV, uma metodologia hi-
erárquica para desenvolver soluções de aceleração por GPU em
aplicações de codificação de vídeo. A abstração proporcionada pela
MIAG-CV permitiu que procedimentos de aceleração semelhantes
fossem aplicados em ferramentas de codificação distintas. Resulta-
dos experimentais mostraram que os kernels de computação em
GPU, desenvolvidos com a MIAG-CV, são centenas de vezes mais
rápidos que o codificador em CPU, consumindo menos de 22% da
energia utilizada pela CPU. Os impactos na eficiência de codificação
foram praticamente desprezíveis.

--- FIM DO ARQUIVO: 30469-829-24917-2-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30472-829-24920-1-10-20241001.txt ---
A social curiosity-driven approach to analyzing the information
dissemination in Telegram political groups
Francisco F. Vasconcelos
francisco.vasconcelos@aluno.ufop.edu.br
Universidade Federal de Ouro Preto
Alexandre M. de Sousa
alexandre.sousa@ufop.edu.br
Universidade Federal de Ouro Preto
Jussara M. Almeida
jussara@dcc.ufmg.br
Universidade Federal de Minas Gerais
2022. Para isso, este trabalho pretende responder
questões de pesquisa (QP) definidas a seguir:
– QP1: Como o compartilhamento de mensagens em grupos do Telegram pode
ser caracterizado pelo estímulo de curiosidade social?
– QP2: Como o comportamento dos usuários em grupos do Telegram pode ser
caracterizado pelo estímulo de curiosidade social?
CTIC’2024, Juiz de Fora/MG, Brazil
Vasconcelos et al.
– QP3: Como o comportamento agregado dos grupos do Telegram pode ser
caracterizado pelo estímulo de curiosidade social?
– QP4: Como a dinâmica do estímulo de curiosidade social dos grupos do
Telegram em relação aos conteúdos compartilhados evolui ao longo do tempo?
Para responder a essas questões de pesquisa, serão aplicadas a
metodologia e o modelo de curiosidade social proposto por Sousa
et al. [9]. Dessa forma, este trabalho traz uma nova contribuição
para a literatura ao ser o primeiro estudo a aplicar o modelo de
curiosidade na modelagem do comportamento do usuário na análise
de grupos políticos do Telegram. Vale ressaltar que o ano de ativi-
dade desses grupos é marcado por diversas polêmicas envolvendo a
disseminação de fake news, desinformação e coordenação de mani-
festações sociais. Isso forma o ambiente ideal para a pesquisa sobre
curiosidade em uma plataforma com número de membros ilimitado
pois fornece uma nova perspectiva da disseminação de informação.
COLEÇÃO DE DADOS E METODOLOGIA
Este trabalho utiliza o modelo de curiosidade e a metodologia pro-
posta por Sousa et al. [9] e Sousa et al. [8] para analisar o papel da
curiosidade social no processo de disseminação de informação em
grupos do Telegram. Para tanto, foi utilizada a coleção de dados
do Telegram do trabalho de Venâncio et al. [11] que construiu um
crawler que buscou links de grupos relacionados à política no Twit-
ter/X e que, por meio da API Telethon2, coletou mensagens e dados
de grupos do Telegram. Cada registro da coleção de dados conta
com o texto da mensagem, o tipo de mídia (e.g., texto, URL, imagem
ou vídeo), o tempo em que a mensagem foi compartilhada, um iden-
tificador anônimo do usuário e a identificação do grupo. O período
de análise considerado neste trabalho é de 1° de janeiro de 2022 a
28 de fevereiro de 2023 e conta com 4.807.516 mensagens compar-
tilhadas por 137.111 remetentes únicos em 282 grupos. Ressalta-se
que canais não foram considerados para análise. A metodologia
deste trabalho é dividida em 5 etapas conforme descrito a seguir.
I) Definição das premissas do modelo de curiosidade: uma
vez que o ambiente de análise é composto por um conjunto de
mensagens compartilhadas em grupos do Telegram, este trabalho
utiliza as mesmas premissas do modelo de curiosidade definidas no
trabalho de Sousa et al. [9] para o WhatsApp, principalmente pelo
uso de recursos em comum existentes nas duas plataformas. Dessa
forma, as ações do usuário de interesse são o compartilhamento de
mensagens de diferentes tipos de mídias (e.g., texto, URL, imagem
e vídeo) as quais formam o histórico de eventos de ação do usuário.
As premissas a serem consideradas são definidas a seguir:
(1) A curiosidade do usuário pode ser estimulada de diferentes formas
dependendo dos recursos da plataforma e do tipo de ação do usuário.
(2) A forma com que um usuário reage a um estímulo de curiosidade
muda no decorrer do tempo.
(3) A curiosidade do usuário tem um período de ativação que é definido
de acordo com o intervalo de tempo que precede à sua ação de com-
partilhamento, que é chamado de janela de interação, durante a qual
as ações dos membros do grupo podem provocar sua curiosidade.
(4) A curiosidade social de um indivíduo varia dependendo das pessoas
com quem ele interage em um grupo particular.
(5) A curiosidade social é estimulada por outros usuários que compar-
tilham mensagens durante a janela de interação.
(6) A curiosidade social pode ser estimada pelo histórico da frequência
das interações entre os usuários dentro de cada janela de interação.
2https://docs.telethon.dev/en/stable/
Para o cálculo das métricas, são considerados somente usuários
com no mínimo 30 mensagens e uma janela de interação de 30
minutos com pelo menos 10 mensagens. Após esse filtro, restaram
277 grupos e 3.026.345 mensagens de 10.277 usuários únicos.
II) Definição das métricas de curiosidade: neste trabalho são uti-
lizadas quatro métricas de curiosidade social propostas no trabalho
de Sousa et al. [9], as quais são derivadas de métricas da teoria da
informação e capturam aspectos do estímulo de curiosidade no com-
partilhamento de mensagens em nível individual: (𝑎) curiosidade
social direta máxima; (𝑏) curiosidade social direta média; (𝑐) curiosi-
dade social indireta máxima; e (𝑑) curiosidade social indireta média.
Ainda, para análise em nível agregado (comportamento coletivo),
são utilizadas métricas de grupo tais como entropia, que representa
a incerteza em termos da quantidade de informação disponível no
grupo, e informação mútua, que denota a redução da incerteza em
função da existência da curiosidade social no grupo. Além disso, são
calculadas métricas de curiosidade referentes às variáveis colativas
tradicionais, a saber, novidade, incerteza e conflito para o tipo de
mídia e para os usuários que compartilharam mensagens dentro da
janela de interação. E, por fim, a métrica de complexidade do tipo de
mídia. Estas 7 métricas são utilizadas para mostrar até que ponto as
métricas de curiosidade social capturam aspectos que não são cap-
turados pelas variáveis colativas. Todas as métricas são calculadas
para cada mensagem compartilhada por um usuário em um grupo.
III) Processo de seleção das métricas de curiosidade: esta etapa
é realizada com o objetivo de identificar quais métricas capturam as-
pectos complementares do estímulo de curiosidade, principalmente
para evitar possíveis problemas de multicolinearidade. Para esse fim,
foram calculados os coeficientes de correlação de Spearman para
cada par de métricas de cada usuário (11 variáveis resultam em 55
coeficientes). Em seguida, verifica-se qual é o percentual de usuários
que possuem coeficientes de correlação no intervalo (−1, −0.5) e
(+0.5, +1.0), considerados como correlação moderada/alta. Então, o
par de métricas que possui mais de 70% dos casos (usuários) com
correlação moderada/alta é considerado como redundante e, do par,
é selecionada apenas uma. Assim, apenas as métricas consideradas
complementares são selecionadas para análise.
IV) Caracterização da curiosidade social: a caracterização dos
estímulos de curiosidade social foi dividida nos três passos a seguir.
(A) Caracterização de mensagens: utiliza apenas as métricas de cu-
riosidade social em nível individual selecionadas na etapa III e
visa identificar perfis de mensagens por meio da clusterização de
mais de 3 milhões de mensagens utilizando o algoritmo Mini-batch
K-means e o número ideal de clusters é selecionado pelo maior
índice de Silhouette. (B) Caracterização do perfil dos usuários: a
partir dos perfis de mensagens obtidos, cada usuário é representado
por meio de um User Behavorial Model Graph (UBMG). Um UBMG é
um grafo onde cada nó representa um perfil de mensagem (estado)
e as arestas representam as transições entre os perfis de mensagens
de um usuário. O peso de cada aresta é dado pela probabilidade de
transição entre os perfis de mensagens. Um mesmo usuário pode
ser representado por um UBMG diferente para cada grupo em que
participa. Visando identificar perfis de usuários distintos, os 10
mil UBMGs dos usuários são agrupados por meio do algoritmo
Mini-batch K-means e o número ideal de clusters é selecionado pelo
maior índice de Silhouette. (C) Caracterização do comportamento
coletivo: visando identificar diferentes clusters relacionados aos 277
CTIC’2024, Juiz de Fora/MG, Brazil
 
 

90%
65%
79.7%
3.3%
6.6%
22%
13%
16.8%
3.5%
(a) U0: 2.230 usuários (332 msgs/user).
 
 

86%
75%
75%
6.6%
7.3%
12.7%
12.3%
16%
9%
(d) U1: 4.319 usuários (423 msgs/user).
 
 

97.5%
51.0%
59.4%
1.6%
48.8%
21.1%
19.5%
(b) U2: 3.234 usuários (86 msgs/user).
 
 

92.7%
42.3%
20%
2.6%
4.7%
52.2%
5.5%
78%
2%
(c) U3: 1.912 usuários (96 msgs/user).
Figure 1: Centroides relativos aos perfis de usuários
grupos do Telegram, o algoritmo K-means é utilizado. Para isso, os
grupos são representados pelas 2 métricas de curiosidade de grupos
em nível agregado, entropia e informação mútua, e pelo número
total de membros. Por fim, o número ideal de clusters é selecionado
pelo maior índice de Silhouette.
V) Análise de tópicos e do impacto da curiosidade social na
disseminação de informação: a análise de tópicos é realizada a
partir do conteúdo das mensagens de texto compartilhadas para que
elas possam ser rotuladas por assunto, o que facilita a identificação
do conteúdo das mensagens durante as conversas nos grupos. Para
isso, é utilizada a ferramenta Bertopic [2] que faz uso de modelos
pré-treinados disponibilizados na plataforma Hugging Face 3 e o
modelo utilizado é o Bertimbau [10]. Os resultados dos tópicos per-
mitem identificar quais assuntos foram discutidos nos grupos ao
longo do tempo. Dessa forma, assuntos abordados durante a ocor-
rência de eventos importantes (eleições ou manifestações) podem
ser relacionados com o impacto da curiosidade social no grupo.
RESULTADOS
Nesta seção são descritos os resultados para cada etapa da metodolo-
gia que ajudam a responder a cada uma das questões de pesquisa
conforme a seguir. Os resultados da Etapa III da metodologia
mostram que, para mais de 84% dos casos as métricas de curiosi-
dade social são complementares às métricas das variáveis colativas
tradicionais (i.e., capturam aspectos relevantes). Para as métricas de
curiosidade social, são selecionadas as métricas curiosidade social
direta máxima e curiosidade social indireta máxima e média (são
complementares para mais de 59% dos casos). Essas métricas são
então utilizadas na Etapa IV (A) para clusterização das mensagens,
na qual foram identificados os perfis de mensagens (1) Dependente,
com 53% das mensagens (msgs), onde a métrica de curiosidade social
direta domina; (2) Independente em que as métricas de curiosidade
social são menores (20% das msgs); e (3) Indireto no qual as métricas
de curiosidade social indiretas dominam (27% das msgs).
A análise da Etapa IV (B) foi realizada por meio da clusterização
dos UBMGs construídos com os perfis de mensagens obtidos na
Etapa IV (A). Dessa forma, foram encontrados 4 perfis de usuários
cujos centroides são apresentados na Figura 1. No geral, nota-se
que os perfis dos usuários têm uma tendência maior de permanecer
no estado Dependente (perfil de mensagens) bem como de tran-
sitarem dos estados Indireto e Independente para Dependente. Há um
destaque para o UBMG 𝑈2 que não tem transições para o estado In-
direto e possui maior probabilidade de transição para Independente.
3https://huggingface.co/
Informação mútua de grupo
Entropia de grupo
G0
G1
G2
G3
(a) Clusters de grupos.
(a) Tópico 1.
(b) Tópico 2.
(c) Tópico 3.
(d) Tópico 4.
(b) Wordclouds dos Top 4 tópicos.
Figure 2: Resultados das Etapas IV(C) e V.
Na Etapa IV (C), a clusterização de grupos do Telegram resultou
em 4 clusters ilustrados na Figura 2a por diferentes cores. Nessa
figura, cada grupo é um ponto em que o eixo 𝑥representa a in-
formação mútua e o eixo 𝑦denota a entropia. Ainda, o impacto da
curiosidade social na entropia dos grupos é calculado pela razão
entre a informação mútua e a entropia. Essa razão possui valores
entre 0 e 1 e, quanto mais próximo de 1, maior é o impacto da curiosi-
dade social para o grupo. A linha vermelha na diagonal da Figura
2a mostra quando essa razão é igual à 1. Dessa forma, os clusters
𝐺2 (18% dos grupos) e 𝐺3 (34% dos grupos) estão mais próximos
dessa linha vermelha na figura e são os que têm o maior impacto
da curiosidade social. Para os clusters 𝐺0 (12% dos grupos) e 𝐺1
(36% dos grupos) a curiosidade social representa um papel menos
importante na disseminação de informação.
Na análise de tópicos da Etapa V, foram identificados 536 tópicos
das mensagens de texto. Após a remoção e filtragem de tópicos
irrelevantes (saudações, reações ou onomatopeias), foram identi-
ficados 37 tópicos que tratam de assuntos importantes e cobrem
30% de todas as mensagens. A Figura 2b apresenta as wordclouds
dos Top 4 tópicos mais frequentes nas conversas (41% dos tópicos).
Nessa figura, os assuntos abordados são referentes à vacinação
e pandemia da COVID-19 (tópico 1), questões religiosas (tópico
2), processo democrático e urnas eletrônicas (tópico 3), e, por fim,
forças armadas e intervenção militar (tópico 4).
Ainda, para ilustrar os resultados Etapa V, foram selecionados
2 grupos que pertencem a clusters de grupos diferentes e que pos-
suem atividade de compartilhamento de mensagens significativa
no período de análise. Os grupos são: (a) “Pátria News” do cluster
𝐺0 com 1.804 usuários e 24.382 mensagens e impacto de curiosi-
dade social de 53%; (b) “Grupo Controle Brasil REVELATION 13
WWWIII” do cluster 𝐺1 com 34% de impacto de curiosidade social,
460 usuários e 10.704 mensagens. A Figura 3 apresenta as séries
temporais para as métricas de grupo e para o percentual do impacto
da curiosidade social para os 2 grupos de exemplo. Nessa figura, é
possível observar uma queda nas atividades no grupo “Controle
Brasil” em abril de 2022, a qual pode ser relacionado a um bloqueio
de grupos de Telegram ordenado pelo STF em março4, seguido por
um bloqueio no dia 19 de abril5. O mesmo aconteceu no grupo
“Pátria News” em fevereiro de 2023, possivelmente por causa das
manifestações do dia 8 de janeiro de 2023 em Brasília6.
4http://glo.bo/4aNLpPs
5https://bit.ly/3VpWGBm
6https://bit.ly/3yKDMvY
CTIC’2024, Juiz de Fora/MG, Brazil
Vasconcelos et al.
1/2022
2/2022
3/2022
4/2022
5/2022
6/2022
7/2022
8/2022
9/2022
10/2022
11/2022
12/2022
1/2023
2/2023
Bits
Informação mútua de grupo
Entropia de grupo
(a) Pátria News.
1/2022
2/2022
3/2022
4/2022
5/2022
6/2022
7/2022
8/2022
9/2022
10/2022
11/2022
12/2022
1/2023
2/2023
Bits
Informação mútua de grupo
Entropia de grupo
(b) Controle Brasil REV. 13 WWIII.
1/2022
2/2022
3/2022
4/2022
5/2022
6/2022
7/2022
8/2022
9/2022
10/2022
11/2022
12/2022
1/2023
2/2023
0.0
0.2
0.4
0.6
0.8
1.0
Impacto da curiosidade social
Pátria News
Grupo Controle Brasil
(c) Impacto da curiosidade social.
Figure 3: Evolução da dinâmica da curiosidade social nos grupos de exemplo.
Também é possível observar uma relação entre o aumento de
atividade das conversas dos tópicos com as métricas de curiosidade
ao longo dos meses. As Figuras 4a e 4b apresentam uma heatmap
em que a coluna representa o mês e as linhas representam os Top
10 tópicos mais frequentes. Esse heatmap mostra para qual mês
o compartilhamento de mensagens de um determinado tópico au-
menta no decorrer do ano. Para o grupo da Fig. 4a, o tópico 6 sobre
família domina as conversas até 09/23. Já, para o grupo da Fig. 4b,
o tópico 1 foi dominante em quase o ano todo, exceto em 09/22 e
10/22. É possível notar que os tópicos de 1 a 4 (wordclouds da Fig.
2b) tiveram um aumento de atividade em 10/22 até 02/23, período
durante e depois das eleições presidenciais. Esses tópicos se ref-
erem a questões religiosas, questionamentos do processo eleitoral
e incitações à intervenção militar. A Fig. 3c mostra um aumento
constante do impacto de curiosidade social para os 2 grupos após o
2º turno das eleições (10/22) até jan/fev de 2023, mesmo período em
que houve aumento de atividade dos tópicos de 1 a 4 (Fig. 4a e 4b).
1/2022
2/2022
3/2022
4/2022
5/2022
6/2022
7/2022
8/2022
9/2022
10/2022
11/2022
12/2022
1/2023
2/2023
Id do tópico
# messagens(normalizado)
(a) Pátria News.
1/2022
2/2022
3/2022
4/2022
5/2022
6/2022
7/2022
8/2022
9/2022
10/2022
11/2022
12/2022
1/2023
2/2023
Id do tópico
# messagens(normalizado)
(b) Controle Brasil REV.13 WWWIII.
Figure 4: Top 10 tópicos mais abordados por mês.
DISCUSSÃO E CONSIDERAÇÕES FINAIS
Este trabalho apresentou uma análise da curiosidade social a partir
de uma coleção de dados do Telegram, referente a um período de
turbulência no cenário político e social no Brasil. Os resultados
mostram como a curiosidade social está relacionada ao conteúdo
da informação disseminada nos grupos do Telegram. Por meio da
clusterização das mensagens, foi possível responder a QP1 onde 3
perfis de estímulo de curiosidade de mensagens foram encontrados.
Ainda, por meio desses perfis de estímulo, UBMGs dos usuários
foram construídos e sua clusterização possibilitou encontrar 4 perfis
de usuários distintos (QP2). A QP3 também foi respondida através
da clusterização dos grupos. A utilização das métricas em nível
agregado de entropia, informação mútua e impacto de curiosidade so-
cial permitiram a caracterização do comportamento dos grupos em
termos de curiosidade social. Finalmente, a análise de tópicos junto às métricas de curiosidade social dos grupos permitiram responder a QP4. Dessa forma, foi possível constatar que certos assuntos apelativos abordados nas conversas dos grupos apresentam uma relação direta com o impacto da curiosidade social durante o período de análise. Para esses assuntos foi observada uma variação do impacto da curiosidade no momento em que eles sofreram um aumento de atividade no compartilhamento de mensagens.
Os resultados deste trabalho permitem uma comparação direta
com os resultados da análise do WhatsApp de Sousa et al. [9]. Além
disso, este trabalho abre caminho para diversas possibilidades de
novos trabalhos futuros na literatura com modelos de curiosidade
em outros tipos de plataformas de mídias sociais online.

--- FIM DO ARQUIVO: 30472-829-24920-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30473-829-24921-1-10-20241001.txt ---
Análise Comparativa dos Artigos de Filmes Indicados ao Oscar
em Três Versões da Wikipédia
Cecília Junqueira V. M. Pereira
ceciliajunq@ufmg.br
Universidade Federal de Minas Gerais
Belo Horizonte, Minas Gerais
Marisa Affonso Vasconcelos
marisa.vasconcelos@gmail.com
Universidade Federal de Minas Gerais
Belo Horizonte, Minas Gerais
Ana Paula Couto da Silva
ana.coutosilva@dcc.ufmg.br
Universidade Federal de Minas Gerais
Belo Horizonte, Minas Gerais
CTIC’2024, Juiz de Fora/MG, Brazil
Cecília Junqueira V. M. Pereira, Marisa Affonso Vasconcelos, and Ana Paula Couto da Silva
“Wikipedia” em Python3, para buscar artigos de filmes. A função
Wikipedia.search retorna uma lista de títulos correspondentes à
consulta. Para identificar os títulos relevantes, utilizou-se o módulo
“SequenceMatcher” do pacote Python “difflib”, com um valor de
similaridade superior a 0, 6. Caso a similaridade fosse menor, foi feita
uma busca manual para cerca de 20% dos títulos em inglês. Após
identificar os artigos em inglês, a API da Wikipédia foi novamente
usada para obter as versões em espanhol e português4, coletando
os respectivos artigos.
Passo (3): Coleta dos artigos e metadados dos títulos. Após
a normalização dos filmes e de suas páginas correspondentes no
passo anterior, utilizou-se a função Wikipedia.page para coletar o
conteúdo textual de cada artigo, além de links e referências a outras
páginas. Após a coleta dos dados, o número de artigos de filmes
encontrados para cada idioma foi: 587 para inglês e português, e
572 para espanhol. Assim, a cobertura da descrição desses filmes é
de 100% para inglês e português e de 97, 4%. para o espanhol.
ANÁLISE DE CONECTIVIDADE
Nessa seção é analisada a conectividade entre os artigos de cada
versão da Wikipédia, oferecendo uma visão de como as informações
são referenciadas e revelando possíveis diferenças na priorização e
organização das informações em diferentes culturas.
A análise considerou 572 filmes com artigos nas três versões da
Wikipédia. Para cada artigo, foram extraídos links para outros arti-
gos da Wikipédia na mesma versão, selecionando apenas aqueles
dentro do dataset. As relações de referência entre os artigos dos fil-
mes foram usadas para criar um grafo, onde cada filme é um vértice
e as arestas direcionadas representam essas referências. Apenas os
filmes que referenciam ou são referenciados pelo menos uma vez
(ou seja, que têm grau diferente de zero) foram incluídos no grafo,
correspondendo a 84, 15%, 44, 46% e 44, 97% do artigos em inglês, es-
panhol e português, respectivamente. Os grafos gerados podem ser
visualizados na Figura 1. A construção dos grafos foi feita com a fer-
ramenta NetworkX5. Para analisar a composição das comunidades,
foi executado o algoritmo Louvain para identificar as comunidades
de cada grafo e visualizamo-nas com a ferramenta Gephi6. A análise
das comunidades não revelou relações semânticas ou de gênero
entre os filmes. As relações de referências predominantes eram
entre filmes com diretor e/ou atores em comum.
A Tabela 1 sumariza as principais métricas de cada grafo. A aná-
lise revela diferenças significativas na estrutura e conectividade
entre os idiomas. O grafo em inglês tem maior densidade de co-
nexões, refletida em métricas, como o número de vértices, arestas
e grau médio, superando os grafos em espanhol e português. O
inglês também tem o maior valor de closeness médio, indicando
que os vértices estão mais próximos, e o maior betweenness médio,
sugerindo que os artigos em inglês desempenham um papel mais
central nas conexões. O grafo em espanhol mostra maior modula-
ridade, com comunidades mais bem definidas, enquanto o inglês
tem maior transitividade e coeficiente de agrupamento médio, su-
gerindo uma maior coesão local e tendência de agrupamento. Esses
resultados mostram que a versão em inglês possui uma rede de
3 https://pypi.org/project/Wikipedia
4 https://www.mediawiki.org/wiki/API:Langlinks
5 https://networkx.org/
6 https://gephi.org/
Tabela 1: Principais métricas dos grafos
Métricas
Inglês
Espanhol
Português
Número de vértices
Número de arestas
1.381
Diâmetro
Grau máximo de entrada
Grau máximo de saída
Grau médio
2,7955
1,3141
1,2310
Betweenness médio
0,0045
0,0003
0,0008
Closeness médio
0,1117
0,0099
0,0158
Densidade
0,0056
0,0050
0,0046
Transitividade
0,0906
0,0513
0,0317
Modularidade
0,5140
0,7742
0,7320
Coefic. de agrupamento médio
0,0671
0,0180
0,0270
Figura 1: Grafo dos artigos em cada idioma, com nós coloridos
segundo as comunidades definidas pelo algoritmo Louvain.
referências mais integrada, enquanto as versões em espanhol e por-
tuguês têm estruturas menos conectadas e com comunidades mais
bem definidas.
Figura 2: Variação nas descrições do enredo dos filmes.
ANÁLISE TEXTUAL ENTRE VERSÕES
Essa seção analisa como os enredos dos filmes são descritos em
cada versão da Wikipédia, com o foco na seção “Enredo” (ou “Plot”
em inglês e “Argumento” em espanhol). A comparação abrange 471
filmes que contêm essa seção nos três idiomas, permitindo uma
análise consistente das variações culturais.
Primeiro, verificou-se se os textos em português e espanhol eram
traduções literais do inglês. Não foi observada semelhança signifi-
cativa, sugerindo que não são traduções diretas. Em seguida, foram
comparados os tamanhos das descrições dos enredos em cada ver-
são da Wikipédia. A Figura 2 apresenta boxplots que ilustram o
Análise Comparativa dos Artigos de Filmes Indicados ao Oscar em Três Versões da Wikipédia
CTIC’2024, Juiz de Fora/MG, Brazil
Figura 3: Nuvens de palavras dos enredos.
número de palavras em cada idioma. A mediana dos boxplots in-
dica que metade dos textos em inglês é significativamente maior
do que em português e espanhol. Isso pode ser explicado porque
76% dos editores da Wikipédia contribuem primeiro para a versão
em inglês7. As descrições em português são, em média, mais cur-
tas, enquanto os textos em espanhol mostram uma maior variação,
incluindo outliers, como o filme “Sunset Boulevard”.
A seguir, investigou-se a frequência das palavras mais usadas em
cada idioma, gerando nuvens de palavras com o pacote wordcloud8,
conforme mostrado na Figura 3. Os textos dos 471 enredos foram
concatenados, transformados em tokens e as stopwords foram remo-
vidas com a ferramenta NLTK9. As nuvens de palavras revelaram
uma predominância de termos do âmbito familiar e palavras mas-
culinas, como “família”, “filho”, “father” e “padre”, indicando que
muitos filmes indicados ao Oscar abordam temas da esfera familiar.
Embora essa conclusão vá além do escopo do estudo, sugere-se que
as relações entre os personagens são um foco importante para os
editores da Wikipédia. Além disso, os textos em inglês contêm mais
verbos, sugerindo maior ênfase no desenvolvimento e na dinâmica
da história em comparação com os textos nos outros idiomas.
Também foram analisadas as frequências das funções sintáticas
nos enredos em diferentes idiomas revelou que, embora a frequência
é diferente nos três idiomas, os verbos são mais frequentes em in-
glês, enquanto substantivos e adjetivos predominam em português.
Além disso, observou-se que os adjetivos são geralmente pouco
utilizados, o que sugere que os editores da Wikipédia não priorizam
a caracterização dos personagens.
5.1
Análise de Categorias Semânticas
A terceira análise utilizou a ferramenta PyMUSAS10 para classificar
termos em categorias semânticas e identificar padrões em tópicos e
sentimentos na seção “Enredo” de cada idioma, excluindo categorias
sintáticas e gramaticais. Foram identificadas 107 categorias nos tex-
tos analisados cujas frequências foram normalizadas e comparadas
entre idiomas usando o teste de Kruskal-Wallis [4]. As diferenças
significativas foram analisadas entre os pares de idiomas. A Figura
4 mostra essas diferenças nas frequências das categorias entre os
idiomas comparados. Valores positivos indicam maior frequência
no primeiro idioma mencionado na figura, enquanto que negativos
indicam maior frequência no segundo idioma.
7 https://en.wikipedia.org/wiki/Wikipedia:Wikipedians
8 https://pypi.org/project/wordcloud
9 https://www.nltk.org/
10 https://ucrel.github.io/pymusas/
A análise comparativa das categorias semânticas entre português,
inglês e espanhol revela diferenças significativas na ênfase de cada
idioma. O português apresenta uma maior frequência em categorias
relacionadas a localização e direção e atributos físicos, enquanto
o inglês destaca-se em nomes de indivíduos e locais específicos,
aspectos temporais e movimentos. Em comparação com o espanhol,
o português também mostra mais frequência em localização e ava-
liação, enquanto o espanhol se foca em termos gerais e abstratos,
além de ações sociais, refletindo uma abordagem mais abstrata e um
interesse maior em contextos sociais. Entre o espanhol e inglês, o
espanhol destaca-se em ações sociais, estados e processos, e termos
gerais e abstratos, enquanto o inglês se concentra mais em nomes
de pessoas, avaliação e processos de posse, indicando uma maior
atenção a referências específicas e julgamentos de valor. Essas di-
ferenças evidenciam como cada idioma evidencia a descrição dos
filmes e refletindo características culturais distintas na comunicação
e interpretação dos conteúdos.
5.2
Análise dos Tópicos dos Filmes
Nessa seção, analisa-se a similaridade semântica entre grupos de
filmes em cada idioma, utilizando embeddings que são representa-
ções vetoriais contínuas capturando o contexto e o significado das
palavras. Com a ferramenta BERT11, obtiveram-se os embeddings
que refletem com precisão o contexto dos textos. A análise por
gênero foi descartada devido à predominância do gênero Drama,
que representa cerca de 40% dos filmes no Oscar, resultando em
diferenças semânticas pouco claras entre os gêneros.
A partir dos embeddings, aplicou-se o algoritmo K-Means para
identificar clusters de filmes semanticamente semelhantes em cada
idioma. O número ideal de clusters foi determinado como 4 para
inglês e espanhol, e 5 para português, com base no Coeficiente
Silhouette. Para visualizar esses clusters, utilizou-se a técnica t-SNE,
que reduz a dimensionalidade dos dados. Testaram-se diferentes
valores para o hiperparâmetro Perplexity, encontrando-se que o
valor ideal era 355, garantindo uma redução eficaz. A Figura 5
ilustra os clusters formados para cada idioma. Após obter os clusters,
analisaram-se os filmes que compõem cada cluster e para atribuir
uma descrição que resuma a abordagem comum das histórias dentro
de cada conjunto (Tabela 3).
Foi feita também uma análise comparativa dos clusters de filmes
entre os três idiomas, utilizando a métrica de Normalized Mutual
Information Score (NMI), que avalia a similaridade entre duas clus-
terizações, variando de 0 a 1, onde valores mais altos indicam maior
correspondência entre os clusters. A similaridade entre os clus-
ters em inglês e português apresentou um NMI de 0.045, e entre
espanhol e português, o NMI foi de 0.033, ambos indicando uma cor-
respondência bastante baixa entre os agrupamentos desses idiomas.
Em contraste, a similaridade entre os clusters em espanhol e inglês
foi um pouco mais alta, com um NMI de 0.185, sugerindo uma cor-
respondência moderada. Esses resultados sugerem que, enquanto
há alguma sobreposição entre os padrões de clustering dos filmes
em inglês e espanhol, os clusters nos diferentes idiomas tendem
a ser bastante distintos, refletindo diferenças nas características
culturais e contextuais dos filmes em cada língua.
11 https://pypi.org/project/bertopic/0.3.1/
CTIC’2024, Juiz de Fora/MG, Brazil
Cecília Junqueira V. M. Pereira, Marisa Affonso Vasconcelos, and Ana Paula Couto da Silva
(a) Português vs. Inglês.
(b) Português vs. Espanhol.
(c) Espanhol vs. Inglês.
Figura 4: Diferenças semânticas nas categorias do PyMUSAS.
Tabela 3: Clusters por idioma.
Idioma
Cluster
# Filmes
Descrição
Cluster (EN)
Cluster (ES)
Cluster (PT)
Inglês
Decisões cruciais na vida dos personagens
-
3 (4, 76%)
3 (4, 76%)
Mudanças drásticas na vida dos personagens
-
0 (9, 82%)
0 (6, 20%)
Histórias de guerra e conflitos
-
1 (6, 79%)
0 (4, 97%)
Dramas familiares e românticos
-
2 (2, 42%)
2 (3, 84%)
Espanhol
Enfrentamento de conflitos pessoais
1 (9, 82%)
-
0 (5, 82%)
Relações de poder e influência
2 (6, 79%)
-
0 (4, 31%)
Relacionamentos dos personagens
0 (7, 30%)
-
0 (4, 31%)
Mudanças significativas no estilo de vida
1 (2, 10%)
-
3 (2, 45%)
Português
Papel social dos personagens na sociedade
1 (6, 20%)
0 (5, 82%)
-
Mudanças na visão do personagem principal
0 (4, 56%)
2 (3, 35%)
-
Conflitos e suas descrições detalhada
3 (3, 84%)
2 (3, 46%)
-
Relações e interações entre personagens
1 (6, 09%)
0 (4, 86%)
-
Biografias reais ou fictícias
0 (3, 96%)
0 (2, 72%)
-
Figura 5: Clusters dos artigos de cada idioma visualizados
com o método t-SNE.
Em seguida, foi analisada a interseção entre clusters de dois
idiomas diferentes. O resultado dessas interseções é mostrado na
Tabela 3 nas colunas “Cluster (EN)”, “Cluster (ES)” e “Cluster (PT)”,
que indicam qual cluster de cada idioma é mais semelhante ao
cluster da linha e a taxa de semelhança. A baixa porcentagem de
semelhança entre os clusters dos idiomas reforça a diferença se-
mântica na forma como as histórias dos filmes são abordadas nos
três idiomas. Exemplos que ilustram essa diferença incluem o filme
“Uma Mente Brilhante” que, na Wikipedia em português, foca nas
relações dos personagens, enquanto, em inglês e espanhol destaca
os conflitos enfrentados pelo personagem. Outro exemplo é “E o
Vento Levou”, que no inglês enfatiza às decisões cruciais dos perso-
nagens, no espanhol foca nos relacionamentos dos personagens, e
no português no papel social dos personagens.
CONCLUSÃO E TRABALHOS FUTUROS
Este estudo analisou os artigos das versões em inglês, espanhol e
português da Wikipédia sobre os indicados à categoria “Melhor
Filme” do Oscar. Os resultados mostram diferenças significativas na
forma como os filmes são retratados: a versão em inglês tem uma
rede de referências mais integrada, enquanto as versões em espa-
nhol e português mostram comunidades mais definidas e menor
densidade de conexões. Essas variações semânticas e de conec-
tividade refletem as distintas abordagens culturais e prioridades
informacionais de cada comunidade linguística.
Durante o estudo, foram enfrentadas limitações como o BER-
Topic, que não forneceu resultados satisfatórios sobre os tópicos
predominantes em cada artigo. Em trabalhos futuros, serão explo-
radas outras ferramentas de análise de tópicos, como o LDA[2]
e o LaBSE [3], e avaliado diferentes públicos para aprofundar a
compreensão das diferenças na percepção dos filmes entre falantes
desses idiomas.
Agradecimentos: Trabalho financiado pela FAPEMIG e CNPq.

--- FIM DO ARQUIVO: 30473-829-24921-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30474-829-24922-1-10-20241001.txt ---
ARIA e Interactive Access: projetando chatbots para idosos
Carlos Nery Ribeiro, Cynthya Letícia Teles de Oliveira, Lucas Padilha Modesto de Araujo,
Kamila Rios da Hora Rodrigues, Marcelo Garcia Manzato
carlosnribeiro@usp.br,cynthya@usp.br,padilha.lucas@usp.br,kamila.rios@icmc.usp.br,mmanzato@icmc.usp.br
Instituto de Ciências Matemáticas e de Computação
Universidade de São Paulo
São Carlos, SP
2022. O estudo identificou tendências princi-
pais, como design de chatbots, experiência do usuário, expressão
emocional, privacidade e ética. Os resultados mostram um cresci-
mento exponencial de publicações nos últimos cinco anos, com foco
crescente na experiência do usuário e comunicação emocional.
Furini et al. [4] exploraram o impacto das interfaces de conver-
sação, como chatbots, na acessibilidade web. O artigo discute os
benefícios dessas interfaces para pessoas com deficiência, como
as visuais e motoras, permitindo o acesso à informação através da
interação por voz. Os autores concluem que, embora as interfaces
de conversação ofereçam benefícios significativos para a acessibi-
lidade, ainda há problemas a serem resolvidos para garantir uma
experiência de usuário ideal para todos.
Yücel and Rizvanoğlu [12] investigaram o impacto da empatia e
das características da voz na percepção de assistentes de voz por
idosos (65-75 anos) na Turquia. Utilizando uma abordagem qualita-
tiva centrada no usuário, o estudo descobriu que um assistente de
voz empático leva a uma maior abertura dos participantes, que se
sentiram validados em suas experiências. Os participantes expres-
saram preferência por um assistente de voz que oferecesse suporte
emocional e social, em vez de apenas informações.
1https://www.notion.so/
CTIC’2024, Juiz de Fora/MG, Brazil
Ribeiro et. al.
É possível notar a tendência de uso de LLMs em produtos volta-
dos para idosos. A maioria das aplicações encontradas atualmente
são da área da saúde, por exemplo, Ramjee et al. [9] desenvolveram
um chatbot, treinado com uma base de dados considerada segura,
que tira as dúvidas de idosos sobre a cirurgia de catarata, porém,
outras aplicações têm foco em pacientes psiquiátricos idosos [6],
que ajudem na comunicação entre médico e paciente [11] ou até
mesmo em robôs de companhia [3].
As pesquisas demonstram um crescente interesse no desenvolvi-
mento e aprimoramento de chatbots acessíveis, com foco particular
na experiência do usuário. No entanto, desafios como a garantia da
acessibilidade para públicos específicos, como idosos, e a aplicação
ética da IA generativa, ainda precisam ser abordados de forma mais
abrangente. Ao encontro dessa informação, também se destaca a
ausência de material intuitivo que dê suporte ao desenvolvimento
de sistemas acessíveis.
DESENVOLVIMENTO DOS ARTEFATOS
TÉCNICOS
Nesta seção serão apresentados os artefatos e as metodologias em-
pregadas no desenvolvimento dos mesmos.
3.1
ARIA
O primeiro artefato técnico desenvolvido foi o chatbot ARIA (Assis-
tente de Recomendação e Interação Acessível). O ARIA foi projetado
para ser uma ferramenta prática que implementa as diretrizes de
acessibilidade documentadas em [1], servindo como um exemplo
funcional e uma plataforma de interação acessível.
O principal objetivo do ARIA é atuar como um assistente virtual
voltado para a recomendação de conteúdos culturais, como filmes,
séries, livros e músicas, tendo um enfoque particular na experiência
dos idosos.
3.1.1
Tecnologia Utilizada. O ARIA foi desenvolvido utilizando a
linguagem de programação Python, uma escolha popular para o
desenvolvimento de chatbots devido à sua simplicidade e extensa
variedade de bibliotecas e frameworks. Para a geração das respostas,
o ARIA utiliza o Gemini Pro2, um LLM open-source criado pela
Google, que oferece uma capacidade robusta de compreensão e
geração de texto em linguagem natural.
Ao empregar o modelo de linguagem Gemini Pro, o ARIA é capaz
de compreender contextos complexos e nuances linguísticas, além
de adaptar suas respostas ao histórico de interações e preferências
dos usuários, criando uma experiência personalizada e elevando a
qualidade da experiência do usuário.
3.1.2
Funcionalidades do ARIA.
• Geração de Respostas Textuais: Conforme mencionado
anteriormente, o ARIA utiliza o modelo Gemini para gerar
respostas textuais em tempo real, proporcionando interações
naturais e relevantes com os usuários;
• Transcrições em Áudio: Além das respostas textuais, o
ARIA oferece transcrições em áudio das respostas geradas.
Esta funcionalidade é crucial para usuários com deficiências
visuais ou dificuldades de leitura, permitindo-lhes interagir
2https://deepmind.google/technologies/gemini/pro/
com o chatbot de forma auditiva. Para realizar esta função,
é utilizada a API (Interface de Programação de Aplicação
ou Application Programming Interface, em Inglês) Text-to-
Speech3 da Google;
• Possibilidade de Utilizar o Microfone: O ARIA permite
que os usuários utilizem o microfone para enviar mensagens
ao chatbot. O áudio captado pelo microfone é transcrito em
tempo real para a caixa de entrada de texto, podendo ser
facilmente ajustado pelo usuário;
• Encontrar Links de Conteúdos Culturais: Uma das fun-
cionalidades distintivas do chatbot ARIA é a sua capacidade
de encontrar links de filmes, séries e músicas por meio de
técnicas de web scraping.
Figura 1: Fluxograma do chatbot ARIA.
3.1.3
Funcionamento do ARIA. O fluxo de funcionamento pode ser
ilustrado na Figura 1. Inicialmente, o usuário envia uma mensagem
na interface do ARIA. Essa mensagem pode ser escrita digitando
ou utilizando o microfone que transcreve a fala do usuário.
Em seguida, a mensagem enviada para o ARIA é transmitida
para o Gemini Pro por meio de um prompt personalizado, conforme
ilustrado na Figura 1. Essa personalização na mensagem enviada
para o LLM proporciona melhores respostas e o cumprimento das
diretrizes de acessibilidade necessárias.
A resposta gerada pelo Gemini é retornada para o ARIA e exibida
para o usuário. Além disso, também é gerado o áudio do texto
retornado, o qual aparece junto da resposta textual.
No caso especial do usuário solicitar que o ARIA encontre um link
de um filme, série ou música, a mensagem não é transmitida para
o Gemini. Ao invés disso, uma função de web scraping é chamada,
e ela retorna o link, caso seja possível encontrá-lo. Por fim, o link
encontrado é exibido pelo ARIA para o usuário.
O ARIA está disponível em um repositório4, onde podem ser
encontradas mais informações, bem como imagens ilustrativas do
chatbot em funcionamento.
3https://cloud.google.com/text-to-speech
4Para acessar o repositório, utilize o seguinte link: https://github.com/carlosnrib/
chatbot_v2.
ARIA e Interactive Access: projetando chatbots para idosos
CTIC’2024, Juiz de Fora/MG, Brazil
3.2
Interactive Access
Durante a elaboração do ARIA, descrito na Seção 3.1, percebeu-se
que algumas das diretrizes de acessibilidade não apresentavam o
grau de clareza necessário para que fossem implementadas correta-
mente. Assim, a segunda etapa do desenvolvimento dos artefatos
técnicos envolveu a criação de uma interface web dedicada a organi-
zar e exibir as diretrizes. A plataforma desenvolvida tem como título
Interactive Access: Explorando Diretrizes para Chatbots Acessíveis, e
está disponível publicamente5.
3.2.1
Tecnologia Utilizada. O Notion foi a ferramenta escolhida
para a elaboração da página, devido à sua versatilidade e capacidade
de integração de diferentes tipos de conteúdo, como textos, ima-
gens, tabelas e links, o que facilita a construção de uma plataforma
abrangente e interativa.
3.2.2
Estrutura da Página. A página no Notion foi estruturada de
forma que a apresentação das diretrizes de acessibilidade fossem fei-
tas de duas formas de visualização: galeria principal ou pelo menu
lateral. Ao clicar em uma diretriz, de ambos os modos citados, o
Notion direciona o usuário para uma página específica dedicada
a essa diretriz. Nessa página, são apresentadas descrições detalha-
das, recomendações práticas, justificativas claras, exemplos de uso
concretos e ferramentas relevantes para auxiliar na implementação.
ESTUDOS DE AVALIAÇÃO
4.1
Avaliação de Desempenho
Para avaliar o desempenho dos artefatos técnicos elaborados, foram
conduzidos testes de carga e estresse em ambos os artefatos e o
teste de tempo de resposta apenas no chatbot.
4.1.1
Teste de Carga/Estresse. Os testes de carga e estresse foram
realizados em três cenários distintos para garantir a escalabilidade
do sistema. Esses testes avaliaram a capacidade do chatbot de su-
portar diferentes volumes de tráfego e solicitações simultâneas. Os
cenários de teste considerados estão descritos na Tabela 1.
Tabela 1: Cenário de Teste.
Cenário
Número de usuários
Taxa de crescimento
Duração
50 por seg.
5 min
10 por seg.
10 min
20 por seg.
15 min
4.1.2
Testes de Tempo de Resposta. Além dos testes de carga e
estresse, também foram realizados testes de tempo de resposta para
garantir que o chatbot responda de forma eficiente às solicitações
dos usuários. As tarefas selecionadas para a avaliação dos tempos
de resposta foram:
1 Tarefa 1: Realizar uma primeira interação com o chatbot.
– Exemplo: "Bom dia"
2 Tarefa 2: Solicitar a recomendação de um filme de um de-
terminado gênero.
– Exemplo: "Recomende um filme de ação"
3 Tarefa 3: Solicitar uma explicação sobre um filme específico.
5O Interactive Access pode ser acessado pelo seguinte link: https://encurtador.com.br/
x4oxC.
– Exemplo: "Conte mais sobre o filme Toy Story"
Para executar todas as três tarefas, participaram quatro pes-
soas da Universidade de São Paulo, todas pesquisadoras na área da
computação e do mesmo grupo de pesquisa responsável por este
trabalho.
Os tempos de resposta foram calculados através de um script
integrado na própria estrutura do chatbot. Em cada uma das tarefa
foram consideradas três medidas distintas: tempo da geração da
resposta textual, tempo da geração do áudio e tempo total.
4.2
Avaliação de Usabilidade
Para executar a avaliação de acessibilidade participaram três pes-
soas da Universidade de São Paulo, todas pesquisadoras na área da
computação e do mesmo grupo de pesquisa responsável por este
trabalho, sendo duas delas com experiência prévia em Interação
Humano-Computador6.
Foi empregada uma avaliação de usabilidade com base nas dez
heurísticas de Nielsen [7, 8], onde uma ou mais tarefas são apresen-
tadas aos participantes que devem executá-las.
Os participantes deveriam procurar, individualmente, por pro-
blemas que firam as heurísticas e classificá-las em um dos quatro
graus de severidade:
• Cosmético, quando não há necessidade imediata de solução;
• Simples, quando o problema é considerado de baixa priori-
dade, isto é, pode ser reparado;
• Grave, nesse caso, o problema é considerado de alta priori-
dade e deve ser reparado;
• Catastrófico, quando é considerado muito grave e deve ser
reparado de qualquer forma.
Após as análises individuais os participantes se reuniram usando
uma plataforma de reuniões online para consolidar os resultados.
Nessa etapa, eles deveriam conversar para discutir seus achados,
suas concordâncias, discordâncias e chegarem em um acordo e
finalizarem os resultados em um único documento.
RESULTADOS
Nesta seção serão apresentados os resultados obtidos por meio
dos testes de carga e de usabilidade realizados com o ARIA e o
Interactive Access.
5.1
Resultados da avaliação de desempenho
5.1.1
Resultados do Teste de Carga/Estresse. Foram realizados tes-
tes de carga somente no ARIA, hospedado em uma máquina virtual
da Universidade de São Paulo, pois é o artefato a que temos acesso
ao código. Os resultados estão dispostos na Tabela 2.
Tabela 2: Resultados do Teste de Carga/Estresse.
Cenário
Requisições
Falhas
Média (ms)
Mediana (ms)
Cenário 1
5,07
Cenário 2
33720
4,64
Cenário 3
126730
4,51
6Os autores entendem a necessidade de avaliar as ferramentas com o público-alvo,
entretanto, até a data da submissão deste artigo o pedido de estudo com idosos está
em análise pela Plataforma Brasil.
CTIC’2024, Juiz de Fora/MG, Brazil
Ribeiro et. al.
Os resultados do teste de carga indicam que o sistema demonstra uma
capacidade notável de lidar com diferentes níveis de requisições. Em todos
os cenários, o sistema operou com alta confiabilidade, menos de 0,05% de
falhas. Além de ser rápido e consistência, dado que as médias e medianas
foram muito próximas e com valores muito pequenos. Assim, o sistema
demonstrou ser capaz de suportar até 500 usuários simultâneos de forma
eficiente.
5.1.2
Resultados dos Testes de Tempo de Resposta. Pelo mesmo motivo, fo-
ram realizados testes de tempo de resposta apenas no chatbot. Os resultados
estão descritos na Tabela 3.
Tabela 3: Tempo Médio por Tarefa.
Tarefa
Mensagem (s)
Áudio (s)
Total (s)
Tarefa 1
4,93
0,84
5,89
Tarefa 2
5,85
6,17
12,14
Tarefa 3
7,32
13,25
20,69
A análise dos resultados do teste de tempo revela que o chatbot é capaz
de responder em tempos satisfatórios para o usuário. É importante salientar
que as respostas textuais são bem rápidas, e que o tempo da geração de
áudio, como esperado, aumenta conforme o aumento do texto de resposta.
Um ponto a se destacar é que, mesmo em uma tarefa mais complexa, como
o caso da Tarefa 3, o chatbot responde em um tempo médio bastante acei-
tável, próximo de 20 segundos, proporcionando uma resposta completa e
interativa, com áudio e texto, para o usuário.
5.2
Resultados da avaliação de usabilidade
Os resultados das avaliações de usabilidade realizadas com os artefatos serão
apresentadas a seguir.
5.2.1
ARIA. Os avaliadores encontraram quatro problemas de usabilidade
no ARIA. São eles:
(1) Por vezes o ARIA teve algum problema de processamento que ocasi-
onou em erro, mas não deixa isso claro ao usuário;
(2) Em caso de erro no ARIA, independente do motivo, basta que o usuá-
rio envie alguma mensagem que o diálogo será retomado, entretanto,
isso pode não ficar claro para o usuário;
(3) Os avaliadores não encontraram estratégias que ajudassem o usuário
a evitar erros;
(4) Apesar de o usuário tirar suas dúvidas usando o próprio diálogo
com o ARIA, isso não fica claro para o usuário. O ARIA também não
possui documentação que auxilie na navegação.
Todos os problemas de usabilidade encontrados no ARIA foram classifi-
cados como grave e devem ser corrigidos em trabalhos futuros. Entretanto,
apesar de terem sidos considerados graves, não houve nenhum problema
que impedisse o andamento das interações seguindo as tarefas definidas
para a realização da avaliação, o que caracteriza que o ARIA possui boa
usabilidade.
5.2.2
Interactive Access. Com a avaliação realizada, os avaliadores concor-
daram com a existência de dois problemas de usabilidade, apresentados a
seguir.
(1) O site não possui documentação e ajuda;
(2) Ao clicar na diretriz usando o conteúdo central da página, é aberta
uma janela menor no estilo pop-up e, nele, não há opção para voltar
ou fechar a página.
Assim como no ARIA, os problemas de usabilidade encontrados no In-
teractive Access devem ser corrigidos em trabalhos futuros. Porém, os
problemas relatados não interferiram na execução de nenhuma das tarefas,
indicando que o Interactive Access possui boa usabilidade e, tanto o ARIA
quanto o Interactive Access, podem seguir para a etapa de avaliação com
usuários idosos.
CONSIDERAÇÕES FINAIS
Este trabalho relatou o desenvolvimento e avaliação de um chatbot acessível
que faz recomendações, ARIA, e de um site com um conjunto de diretrizes
para o desenvolvimento de chatbots acessíveis, Interactive Access.
Os resultados encontrados foram encorajadores para a continuação da
pesquisa, pois, nenhum problema grave ou que, de acordo com a experiência
dos autores, influencie de forma negativa ou cause frustração nos usuários,
principalmente os idosos.
A principal limitação deste trabalho é ainda não ter realizado estudos
com usuários reais e em contextos de uso reais. Como trabalhos futuros, se
pretende resolver os problemas de usabilidade encontrados no ARIA, realizar
estudos com usuários reais e, após essa etapa, verificar se há necessidade de
refinamento do chatbot.
AGRADECIMENTOS
Os autores gostariam de agradecer o apoio financeiro da FAPESP (projeto
número 2022/07016-9) e CNPq. O presente trabalho também foi realizado
com apoio da Coordenação de Aperfeiçoamento de Pessoal de Nível Superior
– Brasil (CAPES) – Código de Financiamento 001.

--- FIM DO ARQUIVO: 30474-829-24922-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30476-829-24924-1-10-20241001.txt ---
Impulsionando a descoberta de tratamentos na medicina através
da representação distribuída de palavras
Matheus V. V. Berto∗
Tiago A. Almeida
matheus.berto@estudante.ufscar.br
talmeida@ufscar.br
Departamento de Ciência da Computação (DComp-So), Universidade Federal de São Carlos (UFSCar)
Sorocaba, São Paulo
Figure 1: Framework proposto neste estudo. Adaptado de Berto et al. [2].
CTIC’2024, Juiz de Fora/MG, Brazil
Matheus V. V. Berto and Tiago A. Almeida
possibilitam a descoberta de relações – explícitas ou implícitas – en-
tre os textos [5]. Assim, termos próximos no espaço vetorial tendem
a compartilhar características sintáticas ou semânticas.
1.1
Motivação
O presente estudo foi motivado por recentes demonstrações de uso
de word vectors para extração de conhecimento latente codificado
em documentos da literatura científica. Mais precisamente, um dos
trabalhos mais notórios sobre o tema, conduzido por Tshitoyan
et al. [8], provou ser possível empregar word embeddings de forma
a identificar relações complexas entre materiais termoelétricos e
antecipar sua descoberta através de análises temporais de modelos
não supervisionados. A avaliação de tal estratégia sobre outras
áreas de conhecimento também foi sugerida pelos autores.
Seguindo a recomendação de Tshitoyan et al. [8], Shetty and
Ramprasad [7] e Yang [9] também propuseram protocolos para
avaliação de inferência de informações sobre outros temas. Entre-
tanto, nenhuma dessas pesquisas abordou textos relacionados à
saúde humana. O aprimoramento da busca por descobertas científi-
cas nessa área é de considerável importância e relevância.
1.2
Objetivos e contribuições
Com base nos estudos de Tshitoyan et al. [8], Shetty and Ramprasad
[7] e Yang [9], este trabalho tem por objetivo avaliar a possibilidade
de decodificação de conhecimento latente na literatura biomédica.
Mais precisamente, a pesquisa busca analisar se o emprego de word
embeddings é capaz de acelerar descobertas acerca de tratamentos
atualmente aprovados para a Leucemia Mieloide Aguda (LMA ou,
em inglês, AML) – uma forma rara e altamente letal de câncer [4].
O restante deste artigo apresenta uma breve revisão da litera-
tura sobre o tema e indica a metodologia empregada no estudo,
além dos resultados obtidos. Entretanto, este texto representa uma
versão sumarizada do texto completo do estudo, redigido exclusi-
vamente para participação no CTIC/WebMedia 2024. O artigo que
descreve em detalhes toda a pesquisa desenvolvida foi publicado
pelos autores e está disponível em Berto et al. [2].
TRABALHOS RELACIONADOS
Dentre os primeiros métodos para representação computacional
de textos, a codificação one-hot constitui-se em transformar ter-
mos de um documento em vetores esparsos. Estes vetores possuem
dimensionalidade igual ao tamanho total de palavras presentes
no vocabulário do corpus (𝑉). Todos os elementos dos vetores são
preenchidos com zeros, exceto aquele o qual corresponde ao índice
do termo em questão no vocabulário. Entretanto, essa estratégia
apresenta alto custo computacional, visto que quanto maior a exten-
são do vocabulário, mais esparsos os vetores se tornam, dificultando
operações aritméticas e exigindo mais uso de memória.
Por outro lado, word embeddings são capazes de representar
coerentemente características semânticas e sintáticas das palavras,
além de não apresentarem dimensionalidade obrigatoriamente equi-
valente à quantidade de termos distintos presentes nos documentos.
Um dos trabalhos pioneiros responsáveis pela popularização da re-
presentação distribuída foi proposto por Mikolov et al. [5]. Nele, um
novo algoritmo denominado Word2Vec foi implementado através
de duas arquiteturas distintas: Continuous Bag-of-Words (CBOW) e
Skip-Gram. Ambos os métodos empregam redes neurais rasas com
o objetivo de predizer termos próximos vinculados ao contexto de
uma palavra-alvo ou vice-versa.
A Figura 2 ilustra o funcionamento da arquitetura Skip-Gram.
A partir da codificação one-hot inicial de um termo na camada
de entrada, cálculos são executados pelos neurônios da camada
oculta. Finalmente, na camada de saída, uma função exponencial
normalizada (ou softmax) gera um vetor, no qual os elementos
indicam a probabilidade de cada termo de 𝑉pertencer à vizinhança
da palavra de entrada.
Figure 2: Arquitetura Skip-Gram. Adaptado de Berto et al.
[2].
O vetor normalizado de probabilidades gerado pela camada de
saída da arquitetura não é o artefato mais útil da arquitetura. Na
verdade, são duas matrizes internas – 𝑊e 𝑂– populadas durante
o treinamento dos modelos que armazenam as representações dis-
tribuídas. As linhas da matriz 𝑊são as word embeddings, enquanto
as colunas da matriz 𝑂são chamadas de output embeddings. As
matrizes possuem uma quantidade 𝑉de linhas e colunas, respec-
tivamente. O produto escalar entre dois vetores, sendo cada um
deles pertencente a uma dessas matrizes, expressa a probabilidade
de ambos os termos coocorrerem em um mesmo texto (Figura 3).
Dessa forma, o valor do produto escalar é diretamente proporcional
ao nível de correlação entre as palavras analisadas.
Figure 3: Cálculo de probabilidade de contexto. Adaptado
de Berto et al. [2].
Outro método popular para se computar word vectors é o Fast-
Text [3]. Através dele, além de vetores para cada palavra de um
vocabulário 𝑉, também são gerados vetores de sub-palavras, repre-
sentadas por 𝑛-gramas (i.e., subconjuntos de 𝑛caracteres contíguos
em um texto). Tal método é capaz de melhorar a interpretação sub-
jetiva codificada a partir dos vetores e representar termos pouco
frequentes ou com estrutura complexa.
Como uma aplicação refinada dos algoritmos e arquiteturas de
representação distribuída acima mencionados, Tshitoyan et al. [8]
empregaram modelos baseados em Word2Vec e Skip-Gram gerados
de forma cronológica e incremental capazes de predizer elementos
da ciência dos materiais atualmente conhecidos. Semelhantemente,
Yang [9] também demonstraram a possibilidade de codificar con-
hecimento em textos relacionados a polímeros. A partir do proces-
samento de 500.000 documentos, o estudo foi capaz de identificar
os principais materiais citados pela literatura ao longo dos anos.
Impulsionando a descoberta de tratamentos na medicina através da representação distribuída de palavras
CTIC’2024, Juiz de Fora/MG, Brazil
Yang [9] também demonstrou a alta capacidade de usar word
embeddings para realizar predições significativas relacionadas a
elementos sobre os estudos de células eletroquímicas de combustão.
Dessa forma, com base nos resultados recentes publicados na lite-
ratura, este trabalho estende as estratégias empregadas de forma a
expandir a análise de conhecimento latente no contexto da LMA.
METODOLOGIA
Esta seção contempla as principais etapas executadas na realização
do presente estudo. Elas estão sumarizadas na Figura 1.
3.1
Coleta e pré-processamento de dados
A partir de um conjunto pré-definido de palavras-chave escolhidas
por um especialista em LMA, somado a nomes e sinônimos de com-
postos atualmente aprovados para o tratamento da doença1, cerca
de 272.000 artigos científicos foram coletados. Todos os artigos
recuperados estão na língua inglesa e tiveram sua coleta ou fil-
tragem através de scripts escritos na linguagem Python com acesso
ao motor de buscas PubMed2.
O conjunto de textos coletados passou então por uma etapa de
pré-processamento, na qual sinônimos de drogas e outras substân-
cias relevantes ao projeto foram normalizadas para um único termo
comum. A substituição automática foi realizada a partir da con-
sulta a arquivos contendo milhões de registros biológicos disponi-
bilizados pela plataforma PubChem 3 e teve por objetivo mitigar
a sensibilidade dos algoritmos à coocorrência de termos no texto.
Sinônimos da LMA também foram todos transformados para a
abreviação universal “AML”. Por fim, outras técnicas, como o trata-
mento de caracteres especiais, palavras irrelevantes (stopwords) e
lematização foram aplicadas utilizando-se o conjunto de bibliotecas
computacionais NLTK 4.
3.2
Modelos de representação distribuída
Os algoritmos Word2Vec e FastText foram executados sobre conjun-
tos de documentos cronologicamente ordenados. O corpus abrangeu
artigos escritos entre os anos 1963 e 2022, totalizando 60 modelos
de representação distribuída que agregam textos publicados antes
ou durante seu respectivo ano. As implementações dos algoritmos
mencionados foram realizados com o uso da biblioteca Gensim 5.
3.3
Análise de conhecimento latente
A partir dos artigos coletados da literatura, foi construída uma
linha do tempo responsável por registrar a primeira ocorrência de
cada uma das drogas-alvo deste estudo em publicações diretamente
ligadas ao contexto da LMA.
A evolução histórica da correlação entre cada uma das drogas-
alvo e o termo “AML”, calculada a partir do produto escalar (Figura 3),
foi analisada anualmente. Períodos de tamanho fixo 𝑤que apresen-
tam um crescimento significativo na correlação entre os termos são
de especial interesse. Tal aumento expressivo indica que o composto
em análise deve ser recomendado para investigação por especialis-
tas em LMA.
Os compostos avaliados em cada intervalo precisam atender
critérios rígidos para serem considerados significativos. Caso um
composto respeite algum dos critérios estabelecidos, o framework
1https://www.cancer.gov/about-cancer/treatment/drugs/leukemia#3
2https://pubmed.ncbi.nlm.nih.gov/
3https://pubchem.ncbi.nlm.nih.gov/
4https://www.nltk.org/
5https://radimrehurek.com/gensim_3.8.3/
proposto emite uma notificação de testagem no ano seguinte à tal
identificação.
(1) a derivada da curva de produto escalar é positiva e a taxa de
crescimento é maior ou igual a valor mínimo 𝑥; ou
(2) o aumento final do produto escalar durante o período, cal-
culado pela subtração entre o último e o primeiro valor reg-
istrado, deve ser igual ou superior a um segundo parâmetro
𝑡, sendo 𝑡>> 𝑥.
RESULTADOS
Diferentes valores para os parâmetros 𝑤, 𝑥e 𝑡envolvidos nos
critérios de análise foram testados empiricamente com ajuda de
especialistas, sendo {3; 2; 4} a combinação final utilizada. A partir
da aplicação de tais parâmetros ao protocolo de validação descrito
na seção anterior sobre os históricos de produto escalar de cada um
dos compostos-alvo, o framework construído emitiu notificações
indicadas na Figura 5.
O sistema foi capaz de emitir 11 notificações para 6 drogas distin-
tas, sendo Azacitidine e Prednisone as mais frequentes (4 e 3 vezes,
respectivamente). Considerando apenas a primeira notificação para
cada droga, o framework proposto sugeriu a testagem com cerca de
5 anos antes da real data de associação entre o composto e a doença.
Notavelmente, o composto Arsenic Trioxide apresentou maior ante-
cedência em seu valor de correlação, sendo sugerido para testagem
11 anos antes da sua primeira associação com LMA.
Também foi analisado o real impacto do uso do framework pro-
posto, i.e., foi quantificado o quanto as notificações de testagem
emitidas seriam capazes de acelerar as descobertas. Iniciando em
1963, a estratégia de ranqueamento por produto escalar foi empre-
gada. Para cada ano, foram selecionados apenas os três primeiros
compostos que apresentaram maior correlação com LMA. Assim,
apenas as melhores predições dos modelos foram consideradas.
Conforme indicado na Figura 4, o uso dessa estratégia (curva de
cor laranja) foi capaz de acelerar a porcentagem de descoberta dos
21 compostos-alvo deste estudo em até 2.3× após 1968, quando
comparado ao cenário sem o emprego do framework proposto.
Figure 4: Porcentagem de compostos preditos pelos modelos e
que foram posteriormente associados à LMA. A linha laranja
compara o uso de apenas as três primeiras predições anuais
a uma escolha aleatória de drogas. Adaptado de Berto et al.
[2].
CTIC’2024, Juiz de Fora/MG, Brazil
Matheus V. V. Berto and Tiago A. Almeida
Figure 5: Notificações de testagem de compostos emitidas pelo sistema proposto. Adaptado de Berto et al. [2].
CONCLUSÕES E TRABALHOS FUTUROS
O presente artigo apresentou um resumo da proposta detalhada
em Berto et al. [2]: um sistema baseado em modelos de represen-
tação distribuída capazes de codificar coerentemente informações
relacionadas à saúde humana, especificamente sobre a Leucemia
Mieloide Aguda (LMA). O estudo foi baseado em diversas pesquisas
da literatura científica que demonstraram êxito de suas aplicações
em áreas de conhecimento distintas. Portanto, a extensão de tais
pesquisas sobre o contexto da LMA apresenta originalidade e em-
basamento técnico.
Os resultados obtidos comprovaram que o framework proposto
foi capaz de orientar investigações de forma a potencialmente im-
pulsionar o descobrimento de tratamentos para a referida doença.
A estratégia desenvolvida apresenta vantagens como o emprego de
algoritmos consolidados e computacionalmente baratos, exigindo
pouco recurso computacional para sua execução [2].
Apesar dos resultados encontrados serem bastante promissores,
o estudo descrito possui certas limitações. As representações dis-
tribuídas geradas pelos algoritmos Word2Vec e FastText são estáticas,
i.e., uma determinada palavra do corpus será sempre vinculada ao
mesmo vetor, independentemente do contexto da sentença no qual
ela esteja presente. Portanto, a adaptação do sistema para o em-
prego de representações contextuais consideradas estado-da-arte
em diversas aplicações pode potencializar ainda mais os resultados.
AGRADECIMENTOS
Este trabalho foi financiado pelas agências de fomento FAPESP
(2021/13054-8 e 2022/07236-9), Capes e CNPq. Os autores agrade-
cem a estreita e inestimável colaboração de Breno L. Freitas (Shopify
Inc.), Profa. Dra. Carolina Scarton (The University of Sheffield)
e Prof. Dr. João A. Machado-Neto (ICB/USP) nesta pesquisa. Os
autores também são gratos à Priscila Portela Costa pela sua con-
tribuição durante o planejamento inicial do projeto.

--- FIM DO ARQUIVO: 30476-829-24924-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30477-829-24925-1-10-20241001.txt ---
iRev: Um framework de avaliação de sistemas de recomendação
baseados comentários textuais
Guilherme Bittencourt
bittenkourt.gmf@aluno.ufsj.edu.br
UFSJ - Minas Gerais - Brasil
Naan Vasconcelos
naan.vasconcelos@aluno.ufsj.edu.br
UFSJ - Minas Gerais - Brasil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ - Minas Gerais - Brasil
2023. Realizamos um estudo detal-
hado dos principais avanços, dos principais conjuntos de dados
e das métricas utilizadas. Observamos que as coleções de dados
In: IV Concurso de Trabalhos de Iniciação Científica (CTIC 2024). Anais Estendidos do
XXX Simpósio Brasileiro de Sistemas Multimídia e Web (CTIC’2024). Juiz de Fora/MG,
Brazil. Porto Alegre: Brazilian Computer Society, 2024.
© 2024 SBC – Sociedade Brasileira de Computação.
ISSN 2596-1683
mais utilizadas dentre os trabalhos relevantes são as bases de da-
dos de produtos da Amazon e de pontos de interesse da Yelp, por
disponibilizarem as interações usuário/item e também os comen-
tários provenientes das interações. Em relação às métricas de avali-
ação, é possível observar que métricas de erro são as formas de met-
rificação mais utilizadas pelos trabalhos, seguidas pelas métricas de
avaliação de ranking. Porém, apesar do consenso na comunidade de
recomendação de que é necessário mais do que precisão para avaliar
a eficácia dos SsR, a grande maioria dos trabalhos priorizam a pre-
cisão sobre outras dimensões de qualidade, tais como serendipidade
e diversidade. Outra limitação surge em relação aos algoritmos que
são considerados estado-da-arte e suas configurações. Não existe
um consenso das linhas de bases a serem consideradas, cada artigo
utiliza um conjunto distinto e as configurações dos parâmetros
raramente são reportadas. Menos de 50% dos trabalhos analisa-
dos disponibiliza código fonte de suas propostas, e menos de 30%
fornece as configurações de parâmetros dos algoritmos propostos.
Nesse sentido, como segunda contribuição, implementamos os
10 principais algoritmos de recomendação review-aware, consoli-
dando todos os códigos fontes gerados, bem como todos os artefatos
levantados durante o mapeamento sistemático (métricas e bases
de dados) em um framework aberto e publicamente disponível1,
denominado iRev, com o objetivo de facilitar a pesquisa e com-
parações entre abordagens na área de review aware. Por fim, como
terceira contribuição, realizamos uma análise experimental de difer-
entes algoritmos, com diversas coleções e métricas, destacando as
principais direções para desenvolvimentos futuros.
Todas as implementações, execuções e avaliações dos resultados
foram realizadas pelo aluno Guilherme Bittencourt, com auxílio do
aluno Naan Vasconcelos, sob a orientação do professor Leonardo Rocha.
MAPEAMENTO SISTEMÁTICO
Nesta seção abordamos o processo de coleta, filtragem e seleção
dos artigos relacionados a RARSs.
2.1
Fase 1: Questões de pesquisa, palavras de
busca e fontes digitais
As questões de pesquisa que deverão ser respondidas são:
• QP1: Como os algoritmos de recomendação utilizam técnicas de
PLN para definir as preferências dos usuários por meio de seus
comentários?
• QP2: Quais são as bases de dados e métricas mais prevalentes uti-
lizadas na avaliação de algoritmos em estudos relacionados a esse
tema?
• QP3: Como as avaliações experimentais são conduzidas nos estudos
analisados, considerando estados da arte, configurações e parâmet-
ros dos modelos?
1https://github.com/guibitten03/iRev
CTIC’2024, Juiz de Fora/MG, Brazil
Bittencourt G. et al.
Recorremos ao mecanismo de pesquisa do Google Scholar para
realizar as três consultas abaixo:
• SS-Q1: ("review based” OR “review aware” OR "review modeling")
AND ("recommender systems" OR "recommendation systems" OR
"recommender system") AND (“text” OR "textual" OR "review")
• SS-Q2: (“review based” OR “review aware” OR "review modeling")
AND (“recommender systems“ OR “recommendation systems“ OR
“recommender system“) AND (“evaluation” OR "measure" OR "met-
rics")
• SS-Q3: (“review based” OR “review aware” OR "review modeling")
AND ("recommender systems" OR "recommendation systems" OR
"recommender system") AND (“source code” OR “reproducibility”
OR “empirical” OR “experimental”)
2.2
Fase 2: Seleção de trabalhos relevantes
Após a coleta, aplicamos um filtro de data entre 2014 e 2023, assegu-
rando um escopo de 10 anos de publicações, acumulando um total
de 1.190 artigos. Eliminamos as duplicadas e aplicamos um segundo
filtro considerando apenas artigos das 100 conferências de maior
fator de impacto de acordo com a research.com), tais como RecSys,
WWW, WSDM, SIGIR, etc., resultando em 681 artigos. Por fim, em-
pregamos um filtro avançado com critérios de inclusão e exclusão.
Realizamos uma análise manual de cada artigo, classificando-os
como relevantes ou não, de acordo com os critérios estabelecidos:
Critérios de Inclusão
• O método principal empregado para realizar as recomendações é
o uso dos comentários dos usuários, considerando as avaliações
numéricas como um suporte adicional.
• Propõem avanços e inovações no domínio, não se limitando à otimiza-
ção de algoritmos preexistentes.
• Realizam avaliações experimentais comparativas entre os algorit-
mos que utilizam comentários do usuário e o método proposto nos
artigos correspondentes.
Critérios de Exclusão
• Além dos comentários, empregam outras fontes de informação, como
imagens, áudios ou vídeos para prever as preferências do usuário.
• São surveys, casos de estudo, revisões sistemáticas ou experimentais
sobre os algoritmos do cenário.
• Utilizam os comentários exclusivamente para justificar as recomen-
dações, focando na explicabilidade..
Após a aplicação desses critérios, restaram 117 artigos que foram
identificados como mais relevantes.
2.3
Fase 3: Extração das informações dos artigos
Realizamos uma leitura detalhada dos 117 artigos restantes para
identificar as principais características das soluções propostas, suas
principais inovações e contribuições para a literatura e as metodolo-
gias de avaliação utilizadas. A seguir, detalhamos o resultado dessa
análise visando responder as três questões de pesquisas levantadas
no início desta seção.
AVALIAÇÃO SISTEMÁTICA
Essa seção detalha como os SsR vêm sendo avaliados por meio de
uma inspeção das avaliações experimentais dos 117 artigos sele-
cionados.
3.1
Bases de dados
Conforme podemos observar na Figura 1, que as bases de dados da
Amazon e da Yelp são as mais utilizadas. Ambas tratam de cenários
clássicos para análises de recomendação review aware devido ao
grande número de usuários que comentam sobre os itens. A Ama-
zon é composta de subcoleções de acordo com a categoria de item
e a Yelp por diferentes cidades. A grande maioria dos trabalhos não
especifica qual categoria/cidade utilizada nos experimentos. Outra
questão crítica é que essas coleções têm cortes temporais que tam-
bém não são mencionados. Essas questões impactam negativamente
na reprodutibilidade desses trabalhos.
Figure 1: Frequência de bases de dados em experimentações.
3.2
Métricas
Conforme podemos observar na Figura 2, há um constante interesse
sob a precisão dos ratings preditos. O consenso na comunidade de
SR é que a precisão por si só não é suficiente para avaliar a eficácia
prática e o valor agregado das recomendações, sendo necessário
outras técnicas de avaliação como diversidade e serendipidade. Não
identificamos nenhum trabalho dentre os 117 analisado que busca
avaliar os modelos nessas dimensões, sendo esse um importante
ponto fraco que avaliações futuras precisam considerar.
Figure 2: Frequência de métricas em experimentações.
3.3
Algoritmos e Configuração de Parâmetros
Com respeito a disponibilidade do código fonte dos algoritmos
propostos, temos que pelo menos 50% dos trabalhos disponibiliza
código fonte, o que dificulta a utilização dos mesmos como linhas
de base. Um novo algoritmo é comparado, em média, com apenas
três outros algoritmos e, no máximo, nove. Outra observação im-
portante é sobre o processo de calibração dos algoritmos, em que
menos de 30% dos trabalhos apresentam em detalhes desse processo.
Todas essas questões impacta na replicabilidade dos trabalhos.
iRev: Um framework de avaliação de sistemas de recomendação baseados comentários textuais
CTIC’2024, Juiz de Fora/MG, Brazil
IREV
Nessa seção detalhamos nossa proposta de um framework de avali-
ação de SsR baseados comentários textuais: iRev (disponível em
https://github.com/guibitten03/iRevRS) .
4.1
Algoritmos Implementados
Dos 117 algoritmos examinados, selecionamos todas as abordagens
utilizadas em pelo menos dois artigos diferentes. Os 10 algoritmos
selecionados são apresentados na Tabela 1, onde a coluna ’Linhas de
Base’ representa quantas vezes o algoritmo foi utilizado em outros
trabalhos.
Algoritmo
# Linhas de Base
# Citações
DeepCoNN
Narre
D-ATTN
Daml
MPCN
CARL
ANR
CARP
HRDR
RGNN
Table 1: Algoritmos mais utilizados como linhas de base.
O DeepCoNN utiliza redes neurais convolucionais para capturar
as informações relevantes nos comentários [10]. O MPCN, por sua
vez, emprega uma arquitetura de co-atenção multi-pontual para
capturar o contexto em diferentes níveis de granularidade [8]. O
D-ATTN adota uma rede neural de atenção dupla considerando
os comentários e as características do usuário/item [7]. O NARRE
utiliza uma abordagem baseada em redes neurais para modelar
a atenção e as interações entre aspectos nos comentários [1]. O
DAML propõe uma abordagem de aprendizado mútuo de atenção
entre avaliações e comentários [4]. O CARL utiliza redes neurais
convolucionais em cápsulas para gerar recomendações e fornecer
explicações sobre as preferências do usuário [9]. O CARP introduz
uma estrutura de rede neural para incorporar a atenção contextual
na modelagem de avaliações e comentários [3]. O ANR adota uma
abordagem baseada em aspectos para recomendação, capturando
a relação entre aspectos e usuários/itens [2]. O HRDR realiza uma
abordagem conjunta de representações de aprendizado profundo
de avaliações e comentários [5]. Por fim, o RGNN propõe uma rep-
resentação hierárquica de comentários de avaliações em forma de
grafo para aprimorar a precisão das recomendações [6].
4.2
Configuração dos Algoritmos
Utilizamos os códigos dos algoritmos provenientes no GitHub dos
respectivos autores e realizamos uma tunagem de parâmetros, de
acordo com o apresentado na Tabela 2.
4.3
Coleções de Dados
A Tabela 3 apresenta alguns detalhes sobre as coleções disponibi-
lizadas pelo iRev. Para garantir reprodutibilidade, todas elas são
divididas em subconjuntos de treino, teste e validação. O conjunto
de treino é composto por 80% dos dados, enquanto os conjuntos de
validação e teste possuem 10% cada. Os reviews presentes nos dados
foram pré-processados utilizando a biblioteca NLTK, que permitiu
Parâmetros
Valores
Épocas de treinamento
10, 20, 50
Função de perda
MSE
Otimizador
ADAM
Dimensões dos vetores de usuário e item
Dimensões dos vetores de palavras
Codificadores utilizados
TF-IDF, Word2Vec e FastText.
Taxa de dropout
0.5
Weight decay
1𝑒−3
Tamanho do lote
128 a 32
Tamanho máximo dos documentos
500 palavras
Taxa de aprendizado
2𝑒−3
# filtros nas camadas convolucionais
Table 2: Configurações dos parâmetros dos algoritmos
realizar tratamentos nos textos, tais como remoção de stopwords e
lematização.
Coleção
# Usuários
# Itens
Esparsidade
Amazon - Video Games
10.000
17.005
99.99%
Yelp - Tampa
18.437
8.664
99,99%
Yelp - Philadelphia
32.376
14.226
99.99%
Table 3: Visão geral das coleções utilizadas na avaliação.
4.4
Métricas
Para avaliar as recomendações dos algoritmos consideramos quatro
métricas de precisão: duas métricas de erro (i.e., MSE e MAE) que
avaliam a diferença entre o rating real e o previsto pelos algorit-
mos; e duas de efetividade (i.e. Accuracy e F1 Score) que avaliam
o quão bem o algoritmo aprendeu o comportamento do usuário.
O consenso na comunidade de SsR é que a precisão por si só não
é suficiente para avaliar a eficácia prática e o valor agregado das
recomendações. Assim, além da precisão, exclusivamente consid-
erada em praticamente todos revisados neste artigo, consideramos
outras duas métricas: serendipidade e diversidade. A serendipidade
se refere a descoberta de itens úteis e inesperados e a diversidade
aos itens recomendados diferentes do histórico de consumo.
4.5
Avaliação Experimental
Com o objetivo de validar o framework proposto, realizamos uma
avaliação experimental de todos os algoritmos implementados, con-
siderando as cinco métricas nas três coleções disponibilizadas e os
resultados são apresentados na Tabela 4 apresentamos os resultados
obtidos. Observamos que não há um destaque único. Na coleção
Amazon, por exemplo, dos 10 algoritmos analisados, cinco deles se
destacaram em distintas métricas. Enquanto os algoritmos HRDR,
D-ATTN e o NARRE se destacaram em métricas de precisão, o Deep-
CONN e o ANR se destacaram em diversidade e serendipidade.
Os resultados também variam de acordo com as coleções. Na
Yelp - Tampa, o segundo algoritmo mais recente proposto, HRDR,
obteve melhores resultados nas métricas de precisão, corroborando
com os experimentos mencionados no artigo original. Além disso,
observou-se que os algoritmos CARP, CARL e MPCN não obtiveram
resultados significativos nessas mesmas métricas, o que contradiz
as afirmações de seus respectivos artigos. Por outro lado, esses al-
goritmos foram destaque em termos de serendipidade e diversidade.
O ANR não obteve o melhor resultado em nenhuma métrica, mas
CTIC’2024, Juiz de Fora/MG, Brazil
Bittencourt G. et al.
Coleção
Amazon - Video Games
Yelp - Tampa
Yelp - Philadelphia
Measure
MSE
MAE
Acc
F1@10
Ser
MSE
MAE
Acc
F1@10
Ser
MSE
MAE
Acc
F1@10
Ser
DeepCoNN
1.541
0.928
0.206
0.323
0.141
0.197▲
1.337
0.892
0.361▲
0.216
0.147
0.097
1.561
0.994
0.300
0.125
0.159• 0.224•
D ATTN
1.127
0.727▲
0.228
0.428
0.048
0.060
1.346
0.912
0.329
0.197
0.159
0.088
1.207
0.871
0.343
0.198
0.141
0.099
MPCN
1.636
0.993
0.121
0.262
0.069
0.1598
1.447
0.965
0.288
0.124
0.164•
0.163
1.322
0.913
0.323
0.121
0.145
0.125
NARRE
1.075
0.691
0.255
0.459▲
0.061
0.141
1.302
0.892
0.348
0.218
0.147
0.049
1.172
0.844
0.364
0.218
0.146
0.063
DAML
1.149
0.744
0.234
0.411
0.035
0.094
1.364
0.935
0.308
0.177
0.146
0.037
1.270
0.899
0.329
0.173
0.152
0.037
CARL
1.286
0.839
0.326
0.152
0.021
0.081
1.525
0.995
0.293
0.134
0.166•
0.125
1.306
0.921
0.332
0.171
0.153• 0.224•
CARP
1.262
0.824
0.340
0.170
0.213
0.124
1.583
0.987
0.285
0.107
0.147
0.225▲
1.336
0.902
0.324
0.136
0.152
0,172
ANR
1.171
0.780
0.381
0.226
0.242▲
0.071
1.288
0.899
0.338
0.215
0.148
0.061
1.115▲0.813▲0.397▲0.267▲
0.124
0.050
HRDR
1.039▲
0.751
0.456▲
0.248
0.082
0.081
1.257▲0.879▲
0.355
0.249▲
0.146
0.050
1.482
0.964
0.324
0.209
0.146
0.057
RGNN
1.179
0.792
0.297
0.150
0.101
0.148
1.364
0.916
0.314
0.186
0.142
0.124
1.296
0.896
0.287
0.128
0.150
0.075
Table 4: Resultados validados com o teste de Wilcoxon com um valor p = 0,05. ▲representa ganhos significativos e • empates estatísticos.
apresentou resultados consistentes e estáveis em termos de desem-
penho. O algoritmo RGNN, embora seja o mais recente em termos
de proposta, apresentou resultados inferiores a muitas outras es-
tratégias avaliadas. Na coleção Yelp - Philadelphia, o algoritmo ANR
mostrou os melhores resultados nas quatro métricas relacionadas
à efetividade, mais uma vez alinhados com o que foi apresentado
no artigo original. O algoritmo CARL foi melhor nessa coleção em
comparação com a anterior, obtendo os melhores resultados de
serendipidade e diversidade, juntamente com o DeepCoNN.
Grande parte dos algoritmos apresentaram resultados consis-
tentes com seus estudos originais. No entanto, alguns algoritmos
não obtiveram bons resultados em comparação com o que foi de-
scrito pelos autores. Algoritmos como CARL, CARP, MPCN e RGNN,
que teoricamente deveriam superar metodologias como DATTN,
ANR e NARRE, não tiveram sucesso em nossa avaliação empírica.
Esses resultados reforçam a importância do iRev por avançar na
questão da reprodutibilidade, por ser um repositório público não
apenas de coleções de dados, como também dos próprios algoritmos
e seus processos de tunagem de parâmetros.
CONCLUSÕES E TRABALHOS FUTUROS
Como primeira contribuição, esse trabalho apresenta um mapea-
mento sistemático dos estudos sobre sistemas de recomendação
review-aware (RARSs) selecionando e investigando os 117 artigos
relevantes publicados nos principais veículos da área (e.g., Rec-
Sys, SIGIR, WWW, etc.), identificando esforços, resultados, con-
tribuições e limitações relevantes. A partir desse levantamento,
propomos e disponibilizamos um framework, denominado iRev,
contendo a implementação dos 10 principais RARSs, bem como
todos os artefatos levantados durante o mapeamento sistemático
(métricas e bases de dados) com o intuito de mitigar a limitação at-
ual de falta de reprodutibilidade devido à ausência de códigos fontes
e de confiabilidade devido à ausência de distintas métricas de avali-
ação. Para validar o iRev, realizamos uma avaliação completa das
principais abordagens, considerando diferentes coleções de dados e
métricas. Nossos resultados mostram que as SsR baseadas em redes
neurais, especialmente as que utilizam mecanismos de extração de
atenção e aspecto, obtiveram os resultados mais competitivos. Por
outro lado, tais resultados também reforçam que não há um único
algoritmo que se destaque de forma absoluta, deixando claro que
ainda há espaço de melhora considerável a ser explorado por novas
estratégias. Como trabalhos futuros, visamos complementar o iRev
com a implementação de outras estratégias, tornando o repositório
uma referência para que pesquisadores da área.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Fapemig, FAPESP e
AWS.

--- FIM DO ARQUIVO: 30477-829-24925-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30478-829-24926-1-10-20241001.txt ---
CTIC’2024, Juiz de Fora/MG, Brazil
Santos et al.
dos usuários por áreas urbanas, visando ainda verificar a validade
em mais de uma fonte de LBSN.
DESCRIÇÃO DOS DADOS E METODOLOGIA
Em uma primeira etapa da pesquisa, foram usadas duas LBSNs
diferentes, Google Places e Foursquare. Os dados do Google Places
foram disponibilizado pelos autores de [7] e [13] para pesquisas e
abrangem o mundo todo. Os dados do Foursquare foram disponi-
bilizadas pelos autores de [18], englobando o mundo todo. Para a
nossa análise, fizemos uma seleção para a cidade de Curitiba, que
resultaram em dois datasets distintos, para os quais foram coletados
dados da cidade de Curitiba no período de 2010-2013, para o Google
Places, formado por 8.372 avaliações de 4.909 usuários em 2.213 es-
tabelecimentos localizados em 69 dos 75 bairros de Curitiba, e 2014,
sendo composto por 5.116 usuários que fizeram 53.253 check-ins
em 8.523 pontos diferentes da cidade de Curitiba em 62 dos bairros,
para o Foursquare.
Ao analisar as categorias mais frequentadas pelos usuários de
cada uma das LBSNs, presentes na Figura 1, por exemplo, a presença
de restaurantes foi maior no Google Places; o Foursquare, por outro
lado, apresentou uma grande ocorrência de categorias como casa e
trabalho, categorias não presentes no Google Places. Ademais, os
usuários do Foursquare apresentaram uma recorrência no uso da
plataforma superior quando comparados aos do Google Places, os
quais costumam avaliar em intervalos de tempo bem diferentes dos
usuários do Foursquare.
Figura 1: Categorias mais avaliadas no Google Places (es-
querda) e no Foursquare (direita)
Como passo metodológico para entender o interesse dos usuários,
construiu-se um grafo não dirigido ponderado 𝐺= (𝑉, 𝐸), no qual
o conjunto 𝑉de nós identifica os bairros de Curitiba, e uma aresta
𝑒𝑖,𝑗∈𝐸conecta o bairro 𝑣𝑖∈𝑉ao bairro 𝑣𝑗∈𝑉, com peso
𝑤𝑖,𝑗∈N indicando a quantidade de usuários que têm avaliações
de estabelecimentos localizados em ambos os bairros. Estas arestas
representam então o interesse de usuários por dois bairros distintos.
Além disso, essa rede possui self-loops, isso é, uma aresta 𝑒𝑖,𝑖∈𝐸
conecta um bairro 𝑣𝑖∈𝑉a ele mesmo, nesta aresta o peso 𝑤𝑖,𝑖∈N
determina a quantidade de usuários que avaliou pelo menos duas
vezes um mesmo bairro. A rede de interesse formada pelos dados
do Google Places na cidade de Curitiba pode ser vista na figura 2.
Nela, arestas de maior largura indicam um peso mais elevado, e o
tamanho do nó representa o seu grau.
Na continuidade desta primeira etapa, de forma a analisar os
fatores que influenciam o comportamento urbano, foram utilizados
os dados socioeconômicos dos bairros de Curitiba, disponibilizados
pelo Censo Demográfico Brasileiro de 2010 realizado pelo IBGE.
Para os dados econômicos, foi coletada a renda média mensal e para
os aspectos sociais, a composição racial de cada bairro, sendo ela
Figura 2: Rede de Interesse de Curitiba (Google Places).
formada pelas categorias: Branca; Preta; Amarela; Parda; Indígena.
Para investigar o impacto da polarização política, foram coletados
em Curitiba no ano de 2014, por ser mais próximos dos datasets
utilizados, dados do Tribunal Regional Eleitoral referentes ao 2°
turno das eleições para presidente.
Com o objetivo de examinar o impacto de diferenças econômicas,
sociais, políticas e espaciais no interesse dos usuários, calculou-se
a diferença entre os bairros, para esses fatores. Para a renda média
mensal foi usada a diferença absoluta, bem como para a polarização
política, na qual usou-se a diferença absoluta entre os percentuais
que votaram na candidata Dilma Rousseff, de modo similar a [8] e
[10]. Para quantificar a diferença racial entre dois bairros, usou-se
a mesma técnica de [8], na qual a diferença entre os bairros 𝐴e 𝐵é
dada por
𝑅𝐴,𝐵= 1
𝑛
∑︁
𝑖=1
𝑃𝑖(𝐴)
𝑃(𝐴) −𝑃𝑖(𝐵)
𝑃(𝐵)
onde 𝑅𝐴,𝐵é a diferença das composições raciais entre os bairros
𝐴e 𝐵, 𝑃(𝐴) representa o tamanho da população do bairro 𝐴, com
𝑖= 1, ..,𝑛identificando a i-ésima categoria racial em um bairro, con-
forme as categorias definidas pelo Censo Demográfico Brasileiro de
2010: Branca, Preta, Amarela, Parda e Indígena. Para calcular a dis-
tância geográfica entre os bairros (distância enre seus centroides),
foi utilizada a biblioteca geopandas [2] do Python, com as coordena-
das geográficas de latitude e longitude do sistema de coordenadas
WGS84 projetadas para o sistema de projeção plana UTMS22s que
traz a projeção mais precisa da região onde se encontra Curitiba.
Na segunda etapa da pesquisa (ainda em andamento), de modo
a investigar os fenômenos urbanos em regiões que não possuem
divisões administrativas acessíveis, assim como compreender o
fenômenos das cidades em diferentes escalas, foi desenvolvida a h3-
cities. Essa ferramenta, produto da presente pesquisa e disponível
publicamente1, utiliza o OpenStreetMap e o "Hexagonal hierarchical
geospatial indexing system"2 (h3) da Uber que subdivide o mundo
todo.
1https://h3-cities.streamlit.app/
2https://h3geo.org/
Modelagem do Interesse por Áreas Urbanas Usando Redes Sociais Baseadas em Localização
CTIC’2024, Juiz de Fora/MG, Brazil
Considerações éticas: Este trabalho não envolve pesquisas com
seres humanos. Todas as informações sensíveis, por exemplo, os
nomes de usuários, foram anonimizadas previamente para garantir
a privacidade dos usuários. Além disso, são utilizados dados públicos
disponíveis na Web.
RESULTADOS
Etapa 1: Ao construir as redes de Interesse para ambas as LBSNs, na
cidade de Curitiba, primeiramente analisou-se a similaridade entre
as duas através da correlação de Pearson. Foram comparados os
pesos das arestas das duas redes obtendo uma correlação de 0,875,
observando-se assim uma forte indicação da similaridade dessas
redes de interesse. Para investigar se os bairros mais importantes
eram os mesmos para as duas redes de interesse, empregou-se a
correlação de Kendall’s Tau. Para isso, construiu-se um ranking dos
bairros, usando a centralidade por autovetor que indica a importân-
cia do nó na rede, comparou-se esse ranking entre as duas redes
obtendo uma correlação de 0,646. Essa comparação pode ser vista
na Figura 3 para os bairros mais importantes de cada rede. Dessa
forma, observou-se uma conexão moderada entre as características
de importância dos bairros para as duas LBSNs.
Figura 3: Comparação dos nós mais importantes de cada rede
Após calcular a diferença entre os bairros de Curitiba para cada
um dos fatores (socioeconômicos, políticos e raciais), estas diferen-
ças foram associadas aos pesos das arestas para ambas as Redes de
Interesse (Google Places e Foursquare). Esse processo foi feito tanto
para a rede completa, como para uma rede filtrada, selecionado ape-
nas as arestas com peso ≥5, de modo a capturar áreas conectadas
por interesse mais elevado. A correlação de Spearman foi usada
para entender quais desses fatores estão mais associados com o
interesse dos usuários. Foi obtida uma correlação inferior a 0,25
em todos os fatores analisados, exceto a distância geográfica, que
apresentou correlações entre -0,3 e -0,55 nos cenários considerados,
ambas com 𝑝≤10−4. Isso indica que os fatores de renda média
mensal, composição racial e polarização política não ajudam a ex-
plicar o interesse dos usuários pelas áreas urbanas, logo as pessoas
não costumam frequentar, necessariamente, lugares que sejam simi-
lares em renda, raça, ou opiniões políticas, quando seus interesses
são modelados por essas LBSNs. Entretanto, a distância geográfica
teve um impacto mais relevante, mesmo que moderado, indicando
a preferência dos usuários por regiões que são mais próximas entre
si.
Etapa 2: Como forma de ampliar a análise da etapa 1 para dife-
rentes granularidades, foi desenvolvida a h3-cities cujos resultados
ilustrativos estão na Figura 4. Nela, há uma comparação entre os
bairros de Curitiba e uma subdivisão feita pela ferramenta, com a
resolução 8, ou seja, os hexágonos têm uma área média de 0, 737
km2. Com essa ferramenta, é possível dividir os bairros em regiões
menores e verificar se as redes de interesse formadas pelas diferen-
tes LBSNs continuam a apresentar resultados semelhantes. Outra
questão importante ao trabalhar com uma subdvisão customizada
de cenários urbanos é que dados de terceiros, como o IBGE, não
fornecem dados especificamente para essa subdivisão. Isso que pode
ser resolvido usando uma aproximação com dados dos bairros, ou
com a exploração de outras características advindas de datasets ge-
olocalizados, como, por exemplo, as categorias de estabelecimentos
presentes naquela região, para a análise urbana.
Figura 4: Comparação da segmentação de Curitiba através de
bairros e uma subdivisão via h3-cities (resolução 8).
Em termos de produtos gerados pela pesquisa, além da ferra-
menta descrita anteriormente, destacam-se os 3 trabalhos aceitos
para eventos (dois nacionais e um internacional). Os resultados di-
retos da etapa 1, desenvolvidos pelo aluno de IC, foram publicados
em [15], cujo impacto foi atestado por um convite para extensão
no periódico JISA, extensão esta que deverá englobar os resulta-
dos diretos da etapa 2. Entretanto, a pesquisa resultou também
em resultados indiretos, através de contribuições na preparação
de dados, construção de figuras, e desenvolvimento de parte dos
resultados, que possibilitaram a publicação de outros 2 trabalhos em
eventos [5, 6], os quais exploram o conceito de assinaturas culturais,
através da Teoria Scenes, que com dados provenientes de LBSNs,
pode, por exemplo, tentar expressar as características culturais de
uma dada região. Em [5], o trabalho explora o processo de agrupa-
mento de bairros com características culturais (assinatura cultural)
semelhantes na cidade de Curitiba. Este trabalho foi premiado com
o 3o lugar no workshop Courb 2024 e, assim como [15], também
recebeu convite para extensão no JISA. Em [6], o trabalho explora
a transferência de conhecimento de uma LBSN (Yelps) para outra
(Google Places) na formação das assinaturas culturais, explorando
dados de diferentes cidades ao redor do mundo, e também de todos
CTIC’2024, Juiz de Fora/MG, Brazil
Santos et al.
os estados americanos. Como forma de validação dos resultados
para os EUA, os dados foram comparados com aqueles obtidos via
American Values Survey (AVS), um resultado desenvolvido pelo
aluno de IC.
CONCLUSÃO
Esse artigo apresentou um resumo do estudo desenvolvido sobre a
modelagem do interesse de usúarios de redes sociais por diferentes
áreas urbanas. Para isso, foi realizada na primeira etapa da pesquisa
uma investigação na cidade de Curitiba usando redes de interesse,
i.e. Location-Based Social Networks (LBSNs) modeladas em grafos.
Nos experimentos observou-se similaridade na modelagem do com-
portamento urbano, a partir de duas fontes de dados diferentes,
Google Places e Foursquare, em particular quando se consideram
as arestas, e os nós mais centrais das respectivas redes de interesse.
Ainda na primeira etapa da pesquisa, investigou-se a possibili-
dade de entender as escolhas dos usuários de LBSNs, em relação
ao seu interesse por regiões urbanas, usando características socioe-
conômicas, de polarização política e distância geográfica das áreas
frequentadas por esses usuários. Com isso, concluiu-se que a dis-
tância geográfica foi o único fator associado com o interesse dos
usuários, mesmo que de forma moderada. Ou seja, regiões próximas
costumam ser mais frequentadas pelos mesmos usuários do que
regiões mais distantes.
Na última etapa que ainda se encontra em andamento, com o de-
senvolvimento da h3-cities, se torna possível a construção de redes
de interesse em diversas cidades. Por exemplo, obtendo-se dados
de uma LBSN, como os datasets públicos usados para este trabalho,
pode-se investigar o interesse dos usuários, sem precisar buscar
pelas subdivisões de uma cidade, o que pode ser difícil dependendo
da região estudada. Ao segmentar qualquer cidade, que tenha dados
disponíveis no OpenStreetMap, permite-se averiguar como o com-
portamento dos usuários varia sob diferentes granularidades. Logo,
essa ferramenta alavanca um estudo padronizado para o cenário
urbano.
Para trabalhos futuros, prevê-se a possibilidade de investigar
outras cidades, datasets de outros períodos, além de explorar os
efeitos provocados por diferentes granularidadades na definição
das áreas urbanas e validar os resultados aqui apresentados, desen-
volvidos com uma quantidade de dados pequena, em datasets de
maior escala. Dessa forma, seria possível analisar como os dados de
diferentes LBSNs são influenciados pelo espaço temporal e escalas
geográficas na modelagem do comportamento urbano, e ainda infe-
rir quais características ajudam a explicar o interesse dos usuários
pelas diferentes áreas urbanas.
AGRADECIMENTOS
Esta pesquisa foi apoiada pelo trabalho da mestranda Fernanda
Gubert, pelo projeto FAPESP SocialNet (2023/00148-0) e CNPq
(313122/2023-7, 314603/2023-9 e 441444/2023-7).

--- FIM DO ARQUIVO: 30478-829-24926-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30479-829-24927-1-10-20241001.txt ---
Modelagem e análise de redes sociais através de hipergrafos
Matheus H. B. dos Santos
matheusrickbatista@aluno.ufsj.edu.br
Universidade Federal de São João del-Rei
Vinícius da F. Vieira
vinicius@ufsj.edu.br
Universidade Federal de São João del-Rei
Carolina R. Xavier
carolinaxavier@ufsj.edu.br
Universidade Federal de São João del-Rei
Jussara M. de Almeida
jussara@dcc.ufmg.br
Universidade Federal de Minas Gerais
CTIC’2024, Juiz de Fora/MG, Brazil
Vinícius da F. Vieira et al.
Figure 1: Exemplo da representação de autoria de artigos cien-
tíficos (a) modelada como: um grafo clássico; (b) um hiper-
grafo (c).
Figure 2: Projeções de um hipergrafo (a) em grafos 𝑠-linha
(b), grafo clique (c), grafo bipartido (d).
em um artigo científico como uma hiperaresta, ilustrada por uma
região de uma cor.
É possível estabelecer métodos para a projeção de hipergrafos
em grafos sob diferentes perspectivas. A Figura 2 apresenta um
exemplo de projeção de um hipergrafo (Fig. 2(a)) em três tipos de
grafos: grafos 𝑠-linha, para 𝑠= 1, 𝑠= 2 e 𝑠= 3 (Fig. 2(b)), grafo
clique (Fig. 2(c)) e grafo bipartido (Fig. 2(d)). Embora a projeção
de hipergrafos em grafos seja muito útil para a aplicação de um
grande conjunto de técnicas clássicas utilizadas em análise de redes,
descritas e aplicadas de maneira vasta na literatura, sua utilização re-
sulta, inevitavelmente, em modelos com uma perda de informação.
Assim, há um enorme desafio em generalizar técnicas de ciên-
cia de redes baseadas em grafos para um contexto de hiper-redes,
em que as relações de alta-ordem sejam respeitadas. Askoy et al.
[1] fazem uma revisão de definições relacionadas a hipergrafos e
propõem uma série de conceitos baseados em passeios que estão na
base de métodos para a definição de coeficientes e centralidades apli-
cados a hiper-redes que sejam análogos àqueles tradicionalmente
aplicados na análise de redes.
Uma delas é a definição de 𝑠-caminho, que conecta um par de vér-
tices 𝑣𝑖e 𝑣𝑗em um hipergrafo utilizando apenas hiperarestas que
compartilham ao menos 𝑠vértices e permite definir a 𝑠-distância
𝑠−𝑙(𝑣𝑖, 𝑣𝑗), que caracteriza o tamanho do menor 𝑠-caminho entre
𝑣𝑖e 𝑣𝑗em um hipergrafo. Dessa forma, as noções de centralidade
de proximidade (closeness centrality) e centralidade de intermedi-
ação (betweenness centrality), tradicionalmente utilizadas em grafos
clássicos, podem ser utilizadas para hipergrafos como 𝑠-closeness
centrality e 𝑠-betweenness centrality, respectivamente1. A definição
de centralidade de grau (𝑠-degree centrality) de um vértice 𝑣𝑖em um
hipergrafo também pode ser facilmente estendida daquela aplicada
a grafos e pode ser calculada proporcionalmente ao número de
1Neste trabalho, será utilizada a terminologia 𝑠-closeness centrality e 𝑠-betweenness
centrality no idioma inglês, como no trabalho de Askoy et al. [1].
hiperarestas que possuem ao menos 𝑠vértices às quais o vértice 𝑣𝑖
pertence.
Uma das tarefas mais importantes na ciência de redes é a iden-
tificação de comunidades que, de maneira consensual pode ser
descrita como a busca por grupos de vértices com alta densidade
interna quando comparada ao volume de ligações externas [12].
Em um esforço de identificar comunidades em hiper-redes, Kumar
et al. ([9]) definem um método baseado no tradicional método de
Louvain [7], frequentemente utilizado para a caracterização de co-
munidades em grafos. Kumar et al. apontam que uma estratégia
simples para generalização do problema de identificação de comu-
nidades em hiper-redes poderia ser a projeção do hipergrafo em um
grafo clássico e a aplicação de algoritmos tradicionais, mas que essa
abordagem faria com que informações essenciais das hiperarestas
fossem perdidas.
MATERIAIS E MÉTODOS
Os experimentos neste estudo são conduzidos tomando como base
a CSBCSet [8], um conjunto de dados sobre publicações acadêmico-
científicas nos eventos mais longevos realizados no Congresso da
Sociedade Brasileira de Computação (CSBC) 2. A CSBCSet apre-
senta metadados de todas as publicações disponíveis entre 2013
e 2022 de 97 edições de dez eventos. Há na CSBCSet um total de
4961 autores, já desambiguados pelos na base de dados, e 1997
publicações distintas.
A partir de um conjunto de A autores e um conjunto de suas
P publicações são construídas uma rede e uma hiper-rede, repre-
sentadas, respectivamente, por um grafo G = (V, L) e por um
hipergrafo H = (V, E). Cada autor é representado por um vértice
em G e H. Cada publicação 𝑝∈P é representada por uma clique
em G, cujas arestas 𝑙𝑘∈𝐿relacionam os pares de vértices (𝑣𝑖, 𝑣𝑗)
correspondentes a 𝑝e por uma hiperaresta 𝑒𝑘∈𝐸que relaciona o
conjunto de autores 𝑣𝑖correspondentes a 𝑝.
Considerando a definição de Aksoy et al. [1], são também con-
struídas redes baseadas em grafos 𝑠-linha para 𝑠= 1, 𝑠= 2 e 𝑠= 3,
que serão utilizados na investigação sobre os autores mais centrais,
tomando como base os métodos de 𝑠-degree centrality, 𝑠-closeness
centrality e 𝑠-betweenness centrality. Para o restante do trabalho,
as projeções da hiper-rede em grafos grafos 𝑠-linha, para 𝑠= 1,
𝑠= 2 e 𝑠= 3, serão chamadas, respectivamente, de Hiper-rede 𝑠= 1,
Hiper-rede 𝑠= 2 e Hiper-rede 𝑠= 3.
EXPERIMENTOS E DISCUSSÃO
Considerando a metodologia proposta (Seção 3), esta seção apre-
senta os experimentos conduzidos com o objetivo de responder às
questões de pesquisa (Seção 1).
A Figura 3 apresenta uma representação visual dos modelos
de Rede (Figs. 3a e 3b) e Hiper-rede (Figs. 3c e 3d). É importante
destacar que a Hiper-rede de coautorias resultou em uma visual-
ização bastante poluída e pouco atrativa e, por isso, optou-se por
aplicar: um filtro de todos os trabalho, mas apenas do ano de 2022
(Fig. 3c) e um filtro de todos os anos, mas apenas de trabalhos do
BraSNAM (Figs. 3d).
Foi realizado um estudo sobre os autores mais centrais, que po-
dem representar pessoas com maior importância dentro do contexto
2https://csbc.sbc.org.br/
Modelagem e análise de redes sociais através de hipergrafos
CTIC’2024, Juiz de Fora/MG, Brazil
(a) Rede de coautoria.
(b) Rede de coautoria (apenas
2022).
(c) Hiper-rede de coautoria
de todos os eventos (apenas
2022).
(d) Hiper-rede de coautoria
de todos os anos (apenas
BraSNAM).
Figure 3: Representação visual da hiper-rede e da rede de
coautoria da CSBCSet.
(a) 𝑠-degree
(b) 𝑠-closeness
(c) 𝑠-betweenness
Figure 4: Coeficientes AO dos ranks obtidos em cada modelo
de (Hiper-)rede para cada centralidade estudada.
explorado. Para comparar os autores mais centrais identificados
nos modelos de (Hiper-)redes estudados, foram calculados os ranks
de autores considerando as centralidades estudadas para os mode-
los de Grafo, Hipergrafo (𝑠= 1), Hipergrafo (𝑠= 2) e Hipergrafo
(𝑠= 3) descritos na seção 3. Após definir um rank dos autores mais
centrais, concentramos a análise nas top-k posições, com 𝑘= 100,
de forma a minimizar o impacto de divergências que posições infe-
riores (e menos interessantes) pudessem trazer ao resultado. Assim,
é necessário correlacionar ranks com elementos potencialmente
diferentes e, para isso, utilizamos a métrica Average Overlap (AO)
[14], que mede a similaridade entre ranks de items possivelmente
diferentes. A Figura 4 apresenta o resultado dos coeficientes AO, em
forma de mapa de calor, comparando os ranks obtidos em cada mod-
elo de (Hiper-)rede para a 𝑠-degree centrality (Fig. 4a), 𝑠-closeness
centrality (Fig. 4b) e 𝑠-betweenness centrality (Fig. 4c).
Padrões bastante distintos de correlação para a 𝑠-degree cen-
trality podem ser observados na Figura 4, quando comparada às
𝑠-closeness centrality e 𝑠-betweenness centrality. Para a 𝑠-degree cen-
trality (Fig. 4a), há uma correlação apenas moderada entre os ranks
(a) 𝑠-degree.
(b) 𝑠-closeness.
(c) 𝑠-betweenness.
Figure 5: Comparação de valores de centralidade das dez pes-
soas autoras mais centrais tomando como referência o mod-
elo de Rede.
gerados para a Rede e a Hiper-rede (𝑠= 1), o que está ligado à
diferença na própria definição de grau para redes e hiper-redes.
Enquanto o grau em uma rede de coautoria está relacionado à quan-
tidade de coautores aos quais um indivíduo está relacionado, em
uma hiper-rede de coautoria o grau está relacionado à quantidade
de trabalhos dos quais ela participa. À medida em que o valor 𝑠
aumenta para a construção do modelo de Hiper-rede, autores que
participam em artigos com menos coautores são desconsiderados,
e o rank prioriza, em suas posições mais altas, autores que colabo-
ram em trabalhos com mais colaboradores. Dessa forma, aumenta
também a correlação entre os ranks gerados para a Rede e as Hiper-
redes (𝑠= 2) e (𝑠= 3). Nos ranks gerados para 𝑠-closeness centrality
e 𝑠-betweenness centrality, observa-se que há uma baixa correlação
entre os ranks dos autores das Redes e Hiper-redes (𝑠= 1) e (𝑠= 2)
e apenas a Hiper-rede (𝑠= 3) apresenta uma correlação significativa
com a Rede. Novamente, ao desconsiderar autores com participação
em trabalhos com poucos autores, o modelo de Hiper-rede (𝑠= 3)
coloca como mais centrais as pessoas com um maior volume de auto-
rias, assim como o modelo de Rede. O resultado revelado pela Fig. 4
indica que o modelo de grafo clique, tradicionalmente utilizado para
representar redes de coautoria, ao priorizar como autores mais cen-
trais aqueles que possuem um volume maior de colaboração, pode
estar subestimando o potencial de autores com um menor número
de colaborações, mas que mantêm colaborações mais consistentes
em servir como ponte intermediadora entre relações.
Com o objetivo de aprofundar o estudo sobre os autores mais
centrais encontrados em cada um dos modelos de (Hiper-)redes, foi
realizada uma investigação mais próxima dos nomes presentes nas
primeiras posições dos ranks. O resultado desse experimento é apre-
sentado na Figura 5, onde cada uma das subfiguras (Fig. 5a, 5b e 5c)
apresenta a comparação para cada uma das centralidades estudadas.
Para isso, o modelo de rede foi utilizado como referência e suas dez
pessoas autoras mais centrais foram encontradas. Em cada uma das
subfiguras da Fig. 5, o eixo-x apresenta os nomes das pessoas nas
dez primeiras posições do respectivo rank no modelo de Rede. O
eixo-y apresenta a medida de centralidade observada para essas
pessoas no modelo de Rede, mas também a medida observada no
modelos de Hiper-redes 𝑠= 1, 𝑠= 2 e 𝑠= 3, possibilitando, assim,
uma avaliação da estabilidade dos ranks de centralidade nos difer-
entes modelos. Primeiramente, nota-se que há pouca concordância
nas primeiras posições dos ranks encontrados para o modelo de
Rede e os modelos de Hiper-redes, o que pode ser visto pelo fato
que há um comportamento monotônico para as barras referentes ao
modelo de Rede – naturalmente, já que é esse o modelo usado como
CTIC’2024, Juiz de Fora/MG, Brazil
Vinícius da F. Vieira et al.
referência –, mas que não se repete para os modelos de Hiper-redes.
Uma notória exceção ocorre na primeira posição observada para a
𝑠-degree centrality (Fig. 5a), onde há um alinhamento na primeira
posição dos ranks para todos os modelos. Para a 𝑠-closeness cen-
trality (Fig. 5b) e a 𝑠-betweenness centrality (Fig. 5c) nota-se que
há algumas pessoas para as quais nem há valor de centralidade
associado nos modelos de Hiper-redes, indicando que essas foram
desconsideradas por esses modelos. Esse fato é ainda mais evidente
nas primeiras posições do gráfico da Fig. 5c, o que mostra que pes-
soas que colaboram em artigos com poucos coautores têm papel de
destaque como intermediadores de relações no modelo representado
por Redes, corroborando o resultado apresentado pela Figura 4.
Os resultados das Figuras 4 e 5 evidenciam a importância da
modelagem de coautorias como Hiper-redes como ferramenta para
análise do fenômeno de colaboração em artigos científicos sob uma
ótica alternativa àquela fornecida por modelos de Redes para a
investigação local. Uma investigação da organização topológica das
(Hiper-)redes foi também realizada para que as coautorias na CSBC-
Set pudessem ser exploradas sob uma perspectiva global. Tomando
como base o método de Louvain [7] e o método de Kumar et al.
[9], como descrito na Seção 2, foram identificadas as comunidades
considerando os modelos de Redes e Hiper-redes, repectivamente.
A modularidade da partição obtida pelo método de Louvain para o
modelo de Rede foi de 𝑄𝐺= 0.96, enquanto um valor 𝑄𝐻= 0.97 foi
obtido para a partição encontrada pelo método de Kumar et al. Esses
resultados mostram que, quando são consideradas as coautorias de
trabalhos no CSBC no período investigado, há uma clara divisão
topológica da (Hiper-)rede de autores, implicando em um alto valor
de modularidade para ambos modelos considerados.
A extensão da sobreposição da partição obtida nos modelos de
(Hiper-)redes, foi avaliada através do coeficiente Overlapping Nor-
malized Mutual Information (ONMI) [11], que retorna um valor 0
quando não há informação mútua entre as partições e 1 quando há
uma perfeita correlação. Quando todas as comunidades são consid-
eradas, obtém-se um valor de ONMI = 0.91, indicando uma grande
sobreposição. Com o objetivo de verificar se o coeficiente ONMI
está dominado por comunidades de tamanhos muito pequenos, um
filtro foi aplicado para que apenas comunidades com mais que dez
vértices fossem avaliadas. Após a aplicação desse filtro, foi obtido
um valor de ONMI = 0.83, indicando que há, sim, uma sobreposição
concentrada nas comunidades menores, mas, mesmo sem considerá-
las, a sobreposição entre as partições é bastante significativa.
CONCLUSÃO
Este trabalho investiga o uso de hiper-redes para a modelagem de
relações sociais de alta-ordem. Nesse sentido, é importante destacar
que a área ainda é incipiente e oferece um grande potencial de
pesquisa, principalmente considerando a baixa quantidade de tra-
balhos sobre hiper-redes em língua portuguesa (nenhum trabalho
em língua portuguesa foi encontrado em assunto relacionado ao
aqui explorado, por exemplo).
Em relação à QPI, pode-se notar que ainda há uma certa limi-
tação nas ferramentas computacionais para modelagem e análise
de hiper-redes quando compara-se com aquelas voltadas a redes
tradicionais. Porém, a análise de hiper-redes, apesar de não ser uma
área de pesquisa nova, tem recebido, apenas recentemente, uma
atenção mais dedicada, ao contrário da análise de redes tradicionais,
que já encontra uma área bastante consolidada. Mesmo assim, há
uma grande quantidade de métodos relatados na literatura para
investigação de hiper-relações, tanto de maneira local, como de
maneira global, o que permite que se tire proveito da riqueza na
representação de relações de alta-ordem trazida pelas hiper-redes,
mas fazendo análises que sejam coerentes com aquelas realizadas
com ferramentas oriundas da ciência de redes. Assim, é possível
avançar também na resposta à QPII, verificando que a aplicação
de modelos de hipergrafos para a compreensão da coautoria de
artigos científicos pode revelar aspectos importantes sobre, não
apenas o número de interações realizadas por cada indivíduo, mas a
maneira como se dão essas interações. Assim, pode-se compreender
com mais clareza o papel de indivíduos com diferentes padrões de
colaboração na intermediação de relações e, consequentemente, na
difusão de ideias e conhecimento através da hiper-rede.
Como possíveis direções futuras, então, pode-se buscar a verifi-
cação da generalidade das observações aqui realizadas em outras
bases de dados de interações sociais, inclusive de colaboração e,
mais especificamente, de coautoria, mas em outros contextos. Além
disso, diversos outros métodos de análise de hiper-redes podem ser
empregados, como forma de confrontar os resultados aqui obtidos
sob outras perspectivas.

--- FIM DO ARQUIVO: 30479-829-24927-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30480-829-24928-1-10-20241001.txt ---
Arpeggion-H: Uma Interface Interativa para Reprodução de áudio
MPEG-H
Bruno Augusto R. de M. Moreira
brunoarmm@id.uff.br
Universidade Federal Fluminense
Niterói, Brasil
Debora C. Muchaluat-Saade
debora@midiacom.uff.br
Universidade Federal Fluminense
Niterói, Brasil
3. A ferra-
menta visa apresentar ao usuário as opções de configuração deste
áudio, e permitir modificações em tempo real dessas configurações.
Esta ferramenta será útil para pesquisas e usuários comuns por
ser um tocador de áudio de um decodificador que não é comu-
mente disponível. Esta ferramenta não implementa o decodificador,
mas sim, utiliza um já existente [4] e, no futuro, pode permitir a
utilização de qualquer implementação.
A ferramenta visa receber um arquivo de áudio MP4 codificado
com mhm1, e conseguir construir e apresentar ao usuário uma
interface baseada nas propriedades de cena extraídas diretamente do
arquivo MP4, possibilitando a mudança de valores das propriedades
do áudio em tempo real. A ferramenta também dará a possibilidade
In: XXII Workshop de Ferramentas e Aplicações (WFA 2024). Anais Estendidos do
XXX Simpósio Brasileiro de Sistemas Multimídia e Web (WFA’2024). Juiz de Fora/MG,
Brazil. Porto Alegre: Brazilian Computer Society, 2024.
© 2024 SBC – Sociedade Brasileira de Computação.
ISSN 2596-1683
de mudanças de configurações globais e, através da configuração
de linguagem, adaptar os textos apresentados na interface para tal
linguagem, quando possível.
A ferramenta tem como público alvo usuários comuns que pos-
suem interesse em tocar estes áudios, já que as opções disponíveis
atualmente se resumem a ferramenta proprietária MPEG-H Autor-
ing Suite, ou a hardwares de streaming para televisão. Ela também
se destina a desenvolvedores e pesquisadores interessados em como
implementar um player com essas capacidades.
TRABALHOS RELACIONADOS
O trabalho [1] propõe que sejam implementados argumentos, os
mesmos que a ferramenta Arpeggion-H utiliza, que manipulem a
saída de áudio. Esses argumentos são inspirações diretas da proposta
do MPEG-H, o que permite ao usuário manipular informações como
posição dos objetos, pre-definições, ganho de volume, entre outros.
Ambos os trabalhos têm como foco a implementação dessas normas
em plataformas de TV.
O trabalho [2] descreve a norma MPEG-H e categoriza ferra-
mentas como o Arpeggion-H como um “Receptor”. Já o artigo [3]
propõe uma forma de implementar a API que seria disponibilizada
para uma ferramenta front-end, e implementa um protótipo front-
end para a validação dessa API. O Arpeggion-H implementa este
front-end de forma similar em questão de funcionalidades, porém
visualmente distinto.
FERRAMENTA
A ferramenta Arpeggion-H consiste em 3 módulos independentes:
o decodificador, o interpretador da cena e o reprodutor de áudio. E
um módulo principal que une todos os outros módulos, a interface.
Por uma limitação da ferramenta, a comunicação entre os difer-
entes módulos e a biblioteca de decodificação MPEG-H se dá por
comunicação por meio de arquivos, isso inclui o arquivo MP4 de
entrada. Visto isso, ainda não é possível integrar esta ferramenta
com um serviço de streaming de MPEG-H. Na Figura 1, é possível
ver como todos os módulos interagem entre si. Na figura, os mó-
dulos em roxo pertence à ferramenta deste trabalho, já os módulos
em verde pertencem ao mpeghdec, e o amarelo representa a entrada
do usuário. A ferramenta foi implementada na linguagem Python,
com o auxílio das bibliotecas Tkinter para a interface e Pyaudio
para a reprodução de áudio.
3.1
Integração com o Decodificador
Foi utilizado o decodificador Fraunhofer MPEG-H decoder (mpeghdec)
[4], e a integração foi feita através do binário compilado do código-
fonte disponibilizados pela Fraunhofer-IIS. A interação é vista na
Figura 2. Conforme a demanda, a ferramenta irá decodificar trechos
WFA’2024, Juiz de Fora/MG, Brazil
Bruno Augusto R. de M. Moreira, Débora C. Muchaluat-Saade
MP4
MPEG-H (mhm1)
Decoder
Handler
MPEG-H Decoder
Input MP4, metadata
and timestamps
to decode
Script XML
(Metadata)
Decoded MP4 as
.wav file
MPEG-H UI
Manager
Input MP4
Output as file
Interface
Player
UI Manager
Handler
Audio Scene
Outputs
Scene
XML File
Audio Scene
Parsed Objects
Manages the player
Pause, Play, ...
Requests to decode
Changes metadata
Configuration
Interface based on
Audio Scene
Changes MP4 File
of the project
Interacts with
Outputs
Has access to
Figure 1: Arquitetura da ferramenta Arpeggion-H, incluindo
módulos e a biblioteca mpeghdec
pequenos de áudio através do mpeghdec, passando o arquivo a ser
decodificado, as alterações que o usuário fizer às configurações,
e a marca de tempo a ser decodificada. As alterações de configu-
rações são passadas ao decodificador num formato XML seguindo
a especificação das mensagens de ações de evento (Action Event
Messages).
Além dos argumentos no XML, existem quatro argumentos pas-
sados diretamente ao decodificador:
• Composição dos alto-falantes: define o layout. Por exemplo,
mono, estéreo ou 7.1;
• Tipo de ambiente: áudio é adaptado para ambientes de difer-
entes tipos. São eles barulhento, silencioso, noturno, faixa
limitada de reprodução, volume baixo, melhora de diálogo e
melhor compreensão;
• Coeficiente de ganho (não disponível diretamente na inter-
face);
• Modo Álbum: aplica um fade-in e fade-out entre músicas.
MP4
MPEG-H (mhm1)
Decoder
Handler
MPEG-H Decoder
Input MP4, metadata
and timestamps
to decode
Script XML
(Metadata)
Decoded MP4 as
.wav file
Output as file
Has access
to
Interacts with
Outputs
Has access to
Figure 2: Arquitetura do módulo de decodificação. Roxo per-
tence à ferramenta deste trabalho, em verde pertence ao
mpeghdec, e amarelo é entrada do usuário
3.2
Leitor de Cena
O leitor de cena fará o papel de transformar os metadados extraídos
do MP4 em algo legível para a interface, como visto na Figura 3.
O leitor também utiliza do mpeghdec para gerar um arquivo XML
com a descrição de cena de áudio (Audio Scene Configuration), e
através deste arquivo ele gera essa descrição e disponibiliza para os
outros módulos. Além disso, este módulo é responsável em criar o
Script XML que contém as alterações do metadado para ser usado
durante a decodificação de áudio.
A cena possui um conjunto de pre-definições que o usuário pode
escolher, e dentro de cada pre-definição existe um conjunto de
áudios e/ou grupos de áudio. Cada áudio e grupo de áudio possui
um conjunto de propriedades que o usuário pode ou não alterar,
também como seus valores mínimos e máximos. No caso do grupo
de áudios, é possível escolher um áudio dentre uma seleção. A
definição do formato XML pode ser encontrado em [3].
MP4
MPEG-H (mhm1)
Script XML
(Metadata)
MPEG-H UI
Manager
Input MP4
UI Manager
Handler
Audio Scene
Outputs
Scene
XML File
Audio Scene
Parsed Objects
Has Access
to
Has access
to
Interacts with
Outputs
Has access to
Figure 3: Arquitetura do módulo de Leitor de Cena. Roxo
pertence à ferramenta deste trabalho, em verde pertence ao
mpeghdec, e amarelo é entrada do usuário
3.3
Player
O Audio Player, como visto na Figura 4, utiliza de arquivos em
formato wav para reprodução de áudio. Estes arquivos são gerados
através do decodificador, que gera esses trechos de áudio. O Player
pode ser controlado pela interface e sua implementação é a mais
simples possível.
3.4
Interface
A Interface é o componente principal da ferramenta, já que ele
integra todas as outras partes, como visto na Figura 5. A Interface
utiliza da saída do leitor de cena para construir a interface gráfica
visual do usuário, e envia pedidos ao leitor de cena para criar as
mensagens de ação de evento baseado na interação do usuário com a
interface. A interface também define qual o arquivo MP4 é utilizado
pelo resto da ferramenta, e também qual o idioma padrão tanto para
os textos quanto para o áudio. A Figura 6 exibe a interface gráfica
gerada pela Interface.
É possível ver na Figura 6 como a descrição da cena é repre-
sentada. Cada pre-definição é uma aba, enquanto cada elemento
de áudio é um parágrafo diferente, separados por linhas horizon-
tais cinzas. Todo texto da cena apresentado é extraído diretamente
Arpeggion-H: Uma Interface Interativa para Reprodução de áudio MPEG-H
WFA’2024, Juiz de Fora/MG, Brazil
Interface
Player
Managed by the interface
Pause, Play, ...
Requests to
decode
Decoder
Handler
MPEG-H Decoder
Decoded MP4 as
.wav file
Plays audio...
Operating
System Audio
Mixer
Interacts with
Outputs
Has access to
Figure 4: Arquitetura do módulo Audio Player. Roxo pertence
à ferramenta deste trabalho, em verde pertence ao mpeghdec,
e amarelo é entrada do usuário
Interface
Player
Manages the player
Pause, Play, ...
Audio Scene
Parsed Objects
Interface based on
Audio Scene
UI Manager
Handler
MP4
MPEG-H (mhm1)
Changes MP4 File
of the project
Changes metadata
Configuration
Interacts with
Outputs
Has access to
Figure 5: Arquitetura do módulo de Interface. Roxo pertence
à ferramenta deste trabalho, em verde pertence ao mpeghdec,
e amarelo é entrada do usuário
da própria cena. A tradução dos textos depende inteiramente da
descrição de cena possuir tais traduções.
USO DA FERRAMENTA
Ao abrir a ferramenta, o usuário pode acessar a opção de File no
topo esquerdo e selecionar qualquer arquivo MP4 que tenha sido
codificado com mhm1. Após isso, a ferramenta ira atualizar a in-
terface baseado no áudio recebido e haverá a opção de iniciar a
Figure 6: Resultado da leitura da configuração de cena, com
algumas partes em destaque
reprodução do áudio através da opção de Play. O usuário poderá
trocar de arquivo a qualquer momento.
A interface será atualizada ao carregar um novo arquivo, sendo
possível visualizar os diferentes elementos de áudio presentes no
MP4, onde seus nomes serão mostrados mais à esquerda. Na Figura
7 é possível ver esses elementos de áudio chamados “Language” e
“O1”. Ao lado dos elementos, estão presentes os atributos referentes
aquele elemento de áudio em específico. As possíveis propriedades
são: Prominência (Prominence), Rotação (Azimuth), Elevação (El-
evation), Mudo (Muting, não presente na Figura). No caso de um
elemento composto de áudio, é possível escolher um dentre um
conjunto de áudios, como visto em “Language”, a opção “Center:
D1” é menu em cascata com outras opções.
O usuário pode interagir com as abas de pre-definições, neste
exemplo, chamadas de “Default”, “Dialog+”, e “Venue”.Cada pre-
definição altera de alguma forma os valores ou disponibilidade de
cada áudio, influenciando no resultado ouvido pelo usuário.
Nas Figuras 8 e 9, é possível ver dois outros resultados de inter-
face de MP4s diferentes.
Como visto anteriormente na Figura 6, existem cinco opções de
cascata no cabeçalho da interface: File, Language, Speakers, Album
Mode, e DRC. File permite escolher um arquivo diferente, enquanto
Language permite escolher a linguagem padrão do usuário. As
outras três opções são passadas diretamente ao decodificador como
visto na Seção 3.1, sendo elas “Composição dos alto-falantes”, “Modo
Álbum”, e “Tipo de ambiente”, respectivamente. Podem ser vistas, na
Figura 10, todas as opções disponíveis até o momento pela interface.
Serão disponibilizadas mais opções no futuro.
Aplica-se a Arpeggion-H a licença GPLv3. Um vídeo demon-
strando a ferramenta pode ser visto em https://drive.google.com/
file/d/1_q7Gg37E6WmNuF-a6H_21qvAc4NootGP/view?usp=sharing.
WFA’2024, Juiz de Fora/MG, Brazil
Bruno Augusto R. de M. Moreira, Débora C. Muchaluat-Saade
Figure 7: Resultado da leitura da configuração de cena.
Figure 8: Resultado da leitura da configuração de cena do
segundo exemplo
Figure 9: Resultado da leitura da configuração de cena do
terceiro exemplo
O código-fonte pode ser visto em https://github.com/BunnyMerz/
Arpeggion-H
CONCLUSÃO
A ferramenta Arpeggion-H, proposta neste trabalho, consegue re-
produzir áudios seguindo a norma MPEG-H, parte 3, e permite ao
Figure 10: Resultado da leitura da configuração de cena do
terceiro exemplo
usuário interagir com as configurações da cena para modificar o áu-
dio reproduzido. A ferramenta adapta a interface gráfica baseado no
arquivo de áudio fornecido e apresenta o texto conforme a preferên-
cia de linguagem do usuário. Mesmo sendo uma ferramenta simples,
será útil para fins acadêmicos, pois reprodutores de mídia deste tipo
normalmente requerem licenças pagas e não são de código aberto.
Como trabalho futuro, a ferramenta será estendida para poder re-
ceber um streaming de áudio. Para adaptação ao streaming de áudio,
seria necessário utilizar e adaptar o código-fonte da Fraunhofer-IIS,
já que o binário da compilação padrão dá acesso apenas à decodifi-
cação por meio de arquivos. Além disso, estão planejados testes de
usabilidade da ferramenta por meio de experimentos com usuários.

--- FIM DO ARQUIVO: 30480-829-24928-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30481-829-24929-1-10-20241001.txt ---
AvaliApp: instrumento de autoavaliação do cuidado em
instituições de longa permanência do idoso
Bernardo Oliveira
bernoliveiras@gmail.com
Colégio Técnico
Universidade Federal de Minas Gerais
Bruno Luís De Carvalho Vieira
Obrunocarvalho@hotmail.com
Prefeitura Municipal de
Belo Horizonte
Virgínia Fernandes Mota
virginia@teiacoltec.org
Colégio Técnico
Universidade Federal de Minas Gerais
WFA’2024, Juiz de Fora/MG, Brazil
Oliveira et al.
Dessa forma, de acordo com as RDCs de 2005 e 2021, as ILPIs
são estabelecimentos, governamentais ou privados, que oferecem
residência coletiva para pessoas com 60 anos ou mais, com ou
sem apoio familiar. Essas instituições devem garantir a liberdade,
dignidade e cidadania dos residentes. Entre suas responsabilidades,
incluem assegurar o exercício dos direitos humanos, a liberdade de
crença e o direito de ir e vir, dentro de um ambiente respeitoso e
digno. Além disso, devem promover a interação entre os residentes e
com pessoas de outras gerações, incentivar a participação da família
e da comunidade, estimular a autonomia, oferecer opções de lazer
e prevenir qualquer forma de violência.
METODOLOGIA DA AVALIAÇÃO DAS ILPI
Para realizar a avaliação das ILPIs, o AvaliApp utiliza como metodolo-
gia o modelo proposto em [6], que divide a qualificação em 6 di-
mensões diferentes: Ambiente, Equipe de Trabalho, Cuidado, Lar,
Envolvimento Familiar e da Comunidade, Gestão da ILPI.
A dimensão Ambiente avalia a docilidade ambiental da estrutura
da ILPI, ou seja, avalia se há a capacidade de o local sanar de forma
satisfatória todas as necessidades físicas e psicossociais de seus
residentes, para que assim, estes tenham direito a saúde física, bem-
estar psicológico, segurança, boa funcionalidade e uma identidade
com a residência.
A Equipe de trabalho avalia os profissionais que atuam no
cuidado dos idosos que residem na ILPI, assegurando seus requi-
sitos técnicos, o número mínimo de profissionais necessários, e
o estabelecimento do aperfeiçoamento para melhor satisfazer os
residentes das ILPIs.
O Cuidado avalia o cuidado ofertado aos idosos que residem
nessas instituições, garantindo que, além de atender às suas neces-
sidades básicas, eles também possuam estímulos ao autocuidado,
à autoestima e à autovalorização. Ademais, também garante que
a ILPI deve prestar cuidado adequado ao idoso de forma contínua,
proporcionando-lhe respeito, escuta qualificada, empatia e pro-
movendo sua independência.
O Lar avalia os padrões que fazem com que o idoso considere
que a ILPI é seu lar, através da existência de um ambiente que seja
acolhedor, estimule o convívio social do idoso, sua higiene e sua
acessibilidade e privacidade, para que assim, seja possível que o
residente realmente sinta-se em bem e em casa.
A dimensão Envolvimento familiar e da comunidade avalia
as condições da ILPI manter as relações do idoso residente com sua
família e familiares, um dos pontos chaves do Estatuto do Idoso,
garantindo assim o mantimento de sua saúde mental, autonomia,
qualidade de vida e independência.
Por fim, a Gestão da ILPI avalia as condições dos processos de
administração da ILPI, para que seja possível garantir o bem-estar
tanto do idoso quanto dos funcionários responsáveis pelos cuidados
dos idosos, seguindo normas preestabelecidas e legislações vigentes.
Cada uma dessas dimensões contém diversas perguntas fechadas,
cada uma com, em média, três ou mais alternativas. As perguntas
são estruturadas de forma que, dependendo da resposta selecionada,
uma nota é atribuída a cada questão, refletindo o nível de atendi-
mento da ILPI em relação aos critérios definidos na pesquisa.
Ao final, todas as pontuações das questões são agregadas para
gerar um valor numérico que determina o nível de adequação da
ILPI aos requisitos necessários para ser considerada uma instituição
apropriada. De acordo com a quantidade de itens necessária para o
alcance da qualidade do cuidado proposta, as respostas indicam se
o padrão é inexistente, incipiente, intermediário ou avançado.
DESENVOLVIMENTO DO APLICATIVO
O aplicativo AvaliApp foi desenvolvido com uma arquitetura de três
camadas, que divide a complexidade do sistema em componentes
menores, proporcionando uma melhor organização do software.
Essa arquitetura permite uma separação clara entre a interface do
usuário, a lógica de negócios e o acesso aos dados, facilitando a
manutenção e atualização do sistema. Além disso, essa estrutura
modular melhora a escalabilidade e a flexibilidade do aplicativo,
permitindo que diferentes partes do sistema sejam desenvolvidas e
atualizadas de forma independente [13].
O aplicativo deve operar com uma licença de aquisição perpétua,
garantindo que a instituição tenha um acesso contínuo e perma-
nente ao software sem a necessidade de renovação.
O tratamento de dados é restrito exclusivamente à instituição
que utiliza o aplicativo, assegurando a privacidade e a segurança
das informações dos usuários. Essa abordagem protege os dados
sensíveis e garante que a instituição tenha total controle sobre o
uso e a gestão das informações coletadas pelo aplicativo.
AvaliApp é direcionado para gestores de ILPIs e tem como obje-
tivo facilitar a transparência dos dados de autoavaliação para outras
partes interessadas, como os familiares dos residentes e as entidades
reguladoras.
O desenvolvimento do aplicativo AvaliApp foi realizado com o
objetivo de garantir sua utilização por uma ampla variedade de
usuários em diferentes tipos de dispositivos móveis. Para atender
a essa demanda, foi escolhida a linguagem Dart1 e o framework
Flutter2, proporcionando compatibilidade e desempenho adequados
para a execução do aplicativo em diversas plataformas de aparelhos
celulares.
Quanto à persistência dos dados, utilizou-se o Firebase3 para
organizar a estrutura do servidor. Este recurso permitiu a retirada
das perguntas e o envio dos dados coletados, garantindo que esses
dados fossem tratados exclusivamente pela instituição que os utiliza.
Figure 1: Tela de Login e Tela de Cadastro.
1http://dart.dev
2http://flutter.dev
3http://firebase.google.com
AvaliApp: instrumento de autoavaliação do cuidado em instituições de longa permanência do idoso
WFA’2024, Juiz de Fora/MG, Brazil
A criação do aplicativo foi dividida em etapas, atribuindo-se
responsabilidades específicas aos diferentes membros da equipe.
Tarefas como a criação do backend e do frontend de algumas das
telas foram distribuídas entre os integrantes, garantindo assim uma
melhor organização do trabalho e uma divisão eficiente das ativi-
dades. O versionamento foi feito utilizando GitHub4.
O fluxo de uso do aplicativo é explicado a seguir. Ao abrir o
aplicativo, a primeira tela vista pelo usuário é a tela de login, na
qual é possível escolher entre as opções de login – podendo ser feita
a partir de uma conta já criada, ou a partir do login com o Google –
e criação de conta (Figura 1).
Após o login, o usuário é levado para a tela de perguntas, onde
poderá escolher qualquer uma das dimensões, para iniciar o preenchi-
mento dos questionários (Figura 2a). Não é necessário fazer as di-
mensões em ordem, entretanto, uma barra de progresso indica ao
usuário a porcentagem de perguntas já respondidas, sendo possível
somente criar um relatório ao responder todo o questionário. Ao
selecionar uma dimensão, o usuário será levado para uma tela que
contém a pergunta e suas opções, sendo possível pular questões e
mudar respostas ao longo do processo (Figura 2b). Entretanto, o
usuário não poderá alterar suas respostas após o envio. Essa tela
funciona através da leitura dos dados da pergunta, contagem das
opções e criação dos botões com a pontuação adequada para cada
um. Deste modo, é uma tela altamente adaptável, podendo ser uti-
lizada para qualquer uma das perguntas, independentemente de
quantas opções possuir.
Após responder o questionário por completo, o usuário poderá
salvar seus dados e enviá-los para o sistema, o que irá gerar para
o usuário um gráfico mostrando a pontuação de cada dimensão
(Figura 2c). Dessa forma, há uma noção para o responsável pelo
gerenciamento da ILPI de em quais áreas há o atendimento aos
critérios necessários e em quais áreas deve haver uma melhora.
Esses dados também são salvos em um histórico (Figura 2e), para
que seja possível comparar dados antigos com dados obtidos no
presente, e ver como a instituição melhorou (ou não) ao longo do
tempo. Tanto o gráfico quanto o histórico estão disponíveis em
outra tela, a tela de dados (Figura 2d).
Além das duas telas mencionadas, a terceira e última tela do
aplicativo é a tela do perfil (Figura 2f), onde o usuário responsável
pela ILPI poderá verificar e modificar seu nome, sua foto e o nome
da instituição que representa e também sair do aplicativo, voltando
para a tela inicial de login, fazendo com que outra pessoa possa
também utilizá-lo.
O questionário completo é composto por 29 questões, distribuí-
das nas 6 dimensões de avaliação. Ele deve ser respondido em sua
totalidade para gerar o gráfico de pontuação. A ideia do instrumento
de avaliação é ser aplicado semestral ou anualmente.
4.1
Avaliação do instrumento de autoavaliação
O instrumento de autoavaliação foi avaliado com uma metodologia
de três etapas [6, 7].
Ele inicialmente foi submetido a um pré-teste realizado por qua-
tro especialistas na área de saúde do idoso, que avaliaram o formato
adotado, as dimensões e seus padrões de avaliação, a linguagem
utilizada e possíveis inconsistências. Após essa fase, o instrumento
4http://github.com
(a) Tela com escolha de uma
das 7 dimensões.
(b) Tela do questionário de
uma das 6 dimensões.
(c) Tela de dimensões com a
barra de progresso.
(d) Tela de dados com os re-
sultados obtidos.
(e) Histórico dos dados.
(f) Tela de Perfil.
Figure 2: Telas do AvaliApp.
WFA’2024, Juiz de Fora/MG, Brazil
Oliveira et al.
foi aplicado aos especialistas para uma avaliação mais detalhada
[5].
A segunda etapa envolveu a validação do instrumento utilizando
a Técnica Delphi modificada, que consiste em uma consulta intera-
tiva a especialistas para obter consenso sobre questões específicas.
Essa abordagem é útil para problemas de saúde com informações
insuficientes, permitindo a síntese de dados e resolução de incon-
sistências por meio do consenso dos especialistas [10].
A Técnica Delphi facilita a organização de informações inade-
quadas ou inexistentes, complementando os estudos tradicionais
e a meta-análise. As rodadas de consulta permitem que todos os
especialistas vejam e revisem as modificações sugeridas, sem con-
tato direto entre eles, evitando interferências nas respostas [12].
Recomenda-se a participação de pelo menos cinco e no máximo
vinte especialistas com domínio sobre o tema, e todas as sugestões
devem ser justificadas. A frequência das respostas obtidas deve ser
devidamente registrada.
Após a etapa de validação com especialistas, o instrumento foi
enviado para 10 gestores de ILPI filantrópicas e particulares para
validação prática. Esse passo é crucial para integrar o conhecimento
teórico com a aplicação prática do instrumento na autoavaliação
das ILPIs, fortalecendo assim o processo de validação. A inclusão
dos gestores assegura que o instrumento seja eficaz e relevante
para o contexto real das instituições, alinhando a teoria à prática
operacional [4].
O estudo foi aprovado pelo Comitê de Pesquisa da Comitê em
Pesquisa da da Universidade Federal de Minas Gerais e todos os
especialistas registraram sua aceitação em participar da pesquisa
após a leitura do termo de consentimento livre e esclarecido (CAAE:
17002519.4.0000.5149).
É importante ressaltar que essa avaliação foi realizada exclusi-
vamente para o questionário. Há planos para avaliar a ferramenta
AvaliApp sob a perspectiva da experiência do usuário, especial-
mente para os gestores de ILPIs. Tanto o instrumento criado quanto
o aplicativo são inovadores e inéditos na forma como foram desen-
volvidos, especificamente para o contexto brasileiro e fundamen-
tados em um modelo teórico. Eles têm o potencial de contribuir
significativamente para elevar o padrão de qualidade das ILPIs, in-
cluindo a criação de um banco de dados para monitoramento e
comparações.
CONCLUSÃO
Neste trabalho apresentamos o aplicativo AvaliApp, criado para
autoavaliação de Intituições de Longa Permanência para Idosos.
Este instrumento de autoavaliação se revela uma ferramenta
potente e inovadora, podendo ser utilizada pelos coordenadores
das Instituições de Longa Permanência do Idoso para o monitora-
mento contínuo da qualidade dessas instituições. Serve como roteiro
em reuniões com funcionários, gestores e residentes, orientando o
planejamento e a tomada de decisões. Quando iniciada de forma
interna, a autoavaliação promove maior adesão e comprometimento
em comparação com uma avaliação externa. Permite conhecer o
estágio atual da qualidade oferecida e identificar áreas de melho-
ria. A aplicação do instrumento pode ser realizada semestral ou
anualmente.
O instrumento foi submetido à avaliação de conteúdo por espe-
cialistas e pelo público-alvo. Embora idealizado para autoavaliação,
também pode ser usado para avaliações externas e como roteiro ou
base para novos instrumentos de avaliação da qualidade do cuidado
em ILPI.
Futuras pesquisas deverão acompanhar o uso do aplicativo e sua
contribuição para a melhoria da qualidade nas ILPI e consequente-
mente a qualidade de vida dos idosos que vivem nelas. Desta forma,
será possível monitorar o desempenho das ILPI em padrões que
vão de incipiente a consolidado, fortalecendo ações nas áreas com
maior necessidade.

--- FIM DO ARQUIVO: 30481-829-24929-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30482-829-24930-1-10-20241001.txt ---
Campanha Eleitoral Legal: Detecção de Propaganda Eleitoral e
Ações Coordenadas de Campanha
Márcio Silva*†, Marcelo M. R. Araújo*, Carlos H. G. Ferreira⋄, Julio C. S. Reis‡,
Ana P. C. Silva*, Jussara M. Almeida*, Fabrício Benevenuto*
† Faculdade de Computação, Universidade Federal do Mato Grosso do Sul (UFMS), Brasil
* Departamento de Ciência da Computação, Universidade Federal de Minas Gerais (UFMG), Brasil
⋄Departamento de Computação e Sistemas, Universidade Federal de Ouro Preto (UFOP), Brasil
‡ Departamento de Informática, Universidade Federal de Viçosa (UFV), Brasil
marcio.inacio@ufms.br,marceloaraujo@dcc.ufmg.br,chgferreira@ufop.edu.br,jreis@ufv.br
{ana.coutosilva,jussara,fabricio}@dcc.ufmg.br
2017. Neste novo cenário, novos instrumentos de fiscalização
In: XXII Workshop de Ferramentas e Aplicações (WFA 2024). Anais Estendidos do
XXX Simpósio Brasileiro de Sistemas Multimídia e Web (WFA’2024). Juiz de Fora/MG,
Brazil. Porto Alegre: Brazilian Computer Society, 2024.
© 2024 SBC – Sociedade Brasileira de Computação.
ISSN 2596-1683
devem ser propostos, levando em consideração os diversos desafios
de monitoramento de informação nas redes sociais.
Portanto, em um trabalho anterior [16] implementamos uma
arquitetura que objetiva monitorar, a partir de dados coletados do
X, a ocorrência de propaganda eleitoral antecipada no contexto
brasileiro. Neste estudo, utilizamos a abordagem proposta anterior-
mente e implementamos um novo mecanismo que visa aprimorar
o processo de classificação de propaganda eleitoral nos textos das
mensagens postadas nesta plataforma, incorporando-o em um sis-
tema real. Esse mecanismo consiste na adoção da estratégia de
Reinforcement Learning from Human Feedback (RLHF), ou seja, a
utilização da avaliação do usuário final para retreinar e aprimorar
o modelo de classificação da ferramenta. Assim, esta estratégia per-
mite ao classificador se adaptar às mudanças que podem ocorrer no
discurso de cunho eleitoral dentro da rede social, e também apri-
morar a acurácia de detecção de propaganda eleitoral ao longo do
tempo [11], o que potencializa a agilidade da investigação de propa-
gandas eleitorais pelos usuários especialistas. Por fim, adicionamos
um componente responsável por revelar indícios de promoção de
conteúdo dentro deste ambiente.
Dessa forma, nosso sistema possui cinco módulos principais: (i)
módulo coletor de postagens do X, (ii) módulo de detecção de ações
coordenadas de promoção de conteúdo, (iii) módulo de classificação
de propagandas eleitorais, (iv) módulo de interface web e (v) módulo
de treinamento de modelos de aprendizado de máquina. Os módulos
(i), (ii), (iii) foram adaptados do modelo proposto em [16] enquanto
a presença dos módulos (ii e iv) denota a novidade do trabalho.
O restante do trabalho está organizado da seguinte forma. Na
próxima seção apresentamos detalhes da arquitetura do Campanha
Eleitoral Legal, detalhando cada um dos módulos. Em seguida,
descrevemos os requisitos para execução do sistema. Um link de
apresentação do sistema é apresentado na seção seguinte. Por fim,
relacionamos as considerações finais deste artigo.
ARQUITETURA DO SISTEMA
Todos os componentes principais do Campanha Eleitoral Legal
são apresentados na Figura 1 e detalhados a seguir.
2.1
Coletor de Dados
O Campanha Eleitoral Legal tem como entrada de dados um
conjunto de postagens oriundas do X. Desta forma, implementamos
WFA’2024, Juiz de Fora/MG, Brazil
Marcio Silva, et al.
Figura 1: Arquitetura geral do Campanha Eleitoral Legal.
um Módulo Coletor de Dados em Python utilizando a X API1.
Porém, nós estamos interessados em tweets que tenham a intenção
de impulsionar ou prejudicar candidaturas. A partir desta tarefa,
construímos um dicionário de palavras-chave que foram sugeridas
por especialistas de instituições fiscalizadoras das eleições brasilei-
ras. Em resumo, são termos que indicam o pedido de voto explícito
ao eleitor, ou pedidos de voto implícitos (ex.: “...conto com você nes-
tas eleições.”). Após a coleta, observamos que as palavras-chave que
nos permitem encontrar prováveis propagandas eleitorais, também
são usadas por conteúdo não político como votações online. Para
resolver isso, nós criamos uma blacklist2 de termos com o propósito
de que não fossem retornamos esses tipos de conteúdo.
Ao iniciar esta coleta no período pré-eleitoral, é possível investi-
gar casos de campanha eleitoral antecipada, prática vedada pelas
leis eleitorais brasileiras. Porém, nosso sistema pode funcionar den-
tro do período eleitoral com o objetivo de ajudar na investigação
de denúncias de abuso de poder econômico, combater fake news,
desinformação e “caixa dois” (i.e., gastos de companha não declara-
dos ou de fontes não permitidas por lei). Portanto, nosso sistema
mantém a coleta ininterrupta mesmo dentro do período permitido
para campanha eleitoral.
2.2
Classificação Automática de Propaganda
Eleitoral
Existem alguns trabalhos na literatura exploraram o uso de mo-
delos de aprendizado de máquina para a detecção de discurso po-
lítico [1, 13, 21]. Dentre eles, destacam-se trabalhos focados em
dados do Facebook [23] e sem foco no idioma Português [9]. Além
disso, a tarefa de classificação de postagens em propaganda eleitoral
apresenta grandes desafios como a falta de ferramentas eficazes de
processamento de linguagem em português, diferença do discurso
político entre plataformas, a grande quantidade de ruído nos dados
oriundos das redes sociais que não são de caráter eleitoral (e.g., vo-
tações online e reality shows), porém que possuem padrões textuais
muito semelhantes às eleições. Além disso, o discurso eleitoral pode
mudar com o tempo, sendo influenciado por escândalos, crimes,
eventos de alcance nacional ou global (e.g., pandemia) e fake news.
Diante deste cenário, nós contruímos uma base de dados para
treinar a primeira versão do modelo de aprendizado de máquina
para ser utilizado no Módulo Classificador. Nós realizamos a
coleta de dados no X utilizando a API Histórica3 nos períodos pré-
eleitorais de 2016, 2018 e 2020. Estes períodos estão compreendidos
1https://developer.x.com/en/docs/twitter-api
2https://www.facom.ufms.br/~marcio/brasnam2021/blacklist.txt
3https://developer.twitter.com/en/docs/twitter-api/enterprise/historical-powertrack-
api/overview
entre primeiro de janeiro do ano eleitoral em questão e o início da
campanha eleitoral. Desta forma, os tweets coletados foram postados
entre 1 de janeiro de 2016 a 15 de agosto de 2016, 1 de janeiro de
2018 a 15 de agosto de 2018 e 1 de janeiro de 2020 a 25 de setembro
de 2020, respectivamente.
Em seguida, três voluntários independentes rotularam os dados
coletados com um grau de concordância moderado (Fleiss’ Kappa
𝜅=0.53) [12]. Como reportado por Vera [25], a percepção do que é
político ou não político pode mudar entre voluntários, até mesmo
entre especialistas do domínio. Este fato ocorre principalmente
quando a postagem envolve conteúdo político implícito ou con-
teúdo sobre questões sociais que dividem a sociedade, como aborto,
cotas raciais, liberação de armas, religião, entre outros [20]. Após a
rotulação, nós realizamos experimentos com modelos tradicionais
de aprendizado de máquina, a saber: SVM, Naive Bayes e Logistic
Regression), classificadores baseados em árvores (Random Forest
e Gradient Boosting) além de modelos de aprendizado profundo
(CNN, LSTM, RNN, and HAN) [8].
Finalmente, nós avaliamos diversos cenários onde tweets foram
utilizados como treino e teste, combinando tweets de diversos pe-
ríodos pré-eleitorais. Com base em nossos experimentos, nós en-
contramos que o modelo CNN4 foi escolhido para ser utilizado mo
módulo de classificação.
2.3
Interface Web
O Módulo Web do Campanha Eleitoral Legal é dividido em
duas partes: frontend e backend. A interface Web (frontend) funciona
desacoplada do backend do sistema, utilizando o padrão Rest API
para comunicação e JSON Webtoken para segurança e autenticação.
Neste trabalho, nós criamos uma interface Web, porém o backend
do sistema está preprarado para que terceiros possam construir sua
próprias interfaces ou visualizações (e.g., mobile, desktop, etc.).
Todo o backend foi implementado em Python utilizando a Fas-
tAPI5. Por outro lado, o frontend utiliza Javascript, HTML e CSS
orquestrada pela biblioteca de interface ReactJS6.
A priori, nós não apresentamos nenhuma postagem na interface
do sistema, sendo necessário que o usuário realize um filtro inicial
sobre os dados. Neste filtro, o usuário poderá realizar buscas sobre
os dados apresentados informando intervalo de datas, palavras-
chave, hashtags e informando o nome de usuário na plataforma
do X. De forma geral, o sistema apresenta uma lista ordenada de
postagens com alta probabilidade de serem propagandas eleitorais.
Entretanto, o usuário final do nosso sistema tem o conhecimento
do domínio (legislação eleitoral brasileira) por pertencer a órgãos
reguladores ou autoridades eleitorais. Logo, ele será peça chave
para realizar uma “curadoria” das probabilidades fornecidas pelos
modelos de aprendizado de máquina. Ou seja, nosso sistema fornece
na opções na interface para que o usuário especialista rotule cada
postagem em: (1) “Político”, (2) “Não político” e (3) “Em análise”. Em
resumo, o usuário especialista fará parte de um processo de rotula-
ção contínua de novos tweets, que posteriormente serão utilizados
como novas entradas reforçando o treinamento, tornando nosso
modelo sensível às alterações no discurso de propaganda eleitoral
4Modelo sequencial usando word2vec word embedding (300 dim.) Treinamento em 10
épocas e 10-fold cross validation.
5https://fastapi.tiangolo.com/
6https://pt-br.reactjs.org/
Campanha Eleitoral Legal: Detecção de Propaganda Eleitoral e Ações Coordenadas de Campanha
WFA’2024, Juiz de Fora/MG, Brazil
Figura 2: Visão geral da estratégia de detecção de campanhas de promoção de conteúdo acomodada no Campanha Eleitoral
Legal.
ao longo do tempo. Finalmente, as postagens que por ventura fo-
rem rotuladas como “Em análise´’ precisarão de uma análise mais
criteriosa e investigativa pela autoridade competente.
2.4
Evolução dos Modelos de Apredizado de
Máquina
Dentro do cenário de companhas eleitorais, os candidatos precisam
decidir se entram em confronto direto com seus oponentes ou se
compartilham os suas ideias e programas de governo. Diante deste
dilema, é comum a mudança radical na forma de comunicação
realizada por candidatos para reverter cenários desfavoráveis, a fim
de obter mais votos ou retirá-los de seus oponentes [18, 19].
Portanto, ao trabalharmos com modelos de aprendizado de má-
quina no contexto eleitoral, nós precisamos mitigar erros de classi-
ficação devido a evolução de longo prazo ou mudanças repentinas
no discurso político [11]. Logo, nossa abordagem de treinamento
não se encerra com a criação da primeira versão do modelo. Nós
colocamos os usuários especialistas de domínio dentro de um loop
em nossa arquitetura para a evolução do modelo, onde tal tarefa é
realizada pelo Módulo de Treinamento.
Em suma, esse módulo objetiva o aprimoramento progressivo
do classificador da ferramenta, utilizando os tweets rotulados como
"Políticos" pelos especialistas durante a interação com a interface
Web. Nesse sentido, a ferramenta compila os textos dos tweets ro-
tulados com conteúdo de interesse das autoriades eleitorais, a fim
de realizar, de maneira periódica, adição de novas instâncias para
um novo treinamento do modelo. Desta forma, nós realizamos um
reforço no conjunto de treinamentos adicionando tweets ainda não
conhecidos pelo Campanha Eleitoral Legal. Consequentemente,
o modelo poderá evoluir com as novas narrativas e elementos de lin-
guagem utilizados nas campanhas para driblar a fiscalização contra
abusos durante o período eleitoral.
2.5
Ações Coordenadas de Promoção de
Conteúdo
Resumidamente, campanhas de promoção de conteúdo eleitoral são
grupos de usuários enviando um conjunto de conteúdos similares
com referência explícita a campanhas eleitorais antecipadas e, con-
sistentemente, agindo com outros usuários para realizar a promoção
desse conteúdo em larga escala, sem realizar impulsionamento.
Estudos demonstram evidências de campanhas de desinformação
e atividades maliciosas por usuários reais ou automatizados, im-
pactando seriamente a eficácia de campanhas eleitorais. Matteo et
al. [5] evidenciaram a presença de contas orquestradas promovendo
campanhas no X durante as eleições nacionais do Reino Unido em
2020. Badawy et al. [3] analisaram a interferência russa na campa-
nha eleitoral de 2016 nos EUA, identificando contas “trolls” russas
promovendo conteúdo pró-Trump. Marozzo et al. [17] estudaram
a influência de sites de notícias compartilhados no X durante as
eleições italianas de 2016, destacando a influência das plataformas
digitais no processo eleitoral.
Entretanto, não se encontrou na literatura trabalhos que abor-
dem a detecção de campanhas de propaganda no contexto político-
eleitoral brasileiro, possivelmente devido à escassez de estudos
sobre o tema. As atividades deste estudo exploraram esse nicho,
propondo, implementando e avaliando uma estratégia para identifi-
car campanhas de propaganda eleitoral antecipada em um sistema
real. A Figura 2 apresenta uma visão geral da abordagem proposta.
Dentro do Módulo de Identificação de Promoção de Con-
teudo, o processo começa com o pré-processamento dos dados de
tweets, onde são excluídos aqueles que não foram compartilhados
por mais de um usuário, eliminando assim informações que não
evidenciam ações orquestradas. A fase subsequente, denominada
“Modelagem da Rede”, envolve a criação de um grafo onde vértices
representam usuários e arestas ponderadas indicam a quantidade
de vezes que dois usuários retuitaram o mesmo conteúdo. Esta rede
ajuda a identificar explicitamente a concordância entre usuários ao
compartilhar tweets, capturando possíveis propagandas antecipa-
das.
Devido ao grande volume de dados e à variabilidade no com-
portamento dos usuários, a rede gerada tende a ser vasta e densa,
contendo muitas arestas de pouca relevância. Essas arestas podem
resultar de ações independentes e populares, complicando a identi-
ficação de grupos coordenados. Portanto, o desafio reside em reter
apenas as arestas que fornecem evidências significativas de co-
ordenação, removendo aquelas que representam comportamento
aleatório ou esporádico, o que também otimiza o uso de recursos
computacionais.
Para abordar esse desafio, são utilizados métodos de extração do
backbone, que resumem a rede mantendo apenas as arestas mais
relevantes para um dado problema. A solução proposta combina
WFA’2024, Juiz de Fora/MG, Brazil
Marcio Silva, et al.
as técnicas Threshold e Neighborhood Overlapping, que eliminam
arestas de baixo peso e asseguram que as arestas remanescentes
conectem usuários com comportamentos anômalos em termos de
compartilhamento de tuítes [7, 14]. O algoritmo de Louvain [4] é
então aplicado para identificar comunidades bem conectadas dentro
da rede. Cada comunidade, assim identificada, representa um grupo
de usuários que, potencialmente, agiram de forma coordenada para
promover conteúdos específicos, como propaganda eleitoral.
REQUISITOS PARA EXECUÇÃO
Nosso sistema foi inteiramente desenvolvido para ser implantado
dentro de containers, especificamente utilizando a plataforma Doc-
ker7. Desta forma, o sistema se torna escalável e multiplataforma,
preparado para lidar com o volume de dados gerados pelas redes
sociais. A arquitetura necessita de no mínimo 32GB de memória
RAM para realizar o treinamento de novos modelos de deep learning
e aplicação deste modelo no processo de classificação das postagens
coletadas. Além disso, para a coleta de dados do X será necessário
a obtenção de uma chave de coleta de dados junto à plataforma.
APRESENTAÇÃO DO SISTEMA
Um link para uma captura de tela narrada do sistema em funciona-
mento está disponível aqui: https://drive.google.com/file/d/1ZrbDwc9lx_
0391Ujv5dAu-6xZK1--b78/view?usp=sharing.
CONSIDERAÇÕES FINAIS
Monitorar as redes sociais com o objetivo de encontrar propaganda
eleitoral irregular ainda é um problema complexo. Neste trabalho,
apresentamos um sistema chamado Campanha Eleitoral Legal
que ajuda autoridades a investigar possíveis propagandas eleitorais
irregulares. Além disso, o sistema fornece mecanismos para evoluir
o modelo de aprendizado de máquina a medida que o usuário ro-
tula novas instâncias. Portanto, nós acreditamos que ao longo do
tempo nosso modelo se tornará ainda mais preciso na classificação
de postagens, se tornando uma ferramenta indispensável para as
autoridades competentes na investigação de mau uso das redes
sociais para fins eleitorais.
AGRADECIMENTOS
Este trabalho foi parcialmente financiado por CAPES, CNPq, FA-
PEMIG, FAPESP e Ministério Público de Minas Gerais (MPMG),
projeto Capacidades Analíticas.

--- FIM DO ARQUIVO: 30482-829-24930-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30483-829-24931-1-10-20241001.txt ---
Ferramenta para análises descritivas e preditivas em dados
criminais: Um estudo de caso em Minas Gerais
Yan Andrade
yrandrade123@aluno.ufsj.edu.br
UFSJ
Minas Gerais, Brasil
Gabriel Amarante
gabriel.amarante@dcc.ufmg.br
UFMG
Minas Gerais, Brasil
Matheus Pimenta
matheuspimenta@dcc.ufmg.br
UFMG
Minas Gerais, Brasil
Wagner Meira Jr.
meira@dcc.edu
UFMG
Minas Gerais, Brasil
George Teodoro
george@dcc.edu
UFMG
Minas Gerais, Brasil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ
Minas Gerais, Brasil
Renato Ferreira
renato@dcc.edu
UFMG
Minas Gerais, Brasil
WFA’2024, Juiz de Fora/MG, Brazil
Y. Andrade et al.
TRABALHOS RELACIONADOS
A análise de dados criminais é objeto de investigação de diversos
trabalhos envolvendo a exploração de diferentes técnicas de mine-
ração de dados para identificar padrões e relações entre eventos
criminais. Marzan et al. [5] propôs o uso de algoritmos de regras de
associação para encontrar correlações entre atributos descritivos e
tipos de crime. Os autores identificaram regras que correlacionaram
atributos como dia da semana, condições climáticas, feriado, horário
do dia, tipo de crime com a localidade da ocorrência. Notavelmente,
a maior parte dos crimes tendia a ocorrer em dias sem chuva e
em áreas residenciais. As regras encontradas foram contrastadas
com um heatmap de ocorrências apresentando uma grande corres-
pondência. Baseado nessa última observação, optamos em nossa
ferramenta pelo uso de heatmaps.
Outro conjunto de técnicas muito utilizado em caracterizações
criminais são as de clusterização e detecção de outliers. Walter et al.
[8] analisou padrões espaço-temporais de crimes em micro locais
(ruas) em seis grandes cidades dos EUA utilizando algoritmos de
clusterização. Os autores identificam os micro locais de maior inci-
dência criminal em diferentes momentos no tempo e realizam uma
avaliação temporal comparativa entre eles. Em nossa ferramenta,
realizamos um agrupamento georreferenciado de ocorrências cri-
minais, identificando e destacando os pontos de maior incidência,
o qual chamamos de pontos críticos, no mapa.
Outra linha crescente de trabalhos que visam construir mode-
los preditivos espaço-temporais [2], com destaque para as séries
temporais. Estas são notáveis pela indexação cronológica de suas
observações e, consequentemente, cruciais na análise de tendências
e padrões ao longo do tempo. Esta abordagem divide-se em três
componentes principais: a Tendência, que reflete a direção estrutu-
ral de longo prazo dos dados; a Sazonalidade, relacionada a padrões
repetitivos em intervalos fixos; e o Ruído, representando resíduos
após a remoção de sazonalidade e tendência, expressando variação
não sistemática nos dados. Catlett et al. [3] empregou o modelo
ARIMA [9] para realizar regressão temporal, explorando diferentes
parâmetros e seus impactos. Focando na região de Chicago, o ob-
jetivo era identificar áreas críticas do estado. Prieto [7] apresentou
uma técnica de análise temporal para dados criminais denominada
“Heartbeat”, que sobrepõem as informações espaciais, temporais e de
intensidade, separadas por tipo de crime, em modelo preditivo. Os
autores observaram que o “Heartbeat” foi capaz de indicar momen-
tos específicos de maior ocorrência para diferentes tipos de crime.
FERRAMENTA DE ANÁLISE CRIMINAL
O objetivo da ferramenta é transformar registros de ocorrências
criminais em informações capazes de auxiliar na elaboração de: (1)
estratégias assertivas de combate ao crime; e na (2) construção de
políticas públicas de prevenção ao crime. Uma visão geral da mesma
é apresentada na Figura 1, com detalhamento apresentado a seguir.
3.1
Filtragem de Cenário
O primeiro componente da ferramenta são filtros (item 1 na Fi-
gura 1) que permitem analisar diferentes cenários segmentando a
representação dos dados de acordo com critérios específicos: In-
tervalo de tempo: intervalo de datas específico; Granularidade:
diário, mensal ou anual; Dia da semana: finais de semana, apenas
dias de semana ou apenas algum dia específico da semana; Período
do dia: madrugada, manhã, tarde e noite; Bairros: bairros espe-
cíficos dentro de cada município; Endereço: local mais preciso;
Tipos de Crime: roubo, furto, agressão, etc. Departamento: de-
legacias ou postos policiais; e Unidade: batalhão, companhia. A
partir desses filtros, diversos cenários podem ser construídos. Por
exemplo, investigar como é a distribuição de crimes do tipo ”roubo”,
no bairro ”X” nas ”madrugadas” durante os finais de semana dos
meses de "Janeiro” à "Abril", contrapondo com o cenário parecido,
porém avaliando os "dias de semana".
Figura 1: Tela inicial da ferramenta
3.2
Análises de Tendências
Esse módulo centra-se em fornecer uma compreensão abrangente
das tendências temporais dos eventos. O primeiro campo (item 2 na
Figura 1) exibe o número total de incidentes com base nos filtros uti-
lizados, permitindo ver imediatamente o quantitativo de incidentes.
O segundo componente (item 4 na Figura 1) destaca visualmente
o comportamento do evento, permitindo análises cumulativas ao
longo do tempo. Para essa curva, a ferramenta utiliza um algoritmo
de regressão linear de pontos, utilizando a derivada da função fun-
ção resultante para determinar se a tendência está aumentando ou
diminuindo (item 3 na Figura 1).
3.3
Análise de Distribuição
Esse módulo foi projetado para fornecer uma visão detalhada dos
aspectos espaciais e temporais dos eventos criminais, incluindo dis-
tribuição atualizada por bairro (item 5 na Figura 1) e horário do dia
(item 6 na Figura 1). Cada uma dessas distribuições representa as-
pectos criticamente importantes dos dados, permitindo uma análise
mais profunda das nuances espaciais e temporais dos eventos.
3.4
Mapa de Calor Temporal
Esse módulo combina dois conceitos, um mapa de calor (item 7 na
Figura 1) que destaca as áreas de maior concentração e um calendá-
rio (item 8 na Figura 1) que exibe os dias do mês, proporcionando
uma representação visual dinâmica da concentração de inciden-
tes em cada dia do mês, permitindo uma análise comparativa de
padrões temporalmente recorrentes.
3.5
Mapa de Eventos em Camadas
Um dos pilares fundamentais da nossa ferramenta é a capacidade de
analisar dados com base nas suas propriedades georreferenciadas.
Ferramenta para análises descritivas e preditivas em dados criminais: Um estudo de caso em Minas Gerais
WFA’2024, Juiz de Fora/MG, Brazil
Para isso, adotamos uma apresentação cartográfica por meio de
mapas (item 7 na Figura 1). Implementamos quatro camadas distin-
tas para fornecer informações sobre a geografia do crime visando
proporcionar uma experiência rica e completa em análise espacial
de eventos. A primeira camada demarca os bairros da cidade (Figura
2 a). A segunda é um mapa de calor (Figura 2 b) destacando densi-
dade de incidência criminal. A terceira camada marca os pontos que
aparecem no mapa (Figura 2 c agregando data, hora e tipo de evento.
A quarta camada destaca pontos críticos (Figura 2 d), identificados
por meio de um processo de agrupamento baseado em um limite
de distância máxima entre pontos. É importante ressaltar que todas
essas camadas estão integradas em um único mapa, permitindo que
as camadas sejam modificadas e combinadas conforme necessário.
(a) Mapa de Regiões
(b) Mapa de Calor - Densidade Crimi-
nal
(c) Mapa de Pontos de Crime
(d) Mapa de Pontos Críticos
Figura 2: Detalhamento das Camadas do Mapa de Eventos
3.6
Modelos de Regressão
A aplicação de modelos de regressão para forecasting tem se mos-
trado uma ferramenta promissora. Do ponto de vista da análise
temporal, destacamos o modelo ARIMA[4] . Sua versatilidade re-
side na habilidade de capturar tendências, sazonalidades e padrões
temporais em uma variedade de domínios, contribuindo significa-
tivamente para análises preditivas temporais. Do ponto de vista da
predição espaço-temporal, destacamos o st-KDE (spacial-temporal
KDE) [10] que calcula a densidade espacial contínua de um evento
(i.e. crimes), em cada instante de tempo por meio de um somatório
das observações passadas de forma ponderada (i.e. time-series). Am-
bos os modelos estão implementados em nossa ferramenta e têm se
mostrado, conforme veremos na seção seguinte, muito promissores
no auxílio às forças policiais no planejamento estratégico e alocação
eficiente de recursos.
AVALIAÇÃO EXPERIMENTAL
Para avaliar a aplicabilidade da ferramenta de análise criminal pro-
posta nesse artigo, instanciamos-a utilizando os registros crimi-
nais armazenados pela Polícia Militar do Estado de Minas Gerais
(PMMG), um dos estados mais violentos do Brasil. A PMMG está
subordinada à Secretaria de Segurança Pública de Minas Gerais
(SSPMG), a qual mantém atualmente, um Projeto de Pesquisa, De-
senvolvimento e Inovação (PD&I) com as universidades dos autores
do presente artigo. Nossa avaliação divide-se em duas partes com-
plementares: a aplicação prática da ferramenta, permitindo uma
interação dinâmica de diferentes cenários; e a aplicação e testes de
modelos preditivos.
4.1
Aplicação Prática
Nessa seção visamos apresentar estudos de caso que demonstrem a
aplicabilidade de ferramenta focando na compreensão do compor-
tamento criminal durante o Carnaval de 2023 e em como a mesma
pode auxiliar na tomada de decisão. A ferramenta permite identifi-
car padrões e tendências, que podem ser usados para definir pontos
de prioridades para alocação de recursos de segurança, como es-
cala de patrulhas e definição de rotas. Analisaremos os dados do
Bairro Seção Urbana Primeira, em Belo Horizonte, entre os dias
4 a 26 de fevereiro. A Figura 3 apresenta o módulo de tendências
englobando o primeiro trimestre de 2023. onde fica evidente uma
alta expressiva da criminalidade no mês de fevereiro. Outro ponto
a destacar é a semelhança das ocorrências nos meses de janeiro e
março, períodos que antecedem e sucedem o Carnaval, destacando
o mês de fevereiro.
Figura 3: Modulo de tendências da ferramenta para o pri-
meiro trimestre de 2023
(a) Pré-Carnaval
(b) Carnaval
(c) Pós-Carnaval
Figura 4: Comparativo entre a distribuição espacial de ocor-
rências no bairro SUP, antes, durante e após o Carnaval 2023
A Figura 4 representa três momentos distintos: antes, durante
e após o Carnaval. Nota-se um aumento significativo de crimes
WFA’2024, Juiz de Fora/MG, Brazil
Y. Andrade et al.
na região central durante o Carnaval, Figura 4 (b), destacando a
presença de pontos críticos nesse período. Esses pontos críticos
apresentam alta relação com a localização dos tradicionais blocos
de carnaval em BH, ressaltando, portanto, áreas de maior perigo que
devem ter maior atenção. Outro ponto a destacar é que, assim como
acontece em números de ocorrência, os pontos de criminalidade
antes e depois do Carnaval, Figura 4(a e c), tendem a manter um
padrão. A ferramenta permite distribuir recursos e escalar patrulhas
de maneira mais eficiente, em lugares que demandam atenção, tudo
baseado em filtros que podem definir múltiplos cenários.
Figura 5: Modelo Arima; Em azul o último intervalo de treino,
em verde a predição (em verde-claro o intervalo de confiança)
e em laranja/vermelho o valor real
Para ilustrar o funcionamento do módulo de regressão, testa-
mos os algoritmos ARIMA e st-KDE. Utilizamos dados de furtos
de 01/01/2023 a 24/01/2023 para treinar o ARIMA e dados de no-
vembro de 2023 para o st-KDE. Previmos crimes para a semana de
25/01/2023 a 31/01/2023 com o ARIMA (Figura 5). Para o st-KDE,
fizemos previsões por setores para a primeira semana de dezem-
bro de 2023 (Figura 6 e Tabela 11). Ambos os modelos mostraram
desempenho satisfatório, com previsões próximas aos dados reais.
A integração desses modelos oferece uma abordagem completa
para análises preditivas, contribuindo de forma mais eficaz para o
aprimoramento da segurança pública.
Metricas
MSE
MAE
ALS
st-KDE
0.00055
0.01079
-4.15952
Tabela 1: Resultados referente ao modelo st-KDE
CONCLUSÃO
Neste trabalho, apresentamos uma ferramenta para análise de dados
criminais que é capaz de caracterizar os dados e transformá-los em
informações que auxiliam no desenvolvimento de táticas assertivas
de combate e prevenção ao crime. Nossa ferramenta compreende a
dinâmica da ocorrência de crimes a partir de duas perspectivas dife-
rentes: temporal e espacial, considerando dois tipos de análise: dis-
cretiva e preditiva, apresentando uma interface simples e intuitiva
para ser utilizada por agentes públicos responsáveis pela segurança.
1As métricas medem o erro do modelo, para o MSE e MAE quanto menor melhor, no
caso do ALS quanto mais próximo de zero melhor.
Figura 6: Modelo st-KDE; Escala da cor amarela (menor con-
centração) para vermelha (mais concentração).
Para avaliar a aplicabilidade da ferramenta, instanciamos-a utili-
zando dados criminais armazenados pela Polícia Militar do Estado
de Minas Gerais (PMMG). Realizamos duas análises descritivas e
preditivas, considerando dados de Belo Horizonte. Na análise descri-
tiva, comparamos o mapa de crimes antes, durante e após o Carna-
val, demonstrando uma mudança significativa no comportamento
criminoso, com um aumento de crimes concentrados em pontos
turísticos específicos. Na análise preditiva, treinamos e avaliamos
dois modelos de previsão de crimes, um temporal e outro espaço-
temporal onde ambos apresentram valores previstos próximos aos
dados reais, permitindo consequentemente, que a segurança pública
desenvolva ações preventivas de forma mais assertiva.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq e Fapemig.

--- FIM DO ARQUIVO: 30483-829-24931-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30485-829-24933-1-10-20241001.txt ---
Integração de DAW com Infraestruturas Remotas de
Processamento de Áudio
Carlos E. C. F. Batista
LAViD/UFPB
João Pessoa, Brasil
bidu@lavid.ufpb.br
Gabriel B. F. Andrade
LAViD/UFPB
João Pessoa, Brasil
gabriel.ferraz@lavid.ufpb.br
Geovana M. S. Lima
LAViD/UFPB
João Pessoa, Brasil
geovana.lima@lavid.ufpb.br
Caio M. C. Guedes
Music.AI
João Pessoa, Brasil
caio.marcelo@moises.ai
WebMedia’2024, Juiz de Fora, Brazil
Batista et al.
seção descreve o plug-in Orchestrator Pad, abordando sua
arquitetura de software e principais funcionalidades. A última
seção oferece conclusões relacionadas ao desenvolvimento e
teste do plug-in e discute novas funcionalidades para versões
futuras.
2 API C++
A API C++ foi desenvolvida com o intuito de facilitar o acesso
aos recursos da plataforma Music.AI. Ela encapsula as
requisições REST e retorna objetos que contém as informações
recebidas.
O CMake [6] foi empregado para configurar parâmetros de
compilação e gerenciar dependências do projeto de software,
assegurando sua portabilidade. A biblioteca cpr [7] foi escolhida
por sua capacidade de realizar requisições HTTP de forma
simples e eficaz, essencial para a comunicação com a API REST
da plataforma Music.AI. Finalmente, a biblioteca nlohmann-json
[8] foi fundamental para manipular e gerenciar dados no formato
JSON, para o tratamento de informações recebidas e enviadas
pela API.
Para evitar dependências excessivas no uso de JSON fora do
escopo da API, foi necessário mapear e abstrair os dados
recebidos para formatos que melhor representassem as
informações usando recursos nativos do C++. As classes
desenvolvidas incluem:
●
Application: informações relacionadas à chave de acesso
utilizada, para identificação do usuário solicitante em todas
as requisições.
●
Workflow: representa um workflow (fluxo de trabalho), com
ID, nome, slug e descrição. Um workflow pode ser criado na
plataforma Music.AI a partir da integração dos múltiplos
serviços oferecidos em uma sequência que pode resultar em
um ou mais arquivos de áudio.
●
WorkflowParams: encapsula a URL de entrada de um
workflow.
●
Job (tarefa): encapsula um processo na plataforma, contendo
informações como entrada de dados, o workflow a ser
utilizado no processamento e as saídas geradas.
●
Load: armazena URLs para upload e download de arquivos.
●
Metadata: armazena os metadados de uma tarefa.
●
RequestResult: armazena os resultados do processamento de
uma tarefa.
●
JSONUtils: auxilia na conversão entre JSON e as classes
criadas.
●
MusicAIException: exceções lançadas em caso de falhas nas
requisições.
Por fim, a classe MusicAI integra a lógica central da API,
oferecendo funções como:
●
getApplication: obtém informações sobre a chave de acesso
em uso.
●
getUpload: recupera URLs para upload e download de
arquivos.
●
getWorkflows: obtém todos os workflows (ou um número
específico) vinculados a uma chave.
●
createJob: cria uma nova tarefa.
●
getJob: obtém uma tarefa específica.
●
getJobs: obtém todas as tarefas associadas à chave de acesso.
●
deleteJob: exclui uma tarefa.
●
uploadFile: envia um arquivo de áudio para o servidor.
●
getSession: cria uma sessão para verificar uma tarefa.
●
monitorJob: verifica o status de uma tarefa.
●
downloadFromJob: baixa os resultados de uma tarefa para um
diretório.
●
downloadFromResult: baixa os resultados de um objeto
RequestResult para um diretório.
A API C++ padroniza e simplifica significativamente a
interação com a plataforma ao encapsular as requisições
possíveis e definir objetos que representam as informações de
resultado. A proposta se destaca em um cenário onde poucos
serviços disponibilizam API para manipulação de áudio
utilizando processamento remoto. Alguns dos serviços que
oferecem API C++ focam em funções mais específicas e
limitadas, como otimização de áudio e tratamento de voz - o
krisp.ai [9] se especializa na remoção de ruído de fundo em
chamadas e gravações, enquanto o sonicAPI.com [10] oferece
funcionalidades para processamento de áudio, como
transformação de texto em fala e reconhecimento de voz. Em
contraste, a API C++ desenvolvida para o Music.AI se distingue
por possibilitar o acesso a um conjunto mais amplo e versátil de
ferramentas de manipulação de áudio.
3 ORCHESTRATOR PAD
O plug-in Orchestrator Pad (Figura 1) foi desenvolvido utilizando
a API C++ apresentada na seção anterior e o framework JUCE
[5], que é amplamente utilizado para criação de plug-ins de
áudio. A arquitetura do plug-in é composta por várias classes que
desempenham funções específicas, como a interface gráfica do
usuário, manipulação de áudio e interação com a API Music.AI.
As principais classes que compõem a arquitetura do plug-in
são:
●
AudioProcessor: é responsável pelo processamento de áudio -
implementação padrão do JUCE para roteamento dos fluxos
de áudio entre o plug-in e a DAW.
●
AudioProcessorEditor: é a principal responsável pela interface
do usuário. Esta classe é responsável por inicializar e
gerenciar todos os componentes visuais do plug-in.
●
ConfigWindow: é uma janela de diálogo usada para
configurar a chave da API. Ela exibe campos de entrada e
botões para salvar e cancelar a configuração. A chave da API
é salva em um arquivo chamado music_ai.apikey no diretório
de trabalho atual.
Integração de DAW com Infraestruturas de Processamento de
Áudio Remotas
WebMedia’2024, Juiz de Fora, Brazil
●
ContentComponent: é responsável pela reprodução dos
arquivos de áudio gerados pelo processamento da tarefa. Ela
utiliza juce::AudioAppComponent para gerenciar o áudio e
juce::AudioTransportSource para controlar a reprodução. O
estado da reprodução é gerenciado pela enumeração
TransportState, que inclui estados como Stopped, Playing,
Paused, etc. A classe também implementa métodos para
manipulação de volume e panorama, além de controle de
progresso.
Figura 1: Interface gráfica do Orchestrator pad
A partir da Figura 1 identificam-se os elementos da interface
do plug-in Orchestrator pad e então discutir como suas
funcionalidades são exploradas. Os seguintes elementos são
apresentados:
●
Chave de Acesso (API Key): Permite inserir uma chave de
acesso para autenticação com a API do Music.AI. A chave é
definida em uma janela exibida através do botão “...”.
●
Botão “Load Audio”: Permite carregar um arquivo de áudio
(no exemplo, “walk.mp3”) para processamento.
●
Seleção de Workflow: Permite selecionar um dos workflows
vinculados à conta da chave utilizada, para utilização na
criação de uma tarefa.
●
Botão “Run Job”: Habilitado após a carga do arquivo de áudio
a ser processado, inicia a execução nos servidores da
plataforma de uma tarefa onde o workflow selecionado será
aplicado ao arquivo carregado.
●
Seleção de Saída: Habilitado após a realização da tarefa,
permite selecionar um dos arquivos de saída da execução. No
exemplo, “Drums.wav” está selecionado como o arquivo de
saída.
●
Botões de Controle de Reprodução: Habilitados após a
realização da tarefa, são botões para reproduzir, pausar, parar
e salvar o áudio processado.
●
Barra de Progresso: Permite visualizar e definir o ponto de
reprodução da saída selecionada.
●
Barra de Volume: Permite definir o volume de reprodução da
saída selecionada.
●
Barra de Panorama: Permite definir o panorama (distribuição
nos canais estéreo) da reprodução da saída selecionada.
●
Console de Status: Exibe mensagens de estado da realização
da tarefa e da reprodução dos arquivos de resultado.
3.1 Usando o plug-in
O Orchestrator Pad foi desenvolvido principalmente para ser
utilizado em conjunto com uma DAW (Figura 2), funcionando
como um plug-in, conforme explicado anteriormente. As opções
de compilação oferecidas pelo projeto de software permitem a
criação não só de um plug-in VST3, mas também de uma
aplicação autônoma, capaz de ser executada em múltiplas
plataformas. Apesar dessa flexibilidade, o uso como plug-in em
uma DAW é considerado o cenário padrão para a ferramenta.
Portanto, este contexto será utilizado para apresentar a jornada
do usuário.
Para usar o plug-in Orchestrator Pad, o usuário primeiro deve
carregar e selecionar o plug-in dentro de uma DAW (compatível
com VST3 [1]), vinculando-o a uma trilha de um projeto. Com o
plug-in carregado, o usuário poderá definir a chave de acesso
para a API da plataforma (que é salva). A manipulação parte do
carregamento de um arquivo de áudio local - a atual versão ainda
não permite o tratamento de fluxos de áudio vindo da DAW para
o plug-in.
Figura 2: Orchestrator pad usado em conjunto com a DAW
Audacity.
Após o carregamento do arquivo, o usuário precisa selecionar
um dos workflows disponíveis. Esses workflows estão vinculados
à conta do usuário na plataforma Music.AI e são configurados
previamente. Cada
workflow
integra um conjunto de
funcionalidades específicas de processamento de áudio, como
separação de stems, transcrição de letras, masterização
automática ou otimização vocal. A atual versão do plug-in trata
apenas workflows que geram saídas no formato de arquivo de
áudio.
Com o arquivo de áudio carregado e o workflow escolhido, o
usuário pressiona o botão “Run Job” para iniciar a tarefa. O plug-
in então envia as informações para a plataforma realizar o
WebMedia’2024, Juiz de Fora, Brazil
Batista et al.
processamento de acordo com o que é definido no workflow.
Durante esse processo, o console do plug-in exibe informações
sobre o estado da execução.
Depois da conclusão do processamento, a interface ativa o
componente que permite a seleção dos arquivos de áudio gerados
(saída). Os controles play, pause e stop possibilitam a
manipulação da reprodução do áudio selecionado. O usuário
também tem a opção de salvar o arquivo de áudio selecionado
em uma pasta externa.
Durante a reprodução, os usuários podem visualizar e ajustar
o progresso, volume e o panorama utilizando os sliders
oferecidos pelo plug-in. O resultado da reprodução é vinculado à
trilha associada ao plug-in na DAW (Figura 2), podendo ser
posteriormente manipulado e reproduzido utilizando os recursos
da própria DAW e outros plug-ins. O vídeo de teste do
Orchestrator pad1 apresenta a visualização completa do processo
de uso do plug-in.
4 CONCLUSÕES E TRABALHOS FUTUROS
O Orchestrator Pad é um plug-in que viabiliza a integração das
funcionalidades da API Music.AI dentro das ferramentas de
edição de áudio. Os testes realizados demonstraram que o plug-in
opera com sucesso tanto em modo autônomo quanto integrado a
uma DAW, garantindo sua versatilidade e eficácia em diferentes
cenários de uso.
Os códigos-fonte da API em C++ e do plug-in estão
disponíveis em um repositório público no GitHub2, sob uma
licença de código aberto. Isso permite que desenvolvedores e
entusiastas contribuam para o contínuo aprimoramento da
ferramenta. O desenvolvimento do Orchestrator Pad continua
após o lançamento inicial, com a incorporação de novas
funcionalidades e melhorias.
Para as próximas versões do Orchestrator Pad, são planejadas
várias melhorias e novas funcionalidades que visam otimizar a
experiência do usuário e ampliar as capacidades do plug-in. Uma
nova interface gráfica que incorpora de forma mais intuitiva e
eficiente o fluxo de trabalho utilizado na manipulação do plug-in
está em desenvolvimento. Esta interface redesenhada facilitará a
navegação e a utilização das funcionalidades oferecidas,
proporcionando uma experiência de usuário mais agradável e
produtiva. Outra melhoria significativa será a implementação da
funcionalidade de arrastar e soltar (drag and drop) tanto para a
entrada quanto para a saída de arquivos de áudio.
Será desenvolvida a capacidade de tratamento de fluxos de
áudio de entrada diretamente da DAW, com comunicação em
tempo real com a API REST usando streaming de áudio. Por fim,
serão adicionadas funcionalidades de visualização e edição dos
workflows diretamente no plug-in, oferecendo aos usuários uma
flexibilidade maior para configurar e ajustar seus processos de
áudio de acordo com suas necessidades específicas.
O processamento remoto de áudio tem impulsionado
ferramentas cada vez mais relevantes para a produção musical
1 Vídeo de demonstração acessível através da URL:
https://tinyurl.com/opadvideo
2 https://github.com/moiseslabs/orchestrator-pad/
moderna, permitindo a execução de tarefas computacionalmente
intensivas em ambientes remotos, o que otimiza o fluxo de
trabalho e expande as possibilidades criativas.
Uma parte essencial do projeto é a análise contínua do estado
da arte, abrangendo desde sistemas mais antigos, como o
AudioGridder [11], que foca na distribuição da execução de
plugins, passando por sistemas baseados em WebAudio [12], até
as plataformas mais avançadas, como o HARP [13] que realiza
processamento de deep learning remoto. Este estudo tem como
objetivo fornecer uma visão mais precisa sobre o uso do
processamento remoto na produção musical e a estruturação
dessas soluções sob a perspectiva da arquitetura de software. Já
há uma compreensão inicial de que essas soluções são bastante
heterogêneas, refletindo a diversidade de funcionalidades
disponíveis. Parte dessa análise e compreensão será consolidada
em um artigo, que apresentará um panorama abrangente sobre o
tema.

--- FIM DO ARQUIVO: 30485-829-24933-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30486-829-24934-1-10-20241001.txt ---
Jogos Sérios para Auxiliar o Exercício Cognitivo de Idosos
Utilizando o Robô Social EVA
Pedro Lucas Pereira da Silva
pl_silva@id.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, RJ, Brasil
João Vitor Carvalho Cecim
joaocecim@id.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, RJ, Brasil
Marcelo Marques da Rocha
marcelo_rocha@midiacom.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, RJ, Brasil
Débora Christina Muchaluat-Saade
debora@midiacom.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, RJ, Brasil
WFA’2024, Juiz de Fora/MG, Brazil
Pereira da Silva et al,
TRABALHOS RELACIONADOS
Com o avanço das tecnologias e o crescente interesse em métodos
inovadores para a estimulação cognitiva, a integração de jogos sé-
rios no tratamento de condições relacionadas ao envelhecimento
tem se mostrado promissora. Neste contexto, em [6], foi apresen-
tada uma proposta de um jogo sério baseado no teste de Stroop,
uma conhecida técnica de psicologia cognitiva que envolve a leitura
de palavras que nomeiam cores impressas em cores conflitantes
(por exemplo, a palavra ’vermelho’ escrita em tinta azul). O jogo
exige a inibição de respostas automáticas e a resolução de conflitos
cognitivos, sendo desenvolvido com base na tarefa Stroop clássica
e adaptado para um formato digital interativo. Este jogo cognitivo,
voltado para idosos e desenvolvido utilizando o middleware Ginga-
NCL [1] para a TV digital brasileira, integra efeitos sensoriais de
luz para aumentar a atratividade e eficácia na estimulação cognitiva
dos usuários idosos. Estudos anteriores destacam a importância dos
jogos virtuais na melhoria da saúde mental e cognitiva, especial-
mente em tratamentos de doenças relacionadas ao envelhecimento.
Testes realizados com idosos demonstraram boa aceitação do jogo e
indicaram que os efeitos sensoriais de luz influenciaram a percepção
dos jogadores, tornando o jogo mais envolvente.
O trabalho descrito em [15] explora o uso de jogos sérios em SARs
para terapias de regulação emocional para crianças com Transtorno
do Espectro Autista (TEA). O artigo descreve o robô EVA, que é
capaz de falar, ouvir e expressar emoções através do olhar, e destaca
a melhoria das capacidades do EVA para reconhecer emoções dos
usuários por meio do reconhecimento facial e criar efeitos sensoriais
de luz, tornando a terapia mais atraente para as crianças. Esse
avanço representa um passo significativo na oferta de terapias
imersivas para crianças autistas, integrando comunicação verbal,
não-verbal e interação social.
Ambos os estudos mostram a relevância dos efeitos sensoriais em
intervenções tecnológicas voltadas para diferentes grupos etários,
seja para a estimulação cognitiva de idosos ou para a regulação
emocional de crianças com TEA. A utilização de luzes e outros
estímulos sensoriais demonstrou ser uma estratégia eficaz para
aumentar o engajamento em jogos sérios e atividades propostas,
refletindo o potencial das tecnologias assistivas na área da saúde.
Este trabalho segue uma abordagem similar, propondo novos jogos
sérios para o robô EVA voltados para o público idoso.
LINGUAGEM EVAML E SIMULADOR EVASIM
A linguagem EvaML é uma linguagem de programação baseada em
XML, criada para facilitar o desenvolvimento de scripts de interação
para o robô EVA. Ela oferece abstrações que facilitam a construção
dos scripts, permitindo que sejam criados utilizando um editor de
texto simples. Por ser baseada em XML, a EvaML é mais legível
para não-programadores.
A EvaML possui elementos que permitem a interação por voz,
o controle de uma lâmpada inteligente, a execução de arquivos de
áudio e o controle da movimentação da cabeça e dos braços do
robô. Além disso, oferece elementos para a criação e manipulação
de variáveis e para o controle do fluxo de execução do script, entre
outros.
Com o objetivo de auxiliar no desenvolvimento e teste de scripts
para o robô EVA, os autores em [5, 11] propuseram o simulador
EvaSIM. O simulador busca simular, através de elementos gráficos
em sua interface, o comportamento do robô físico. Em sua versão
atual, o EvaSIM possui recursos aprimorados de interação multimo-
dal, como reconhecimento de expressões faciais, reconhecimento
de gestos e leitura de QR codes utilizando uma webcam, além da
interação por voz, utilizando um microfone. Todos os jogos sérios
propostos neste trabalho utilizaram a linguagem EvaML e o simu-
lador EvaSIM durante o processo de desenvolvimento e teste dos
scripts para o robô.
JOGOS SÉRIOS PROPOSTOS
Todas as aplicações criadas são jogos sérios interativos que utili-
zam o robô EVA como plataforma. Os jogos foram projetados para
engajar o usuário por meio de uma série de interações dinâmicas e
multimodais.
Figura 1: Fluxograma do funcionamento geral dos jogos
No início de cada jogo, o robô EVA dá as boas-vindas ao usuário
e o convida a participar do jogo. Utilizando efeitos de luz e expres-
sões emocionais, o robô cria um ambiente imersivo e atraente. Em
seguida, o robô faz uma pergunta para saber o nome do usuário,
personalizando a interação conforme o nome fornecido. Na Figura
1, é possível observar o fluxo principal presente nas aplicações.
4.1
Jogo Genius
Esta proposta de jogo sério foi baseada no brinquedo Genius, um
jogo eletrônico muito popular na década de 80, lançado pela empresa
de brinquedos Estrela.
Assim como no jogo original, o jogo sério proposto tem como ob-
jetivo estimular a memória utilizando cores e sons. O jogo consiste
em acertar a sequência de cores que será mostrada através de uma
lâmpada inteligente. Cada cor tem uma nota musical associada, que
será reproduzida pelo robô para estimular o raciocínio. Conforme
o jogador acerta a sequência, uma nova cor é acrescentada a cada
rodada. A Figura 2 apresenta o jogo original Genius e o robô social
com a lâmpada inteligente conectados através de uma rede Wi-Fi.
No início do jogo, o EVA se apresenta ao usuário perguntando
seu nome. Em seguida, pergunta se o jogador está interessado em
participar. Se o usuário se recusar a jogar, o EVA se despede e
encerra a interação. Uma vez confirmada a participação do usuário,
o robô explica as regras do jogo e mostra as quatro cores que podem
Jogos Sérios para Auxiliar o Exercício Cognitivo de Idosos Utilizando o Robô Social EVA
WFA’2024, Juiz de Fora/MG, Brazil
Figura 2: (a) Brinquedo eletrônico Genius. (b) Robô EVA.
aparecer durante a interação: amarelo, verde, azul e vermelho. Além
de mostrar cada cor, o robô também toca a nota musical associada
a cada uma delas. Quando o EVA termina de apresentar as cores e
explicar as regras, ele sinaliza que o jogo vai começar.
O jogo começa com uma sequência pré-definida, que será sorte-
ada no início de cada sessão. A aplicação disponibiliza sete sequên-
cias, e cada uma delas pode ter até um máximo de cinco cores. Se o
jogador acertar a sequência correta de cores cinco vezes seguidas,
o robô expressa felicidade e parabeniza o jogador por ter vencido
o jogo. Além disso, o robô diz uma frase de motivação elogiando
a capacidade de memória do usuário. Se o jogador errar alguma
cor da sequência em qualquer rodada, o robô demonstra tristeza e
informa ao usuário que ele errou a sequência.
Figura 3: Fluxograma do funcionamento lógico do jogo ge-
nius
Após o EVA interagir com o jogador, tanto quando ele acerta
todas as cores quanto quando ele erra alguma cor, o robô sempre
pergunta se a pessoa tem interesse em jogar novamente. Se a res-
posta for afirmativa, o robô se mostra feliz e diz que o jogo será
reiniciado. Nesse momento, uma nova sequência de cores é gerada
aleatoriamente para uma nova tentativa. Se a resposta for negativa,
o robô demonstra tristeza e agradece a participação do usuário,
encerrando o jogo. Os processos presentes na etapa entre o início
e o fim do jogo Genius podem ser analisados detalhadamente na
Figura 3.
Um vídeo de demonstração do jogo sério Genius utilizando o
robô EVA pode ser assistido a partir deste link.
4.2
Jogos Inspirados no Teste MoCA
Dificuldades cognitivas são comuns em idosos, e a intervenção pre-
coce pode retardar a progressão para condições mais graves, como
a demência. Um dos instrumentos utilizados para o diagnóstico de
comprometimento cognitivo é o teste MoCA (Montreal Cognitive
Assessment). Os autores em [3] mostram a eficácia do MoCA para o
diagnóstico de Comprometimento Cognitivo Leve (CCL). Esta seção
apresenta a proposta de três jogos sérios desenvolvidos com base no
teste MoCA, utilizando a plataforma de robótica social EVA. Cada
jogo foi projetado para avaliar diferentes capacidades cognitivas.
As subseções a seguir descrevem cada um dos jogos.
4.2.1
Jogo de Pares de Palavras. No exercício de abstração, o obje-
tivo é avaliar a capacidade do usuário de pensar de forma abstrata
e estabelecer analogias. Nesta aplicação, o robô EVA apresenta
duas palavras ao paciente e solicita que ele identifique o que essas
palavras têm em comum.
Figura 4: Fluxograma de funcionamento lógico do jogo de
pares de palavras
Por exemplo, se o EVA diz ’maçã’ e ’laranja’, a resposta esperada
seria ’fruta’. Esse exercício ajuda a testar a habilidade do usuário
em reconhecer categorias e relações entre conceitos aparentemente
distintos. A Figura 4 retrata especificamente o estado entre o início
e o término do jogo de pares de palavras.
Através do link, pode-se acessar uma demonstração da aplicação
Jogo de Pares de Palavras no simulador do robô EVA link.
4.2.2
Jogo da Repetição de Letras. O jogo de repetição consiste em
contar quantas vezes a letra ’X’ aparece em uma sequência de letras
WFA’2024, Juiz de Fora/MG, Brazil
Pereira da Silva et al,
falada pelo robô. No início do jogo, são explicadas as regras e, em
seguida, a sequência é mostrada. A aplicação utiliza quatro macros
para gerar sequências diferentes e verificar a quantidade de ’X’. O
robô escuta a resposta do usuário, fornece feedback com base na
precisão e expressa emoções de felicidade ou tristeza, dependendo
se houve acerto ou erro.
Após o jogo, o robô pergunta se o usuário deseja jogar novamente
e reinicia ou encerra o jogo conforme a resposta. No código, é
possível configurar o robô com voz, áudio e efeitos de luz, e utilizar
a macro ’apresentação’ para introduzir o robô e perguntar o nome
do usuário. Dependendo da resposta do usuário sobre jogar ou não,
o robô inicia ou encerra a interação, oferecendo uma experiência
interativa e educativa com feedback ajustado.
Um vídeo de demonstração do Jogo da Repetição de Letras ro-
dando no simulador do robô pode ser visto através deste link. Um
outro vídeo demonstrando o Jogo no simulador antigo, onde ocorre
o looping da aplicação, pode ser visto através deste link.
4.2.3
Jogo de Cálculo. O exercício de cálculo e atenção envolve a
subtração de 7 em 7 e é outra aplicação baseada no MoCA. Neste
teste, o robô EVA solicita ao usuário que comece com um número
aleatório fornecido pelo programa, que deve ser maior que 35 e
menor ou igual a 100, e diminua esse número em 7 repetidamente
até um total de 5 subtrações.
Por exemplo, se o jogo começar com o número 100, o usuário deve
continuar subtraindo 7 e fornecer a sequência correta (100, 93, 86,
etc.). Esse exercício visa avaliar a capacidade do usuário de realizar
cálculos mentais e manter a atenção durante o processo. A tarefa
ajuda a testar a agilidade mental e a capacidade de concentração
do usuário.
Entre no link para visualizar a demonstração da aplicação no
simulador do robô EVA link.
CONSIDERAÇÕES FINAIS
Através deste trabalho, podemos observar que as aplicações desen-
volvidas para o robô social EVA, utilizando a linguagem EvaML,
se destacam pela capacidade de criar experiências interativas e
educativas focadas no aprimoramento cognitivo de idosos. Desde a
introdução do EVA, o objetivo tem sido usar a robótica para oferecer
suporte assistivo em atividades cognitivas e sociais. A linguagem
EvaML, projetada para facilitar a criação e personalização dessas
atividades, tem sido essencial para desenvolver jogos sérios, como
o ’Jogo Genius’ e os jogos inspirados no MoCA, que demonstram
como a tecnologia pode engajar usuários em atividades que estimu-
lam a memória, a atenção e outras funções cognitivas.
A integração de elementos sensoriais, como luzes, sons e expres-
sões emocionais, enriquece a interação do robô com os usuários.
Este trabalho contribui para a utilização de robôs socialmente assis-
tivos em contextos terapêuticos, promovendo a saúde mental e o
bem-estar da população idosa, além de oferecer novas possibilidades
para o cuidado e a inclusão social de grupos vulneráveis.
Para trabalhos futuros, serão desenvolvidas novas aplicações que
aproveitam ao máximo as recentes capacidades do robô, incluindo
funcionalidades avançadas, como leitura de QR codes e reconhe-
cimento de expressões faciais. Essas inovações permitirão ao robô
interpretar uma gama mais ampla de sinais humanos, resultando
em interações mais naturais.
Além disso, um importante trabalho futuro é a realização de tes-
tes com idosos em ambientes geriátricos para coletar dados cruciais
sobre a aceitação e o impacto emocional dessas tecnologias em
populações idosas. Um projeto já submetido ao Comitê de Ética em
Pesquisa com esse foco permitirá a realização desses testes em um
futuro próximo.
AGRADECIMENTOS
Os autores agradecem o apoio recebido do CNPq, FAPERJ, Google
Research, CAPES, CAPES PRINT, FINEP, INCT-MACC e INCT-
ICONIoT.

--- FIM DO ARQUIVO: 30486-829-24934-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30487-829-24935-1-10-20241001.txt ---
LightningBR: Ferramenta Web para Visualização e Análise de
Ocorrência de Raios no Brasil
Elton R. Alves
Warley M. V. Júnior
eltonalves@unifesspa.edu.br
wmvj@unifesspa.edu.br
Universidade Federal do Sul e Sudeste
do Pará
Alife S. de Moraes
Diogo da S. Alvino
alife.silva@unifesspa.edu.br
diogo.alvino@unifesspa.edu.br
Universidade Federal do Sul e Sudeste
do Pará
Andson M. Balieiro
amb4@cin.ufpe.br
Universidade Federal de Pernambuco
WFA’2024, Juiz de Fora/MG, Brazil
Elton R. Alves et al.
Este artigo está organizado da seguinte forma: A Seção 2 descreve
a arquitetura geral da aplicação e detalhes das camadas de front-end
e back-end. Na Seção 3 são apresentados os principais resultados
relacionados as funcionalidade da ferramenta. Por fim, a conclusão
e trabalhos futuros são apresentados na Seção 4.
A FERRAMENTA WEB LIGHTNINGBR
Nesta seção, detalha-se a arquitetura geral da ferramenta, descre-
vendo cada um dos componentes desenvolvidos, bem como a inte-
ração entre eles.
2.1
Arquitetura Geral
A Figura 1 exibe a estrutura geral do sistema proposto, demons-
trando um fluxo adequado para o tratamento dos dados, de modo
que os usuários possam acessar as funcionalidades que incluem
um sistema de filtragem, importação de arquivos, mapa e gráficos
dinâmicos por meio da interface.
Figura 1: Arquitetura Geral.
Conforme ilustrado pela Figura 1, os dados que alimentam o
sistema são obtidos através da Earth Networks - Sferic Maps (Etapa
1) [2], sendo disponibilizados em formato .csv (Comma Separated
Values). A Etapa 2 envolve a conversão desses arquivos CSV para o
formato JSON (JavaScript Object Notation) para ser compreendido
pelo banco de dados. Uma vez que os dados são transformados
em formato JSON, dá-se início ao processo da Etapa 3, ou seja, a
interface da camada de front-end oferece uma funcionalidade de
importação, o que permite a inclusão dos dados no banco de dados
por meio de um campo incorporado na tela.
Antes da inserção no banco de dados, os dados passam por uma
breve validação em nível de código (Etapa 4) para garantir que
as chaves e valores estejam formatados corretamente. Se a vali-
dação ocorrer sem problemas e nenhuma anomalia for detectada
na estrutura do arquivo JSON, os dados são inseridos (Etapa 5). É
importante observar que, se os arquivos já tiverem sido adicionados
anteriormente, o sistema notificará o usuário de que informações
semelhantes já foram incluídas.
Uma vez que os dados estão no banco de dados, é estabelecido
um relacionamento com a API (Application Programming Interface)
de mapas (Etapa 6). Isso é realizado por meio de um ambiente de
execução baseado em JavaScript no lado do servidor (JavaScript
Server-Side) na Etapa 4. O servidor carrega as informações do banco
de dados e as armazena em uma variável temporária que pode
ser acessada por todas as rotas da aplicação. A API de mapas é
uma biblioteca que é chamada na camada de front-end e possibilita
a geração de um mapa geográfico, o qual é exibido em uma das
páginas da aplicação. As propriedades da variável temporária, como
latitude e longitude, são usadas por esta API para personalizar tanto
a exibição dos marcadores no mapa quanto a clusterização destes,
conforme sugerido na documentação da API [1].
Neste cenário, outra funcionalidade foi desenvolvida com base
na interação entre o servidor e o banco de dados (Etapa 4). Tal
funcionalidade trata-se de um sistema de filtragem dinâmica que,
através de uma série de campos interativos na camada do front-
end (Etapa 3), permite aos usuários manipular e explorar os dados
previamente incluídos no banco. Isso possibilita análises específicas,
como a contagem de ocorrências de raios por mês, classificação por
tipo de raio, seleção de um intervalo inicial e final de dias no ano e
filtragem por quantidade de ocorrência.
Após a aplicação da filtragem, o sistema retorna a quantidade
de raios que atendem aos critérios especificados, e cada raio é re-
presentado no mapa por marcadores que facilitam a visualização
geográfica das ocorrências.
2.2
Camada Back-End
A camada back-end tem papel fundamental na ferramenta, pois ela é
constituída pelo banco de dados, um servidor, a API de mapas e pela
comunicação entre esses três componentes, além de conter as fun-
cionalidades desenvolvidas a nível de código, que são transmitidas
para a interface.
2.2.1
Modelo de Dados. O banco de dados da ferramenta proposta
armazena as informações vinda do campo de inserção na camada do
front-end. A entidade tem os atributos que constituem em informa-
ções usadas tanto nas filtragens dinâmicas, nos gráficos dinâmicos,
quanto nos marcadores no mapa da API. A partir do exposto, os
atributos do modelo de dados são descritos conforme:
• id (PK): É um atributo numérico que representa a chave
primária da tabela, cujo propósito é garantir que cada dado
inserido tenha um identificador único controlado automa-
ticamente pelo SGBD (Sistema Gerenciador de Banco de
Dados).
• time_utc: É o atributo que contem informações temporais
de ocorrência da descarga atmosférica, tais como dia, hora,
minutos e segundos.
• type: É um atributo categórico-nominal referente ao tipo do
descarga atmosférica.
• latitude: Este atributo é responsável por armazenada a lati-
tude geográfica, que é a distância angular medida em graus
entre qualquer ponto na superfície terrestre e a Linha do
Equador, sendo essencial para localizar e mapear pontos no
planeta.
• longitude: Este atributo é medido em graus, porém em rela-
ção ao meridiano de Greenwich. O sistema de coordenadas
formado por latitude e longitude é amplamente utilizado
para identificar a localização precisa de pontos na Terra.
• pear_current: Este atributo refere-se ao valor máximo da
corrente elétrica gerada durante um raio.
• ic_height: É um atributo relacionado a altura em que foi
detectada a ocorrência do raio.
LightningBR: Ferramenta Web para Visualização e Análise de Ocorrência de Raios no Brasil
WFA’2024, Juiz de Fora/MG, Brazil
• number_of_sensors: Este atributo está relacionado ao nú-
mero de sensores que captou a ocorrência do raio.
2.2.2
API de mapas. A API de mapas tem o objetivo de fornecer
além do próprio mapa geográfico, os marcadores de cada ocorrência
de raio. Além disso, com algumas bibliotecas importadas dessa
API, ela pode proporcionar o desenvolvimento de clusterização
dos marcadores no mapa. Dessa forma, ao filtrar uma quantidade
grande de pontos na tela, essa funcionalidade serve para agrupar os
marcadores próximos, melhorando o desempenho da ferramenta.
2.3
Front-end
O ambiente front-end refere-se à parte onde os usuários finais veem
e interagem diretamente com a aplicação. Em vista disso, criar um
cenário para que o usuário possa ter uma boa experiência com a
ferramenta torna-se algo de grande importância. Das ferramentas
usadas para alcançar um ambiente adequado, foi feito o uso de
HTML. Além de ser a linguagem de marcação mais amplamente
utilizada, o HTML é considerado o padrão da internet, o que signi-
fica que ele estabelece as diretrizes e as regras fundamentais para a
criação e a formatação de aplicações web.
A principal linguagem de programação usada neste projeto foi
o JavaScript. Isso ocorre porque, quando uma página web exibe
informações além de estáticas, como conteúdo que se renova pe-
riodicamente, sistemas de filtragem, gráficos e mapas dinâmicos,
a linguagem JavaScript está presente por trás de toda a lógica,
fornecendo esse suporte por meio de APIs, bibliotecas e pacotes.
O projeto inclui uma API em JavaScript chamada Leaflet. É uma
API de mapas de código aberto amplamente empregada na constru-
ção de mapas interativos para websites. Ela oferece uma estrutura
ágil e adaptável para integrar mapas interativos em aplicações da
web por meio das tecnologias HTML, CSS e JavaScript. Além disso,
essa API de mapa tem uma excelente interatividade, que vai desde
um evento de clique ou zoom até funcionalidades como a clusteriza-
ção dos marcadores no mapa, que por sua vez se caracteriza como
uma técnica usada para agrupar marcadores que estão próximos
uns dos outros. Isso é especialmente útil quando há muitos pontos
no mapa, evitando a poluição visual e tornando a visualização mais
clara e organizada.
2.4
Plataforma Web
A Figura 2 demonstra a interface gráfica para que o usuário possa
realizar filtragens de ocorrência de raios 3, 4 . Esta prototipagem
foi realizada através da ferramenta Figma5. Essa primeira página
caracteriza-se por um menu lateral com campos usados para realizar
análises por meio de filtragem, um campo para inserção de um
arquivo JSON e um botão localizado na parte inferior que leva à
página de gráficos. Ao lado, encontra-se o mapa geográfico. Quando
a funcionalidade dos filtros for acionada, os pontos retornados serão
exibidos como marcadores no mapa e a quantidade desses pontos
será mostrada na parte inferior do menu lateral. Na plataforma web,
há exemplos de arquivos JSON que o usuário pode fazer download
e upload.
3Disponível em: https://lightmab.onrender.com/mapa.html
4O funcionamento pode ser visto em: https://youtu.be/EmnUSG94CcA
5Disponível em: https://www.figma.com
Figura 2: Interface da primeira página
Outra funcionalidade presente na ferramenta é a possibilidade
de análise estatística por meio de gráficos dinâmicos. A Figura
3 demonstra esta funcionalidade, onde o usuário poderá analisar
melhor o quantitativo de cada tipo de raio. É possível selecionar o
mês desejado e o gráfico ao lado evidenciará as informações sobre as
ocorrências de raios nesse mês escolhido. Por fim, na parte inferior
do menu, há um botão que permite retornar para a página onde se
encontra o mapa geográfico.
Figura 3: Interface da segunda página
ANÁLISE E DISCUSSÃO DOS RESULTADOS
Nesta seção serão mostrados resultados obtidos com a ferramenta
web. Os resultados serão exibidos para uma cidade do Brasil. Es-
ses resultados mostram algumas das principais funcionalidades do
sistema desenvolvido.
3.1
Filtragem por Mês
A Figura 4 ilustra uma filtragem realizada para um mês específico,
sem especificação do tipo de raio. Neste caso, o sistema mostrou
na parte inferior do menu o quantitativo dos pontos retornados.
No mapa, por sua vez, foram exibidos os marcadores, ou seja, os
pontos relacionados ao local exato de cada ocorrência.
É importante ressaltar que a ferramenta possui a funcionalidade
de clusterização dos marcadores, que é uma forma para otimizar a
experiência do usuário e não deixar o mapa poluído com inúmeros
marcadores. Isto é, caso vários pontos estejam próximos, o mapa
apresenta um determinado perímetro com a borda em azul com
WFA’2024, Juiz de Fora/MG, Brazil
Elton R. Alves et al.
um círculo laranja mostrando o número total de marcadores que
contém neste perímetro, de maneira que, quanto mais o usuário se
aproximar do chão, mais grupos serão mostrados, até que o zoom
esteja tão ampliado que seja possível ver apenas um marcador na
tela. Outrora, quanto menos ampliado estiver o zoom, os números de
grupos irão diminuir, de forma que no mapa, permaneça apenas um
único grupo clusterizado, contendo todos os marcadores referente
a filtragem feita.
Figura 4: Filtragem raios por mês.
Além disso, no mapa, é possível perceber que há áreas mais
afetadas do que outras, a coloração mais avermelhada significa que
há muitos marcadores, ou seja, que se trata de uma região com
ocorrências de raios bastante elevada, analogamente as regiões com
a coloração azul quase transparentes informam um local afetado
com menos ocorrências.
3.2
Filtragem por Mês e por tipo de Raio
A Figura 5 exibe uma filtragem de todas as ocorrências de um deter-
minado tipo de raio (IN) para um mês específico. Nessa filtragem, o
usuário pode escolher qual o tipo de raio será exibido, ou seja, NS
ou IN.
Figura 5: Filtragem por Mês Limitado a um Tipo de Raio.
3.3
Gráficos Dinâmicos
No menu lateral, na página onde se encontra o mapa mostrado
na Figura 7, há um botão na parte inferior destacado em azul. Ao
clicar, o usuário será redirecionado para outra página do sistema,
onde encontrará um gráfico dinâmico empilhado correspondente às
ocorrências de raios de cada mês, representadas por cores diferentes,
como demonstrado na Figura 6.
Figura 6: Gráfico Dinâmico Empilhado.
Esta funcionalidade é importante, pois permite ao usuário reali-
zar análises estatísticas de ocorrência de raios para uma determi-
nada região. Outras funcionalidades que podem ser realizadas são:
filtragem por mês limitado a quantidade de raios e filtragem por mês
limitado em certo período de dias. Ressalta-se que a ferramenta de-
senvolvida não requer nenhum tipo de licença para uso acadêmico
e social. Caso o usuário, em sua localidade, não disponha de dados
da ENTLN, ele poderá entrar em contato para a disponibilização
no formato JSON para inseri-los na ferramenta.
CONSIDERAÇÕES FINAIS
Este trabalho apresentou o desenvolvimento e as funcionalidades
de um sistema web que permite o monitoramento de ocorrência
de raios IN e NS, a partir dos dados da ETLN. A construção da
ferramenta web foi dividida em duas partes, sendo a primeira o
back-end e a segunda o front-end.
A partir das principais funcionalidades apresentadas, a ferra-
menta web mostrou potencial para a realização de estudos sobre
regiões mais afetadas pela ocorrência de determinados tipos de raio.
Isso pode levar à mitigação dos danos causados pelos raios. Essa
análise é viabilizada pela funcionalidade de clusterização dos pontos
de raios gerados. Ressalta-se que as funcionalidades apresentadas
podem ser combinadas, tornando as filtragens mais criteriosas.
A ferramenta web ainda não consegue realizar um monitora-
mento em tempo real, pois não está ligada aos sensores da ETLN.
Outra limitação é que os dados JSON precisam ser disponilizados na
plataforma para serem baixados e importados na ferramenta, caso
o usuário não disponha de dados da ETLN em sua região. Assim,
como trabalhos futuros, destaca-se, vincular a ferramenta web a
um sensor que a alimente automaticamente quando captar um raio,
mostrando em tempo real as ocorrências de raios NS e IN para
qualquer localidade do Brasil. Além disso, pretende-se desenvolver
um algoritmo de predição de ocorrências de raios em tempo real
na plataforma.

--- FIM DO ARQUIVO: 30487-829-24935-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30488-829-24936-1-10-20241001.txt ---
OlivesApp: Aplicativo de Classificação do Índice de Maturação de
Azeitonas através do YOLOv8
Diogo da Silva Alvino*, Jhon Randler dos Santos Belarmino*, Warley M. Valente Junior*, Elton Rafael
Alves*, Aline Santana Oliveira Valente*, Cleiton Antônio Nunes†, Amanda Carolina Souza Andrada
Anconi†
diogo.alvino@unifesspa.edu.br,jhon.randler@unifesspa.edu.br,wmvj@unifesspa.edu.br,eltonalves@unifesspa.edu.br
asoliveirav@gmail.com,cleiton.nunes@ufla.br,quimica.amandasouza@gmail.com
*Universidade Federal do Sul e Sudeste do Pará - Marabá - Pará - Brasil
†Universidade Federal de Lavras - Lavras - Minas Gerais - Brasil
WFA’2024, Juiz de Fora/MG, Brazil
Alvino et al.
tecnológicos das camadas de front-end e back-end. Nas subseções
2.3 e 2.4 são apresentados os fluxos de interações e as principais fun-
cionalidades da ferramenta, respectivamente. Por fim, a conclusão
e trabalhos futuros são apresentados na Seção 3.
APLICATIVO DE CLASSIFICAÇÃO
2.1
Metas do Aplicativo
O OlivesApp tem como objetivo principal aumentar a eficiência do
processo de classificação do índice de maturação das azeitonas de
modo a permitir aos olivicultores tomarem decisões mais precisas
durante a colheita. Para atingir esse objetivo, o aplicativo oferece
funcionalidades como: (1) captura de imagens de azeitonas pela
câmera do smartphone e por meio do espaço de armazenamento lo-
cal; (2) cadastro de novos lotes de imagens de azeitonas; (3) definição
de padrões de cores das azeitonas; (4) classificação e ilustração do
índice de maturação do lote de azeitonas; e (5) armazenamento
local dos resultados coletados para posterior análise. Todas estas
funcionalidades são acessíveis via uma interface intuitiva destinada
a olivicultores e pesquisadores da área. Como benefício, o aplica-
tivo permite que o olivicultor faça uma determinação mais exata
do índice de maturação de azeitonas, já que atualmente ele é deter-
minado visualmente e consequentemente suscetível a erros. Outro
benefício é garantir decisões mais acertadas no processo de colheita
das azeitonas, pois o olivicultor precisa decidir se as azeitonas de
uma determinada área devem ser colhidas ou não, pois há um nível
de maturação considerado "ideal" para colher, o qual resultaria em
um azeite de melhor qualidade.
2.2
Arquitetura Proposta
De acordo com a Figura 1, o modelo arquitetural Cliente-Servidor foi
selecionado para o desenvolvimento do aplicativo, levando em con-
sideração sua eficiência e escalabilidade. Esta abordagem permite
que o aplicativo OlivesApp mantenha uma interface de usuário leve
e responsiva, enquanto o servidor (API REST) lida com o processa-
mento mais intensivo para tratamento da imagem e classificação das
azeitonas, além do armazenamento dos resultados da classificação.
API REST
Módulo de
tratamento de imagem
Módulo de classificação
de azeitonas
Olivicultor
Imagem de
azeitonas
Resultado da
classificação
Figure 1: Arquitetura da aplicação
O processo inicia com o olivicultor usando um aplicativo móvel
para capturar e enviar uma imagem das azeitonas. Essa imagem é
transmitida para uma API REST desenvolvida especificamente para
o processamento. A API utiliza o YOLOv8 para identificar azeitonas
na imagem, gerando uma máscara que identifica a localização da
azeitona na imagem. Após essa etapa, o módulo de tratamento de
imagem é executado, utilizando a máscara gerada pelo YOLOv8 para
remover o fundo da imagem original e deixando apenas as azeitonas
sobre um fundo branco. Com isso, a imagem das azeitonas com
fundo branco é analisada pelo módulo de classificação de azeitonas,
que irá categorizar as azeitonas e retornar para o aplicativo móvel
do olivicultor o resultado da classificação.
Os três principais módulos do aplicativo são descritos a seguir:
• YOLOv8: está sendo utilizado para detectar azeitonas na
imagem enviada pelo usuário. Quando o usuário envia uma
imagem, o YOLO irá realizar todo o processamento da im-
agem para que seja gerado uma máscara da azeitona na
imagem enviada. Ou seja, o YOLO irá entregar o resultado
da sua detecção com uma imagem de fundo preto, e apenas
a azeitona detectada em branco.
• Módulo de tratamento de imagem: é responsável por
ajustar a imagem enviada pelo usuário para a forma que o
módulo de classificação necessita para entregar um resultado
preciso. Com a máscara gerada pelo YOLO, esse módulo irá
fazer com que a máscara sobreponha a imagem original en-
viada pelo usuário. Assim, após essa sobreposição o módulo
recorta a azeitona da imagem original, gerando uma imagem
apenas da azeitona com o fundo branco. Assim, o módulo de
classificação poderá analisar cada pixel da azeitona sem o
risco de ter impacto por conta das cores de fundo da imagem.
• Módulo de classificação de azeitonas: realiza a análise da
imagem da azeitona com fundo branco para determinar a
classe de maturação da azeitona. Utilizando padrões de cores
predefinidos, ele calcula a porcentagem de pixels em cada
cor padrão e classifica a azeitona em uma das 8 classes de
maturação. O resultado final é uma classificação detalhada
que inclui a distribuição percentual das diferentes classes de
azeitonas na imagem analisada.
2.2.1
Tecnologias da camada front-end. O front-end da aplicação
foi todo desenvolvido em Flutter, um framework para criar inter-
faces de usuário (UI) atraentes e nativas para aplicativos móveis
(iOS e Android), bem como para aplicações web e desktop. O flutter
utiliza a linguagem de programação Dart e oferece uma rica coleção
de widgets personalizáveis para construir interfaces de usuário (UI)
atraentes e responsivas.
2.2.2
Tecnologias da camada back-end. Toda a codificação do back-
end da aplicação foi feita em Python devido sua ampla gama de
bibliotecas, oferecendo suporte para bibliotecas de aprendizagem de
máquina e visão computacional, e também para criação de APIs. A
criação de endpoints da API e manipulação de requisições HTTP foi
feita com Flask. Atualmente o Flask gerencia as rotas, recepciona
os dados JSON enviados pelo usuário (como padrões de cores e
imagens) e devolve respostas JSON após o processamento. Além
disso, também controla a execução do servidor web, tornando a API
acessível aos dispositivos móveis ou outros clientes.
O YOLOv8 é utilizado para a detecção de azeitonas em imagens.
O YOLO é um modelo de detecção de objetos em tempo real que
OlivesApp: Aplicativo de Classificação do Índice de Maturação de Azeitonas através do YOLOv8
WFA’2024, Juiz de Fora/MG, Brazil
é rápido e preciso, ideal para a tarefa de identificar azeitonas nas
imagens. No projeto ele está sendo utilizado para detectar azeitonas
nas imagens enviadas. Ele processa a imagem e gera binárias que
são utilizadas para isolar a azeitona do restante da imagem.
Ainda no back-end, o NumPy, uma biblioteca fundamental para
computação científica em Python, foi essencial para o desenvolvi-
mento deste projeto devido às suas capacidades de manipulação
de arrays e operações matemáticas eficientes. O NumPy permite
a conversão rápida de dados de imagem para arrays, facilitando
operações como cálculo de distâncias e manipulação de pixels. No
projeto do aplicativo, o NumPy é usado para transformar as ima-
gens em matrizes numéricas, realizando cálculos necessários para
o processamento de imagem e classificação de azeitonas.
2.3
Fluxograma da aplicação
A Figura 2 apresenta todo o fluxograma da proposta, deixando claro
os principais pontos de inserção de dados, validações, detecção de
objetos, resultados, etc. Cada etapa é de extrema importância para
o funcionamento correto do aplicativo, visando um fluxo simples e
bem validado.
Analisando a Figura 2, o fluxo de utilização da principal fun-
cionalidade do aplicativo está dividido da seguinte maneira:
• Evento Inicial: Nesta etapa o usuário acessa o menu principal
do aplicativo.
• Nova Amostragem: O fluxo de nova amostragem se trata da
principal funcionalidade da plataforma, onde o usuário pode
cadastrar um lote para verificar o índice de maturação do
mesmo.
• Preenchimento de dados: Nessa etapa o usuário deve preencher
os dados solicitados após clicar na opção de nova amostragem,
como nome do lote, quantidade de amostras, padrões de
cores, etc. Importante lembrar que esses dados estão sendo
validados, ou seja, se faltar alguma informação obrigatória o
usuário não conseguirá prosseguir com o cadastro do lote.
Caso contrário, o botão de iniciar será liberado para dar
continuidade com o cadastro.
• Seleção de imagens: O usuário deverá selecionar as imagens
das azeitonas que irão compor o lote cadastrado, onde o
aplicativo irá validar se a quantidade de imagens inseridas
é a mesma quantidade de amostras informadas no cadastro.
Desse modo, caso seja informado que o lote é formado por
15 amostras, o usuário deve realizar o upload de 15 imagens
de azeitonas.
• Reconhecimento da IA (YOLOv8): Nessa fase o aplicativo irá
trabalhar em volta das imagens inseridas, onde cada uma das
amostras passará pela detecção de objetos visando um "refi-
namento" da imagem para que o algoritmo de classificação
funcione corretamente.
• Classificar a maturação do lote: Com todas as imagens tratadas
pelo YOLOv8, o aplicativo irá passar cada uma das imagens
pelo algoritmo de classificação de índice de maturação e
armazenar os resultados de todo o lote em uma lista.
• Entrega dos resultados: Por fim, a lista contendo dados como
índice de maturação do lote e percentual de cada classificação
será apresentada ao usuário, onde o mesmo poderá visualizar
e analisar as informações do lote cadastrado.
Evento Inicial
Nova Amostragem
Libera o botão de
"iniciar"
Preenchimento de
dados
Falta dados?
Sim
Não autoriza
iniciar
Não
Seleção de imagens
Número de
imagens é igual ao de
amostras?
Retorna para
a seleção de
imagens
Reconhecimento da IA
(YOLOv8)
Classificar a maturação do
lote
Entrega dos
resultados
Não
Sim
Figure 2: Fluxograma da aplicação
2.4
Principais Funcionalidades
A Figura 3 ilustra as principais interfaces do aplicativo OlivesApp.
Para detalhes, é possível visualizar as interações por meio do seguinte
link: https://youtu.be/DpYgObAVmgo.
Na Figura 3a, as imagens das azeitonas são inseridas pelo olivicul-
tor, sendo que neste cenário foram selecionadas quatro imagens
sendo duas pertencentes a mesma classe e as duas restantes a classes
distintas. De acordo com a Figura 3b o olivicultor insere os dados
referente ao lote que deseja cadastrar, onde o campo "Nome do
lote", corresponde a uma identificação para a classificação do lote
de amostras; "Nº de amostras", refere-se a quantidade de amostras
que o lote possui (o número de imagens enviadas deve ser o mesmo);
"Padrão de Cor", refere-se ao padrão de classificação usado para
aquele lote (no aplicativo é possível definir a cor que representa
um verde escuro, por exemplo); por fim o campo "Informações"
destinado para anotações extras sobre o lote. Nas Figuras 3c e 3d
é gerado o resultado do lote cadastrado com todos os detalhes in-
cluindo o valor para o "Índice de Maturação" que neste exemplo
corresponde ao valor "3", pois foram identificadas três classes dis-
tintas de IM para a amostra das azeitonas. A seguir é ilustrado uma
tabela com todas as possíveis classes de IM, respectiva imagem
representativa, percentual de acordo com o número de amostras, e
a quantidade exata. Para este exemplo, o aplicativo revela que uma
azeitona pertence a classe 1 (cor totalmente verde), uma azeitona
pertence a classe 3 (cor < 1/2) e duas azeitonas pertencem a classe 4
(cor > 1/2), sendo que as duas ultimas classes são classificadas como
azeitonas mistas.
WFA’2024, Juiz de Fora/MG, Brazil
Alvino et al.
(a) Imagem de quatro amostras de
azeitonas.
(b) Configuração de um novo lote de
azeitonas.
(c) Resultado da classificação do IM
das azeitonas.
(d) Gráfico interativo com os resulta-
dos.
Figure 3: Funcionalidades do OlivesApp.
2.5
Do tipo de licença e perspectivas criadas
pelo Aplicativo
Atualmente o OlivesApp está em processo de registro de soft-
ware junto ao INPE (Instituto Nacional da Propriedade Industrial)
e encontra-se licenciado como software gratuito (sem acesso ao
código fonte) para fins de validação das interfaces e da experiên-
cia de uso junto aos olivicultores e pesquisadores interessados. A
perspectiva é que seja comercializado como um software freemium
com assinatura premium, ou seja, oferecemos uma versão básica
do aplicativo gratuitamente com funcionalidades essenciais como,
cadastro de até cinco lotes de imagens de azeitonas, configuração
do padrão de cor, capacidade de classificar até cinco azeitonas em
uma só imagem e armazenamento dos resultados diretamente no
smartphone. Para funcionalidades avançadas que corresponde a
assinatura premium, o aplicativo permite classificar quantidade
indeterminada de azeitonas em uma só imagem, fazer captura dire-
tamente da câmera do smartphone, opção de classificação off-line,
além de todas as funções previamente listadas para a versão básica
sem qualquer restrição.
CONCLUSÃO
O presente trabalho teve por objetivo apresentar o OlivesApp,
aplicativo para detecção e classificação do IM de azeitonas através
do YOLOv8 e algoritmos de tratamento de imagem e classificação
das azeitonas. Os resultados alcançados mostram que o aplica-
tivo é promissor como produto final para apoiar olivicultores e
pesquisadores, utilizando tecnologias avançadas de visão computa-
cional e aprendizado de máquina.
Das limitações do trabalho, além da quantidade limitada de
olivicultores, destacamos que por tratar-se de um protótipo, ainda
não foi possível disponibilizar para download. Como trabalhos
futuros, planejamos realizar avaliação de usabilidade junto aos po-
tenciais usuários e parcerias com as associações dos olivicultores
de todas as regiões do país para a implementação da proposta e
assim atingir o impacto pretendido com a solução.

--- FIM DO ARQUIVO: 30488-829-24936-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30489-829-24937-1-10-20241001.txt ---
PAR Digital: Uma Ferramenta Web em Prol da Educação Inclusiva
Daniele C. S. Diniz, Adriano C. M. Pereira
Departamento de Ciência de Computação | DCC
Universidade Federal de Minas Gerais | UFMG
Belo Horizonte, Brasil
danielecassia@ufmg.br,adrianoc@ufmg.br
Adriana Borges, Adriana Valladão, Maria Luisa
Nogueira
FAE | EEFFTO | FAFICH - UFMG
Belo Horizonte, Brasil
{adrianaapb,avaladao,marialuisamn}@ufmg.br
WFA’2024, Juiz de Fora/MG, Brazil
Daniele C. S. Diniz, Adriano C. M. Pereira and Adriana Borges, Adriana Valladão, Maria Luisa Nogueira
OBJETIVO
O objetivo do projeto PAR Digital é proporcionar uma ferramenta
acessível e intuitiva que facilite o preenchimento e a gestão do Plano
Educacional Individualizado (PEI) [1], essencial para o desenvolvi-
mento educacional dos alunos com deficiência. Desenvolvido a par-
tir de princípios do Desenho Universal de Aprendizagem (DUA) e de
uma parceria com o Atendimento Educacional Especializado (AEE),
o sistema visa integrar-se de maneira eficiente na rotina diária dos
educadores, promovendo um ensino inclusivo e personalizado.
ARQUITETURA
A estrutura do projeto é organizada de maneira a facilitar a manutenção
e a expansão do mesmo. A Figura 2 ilustra a arquitetura do sistema
PAR Digital, que foi concebido seguindo boas práticas de padrões
de desenvolvimento Web.
Figure 2: Arquitetura do sistema PAR Digital
As principais tecnologias utilizadas no projeto incluem React.js,
uma biblioteca JavaScript para construção de interfaces de usuário,
escolhida por sua eficiência na criação de componentes reutilizáveis
e desempenho otimizado. O TypeScript é usado para adicionar
tipagem estática ao código JavaScript, aumentando a robustez e
facilitando a manutenção. O Redux é empregado para o gerenci-
amento de estado da aplicação, ideal para aplicações de médio e
grande porte que necessitam de um controle mais sofisticado do es-
tado. A biblioteca Material-UI é utilizada para implementar o design
system do Google Material Design, enquanto o Axios serve como
cliente HTTP para realizar requisições a APIs. Ferramentas como
Jest e React Testing Library são utilizadas para testes automatizados,
garantindo a qualidade e a funcionalidade do código.
O fluxo de dados na aplicação é gerenciado pelo Redux, que
centraliza o estado da aplicação em um único store. As ações são
despachadas a partir dos componentes, que são tratadas pelos ‘re-
ducers‘ para atualizar o estado global. Essa abordagem facilita a
depuração e o desenvolvimento de novas funcionalidades, uma vez
que o estado da aplicação se torna previsível e controlado.
A estilização da aplicação é feita utilizando CSS-in-JS com a
biblioteca Material-UI, permitindo uma aplicação consistente do
design system através dos componentes. A utilização de temas
facilita a customização e a manutenção do estilo visual da aplicação.
Para a integração e entrega contínua (CI/CD), o projeto utiliza
o GitHub Actions, configurado para executar testes automatizados
e builds a cada commit, garantindo que a aplicação se mantenha
estável e pronta para deployment.
FUNCIONALIDADES
O projeto oferece diversas funcionalidades voltadas para o suporte
educacional de estudantes com necessidades especiais. A Figura 3
ilustra o painel de funcionalidades vinculadas ao aluno. Nesta seção,
são descritas as principais funcionalidades do sistema, cada uma
desempenhando um papel crucial na promoção de um ambiente
inclusivo e eficaz para o aprendizado do aluno.
Figure 3: Tela do painel do aluno do sistema PAR Digital
4.1
CRUD de Usuários
No sistema, os administradores são responsáveis por gerenciar as
escolas, acesso a relatórios e detalhes como professores e estratégias.
As escolas criam professores, responsáveis e desenvolvem o PEI,
além de visualizar estratégias e dados dos alunos e professores.
Os coordenadores acompanham o progresso dos alunos e es-
tratégias, visualizando o andamento dos alunos.
Os professores regulares são os usuários chave, criando e
aplicando estratégias, e adicionando relatórios de progresso.
Os professores AEE auxiliam os regulares, visualizando estraté-
gias do PEI e relatórios, e participando de fóruns.
Os responsáveis adicionam documentos necessários e acom-
panham o desenvolvimento dos alunos, visualizando seus dados e
relatórios parciais.
4.2
Criação do PEI
A funcionalidade de criação do Plano Educacional Individualizado
(PEI) é uma das mais importantes, permitindo que educadores e
coordenadores elaborem planos personalizados para atender às
necessidades específicas de cada aluno. A Figura 4 ilustra a tela de
criação do PEI.
4.3
Estratégias do PEI
Dentro do PEI, os educadores podem definir e documentar diversas
estratégias pedagógicas adaptadas às necessidades do aluno. Essas
estratégias são orientações práticas que guiam os professores na
implementação do plano e ajudam a monitorar o progresso do aluno.
A Figura 5 ilustra a tela de listagem das estratégias.
PAR Digital: Uma Ferramenta Web em Prol da Educação Inclusiva
WFA’2024, Juiz de Fora/MG, Brazil
Figure 4: Tela de criação do PEI do sistema PAR Digital
Figure 5: Tela de listagem de todas as estratégias de um PEI
do sistema PAR Digital
4.4
Relatório sobre Estratégias
A funcionalidade de relatórios permite que educadores e coorde-
nadores acompanhem o andamento das estratégias definidas no
PEI. Esses relatórios são fundamentais para avaliar o progresso do
aluno, identificar áreas que necessitam de ajustes e garantir que
as estratégias estão sendo eficazmente implementadas. A Figura 6
ilustra a tela que apresenta o relatório.
Figure 6: Tela que exibe relatório parcial baseado nos status
de finalização das estratégias por disciplina
4.5
Fórum de Acompanhamento do Aluno
O fórum de acompanhamento do aluno é uma plataforma colabo-
rativa onde professores, coordenadores e outros profissionais po-
dem discutir o progresso do aluno, compartilhar insights e propor
ajustes no PEI. Este fórum promove uma abordagem colaborativa
e integrada ao acompanhamento educacional. A Figura 7 ilustra a
tela do fórum.
Figure 7: Tela que exibe um fórum entre professores sobre o
aluno do sistema PAR Digital
4.6
Reutilização de Estratégias
O sistema possui um banco de estratégias pedagógicas que podem
ser reutilizadas. Educadores podem consultar esse banco para en-
contrar estratégias que já foram aplicadas com sucesso em situações
similares , facilitando a implementação de práticas comprovada-
mente eficazes. A Figura 8 ilustra a tela do banco de estratégia.
Figure 8: Tela com lista das estratégias do sistema PAR Digital
4.7
Compartilhamento de Recursos
Uma funcionalidade vital do PAR Digital é o compartilhamento
de recursos, especialmente Tecnologias Assistivas. Esse recurso
permite que professores e coordenadores acessem ferramentas e
materiais que podem ser utilizados para apoiar o aprendizado dos
alunos com necessidades especiais. A Figura 9 ilustra a tela de
compartilhamento de recursos externos.
4.8
Perfil do Aluno
O sistema mantém um perfil detalhado do aluno, que inclui in-
formações educacionais e fundamentais. Esse perfil permite que
educadores e coordenadores tenham uma visão abrangente das
necessidades, capacidades e progressos do aluno, facilitando a per-
sonalização do ensino e a monitorização do desenvolvimento edu-
cacional. A Figura 10 ilustra a o formulário para preenchimento do
Perfil Educacional do aluno.
WFA’2024, Juiz de Fora/MG, Brazil
Daniele C. S. Diniz, Adriano C. M. Pereira and Adriana Borges, Adriana Valladão, Maria Luisa Nogueira
Figure 9: Tela que disponibiliza materiais externos para aux-
iliar no desenvolvimento do ensino no sistema PAR Digital
Figure 10: Tela do Perfil Educacional Infantil do aluno
LICENÇA E PERSPECTIVAS DO PROJETO
5.1
Licença
O projeto PAR Digital foi concebido como um software acadêmico,
com objetivo de ser utilizado de forma tutorada por Secretarias de
Ensino. A propriedade intelectual da ferramenta pertence a uma
Instituição de Ensino Superior (IES) brasileira e foi validada em
escolas de ensino público.
5.2
Perspectiva Acadêmica
O PAR Digital representa uma inovação significativa no campo
da educação inclusiva, oferecendo uma ferramenta tecnológica
poderosa para a gestão eficiente de Plano Educacional Individual-
izado (PEI). Academicamente, essa ferramenta pode servir como
um catalisador para a pesquisa e o desenvolvimento contínuo de
estratégias pedagógicas adaptativas.
O acesso a dados detalhados sobre o progresso dos alunos, bem
como o compartilhamento de estratégias bem-sucedidas entre os
educadores, pode promover a produção de conhecimento acadêmico
relevante na área da educação inclusiva.
Além disso, o projeto pode facilitar estudos longitudinais sobre
o impacto de intervenções específicas no desenvolvimento educa-
cional de alunos com deficiência, fornecendo insights valiosos para
a comunidade acadêmica.
5.3
Perspectiva Social
Socialmente, o PAR Digital tem o potencial de promover a inclusão
e a igualdade de oportunidades na educação. Ao oferecer uma
plataforma acessível e intuitiva para a gestão de PEIs, o sistema
capacita os educadores a oferecer um suporte mais eficaz aos alunos
com deficiência, adaptando seus métodos de ensino às necessidades
individuais de cada estudante. Isso não apenas melhora a exper-
iência educacional desses alunos, mas também fortalece a coesão
social ao reconhecer e valorizar a diversidade no ambiente escolar.
Além disso, ao envolver os pais e responsáveis no processo edu-
cacional por meio do acompanhamento do progresso dos alunos,
o software promove uma parceria colaborativa entre escola e co-
munidade, fortalecendo os laços sociais e a confiança na instituição
educacional.
CONCLUSÃO E TRABALHOS FUTUROS
Este artigo apresentou a ferramenta PAR Digital, que é resultado
de muitos anos de pesquisa de pesquisadores e professores de difer-
entes áreas do conhecimento, em especial da educação, terapia
ocupacional, psicologia e ciência da computação.
Baseado no Planejamento de Ensino Individualizado (PEI), o soft-
ware se organiza a partir dos princípios do Desenho Universal de
Aprendizagem – Engajamento, Apresentação, Ação e Expressão. A
parceria com o responsável pelo Atendimento Educacional Espe-
cializado (AEE) é fundamental nesse processo.
O PAR é um instrumento que pretende facilitar o acompan-
hamento do aluno com deficiência no ambiente escolar. Sua utiliza-
ção deve estar presente no dia-a-dia da sala de aula. Esperamos que
com esta tecnologia pode contribuir de forma relevante com a edu-
cação de crianças e adolescentes com autismo e outras deficiências,
auxiliando os professores, pedagogos e profissionais de diferentes
áreas que atuam para contribuir para a formação e desenvolvimento
de importantes habilidades dessas crianças com deficiência.
Como trabalhos futuros, pretendemos adicionar novas funcional-
idades à ferramenta, tais como um novo instrumento de avaliação
de crianças do ensino fundamental para traçar o perfil e auxiliar
o trabalho dos profissionais e uma nova ferramenta de auxílio na
gestão de carreira e vocação profissional.
AGRADECIMENTOS
Gostaríamos de expressar nossa sincera gratidão pelo apoio pro-
porcionado pela FAPEMIG, pelo CNPq, pela CAPES e por todos
os colaboradores deste projeto. A colaboração e o apoio recebidos
foram fundamentais para o desenvolvimento deste projeto.

--- FIM DO ARQUIVO: 30489-829-24937-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30490-829-24938-1-10-20241001.txt ---
Proposta de um framework de apoio à etapa de avaliação de
soluções computacionais
Suzane Santos dos Santos
suzanesantos@usp.br
Universidade de São Paulo
São Carlos, São Paulo, Brasil
Marcus V. Santos Rodrigues
marcus.v.rodrigues@usp.br
Universidade de São Paulo
São Carlos, Brasil
Kamila Rios da Hora Rodrigues
kamila.rios@icmc.usp.br
Universidade de São Paulo
São Carlos, São Paulo, Brasil
WFA’2024, Juiz de Fora/MG, Brazil
Santos, Rodrigues e Rodrigues
Psicologia e Gerontologia, sendo eles: 1) Self-Assessment Manikin
(SAM) [2]; 2) System Usability Scale (SUS) [4]; 3) Escala de Humor
de Brunel (BRUMS) [3]; 4) Escala de Afetos Positivos e Negativos
(PANAS) [10]; 5) Escala de Afetos de Zanon (EA) [12]; 6) Lista de
Estados de Ânimo Presentes (LEAP) [5]; 7) Escala de Depressão
Geriátrica (GDS) [11].
ARQUITETURA
Nesta seção são descritos os aspectos técnicos sobre a arquitetura do
framework. É válido ressaltar que há dois tipos de perfis que podem
acessar o framework: usuário respondente e usuário especialista.
Para cada perfil há um conjunto de funcionalidades específicas e
uma interface distinta.
O framework contempla a implementação da lógica das funcio-
nalidades do servidor em uma arquitetura de microsserviços. Nesse
modelo, requisitos como autenticação e cada um dos instrumentos
de autorrelato são tratados como microsserviços independentes,
utilizáveis tanto pelo framework quanto por outras plataformas. Ini-
cialmente, considerou-se que uma arquitetura totalmente modular
ofereceria diversas vantagens. No entanto, optou-se por priorizar as
funcionalidades principais, resultando em uma arquitetura centrali-
zada que se assemelha a um monólito, com uma forte integração
entre suas partes, todas sob a mesma API (do inglês, Application
Programming Interface). Com o avanço das funcionalidades, uma
alteração arquitetural está sendo planejada.
Atualmente, o servidor está organizado da seguinte forma: o
código é altamente modularizado e dividido em componentes de
servidor e de cliente. Os componentes de cliente referem-se às par-
tes que demandam interação direta do usuário, como formulários e
elementos de interface. Os componentes de servidor são responsá-
veis pela interação com a base de dados, autenticação e proteção de
rotas, otimização do desempenho das funcionalidades e redução de
erros decorrentes de tratamento inadequado de dados.
Os componentes são divididos de acordo com suas funcionali-
dades. Primeiramente, existem os componentes de interface, que
englobam os elementos mínimos necessários para a interação do
usuário, como botões, campos de preenchimento e tabelas. Em se-
guida, há os formulários, os quais são subdivididos entre aqueles
que realizam funções práticas, como o cadastro de novos usuários,
e os instrumentos de autorrelato, responsáveis pela coleta de res-
postas. Além disso, há os componentes responsáveis por gerar os
gráficos apresentados na visualização dos resultados de uma avali-
ação, bem como os provedores encarregados de transmitir dados
globalmente.
Além dos componentes mencionados, existem módulos que não
compõem diretamente as páginas da interface. O módulo de au-
tenticação inclui um provedor de credenciais que, acessado pelo
formulário de login, coleta as credenciais e verifica sua autentici-
dade na base de dados. Se válidas, gera uma sessão de oito horas
com os dados essenciais da conta. Há também o módulo de acesso
à base de dados, que abriga todas as funções responsáveis por so-
licitar, receber e tratar os dados, incluindo os tokens que validam
esse acesso. Este módulo é acessado por todos os componentes que
necessitam desses dados.
Os componentes que definem as páginas do sistema são divididos
em diferentes categorias. Primeiramente, há aqueles responsáveis
por implementar os componentes de cadastro e login de usuários.
Em seguida, existem os componentes que permitem a visualização
dos dados pessoais do usuário. Além disso, há componentes especí-
ficos designados para usuários respondentes e especialistas, cada
um com funcionalidades distintas e direcionadas às necessidades
específicas desses grupos de usuários. Os componentes dos especi-
alistas consistem nas páginas que predominantemente expõem as
funcionalidades de criação de avaliações e de usuários respondentes.
Enquanto isso, os componentes dos respondentes disponibilizam a
capacidade de preencher os instrumentos das avaliações que lhes
foram designados. É importante destacar que um usuário comum
não pode acessar as páginas de um especialista, e vice-versa. Esse
bloqueio é efetuado automaticamente quando a conta é validada,
por meio de um middleware que verifica o tipo de conta e restringe
o acesso às páginas do tipo oposto.
A Figura 1 apresenta um diagrama C4 nível 2, detalhando a arqui-
tetura de software. Este diagrama foca nas unidades de implantação
que compõem o sistema, mostrando como os componentes se re-
lacionam e interagem dentro de cada container. No contexto do
framework, o diagrama ilustra a comunicação iniciada pelos dois ti-
pos de usuários, passando pelos diferentes componentes até chegar
à base de dados.
Figura 1: Diagrama C4 nível 2 ilustrando a arquitetura do
sistema.
Proposta de um framework de apoio à etapa de avaliação de soluções computacionais
WFA’2024, Juiz de Fora/MG, Brazil
3.1
Base de dados
O framework explora a utilização de um modelo de base de da-
dos não relacional, cuja principal característica é a capacidade de
armazenar e recuperar dados sem a necessidade de um esquema
definido previamente. Este tipo de banco de dados contrasta com os
sistemas relacionais tradicionais, que exigem um esquema rígido.
A escolha pelo modelo não relacional justifica-se pela flexibilidade
que oferece na gestão de grandes volumes de transações e dados
variáveis, como os gerados por sensores fisiológicos. Além disso,
a natureza dinâmica e imprevisível do sistema, que passa por um
processo contínuo de refatoração evolutiva, torna essa abordagem
ainda mais apropriada.
Os dados são organizados em coleções e documentos. As cole-
ções atuam como contêineres de documentos individuais, cada um
contendo pares de chave-valor que representam um único registro
de dados. Esta estrutura simplificada facilita tanto a organização
quanto a consulta eficiente dos dados.
A estrutura da base de dados em questão é composta por três
coleções: Users, Evaluations e Answers. Esta configuração é moldada
pela natureza não relacional da base, permitindo a inclusão de infor-
mações variadas por diferentes tipos de usuários, mesmo quando
integrados na mesma coleção.
A coleção intitulada Evaluations armazena os registros das ava-
liações realizadas pelos especialistas para uso dos usuários. Cada
avaliação é uma entidade que inclui instrumentos de autorrelato,
juntamente com informações suplementares, como descrição da
atividade e data designada para execução. Inicialmente, a concepção
previa apenas o armazenamento dos instrumentos, com as respos-
tas associadas guardadas nos registros dos usuários. Todavia, essa
abordagem mostrou-se ineficaz devido à dificuldade em diferenciar
diferentes registros do mesmo instrumento. Assim, considerou-se
a implementação de uma camada adicional para os instrumentos,
visando otimizar a estrutura.
Por outro lado, a coleção Answers possui particularidades, visto
que não está localizada na raiz da base de dados, mas sim como uma
subcoleção presente nos registros de cada avaliação respondida.
Essa disposição resulta na existência de múltiplas coleções deste
tipo, facilitando a diferenciação das respostas de um mesmo usuário
em diferentes avaliações.
INTERFACE
A versão atual do framework disponibiliza interfaces para sete ins-
trumentos de autorrelato. A interface dos especialistas é dividida
em cinco abas: página inicial, usuários, avaliações, serviços e resul-
tados. Enquanto a interface do respondente contém apenas uma
página inicial e uma página de avaliações.
A página inicial do especialista foi idealizada para concentrar
informações gerais sobre o sistema, além de vídeos e tutoriais mos-
trando como o mesmo pode ser utilizado. A página de usuários,
foi idealizada para que o especialista possa ver todos os usuários
cadastrados por ele e para cadastrar um novo usuário.
Para cadastrar um usuário, o especialista primeiro deve selecio-
nar quem está respondendo ao cadastro, se é o próprio usuário ou
um responsável legal. Caso seja um responsável, o próprio usuário
pode, posteriormente ao cadastro, alterar as informações preenchi-
das. A página de avaliações, conforme Figura 2, foi idealizada para
que o especialista consiga criar uma sessão de avaliação, em que
ele seleciona os usuários e os instrumentos que serão utilizados na
avaliação. Além disso, ao criar uma nova avaliação, o especialista
pode identificá-la com um título, e definir uma data de aplicação,
conforme a Figura 3.
Figura 2: Página de consulta de avaliações criadas.
Figura 3: Página de criação de avaliação.
A página de serviços foi projetada para ser um “catálogo” do sis-
tema, em que todos os serviços disponíveis estejam concentrados.
Além disso, o especialista tem a funcionalidade de criar um instru-
mento customizado com base em templates pré-definidos a partir de
uma interface de arrasta e solta. Nesta interface, o especialista pode
definir aspectos como título, subtítulo, separador, espaçamento e
questões que variam entre Escala de Diferencial Semântico [8] e
Escala Likert [6], com cinco, sete ou nove opções.
WFA’2024, Juiz de Fora/MG, Brazil
Santos, Rodrigues e Rodrigues
Para consultar os resultados dos instrumentos respondidos, o
especialista deve acessar a aba de resultados da avaliação feita.
Cada instrumento tem uma página de resultados diferente, com
características e visualizações específicas para os dados coletados.
Por exemplo, há a página de visualização do instrumento SAM,
conforme a Figura 4.
Figura 4: Página de resultado do instrumento SAM.
Outras páginas do framework, podem ser vistas no vídeo de
demonstração 1, disponibilizado em: https://shorturl.at/nSNu5.
CONSIDERAÇÕES FINAIS
Este estudo apresentou as etapas iniciais do desenvolvimento de
um framework de apoio à etapa de avaliação de soluções computa-
cionais. As tecnologias utilizadas até o momento foram Node.Js 2,
React 3, Next.js 4, Tailwind CSS 5 e a plataforma Firebase 6.
1Os nomes que aparecem no vídeo não são de usuários reais.
2https://nodejs.org/
3https://react.dev/
4https://nextjs.org/
5https://tailwindcss.com/
6https://firebase.google.com/
Reforça-se que este é um trabalho em andamento e que mediante
a sua conclusão, o resultado esperado é a criação e oferta de um fra-
mework que reúna diversos artefatos de avaliação, permitindo que
diferentes profissionais compreendam o impacto de suas soluções
em diferentes áreas. Com a disponibilização de instrumentos de
avaliação para diversos requisitos, espera-se que esses profissionais
possam medir efetivamente os efeitos de suas aplicações. Além
disso, o uso de sensores, microsserviços e templates deve ampliar o
público-alvo do sistema, beneficiando diferentes profissionais.
Dessa forma, considera-se que as perspectivas criadas pelo fra-
mework são especialmente interessantes para o âmbito acadêmico,
mas também possuem um apelo social, tendo em vista que o uso do
framework para realizar avaliações tem o potencial para promover
soluções interativas mais eficientes e acessíveis para os usuários.
Os trabalhos futuros incluem a implementação dos demais ar-
tefatos e a adição de outros instrumentos de autorrelato. Quando
uma versão “final” for desenvolvida será possível a realização de
testes com usuários (mediante a aprovação de um comitê de ética
em pesquisa) e a disponibilização do framework para o público alvo.
Por fim, ressalta-se que s questões referentes ao licenciamento do
software ainda não foram definidas.
AGRADECIMENTOS
O presente trabalho foi realizado com apoio da Coordenação de
Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) –
Código de Financiamento 001.

--- FIM DO ARQUIVO: 30490-829-24938-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30492-829-24940-1-10-20241001.txt ---
Desenvolvimento de Editores Colaborativos em Tempo Real:
Revisão Rápida
Laurentino Augusto Dantas
laurentino.dantas@ifms.edu.br
Instituto Federal de Mato Grosso do
Sul
Naviraí, MS
Joab Cavalcante da Silva
joab@uems.br
Universidade Estadual de Mato
Grosso do Sul
Naviraí, MS
Maria da Graça C. Pimentel
mgp@icmc.usp.br
Universidade de São Paulo
São Carlos, SP
2022. The results revealed various approaches and techniques
employed in the implementation of RTCEs, offering a comprehen-
sive view of research in this area. This, in turn, allowed for the
identification of challenges that future research should address.
WRSL+’2024, Juiz de Fora/MG, Brasil
Dantas et al.
abordagens e desafios na colaboração em tempo real de modo geral,
e na construção de sistemas apoio à colaboração de modo particular.
Diante da relevância e da atualidade desse tema, o presente tra-
balho se propõe a realizar uma revisão sistemática da literatura,
com o objetivo de elucidar aspectos cruciais no desenvolvimento de
editores colaborativos em tempo real (RCE). Esse esforço está ali-
nhado a vários tópicos de interesse do WebMedia, inclusive “autoria
e anotação” e “interação multiusuário.”
Assim, apresentamos uma Revisão Rápida (RR) para identificar,
de maneira objetiva, sistemática e reprodutível, os trabalhos relevan-
tes sobre o tema. As Revisões Rápidas utilizam métodos abreviados
em comparação com as revisões sistemáticas padrão para acelerar
o processo de tomada de decisão [57], mas mantêm um caráter
sistemático [50]. Seguindo as melhores práticas para RR [16], ini-
cialmente foi definido um protocolo PRISMA2 para especificar os
passos e os resultados esperados da pesquisa.
Um dos motivos de classificarmos nosso trabalho como uma
Revisão Rápida é o fato de utilizarmos a indexação realizada pela
ACM Digital Library para acessar os registros de trabalhos da área
de computação disponibilizados em outras bases de dados. O outro
motivo é que aplicamos a string de busca para a seleção dos artigos
nos títulos e nos resumos dos registros, e não no texto do artigo
como um todo.
O restante deste texto está organizado como segue. A Seção 2
apresenta uma compilação de conceitos importantes para o desen-
volvimento de editores colaborativos em tempo real. A Seção 3
detalha o protocolo adotado na revisão bem como o processo de
seleção dos estudos. A Seção 4 apresenta os resultados obtidos
relativamente às questões de pesquisa e um resumo dos estudos
analisados relativamente às questões. A Seção 5 identifica riscos à
validade dos resultados da revisão. A Seção 6 discute desafios de
pesquisa associados aos resultados obtidos. A Seção 7 sumariza o
trabalho reportado neste estudo.
EDITORES COLABORATIVOS EM TEMPO
REAL (RCE)
Editores colaborativos em tempo real, ou Real-Time Collaborative
Editors (RCEs), são ferramentas de software que permitem a múlti-
plos usuários trabalharem simultaneamente em um mesmo docu-
mento, independentemente de suas localizações geográficas. Esses
editores oferecem uma interface onde um grupo de usuários pode
visualizar e editar o mesmo documento em tempo real, com todas
as alterações sendo propagadas e exibidas instantaneamente para
todos os participantes [5].
O processo de edição colaborativa é complexo e envolve múlti-
plas considerações [22, 36]. A principal delas é a manutenção da
sincronização entre diversas cópias do documento editadas simulta-
neamente por usuários distintos. É essencial que os sistemas de RCE
integrem camadas de controle de acesso para garantir responsivi-
dade e consistência, sem introduzir cargas excessivas. Além disso,
deve-se assegurar que todos os usuários vejam a versão atualizada
do documento compartilhado [7].
Os RCEs são amplamente utilizados em ambientes como desen-
volvimento de software, educação e colaboração em documentos.
Eles suportam a edição de uma variedade de tipos de conteúdo,
2https://www.prisma-statement.org/prisma-2020-statement
incluindo texto, imagens [5], objetos JSON [27], documentos PDF
[28] e objetos 3D [49], entre outros.
Para funcionar eficazmente, um RCE deve atender a vários re-
quisitos comuns, de acordo com Cherif [7]:
• Alta responsividade local: O sistema deve ter uma perfor-
mance comparável àquelas de editores de usuário único [14,
61, 63];
• Alta concorrência: O sistema deve permitir que os usuá-
rios modifiquem simultaneamente qualquer parte do docu-
mento [14, 61];
• Consistência: Todos os usuários devem eventualmente visua-
lizar uma versão convergente do documento [14, 61];
• Coordenação descentralizada: Atualizações concorrentes de-
vem ser sincronizadas de forma descentralizada para evitar
pontos únicos de falha [14, 61];
• Escalabilidade do número de usuários: O sistema deve suportar
um número dinâmico de usuários, permitindo que se juntem
ou saiam do grupo a qualquer momento [23].
Os algoritmos de RCE que suportam esses requisitos são geral-
mente classificados em dois grupos [2]:
• Algoritmos centralizados: Esses algoritmos exigem um ser-
vidor central para coordenar as atualizações do documento,
gerenciando a concorrência e a ordem das operações. Exem-
plos incluem SOCT4 [69], GOT [61], e Jupiter [40].
• Algoritmos descentralizados: Esses algoritmos permitem que
as atualizações sejam executadas em qualquer ordem, sem
a necessidade de um servidor central. Exemplos incluem
adOPTed [48] e SOCT2 [59].
Além de permitir a edição simultânea e visualização em tempo
real das alterações, muitos RCEs oferecem recursos adicionais, e.g.:
• Histórico de alterações: Registra a sequência de operações e
os usuários envolvidos [17];
• Comunicação em tempo real: Inclui recursos como chat para
facilitar a comunicação durante a edição [17];
• Desfazer e Refazer (undo/redo): Permite reverter ações reali-
zadas [20];
• Comentários: Possibilita a inserção de notas em partes espe-
cíficas do documento [28].
Atualmente, existem diversos RCEs disponíveis na web, incluindo:
• Google Docs: Plataforma para edição colaborativa de docu-
mentos, planilhas e apresentações com recursos de histórico
de versões e comentários;
• Microsoft Office Online: Versões online do Word, Excel e
PowerPoint com funcionalidades colaborativas;
• Notion: Ferramenta que combina notas, tarefas, wikis e bases
de dados com colaboração em tempo real;
• Visual Studio Code com Live Share: Permite edição colabora-
tiva de código com colaboração simultânea em projetos;
• Overleaf : Plataforma para edição colaborativa de documen-
tos LaTeX, amplamente utilizada no meio acadêmico.
Conforme Gadea [19], o design e a implementação de algoritmos
de controle de consistência otimista apresentam desafios significa-
tivos. Ao longo de mais de três décadas de pesquisa em coedição,
Desenvolvimento de Editores Colaborativos em Tempo Real: Revisão Rápida
WRSL+’2024, Juiz de Fora/MG, Brasil
surgiram duas principais abordagens para o controle de consistên-
cia otimista: Transformações Operacionais (OT) e Tipos de Dados
Replicados Comutativos (CmRDTs).
Transformações Operacionais (OT) utilizam transformações que
preservam a intenção das operações em ambientes colaborativos,
mesmo diante de concorrência. O primeiro algoritmo de OT foi
introduzido por Ellis and Gibbs [14] e era baseado em uma aborda-
gem ponto-a-ponto. Por outro lado, Oster et al. [43] propuseram os
CmRDTs, que utilizam estruturas de dados projetadas para garantir
a comutatividade das operações, eliminando a necessidade de trans-
formações [53]. Os CmRDTs baseiam-se em operações posicionais
e são projetados para garantir a consistência sem conflitos [52].
Sun et al. [62, 64] analisaram e compararam as duas estratégias, e
detalham as vantagens de OT em relação CRDT.
REVISÃO RÁPIDA
3.1
Protocolo da revisão
Foi definido como protocolo de trabalho para esta pesquisa, o mé-
todo denominado Revisão Rápida (RR) [16, 50] utilizando a base de
dados indexada pela ACM e dividindo a seleção e a análise entre
dois avaliadores apoiados por uma pessoa árbitra de conflitos. Para
análise dos estudos, foram selecionados trabalhos que descreves-
sem o desenvolvimento de sistemas com suporte a RTCE, incluindo
trabalhos que descrevessem a implementação de um RCE. Também
foram analisados trabalhos que, mesmo que não apresentassem a
implementação de um RCE, descrevessem algoritmos ou técnicas
importantes para o seu desenvolvimento.
3.1.1
Fatores PICO.
Conforme descrito por Moher et al. [37], no contexto de uma revisão
sistemática, critérios de elegibilidade são os critérios pré-definidos
que formam a base para a definição de uma boa questão de pes-
quisa. Esses critérios incluem fatores como o tipo de população
alvo estudada (P), a intervenção realizada ou tratamento aplicado
(I), a comparação com outras intervenções (C), e os desfechos de
interesse ou resultados esperados (O). De acordo com Shin et al.
[55], o termo intervenção, no caso de revisão sistemática na área
de computação, corresponde à inovação investigada pela pesquisa
relatada no artigo analisado.
P (População/Problema): Tipos de objetos editáveis, arquiteturas
de sistemas RTCE, tipos de diagramas usados, modelos e roteiros
para desenvolvimento, algoritmos de sincronização OT e CRDT,
tipos de testes realizados, técnicas de undo/redo, e utilização de
comentários, chat ou histórico.
I (Intervenção): Sistemas RTCE, implementação de sistemas RTCE,
representação de editores colaborativos, desenvolvimento de siste-
mas RTCE, algoritmos de OT e CRDT, e técnicas de undo/redo.
C (Comparação): Comparação entre diferentes tipos de objetos,
arquiteturas, modelos ou roteiros, algoritmos de OT e CRDT, tipos
de testes, e técnicas de undo/redo.
O (Resultado): Tipos de objetos editáveis, arquitetura mais utili-
zada e modelos para desenvolvimento, principais tipos de diagramas
utilizados, frequência de algoritmos de OT e CRDT, tipos e número
de testes realizados, discussões sobre técnicas de undo/redo, e uso
de ferramentas de apoio ao trabalho colaborativo.
3.1.2
Objetivos e Questões de Pesquisa.
Utilizamos os fatores PICO para definir as questões de pesquisa.
QP1. Para edição de quais tipos de objetos a literatura apresenta
sistemas RTCE?
QP2. Qual a arquitetura mais utilizada pelos pesquisadores para
implementar sistemas RTCE , como eles são representados
e quais trabalhos apresentam modelos ou roteiros para o
desenvolvimento de RCEs?
QP3. Entre os algoritmos OT e CRDT, qual o que aparece com
mais frequência nos trabalhos selecionados?
QP4. Quais os principais tipos de testes realizados para avaliar os
editores colaborativos, quais os artigos realizaram testes com
usuários e qual o número médio de usuários que realizaram
os testes?
QP5. Quais trabalhos discutem técnicas de undo/redo?
QP6. Quais trabalhos discutem a utilização dos comentários, chat
ou histórico como ferramentas de apoio ao trabalho colabo-
rativo?
A seguir, detalhamos os aspectos específicos que buscamos investi-
gar em nossa revisão.
QP1. Para edição de quais tipos de objetos a literatura apresenta
sistemas RTCE? Buscamos identificar os tipos de objetos que são
alvo de edições em tempo real nos sistemas colaborativos, como
texto, código-fonte, gráficos, multimídia, entre outros.
QP2. Qual a arquitetura mais utilizada pelos pesquisadores para
implementar sistemas RTCE, como eles são representados e quais tra-
balhos apresentam modelos ou roteiros para o desenvolvimento de
RCEs? Analisamos quais arquiteturas são preferidas para a imple-
mentação de sistemas RTCE, como cliente-servidor, peer-to-peer,
entre outras. Investigamos também como essas arquiteturas são re-
presentadas nos artigos e quais deles fornecem modelos ou roteiros
detalhados para o desenvolvimento de editores colaborativos.
QP3. Entre os algoritmos OT e CRDT, qual o que aparece com mais
frequência nos trabalhos selecionados? Verificamos a frequência de
uso dos algoritmos de Transformação Operacional (OT) e Tipos de
Dados Replicados sem Conflitos (CRDT) nos artigos, identificando
qual deles é mais recorrente e em quais contextos são aplicados.
QP4. Quais os principais tipos de testes realizados para avaliar os
editores colaborativos, quais os artigos realizaram testes com usuários
e qual o número médio de usuários que realizaram os testes? Exa-
minamos os tipos de testes empregados para avaliar a eficácia dos
editores colaborativos, distinguindo entre testes de desempenho,
usabilidade, robustez, entre outros. Procuramos identificar artigos
que realizaram testes com usuários e a média de participantes en-
volvidos nesses estudos.
QP5. Quais trabalhos discutem técnicas de undo/redo? Investiga-
mos quais artigos abordam a implementação de técnicas de desfa-
zer/refazer (undo/redo) nos editores colaborativos, destacando as
abordagens utilizadas e os desafios enfrentados.
QP6. Quais trabalhos discutem a utilização dos comentários, chat
ou histórico como ferramentas de apoio ao trabalho colaborativo?
Analisamos artigos que discutem a integração de funcionalidades
WRSL+’2024, Juiz de Fora/MG, Brasil
Dantas et al.
como comentários, chat e histórico nos editores colaborativos, avali-
ando como essas ferramentas contribuem para a eficácia do trabalho
colaborativo.
Essas questões de pesquisa nos orientaram a focar nos aspec-
tos mais relevantes e recorrentes do desenvolvimento de editores
colaborativos em tempo real, garantindo uma análise abrangente
e detalhada das práticas, algoritmos e inovações documentadas
na literatura. Através dessas perguntas, buscamos compreender
as metodologias e tecnologias envolvidas no desenvolvimento de
sistemas RTCE.
Além disso, este estudo também se interessa em entender como
se dá o processo de desfazer e refazer em sistemas colaborativos
em tempo real, assim como em examinar como a literatura aborda
os recursos de histórico, comentários e chat nesses sistemas, que
muitas vezes são utilizados como suporte ao trabalho colaborativo.
3.1.3
Critérios de elegibilidade.
Apresentamos os critérios de inclusão dos estudos com base nos
fatores PICO utilizados na definição da questão de pesquisa.
Population (P): Devem ser incluídos artigos que reportam mode-
los, algoritmos, técnicas ou formas de desenvolver editores cola-
borativos em tempo real, bem como aqueles que discutem formas
originais ou customizadas para implementar recursos como histó-
rico, desfazer/refazer, chat, comentários, etc.
Intervention (I): Devem ser incluídos artigos que apresentam
novos editores colaborativos em tempo real, novas formas de im-
plementação desses editores, ou novos recursos que possam ser
aplicados aos mesmos, além de formas inovadoras de implementar
esses recursos.
Comparison (C): Devem ser incluídos artigos que explicitam o
desenvolvimento de editores colaborativos em tempo real, algo-
ritmos e técnicas utilizados, implementação de novos recursos e
novas formas de implementá-los. Preferencialmente, artigos que
apresentam as avaliações realizadas e os dispositivos empregados.
Outcome (O): Devem ser incluídos artigos que explicitam técnicas,
algoritmos, arquiteturas ou modelos empregados no desenvolvi-
mento de editores colaborativos em tempo real, dando preferência
àqueles que apresentam testes e resultados obtidos.
3.1.4
String de busca.
Considerando que as palavras necessárias ao estudo são comuns
e utilizadas com frequência no texto acadêmico, restringimos a
consulta ao resumo e ao título dos trabalhos. Portanto, a string de
busca foi criada a identificar, no 2024. Como re-
sultado, foram obtidos 370 registros, dos quais 5 (cinco) foram identificados
como duplicados e removidos.4
Uma verificação dos 365 registros obtidos depois da eliminação das
duplicatas permitiu identificar as editoras responsáveis pela publicação dos
trabalhos: ACM (162), IEEE (64), Springer-Verlag (41), Elsevier (12), Kluwer
(7), Addison-Wesley (5), e os 74 trabalhos restantes foram publicados por
outras editoras como Microsoft Press e Australian Computer Society.
A Figura 2 mostra a distribuição do ano de publicação dos 365 artigos.
Nota-se que o período de maior ocorrência foi entre 2010 e 2017 e que, nos
últimos cinco anos, o número de registros está entre quatro e dez por ano.
Utilizamos a ferramenta Rayyan5 para apoiar a triagem de artigos e a
extração de dados.
3.2.2
Seleção. Considerando os critérios de elegibilidade enumerados na
Seção 3.1.3, a triagem correspondente à leitura dos títulos e dos resumos
foi realizada paralelamente por dois pesquisadores, que avaliaram todos os
registros.
A análise foi conduzida em modo cego, de forma que cada pesquisador
realizou sua análise sem tomar conhecimento da decisão do outro. Ao final
dessa etapa, o modo cego foi desativado. As divergências entre as classifica-
ções dos revisores foram arbitradas por uma terceira pessoa revisora. Do
total de 365 trabalhos, foram selecionados 65 artigos completos.
Embora todos os tipos de materiais tenham sido analisados, a fase de
análise resultou na seleção exclusivamente de artigos completos.
3.2.3
Elegibilidade. Na etapa de elegibilidade, dois pesquisadores fizeram a
leitura completa de cada artigo e os classificaram de acordo com os critérios
de elegibilidade. A seguir, as divergências foram sanadas pela terceira pessoa
revisora. Além disso, nesta etapa foi possível identificar a língua utilizada
nos artigos, e foram considerados artigos em inglês ou português.
3.2.4
Inclusão. Ao final foram identificados 23 artigos que atendiam aos
critérios de elegibilidade. A Figura 3 apresenta a distribuição dos estudos
ao longo do período entre 2002 e 2021, sendo que 2012 apresenta o maior
número de publicações (4).
Uma síntese da análise desses 23 artigos, relativamente às questões de
pesquisa, é apresentada na Seção 4.1. O resumo de cada um dos artigos é
apresentado na Seção 4.2.
3https://libraries.acm.org/digital-library/acm-guide-to-computing-literature
4Observamos que a consulta realizada na data de conclusão deste manuscrito
(27/08/2024), utilizando a URL informada, retorna 381 registros.
5https://rayyan.ai/
Desenvolvimento de Editores Colaborativos em Tempo Real: Revisão Rápida
WRSL+’2024, Juiz de Fora/MG, Brasil
Figura 1: Síntese da estratégia de identificação dos estudos.
Figura 2: Distribuição dos 365 registros retornados por ano.
RESULTADOS
4.1
Síntese das respostas às questões da RR
A etapa de análise de cada artigo permitiu extrair as respostas às questões
de pesquisa, sumarizadas nesta seção.
A Tabela 1 sumariza o tipo de objeto editado, de arquitetura utilizada, de
algoritmo de sincronização adotado, bem como se o editor foi desenvolvido
ou não como uma aplicação Web. A Figura 4 apresenta graficamente os
resultados descritos na Tabela 1.
A Tabela 2 indica quais trabalhos contemplaram testes de modo geral, e
os que realizaram testes com usuários. A Figura 5 apresenta graficamente
os trabalhos que executaram testes.
A Tabela 3 especifica quais trabalhos oferecem quais recursos de edição.
A Figura 6 apresenta os totais referentes aos trabalhos que exploraram os
recursos de undo/redo, chat, histórico e comentários.
Figura 3: Distribuição dos 23 artigos selecionados por ano.
Para edição de quais tipos de objetos a literatura apresenta sistemas
RTCE?
A análise dos estudos identificou editores para os seguintes tipos de objeto:
texto [3, 31, 32, 39, 54, 71, 73], imagens [5] [20] [49], páginas web [24, 44, 45],
diagramas UML [12, 66], modelos de recursos [30], modelo de software [41],
objetos JSON [27], dados geográficos [17], documentos PDF [28], e docu-
mentos XML [72].
Qual a arquitetura mais utilizada pelos pesquisadores para imple-
mentar sistemas RTCE , como eles são representados e quais tra-
balhos apresentam modelos ou roteiros para o desenvolvimento de
RCEs?
A arquitetura predominante nos trabalhos analisados foi a cliente-servidor,
presente em 18 dos 23 estudos, o que representa 75% do total.
WRSL+’2024, Juiz de Fora/MG, Brasil
Dantas et al.
Tabela 1: Objeto editado, arquitetura empregada, algoritmos de sincronização (se informados) e plataforma web
Referência
Tipo de objeto editado
Arquitetura
OT / CRDT
Web
Bath et al. [5]
Raster e Vetor de imagem
Cliente-Servidor
OT
Sim
Kuiter et al. [30]
Modelos de Recursos (gráfico)
Cliente-Servidor
OT
Sim
Nicolaescu et al. [41]
Modelo de software
Centralizada e ponto a ponto
OT
Sim
Alsulami and Cherif [2]
Não especificado
Redes Oportunísticas
OT
Não
Gao et al. [20]
Imagens Bitmap
Ponto a Ponto
-
Não
Jungnickel and Herb [27]
Objetos JSON
Cliente-Servidor
OT
Sim
Nédelec et al. [39]
Documentos de texto
Descentralizado
CRDT
Não
Fechner et al. [17]
Dados geográficos
Cliente-Servidor
-
Sim
Salvati et al. [49]
Imagens Poligonais
Ponto a Ponto
-
Sim
Katayama et al. [28]
Documentos PDF
Cliente-Servidor
-
Sim
Inoue et al. [24]
Páginas WEB
Cliente-Servidor
-
Sim
Ozono et al. [45]
Páginas WEB
Cliente-Servidor
-
Sim
Ozono et al. [44]
Páginas WEB
Cliente-Servidor
-
Sim
Lautamäki et al. [31]
Código fonte Java (texto)
Cliente-Servidor
-
Sim
Thum et al. [66]
Diagramas UML
Cliente-Servidor
-
Sim
Wei et al. [71]
Textos que representam moléculas
Cliente-Servidor
-
Não
Bani-Salameh et al. [3]
Código fonte de software (texto)
Cliente-Servidor
-
Não
De Lucia et al. [12]
Diagramas UML
Cliente-Servidor
-
Não
Lin et al. [33]
Documentos Visio Microsoft
Cliente-Servidor
OT
Não
Leone et al. [32]
Documentos de texto
Cliente-Servidor
-
Não
Wong [72]
Documentos XML
Cliente-Servidor
-
Não
Xia et al. [73]
Documentos de texto
Cliente-Servidor
OT
Não
Shen and Sun [54]
Texto
Cliente-Servidor
-
Não
Figura 4: Arquitetura adotada, algoritmo de sincronização, e plataforma Web
A prevalência da arquitetura cliente-servidor pode ser explicada pelo
fato de ser o modelo mais adotado na internet atualmente. Essa populari-
dade é reforçada pela estatística de que, em 2023, 86% dos desenvolvedores
utilizaram APIs REST [47] que segue a arquitetura cliente-servidor [18].
Entre os algoritmos OT e CRDT, qual o que aparece com mais frequên-
cia nos trabalhos selecionados?
Os algoritmos de Transformação de Operações (OT) foram mais frequen-
temente utilizados do que os algoritmos de Estado Replicado (CRDT): 7
(sete) estudos utilizaram OT enquanto 2 (dois) utilizaram CRDT. Os demais
estudos não especificaram o algoritmo de resolução de conflitos adotado. A
preferência por OT pode ser atribuída ao fato de ser o método mais tradici-
onal e melhor adaptado ao modelo cliente-servidor [19].
Desenvolvimento de Editores Colaborativos em Tempo Real: Revisão Rápida
WRSL+’2024, Juiz de Fora/MG, Brasil
Tabela 2: Avaliação com ou sem envolvimento de usuários
Referência
Testes
Testes com usuários
Quantidade de usuários
Bath et al. [5]
Sim
Sim
27 + 16
Kuiter et al. [30]
Sim
Sim
Nicolaescu et al. [41]
Sim
Sim
20 + 16
Alsulami and Cherif [2]
Não
Não
-
Gao et al. [20]
Não
Não
-
Jungnickel and Herb [27]
Não
Não
-
Nédelec et al. [39]
Sim
Não
-
Fechner et al. [17]
Sim
Sim
Salvati et al. [49]
Sim
Sim
Katayama et al. [28]
Sim
Não
-
Inoue et al. [24]
Não
Não
-
Ozono et al. [45]
Sim
Não
5 (simulados)
Ozono et al. [44]
Sim
Não
Simulado de 3 a 30 com intervalos de 3
Lautamäki et al. [31]
Não
Não
-
Thum et al. [66]
Não
Não
-
Wei et al. [71]
Não
Não
-
Bani-Salameh et al. [3]
Não
Não
-
De Lucia et al. [12]
Não
Não
-
Lin et al. [33]
Não
Não
-
Leone et al. [32]
Não
Não
-
Wong [72]
Sim
Não
-
Xia et al. [73]
Não
Não
-
Shen and Sun [54]
Não
Não
-
Figura 5: Testes gerais e testes com usuários.
Quais os principais tipos de testes realizados para avaliar os editores
colaborativos, quais os artigos realizaram testes como usuários e
qual o número médio de usuários que realizaram os testes?
Entre os 23 trabalhos selecionados, 10 realizaram algum tipo de testes, o
que representa aproximadamente 42% dos trabalhos selecionados. Apenas 5
(cinco) entre os 23 trabalhos realizaram testes com usuários. Desses cinco
trabalhos que realizaram testes com usuários, 2 (dois) reportaram testes pre-
liminares. O maior número de usuários foi 39, no trabalho de Fechner et al.
[17], e o número mínimo foi 12, no trabalho de Salvati et al. [49]. Além disso,
durante a análise dos 23 trabalhos, não foi possível identificar um processo
padronizado que definisse um modelo de teste para o desenvolvimento de
um RCE.
Quais trabalhos discutem técnicas de undo/redo?
Dos 23 trabalhos incluídos, 3 (três) discutem explicitamente técnicas e re-
cursos de undo/redo. No trabalho de Gao et al. [20] foram apresentados e
discutidos os algoritmos BTMVIC e AnyUndo, ambos com o objetivo de
resolver problemas de consistência nas operações de undo/redo. No trabalho
de Shen and Sun [54] foi apresentado o algoritmo SUCA.
Quais trabalhos discutem a utilização dos comentários, chat ou his-
tórico como ferramentas de apoio ao trabalho colaborativo?
De forma diversificada, 6 (seis) dos 23 trabalhos discutiram sobre recursos de
colaboração utilizando chat, anotações ou histórico. No trabalho de Fechner
et al. [17] foram verificadas as razões da utilização, ou não, do chat e do
recurso de histórico como ferramenta de auxílio. No trabalho de Salvati et al.
[49], foi permitido o compartilhamento do histórico de edição para auxílio
aos outros usuários. Katayama et al. [28] registram o compartilhamento
WRSL+’2024, Juiz de Fora/MG, Brasil
Dantas et al.
Tabela 3: Recursos de edição oferecidos
Referência
undo/redo
chat, comentário e histórico
Bath et al. [5]
Não
Não
Kuiter et al. [30]
Não
Não
Nicolaescu et al. [41]
Não
Não
Alsulami and Cherif [2]
Não
Não
Gao et al. [20]
Algoritmos BTMVIC e AnyUndo
Não
Jungnickel and Herb [27]
Não
Não
Nédelec et al. [39]
Não
Não
Fechner et al. [17]
Não
chat e histórico3
Salvati et al. [49]
Não
Compartilhamento de históricos de
edição
Katayama et al. [28]
Não
Compartilhamento das anotações e
chat por texto
Inoue et al. [24]
Não
Não
Ozono et al. [45]
Não
chat por texto
Ozono et al. [44]
Não
Não
Lautamäki et al. [31]
Não
Não
Thum et al. [66]
Não
Não
Wei et al. [71]
Não
Não
Bani-Salameh et al. [3]
Não
chat com Voip
De Lucia et al. [12]
Não
Não
Lin et al. [33]
Não
Não
Leone et al. [32]
Sim
Não
Wong [72]
Não
Não
Xia et al. [73]
Não
Não
Shen and Sun [54]
Algoritmo SUCA
Destaques no documento
Figura 6: Undo/Redo, histórico, chat e comentários.
das anotações (comentários) e a uso de chat por texto. Ozono et al. [45]
também reportam o uso de chat por texto. Bani-Salameh et al. [3] registram
a disponibilização de um chat com recursos de VoIP (Voice over Internet
Protocol). No editor reportado por Shen and Sun [54], era possível fazer
destaques nos textos que eram compartilhados com outros usuários.
4.2
Resumo dos artigos selecionados
O trabalho de Bath et al. [5] centra-se na concepção e na implementação
de um aplicativo web para edição colaborativa em tempo real de imagens
raster e vetoriais. Para compreender melhor os requisitos dos usuários, foi
realizada uma avaliação preliminar, identificando comunicação e sincroniza-
ção como elementos essenciais. O sistema desenvolvido utiliza um modelo
de documento centralizado, mantido por um servidor que sincroniza as
alterações com múltiplos clientes. A implementação prototípica é baseada
em uma arquitetura cliente-servidor escalável, utilizando WebGL para ren-
derização interativa no navegador e WebSocket para manter a sincronização
em tempo real. A sincronização é gerenciada de forma centralizada, com um
sistema de bloqueio que permite apenas a um usuário editar uma camada
Desenvolvimento de Editores Colaborativos em Tempo Real: Revisão Rápida
WRSL+’2024, Juiz de Fora/MG, Brasil
por vez. As atualizações são transmitidas por broadcast e serializadas para
garantir a consistência. O trabalho foi avaliado qualitativamente através de
um estudo como usuários.
O variED é um editor para modelagem de características (feature mo-
deling) colaborativa e em tempo real apresentado por Kuiter et al. [30].
O editor foi desenvolvido para permitir que múltiplos usuários trabalhem
simultaneamente na criação e modificação de modelos de características,
que são essenciais para o desenvolvimento de software orientado a linha
de produtos. A ferramenta aborda desafios críticos, como a manutenção
da consistência do modelo e a sincronização das alterações feitas por dife-
rentes usuários em tempo real. A arquitetura do variED é baseada em uma
infraestrutura distribuída que utiliza algoritmos de OT para gerenciar as
operações concorrentes, garantindo que todas as edições sejam integradas
de forma consistente sem conflitos. Os autores conduziram uma série de
testes experimentais e estudos de caso para avaliar o desempenho do variED,
demonstrando que a ferramenta é capaz de suportar um grande número de
usuários simultâneos sem comprometer a integridade e a consistência do
modelo. A análise dos resultados mostra que variED melhora a eficiência
e a produtividade do processo de modelagem de características, e também
proporciona uma experiência de usuário fluida e interativa.
Nicolaescu et al. [41] abordam a necessidade de colaboração eficiente
entre stakeholders geograficamente distribuídos em ambientes de modela-
gem conceitual. Para atender a essa necessidade, os autores introduzem
a SyncMeta, uma ferramenta de modelagem colaborativa que opera em
navegadores web, permitindo a edição compartilhada em tempo quase real.
SyncMeta é projetada para suportar a modelagem baseada em visões, na
qual diferentes perspectivas de stakeholders podem ser integradas e negoci-
adas de forma eficaz. A ferramenta permite que stakeholders contribuam
simultaneamente, minimizando conflitos e inconsistências nas operações
de modelagem. A ferramenta permite a rápida geração de protótipos a
partir de especificações, facilitando a análise de impacto e a tomada de
decisões. Os estudos conduzidos pelos autores investigaram tanto os re-
quisitos fundamentais da ferramenta quanto opções de design. Entre os
requisitos fundamentais avaliados, os autores destacam a aceitação da abor-
dagem de modelagem baseada em visões. As opções de design exploradas
no trabalho incluem a resolução de pontos de vista de maneira centralizada
versus ponto-a-ponto, considerando o compromisso entre desempenho e
complexidade de implementação. Os resultados dos estudos indicam que a
SyncMeta oferece uma solução robusta para os desafios de colaboração em
ambientes de modelagem distribuídos, melhorando a eficiência e a eficácia
das equipes.
Alsulami and Cherif [2] exploram a viabilidade e os desafios da edição
colaborativa em redes oportunísticas (ON), na qual a conectividade entre
dispositivos é intermitente e imprevisível, focalizando na adaptação de al-
goritmos baseados em OT para RTCE em ON. São discutidos os principais
desafios como alta mobilidade, dinâmica dos nós e atrasos de rede, e revisa-
das as versões dos algoritmos OT segundo critérios relevantes para ON. O
artigo oferece uma visão abrangente para o desenvolvimento e avaliação
de editores colaborativos em ambientes ON. O artigo também aborda as
estratégias para sincronização de dados em tempo real, adaptando-os à na-
tureza assíncrona e intermitente das comunicações em redes oportunísticas,
e métodos são propostos para reconciliar versões divergentes de documen-
tos editados por dispositivos desconectados. Resultados experimentais são
apresentados, demonstrando o desempenho das abordagens propostas em
cenários simulados e reais, com análises quantitativas de consistência de
dados, tempo de sincronização e eficiência operacional.
Nédelec et al. [39] apresentam o desenvolvimento e a implementação
de um sistema de edição colaborativa em tempo real, chamado CRATE,
que é um editor colaborativo descentralizado em tempo real que funciona
diretamente em navegadores da web utilizando o WebRTC. CRATE foi um
editor em tempo real pioneiro na utilização apenas de navegadores para
suportar a edição colaborativa, manejando de forma transparente grupos
pequenos e grandes de usuários. As propriedades do CRATE dependem de
dois avanços científicos principais: 1) uma estrutura de sequência replicada
com limite superior sublinear na complexidade do espaço, o que evita a
necessidade de coletores de lixo distribuídos dispendiosos; 2) um protocolo
adaptativo de amostragem por pares, que evita o superdimensionamento de
tabelas de roteamento, permitindo que redes pequenas não paguem o preço
das redes grandes.
Jungnickel and Herb [27] ampliam o escopo do algoritmo de controle
de consistência OT para a edição simultânea de objetos JSON compartilha-
dos. A pesquisa detalha as estruturas de dados e as mecânicas subjacentes
necessárias para suportar a edição colaborativa de objetos JSON, fornecendo
uma base teórica robusta para desenvolvedores de aplicações web. Além
disso, o artigo discute o design de aplicativos web que utilizam essa extensão
da OT, destacando as vantagens em termos de versatilidade e robustez na
colaboração em tempo real. Esta extensão da OT para objetos JSON abre
novas possibilidades, permitindo a edição colaborativa de uma gama mais
ampla de dados estruturados, desde configurações de software até dados
de aplicações complexas. A implementação desta técnica oferece benefícios
significativos para desenvolvedores e usuários, criando um ambiente cola-
borativo mais flexível e poderoso. A capacidade de editar simultaneamente
objetos JSON em tempo real pode transformar a maneira como dados es-
truturados são manipulados colaborativamente, promovendo uma maior
eficiência e inovação no desenvolvimento de software e em outras áreas
que dependem de dados complexos.
O trabalho de Gao et al. [20] foca nos desafios de garantir que múltiplos
usuários possam editar simultaneamente uma imagem bitmap sem causar
inconsistências ou conflitos nas alterações. Para resolver esses desafios, os
autores propõem um modelo que integra algoritmos de OT para gerenciar e
sincronizar as operações de edição realizadas por diferentes usuários. Este
modelo permite que as operações de undo e redo sejam tratadas de forma
consistente, mantendo a integridade do estado do bitmap ao longo do tempo,
o que é demonstrado através de análises e um protótipo de sistema chamado
CoGraphical Editor. Estudos de caso e simulações reforçam a viabilidade
do sistema em cenários de uso real, destacando melhorias significativas na
precisão e na eficiência das operações de edição colaborativa.
Fechner et al. [17] apresentam uma abordagem para a edição colabora-
tiva em tempo real de dados geográficos, exemplificada pelo Ethermap. O
sistema Ethermap foi desenvolvido para permitir que múltiplos usuários
possam editar simultaneamente mapas geográficos, garantindo a sincroniza-
ção imediata e precisa das alterações realizadas. A arquitetura do Ethermap
utiliza tecnologias avançadas de comunicação e OT para gerenciar opera-
ções concorrentes, assegurando que todas as edições sejam integradas de
maneira consistente. O sistema é projetado para suportar tanto a criação
de novos mapas quanto a edição de mapas existentes, oferecendo uma in-
terface intuitiva que facilita a colaboração entre os usuários. Estudos de
caso são apresentados para demonstrar a eficácia do sistema em melhorar
a colaboração e a produtividade dos usuários durante a manipulação e a
atualização de informações geoespaciais.
Salvati et al. [49] propõem o editor MeshHisto como uma aborda-
gem inovadora para a modelagem colaborativa de objetos 3D, centrada no
compartilhamento e redirecionamento de históricos de edição. MeshHisto
permite que múltiplos usuários colaborem em projetos de modelagem 3D,
compartilhando seus históricos de edição de forma granular e reutilizável. A
metodologia proposta utiliza uma estrutura de dados eficiente para capturar
e armazenar as operações de edição, permitindo que estas possam ser rea-
plicadas ou adaptadas a diferentes modelos 3D. Isso facilita a transferência
de técnicas e estilos de modelagem entre colaboradores, promovendo uma
integração harmoniosa e eficiente do trabalho em equipe. A arquitetura do
sistema MeshHisto incorpora mecanismos para resolver conflitos de edição
e manter a consistência dos modelos em cenários de edição concorrente.
Estudos de caso demonstram a eficácia do sistema em diversos contextos
de modelagem, evidenciando melhorias significativas na produtividade e
WRSL+’2024, Juiz de Fora/MG, Brasil
Dantas et al.
na qualidade dos modelos produzidos. Além disso, a análise empírica dos
resultados revela que o compartilhamento de históricos de edição não ape-
nas acelera o processo de modelagem, mas também enriquece a experiência
colaborativa ao possibilitar a aprendizagem e a inspiração mútua entre os
usuários.
Katayama et al. [28] reportam uma aplicação web colaborativa para
editar documentos PDF usando navegadores. O sistema permite que os
usuários editem o mesmo documento em tempo real e compartilhem anota-
ções em documentos simultaneamente. Os autores propõem um mecanismo
de sincronização eficiente para aplicações web colaborativas utilizando as
capacidades avançadas do HTML5. Este mecanismo visa resolver desafios
de latência e consistência que surgem quando múltiplos usuários intera-
gem simultaneamente com uma aplicação web. A abordagem se baseia
na utilização de WebSockets para comunicação bidirecional em tempo real
entre clientes e servidores, permitindo atualizações instantâneas e redu-
ção de atrasos. O trabalho detalha a arquitetura do sistema, que incorpora
algoritmos de OT para gerenciar as operações concorrentes e assegurar
que todas as mudanças feitas pelos usuários sejam integradas de maneira
consistente. A infraestrutura inclui um servidor central que coordena as
operações de sincronização, garantindo que todas as instâncias da aplicação
web permaneçam atualizadas.
Lautamäki et al. [31] detalham o desenvolvimento e a implementação
de CoRED, um editor colaborativo em tempo real baseado em navegador
para aplicações web Java. O CoRED foi projetado para permitir que múlti-
plos desenvolvedores editem simultaneamente o código fonte de aplicações
Java diretamente em um navegador web, promovendo uma colaboração
eficiente e integrada. O CoRED é um editor Java completo que inclui ve-
rificação de erros e recursos de geração automática de código, além de
ser complementado por funcionalidades comumente associadas às mídias
sociais. A arquitetura do CoRED incorpora tecnologias avançadas como
WebSockets para comunicação bidirecional em tempo real, garantindo a
sincronização imediata das edições feitas por diferentes usuários. Além
disso, o sistema utiliza algoritmos de OT para gerenciar operações concor-
rentes, assegurando que todas as modificações sejam aplicadas de maneira
consistente e sem conflitos. A ferramenta foi testada em diversos cenários
de desenvolvimento colaborativo, demonstrando melhorias significativas
na produtividade e eficiência das equipes de desenvolvimento.
Inoue et al. [24] apresentam o WFE, um RCE que permite a um grupo
de pessoas editarem uma página web em um navegador compartilhando o
conteúdo de edição em tempo real. Além disso, os autores também propõem
a aplicação do WFE para um ambiente de computação em nuvem, chamada
WFE-S, com o objetivo de melhorar a escalabilidade, a elasticidade, a ca-
pacidade de resposta, bem como de reduzir a necessidade de manutenção.
A arquitetura do sistema é baseada em uma combinação de algoritmos de
OT e técnicas de controle de versão, que juntos asseguram que as edições
concorrentes sejam integradas sem conflitos. Os autores detalham a infraes-
trutura técnica, incluindo o uso de servidores intermediários para gerenciar
as comunicações entre os clientes e assegurar a integridade das operações
de edição. Os estudos experimentais realizados demonstram a eficácia do
sistema, mostrando que ele é capaz de lidar com um grande número de
usuários simultâneos sem degradação significativa de desempenho. Além
disso, o sistema inclui funcionalidades como a visualização das edições em
tempo real e a capacidade de desfazer e refazer mudanças, oferecendo uma
experiência de usuário rica e interativa. A análise dos resultados indica que
o mecanismo proposto melhora a eficiência da colaboração em tempo real e
aumenta a produtividade e a qualidade do trabalho colaborativo.
Ozono et al. [44] também apresentam o ambiente WFE-S para edição co-
laborativa de páginas web em tempo real. Além de detalhar a infraestrutura
e os algoritmos, os autores realizaram uma série de testes experimentais
e estudos de caso para avaliar o desempenho do WFE-S, demonstrando
que o sistema é altamente eficiente em termos de latência e escalabilidade.
Os resultados mostram que o WFE-S mantém a consistência dos dados e
proporciona uma experiência de usuário suave, mesmo sob alta carga de
usuários simultâneos.
Thum et al. [66] descrevem o desenvolvimento e a implementação
do SLIM, um ambiente projetado para suportar a modelagem colaborativa
síncrona. O sistema SLIM visa facilitar a colaboração em tempo real entre
múltiplos usuários na criação e edição de modelos, com foco especial em mo-
delos de sistemas de software. A arquitetura do SLIM incorpora tecnologias
que permitem a sincronização imediata de alterações feitas por diferentes
participantes, garantindo consistência e integridade do modelo em tempo
real. O ambiente utiliza técnicas avançadas de OT para gerenciar operações
concorrentes de maneira eficiente, minimizando conflitos e assegurando que
todas as alterações sejam devidamente integradas. O artigo também discute
estudos de caso e experimentos realizados para avaliar o desempenho e
a eficácia do SLIM em cenários reais de uso. Os resultados demonstram
que o ambiente é capaz de suportar uma colaboração eficiente e produtiva,
promovendo uma interação fluida entre os participantes durante o processo
de modelagem.
Wei et al. [71] abordam o desenvolvimento e a implementação de um
ambiente colaborativo de edição científica focado na química. O sistema
foi projetado para facilitar a colaboração entre cientistas e pesquisadores
na criação e edição colaborativa de documentos científicos relacionados à
química. A arquitetura do ambiente colaborativo incorpora funcionalidades
específicas para apoiar a edição em tempo real de documentos científicos
complexos, incluindo formulações químicas, estruturas moleculares, dia-
gramas e textos técnicos especializados. O ambiente oferece recursos como
controle de versões, que permitem rastrear e gerenciar alterações feitas por
diferentes colaboradores ao longo do tempo. Além disso, são implementadas
ferramentas para suportar a análise e a visualização de dados químicos,
facilitando a comunicação e a colaboração entre os membros da equipe de
pesquisa. O artigo também destaca estudos de caso e experimentos práticos
realizados para validar a eficácia e a usabilidade do ambiente colaborativo
em cenários reais de pesquisa química. Os resultados demonstram que o
sistema é capaz de melhorar a eficiência e a produtividade dos pesquisadores,
ao mesmo tempo que promove uma colaboração mais integrada e eficaz no
desenvolvimento de documentos científicos na área da química.
Bani-Salameh et al. [3] registram o design e a implementação de uma
de um IDE colaborativo chamado ICI (Idaho Collaborative IDE) que permite
que desenvolvedores em diferentes locais colaborem em uma variedade de
atividades de desenvolvimento de software em tempo real. O ICI combina
um editor de programa colaborativo síncrono e um depurador colaborativo
em tempo real em um ambiente virtual multiusuário 3D. O ICI permite que
desenvolvedores compartilhem, em tempo real, o processo de edição, com-
pilação, execução e depuração de seus projetos de software. A arquitetura
do ICI é composta por quatro componentes principais: 1) um editor colabo-
rativo 2) um ambiente colaborativo shell 3) um conjunto de ferramentas de
comunicação, como chat de texto e voz, e 4) uma interface para colaboração
controle. A contribuição deste trabalho é uma IDE colaborativa que integra
edição colaborativa responsiva e em tempo real e depuração. Os resultados
de testes de usabilidade e estudos de caso indicam que o ICI melhora a
produtividade e a eficiência da equipe, reduzindo o tempo necessário para
detectar e corrigir erros.
De Lucia et al. [12] apresentam e ferramenta STEVE, destinada à mo-
delagem colaborativa síncrona com gerenciamento de versionamento, que
suporta a modelagem distribuída em UML de sistemas de software. STEVE
permite que desenvolvedores distribuídos editem simultaneamente o mesmo
diagrama UML, decompostos em sub-artefatos gerenciados hierarquica-
mente, oferecendo gerenciamento de alterações e configurações tanto para
diagramas quanto para objetos gráficos. Isso possibilita a reutilização e o
compartilhamento consistentes de componentes de diagramas em diferentes
projetos. Integrada ao sistema ADAMS, que oferece funcionalidades refi-
nadas de gerenciamento de artefatos e compartilhamento de diagramas, a
Desenvolvimento de Editores Colaborativos em Tempo Real: Revisão Rápida
WRSL+’2024, Juiz de Fora/MG, Brasil
ferramenta destaca a importância do controle preciso de versões. Ela incor-
pora mecanismos de versionamento que rastreiam alterações nos elementos
individuais do modelo, promovendo a sincronização eficiente e a resolu-
ção de conflitos. A arquitetura do sistema, que combina ferramentas de
modelagem UML com um robusto sistema de controle de versões, permite
operações colaborativas em tempo real. Comparações com métodos tradici-
onais de versionamento demonstram melhorias significativas em precisão
e eficiência na gestão de alterações concorrentes. Estudos de caso e testes
empíricos validam a eficácia da ferramenta, mostrando que ela facilita a
colaboração contínua e a integração de mudanças de maneira fluida.
Leone et al. [32] reportam TeNDaX, um editor colaborativo em tempo
real, baseado em um sistema de banco de dados, conforme descrito no tra-
balho de . O editor armazena documentos, incluindo conteúdo, estrutura,
tabelas e imagens, em um banco de dados semiestruturado, o que facilita
a edição colaborativa e o layout dos documentos. Além disso, o TeNDaX
oferece funcionalidades avançadas como operações de desfazer e refazer,
definição e execução de processos de negócios, e recursos de segurança
e conscientização do contexto. Durante a criação e uso dos documentos,
metadados são coletados automaticamente, permitindo a criação de pas-
tas dinâmicas, rastreamento da proveniência dos dados, e a realização de
mineração e pesquisa visual e de texto. A plataforma é comparada a uma
"LAN-Party"de processamento de texto, suportando múltiplos editores e
sistemas operacionais diferentes. Esse ambiente colaborativo permite a exe-
cução de operações locais e globais de desfazer e refazer, facilitando a edição
em tempo real. Além disso, o TeNDaX demonstra a utilização eficiente de
dados e metadados para criar pastas dinâmicas, visualizar a proveniência
dos dados, realizar mineração visual de texto e oferecer funcionalidades
de pesquisa avançadas. O sistema também destaca a extensão do banco de
dados para gerenciar texto, mostrando como a integração de dados e metada-
dos pode aprimorar significativamente a experiência de edição colaborativa
em tempo real.
Wong [72] estudam como múltiplos usuários podem colaborar de forma
eficaz na criação e edição de documentos hipertextuais usando disposi-
tivos móveis, considerando as limitações típicas desses ambientes, como
conectividade intermitente e recursos limitados. Os autores apresentam
um protótipo de sistema de edição colaborativa com um mecanismo para
sincronizar atualizações de editores simultâneos, integrado ao sistema de
gerenciamento de banco de dados XML nativo SODA. A pesquisa discute
diversas estratégias e tecnologias para facilitar a colaboração em tempo
real, incluindo o uso de protocolos de comunicação eficientes, algoritmos de
reconciliação de conflitos e interfaces de usuário adaptadas para dispositivos
móveis. São exploradas técnicas para garantir a consistência e a sincroni-
zação das edições realizadas por diferentes usuários, mesmo em condições
adversas de rede e mobilidade. O artigo também apresenta estudos de caso
e experimentos práticos para avaliar a viabilidade e a eficácia das soluções
propostas. Resultados experimentais são discutidos para demonstrar a per-
formance e a usabilidade do sistema em cenários reais de uso, destacando
melhorias na colaboração e na produtividade dos usuários móveis durante
a edição de documentos hipertextuais.
Xia et al. [73] investigam a transformação de aplicações de usuário
único em plataformas colaborativas multiusuário através da abordagem
CoWord. Os autores apresentam um trabalho com o objetivo central de
adaptar o Microsoft Word, uma aplicação originalmente concebida para uso
individual, para suportar a edição colaborativa em tempo real. A metodo-
logia empregada envolve a integração de um mecanismo de sincronização
que permite a múltiplos usuários editar simultaneamente o mesmo docu-
mento, mantendo a consistência e a integridade das alterações. Utilizando
técnicas de OT e algoritmos de resolução de conflitos, o CoWord assegura
que as edições concorrentes sejam harmonizadas eficientemente. O artigo
descreve a arquitetura do sistema CoWord, destacando componentes essen-
ciais como o servidor de sincronização, os clientes distribuídos e o protocolo
de comunicação utilizado para garantir uma colaboração fluida. Resultados
experimentais são apresentados para demonstrar a eficácia do CoWord,
evidenciando melhorias significativas na produtividade e na experiência
colaborativa dos usuários. Estudos de caso exemplificam a aplicabilidade da
abordagem em diversos contextos profissionais e educacionais, realçando a
versatilidade e a robustez da solução proposta.
Shen and Sun [54], considerando oferta do recurso de destacar texto
de modo síncrono, identificaram três necessidades principais: 1) diferenciar
destaques feitos por diferentes usuários; 2) resolver problemas de incon-
sistência causados por operações simultâneas; 3) oferecer uma função de
desfazer flexível, permitindo reverter qualquer operação de destaque a qual-
quer momento. Para atender a essas necessidades, foi desenvolvido o sistema
REDUCE (Real-time Distributed Unconstrained Collaborative Editing). O
artigo apresenta soluções específicas, incluindo os algoritmos de transfor-
mação IT_EH e IT_HH, além do algoritmo de controle OSCA, que resolvem
problemas de divergência causados por operações de destaque sobrepostas.
Adicionalmente, o algoritmo GOTO foi estendido para controlar a transfor-
mação de operações de destaque em relação às operações de edição. A função
de desfazer foi aprimorada com os algoritmos de transformação IT_HU e o
algoritmo de controle SUCA, permitindo desfazer seletivamente qualquer
operação de destaque a qualquer momento. Resultados experimentais, obti-
dos através de estudos de caso e de testes de usabilidade, demonstram que
o uso do highlighting melhora significativamente a compreensão mútua,
reduz o tempo de resolução de tarefas colaborativas e aumenta a satisfação
dos usuários. A análise quantitativa e qualitativa dos dados coletados sus-
tenta a eficácia da ferramenta em diversos cenários de aplicação, incluindo
edição de documentos, design gráfico e programação em pares.
RISCO À VALIDADE
Em relação aos resultados relatados nos artigos, o principal viés está asso-
ciado à fase inicial e exploratória de muitos dos estudos, dado que apenas
10 dos estudos apresentam algum tipo de avaliação e, dentre eles, apenas 5
(cinco) relatam estudos com a participação de usuários.
No viés associado à realização da revisão, os principais riscos e as ações
tomadas para minimizá-los foram:
(1) A pesquisa foi restrita a trabalhos classificados como sendo da área
de Computação pela ACM. Não foram tomadas ações neste caso
pois considerou-se que os principais veículos estão indexados;
(2) A string de busca restringiu a pesquisa aos campos de título e resumo.
Não foram tomadas atitudes porque considerou-se que os trabalhos
mais relevantes foram identificados;
(3) A distinta experiência dos 3 avaliadores. Para diminuir o risco de
viés associado, a seleção foi realizada individualmente por dois ava-
liadores de forma que um não soubesse as escolhas do outro, e
divergências foram definidas por uma pessoa árbitra.
DESAFIOS DE PESQUISA
Os resultados obtidos com esta revisão fornecem uma base para o desenvol-
vimento futuro de sistemas com suporte a RTCE. Os estudos identificados
sugerem múltiplas vias para alcançar eficiência e eficácia na edição colabo-
rativa em tempo real, cujos temas são reportados na literatura recente.
Testes e Avaliação com Usuários: Realizar testes abrangentes é um desa-
fio fundamental para a pesquisa em RTCE. A maioria dos estudos ainda é
limitada em termos de número de usuários e de cenários avaliados. Garantir
a condução de estudos que envolvam um número apropriado de partici-
pantes, participantes com diferentes papéis e em ambientes variados, como
no estudo de de Lange et al. [11], pode proporcionar uma compreensão
mais completa dos desafios enfrentados pelos usuários e das características
desejadas nos sistemas colaborativos.
WRSL+’2024, Juiz de Fora/MG, Brasil
Dantas et al.
Algoritmos de resolução de conflitos: Estudos ainda são necessários
neste tema dado que, apesar das análises de Sun et al. [62, 64] explicitarem
as vantagens de OT em relação CRDT, pesquisadores continuam a advogar
pelo uso de CRDT. Por exemplo, Kleppmann [29] apresenta um algoritmo
para operações de movimentação de elementos em CRDTs de listas, e Litt
et al. [34] tratam da edição de texto rico. Outros exemplos são os frameworks
YJS [26], para compartilhamento de dados, e Hocuspocus [25], para edição
colaborativa.
Diversidade de Objetos Editáveis e ambientes heterogêneos: Um de-
safio significativo na pesquisa de sistemas RTCE é expandir a capacidade
desses sistemas para suportar uma gama mais ampla de objetos editáveis.
Atualmente, muitos sistemas são otimizados para tipos de objetos relati-
vamente simples, como textos e planilhas. No entanto, a demanda está
crescendo por sistemas que possam lidar com modelos gráficos complexos
e diagramas interativos, entre outros [11, 29, 34]. Desenvolver abordagens
que garantam a consistência visual e a integridade dos dados para esses
tipos de objetos apresenta uma oportunidade para avanços significativos na
área. Outro desafio é a construção de sistemas de co-edição heterogêneos,
permitindo que múltiplos usuários utilizem editores diferentes para editar
documentos compartilhados na mesma sessão [8]. Essa abordagem destaca
a necessidade de maior flexibilidade e adaptabilidade nos sistemas de edi-
ção colaborativa, evidenciando a importância de suportar uma diversidade
maior de objetos e editores para melhorar a experiência colaborativa.
Edição Colaborativa de Texto Rico: A edição colaborativa de texto rico,
que inclui formatação e outras características visuais, apresenta desafios
específicos em comparação com a edição de texto simples. Por exemplo, Litt
et al. [34] descrevem um algoritmo CRDT para edição colaborativa de texto
rico. A estratégia armazena spans de formatação ao lado da sequência de
caracteres do texto e deriva o texto formatado final de forma determinística
para que operações concorrentes sejam comutativas.
Edição Colaborativa de Vídeo: O design de ferramentas para edição cola-
borativa de vídeo também levanta questões importantes e oportunidades
de pesquisa. Por exemplo, Okopnyi et al. [42] exploram as complexidades
do design para edição colaborativa de vídeo, destacam a necessidade de
integrar recursos colaborativos específicos para software de edição não-
linear, e discutem os desafios associados à introdução desses recursos. A
pesquisa sugere que, além dos recursos colaborativos convencionais, pode
ser necessário desenvolver representações abstratas alternativas para mí-
dias baseadas em tempo para facilitar a colaboração eficaz na edição de vídeo.
Integração de Modelagem e Edição Colaborativa em Processos de
Desenvolvimento: Além dos avanços na edição colaborativa de textos e
vídeos, a integração de técnicas colaborativas em processos de desenvolvi-
mento web também apresenta desafios. Por exemplo, de Lange et al. [11]
investigam a integração de modelagem colaborativa e edição de código em
um processo de engenharia web orientado a modelos (MDWE). O estudo
propõe uma abordagem que combina edição ao vivo e wireframing com
modelagem colaborativa em tempo real, abordando a sincronização entre
código-fonte, wireframes e modelos.
Implementação de Undo/Redo: A implementação de operações de undo
e redo continua a ser um desafio de pesquisa no contexto de edição cola-
borativa em tempo real, em particular no contexto de objetos complexos
e interativos. Sun [60] propõe uma solução para operações de undo em
editores colaborativos, permitindo desfazer qualquer operação a qualquer
momento, independentemente do contexto de undo. A solução usa OT para
garantir que uma operação possa ser desfeita, suportando múltiplos modos
de undo (único, cronológico e seletivo) na mesma sessão. Já Stewen and
Kleppmann [58] utilizam CRDT para implementar um novo algoritmo que
implementa undo/redo de acordo com a semântica modelada pelos autores
a partir de um conjunto de ambientes de edição investigados.
Representação e Diagramação: Outro desafio importante é o desenvol-
vimento de técnicas e de ferramentas de representação que capturem de
forma eficaz as interações dinâmicas e o comportamento em tempo real
dos sistemas RTCE. Inovar na criação de diagramas e modelos que possam
representar adequadamente essas interações é crucial para melhorar a com-
preensão e o gerenciamento desses sistemas. Por exemplo, Yu and Lu [74]
propõem uma ferramenta de discussão colaborativa adaptada para ambien-
tes MOOC, utilizando de um esquema de Address Space Transformation para
simular a interação entre usuários históricos e atuais. Essa abordagem pode
inspirar novas maneiras de representar interações dinâmicas em sistemas
RTCE, especialmente em ambientes educacionais.
Integração de Comentários, Chat e Histórico: Melhorar a integração
e a gestão de ferramentas de apoio colaborativo, como comentários, chat
e histórico de edições, representa um desafio significativo, entre outros,
relativamente à interação. No contexto da edição de imagens, por exemplo,
Bath et al. [4] optaram por posicionar a janela de troca de mensagens sobre
a imagem sendo editada. Desenvolver métodos que otimizem o uso dessas
ferramentas e avaliem seu impacto na produtividade e na eficácia do traba-
lho em equipe são relevantes para permitir integração sem comprometer a
interação dos usuários nem o desempenho do sistema.
Colaboração Humano-Computador: A colaboração entre humanos e
computadores na edição pode representar um avanço significativo na eficá-
cia e eficiência dos sistemas RTCE. O trabalho de Pan et al. [46] introduz
uma ferramenta de edição colaborativa que divide a tarefa entre o humano e
o computador, permitindo que cada um se concentre em suas especialidades.
Explorar e desenvolver ferramentas que integrem a colaboração humano-
computador pode reduzir o esforço necessário para operações complexas e
melhorar a eficiência na edição colaborativa.
Inclusão de Usuários com Deficiências: Um desafio constante na pes-
quisa em RTCE é garantir que os sistemas de edição colaborativa sejam
acessíveis e eficazes para pessoas com deficiências. O trabalho de Akter
et al. [1] revela as barreiras enfrentadas por facilitadores de reuniões com
deficiência visual ao usar ferramentas de videoconferência, destacando a
necessidade de interfaces mais inclusivas e adaptáveis. Investigar como as
tecnologias de edição colaborativa podem ser projetadas para atender às
necessidades específicas de diferentes grupos de usuários é crucial para
promover a equidade e a acessibilidade no ambiente colaborativo.
CONSIDERAÇÕES FINAIS
No presente trabalho apresentamos uma Revisão Rápida que buscou iden-
tificar pesquisas que descrevessem o desenvolvimento de sistemas com
suporte a RTCE, bem como algoritmos ou técnicas importantes para o seu
desenvolvimento.
Os dados extraídos desses artigos foram organizados de maneira a des-
tacar aspectos importantes como o tipo de objeto editado, a arquitetura
adotada, os algoritmos de sincronização utilizados, os tipos de avaliações
realizadas, e os recursos de edição oferecidos. A análise revelou diversas
abordagens e técnicas empregadas na implementação de RTCE, proporcio-
nando uma visão abrangente das pesquisas nessa área, e permitiu identificar
desafios para pesquisas futuras.
AGRADECIMENTOS
Agradecemos as pessoas revisoras pelas valiosas sugestões que contribuíram
para o aprimoramento do conteúdo e da apresentação deste trabalho.
Desenvolvimento de Editores Colaborativos em Tempo Real: Revisão Rápida
WRSL+’2024, Juiz de Fora/MG, Brasil

--- FIM DO ARQUIVO: 30492-829-24940-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30493-829-24941-1-10-20241001.txt ---
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade
em Sistemas Baseados em Aprendizado de Máquina
Lívia Mancine
Instituto de Informática
Universidade Federal de Goiás
Instituto Federal Goiano
Goiânia, Goiás, Brasil
liviamancine@discente.ufg.br
João Lucas Soares
Instituto de Informática
Universidade Federal de Goiás
Goiânia, Goiás, Brasil
soares.joao@discente.ufg.br
Taciana Novo Kudo
Instituto de Informática
Universidade Federal de Goiás
Goiânia, Goiás, Brasil
taciana@ufg.br
Renato F. Bulcão-Neto
Instituto de Informática
Universidade Federal de Goiás
Goiânia, Goiás, Brasil
rbulcao@ufg.br
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
substancialmente a qualidade dos sistemas de software [11, 13, 39].
Confiar decisões cruciais a um modelo de AM cria a necessidade
de que esses modelos apresentem como característica importante a
explicabilidade para o seu processo de tomada de decisão [23].
A capacidade de explicar está intimamente ligada à forma como
os utilizadores finais e/ou especialistas de domínio podem compre-
ender e explorar os resultados dos modelos de AM [30]. Assim, a
explicabilidade, como um RNF importante, é considerada um requi-
sito transversal. Isso significa que a explicabilidade é importante
para todos os tipos de sistemas baseados em AM, incluindo sistemas
multimídia e Web. A necessidade de explicabilidade transcende a
simples funcionalidade do sistema. Portanto, é essencial estabelecer
uma interação entre a Engenharia de Requisitos (ER) e a explicabi-
lidade em sistemas baseados em AM, uma vez que a ER aborda de
maneira sistemática os requisitos de software, incluindo requisitos
funcionais e não funcionais.
Além disso, a explicabilidade em sistemas baseados em AM, está
relacionada à confiança, justiça, análise de vieses e aspectos éti-
cos, pois permite que os processos de tomada de decisão sejam
compreensíveis e transparentes. Embora a interpretabilidade esteja
associada aos modelos de AM, a explicabilidade sob a perspectiva
da ER pode facilitar a compreensão das previsões complexas desses
modelos, contribuindo para a confiança nos resultados gerados.
O objetivo deste Mapeamento Sistemático da Literatura (MSL)
é investigar não apenas as explicações técnicas, mas fornecer um
panorama da ER em relação ao requisito de explicabilidade em
sistemas baseados em AM. Para isso, planejamos e executamos um
protocolo para um MSL baseado na abordagem de [33].
A metodologia adotada incluiu uma análise sistemática das publi-
cações disponíveis, permitindo-nos identificar lacunas na literatura
e áreas que necessitam de maior investigação. Após a análise de 27
artigos relevantes para o tema, delimitou-se uma base sólida para
futuras pesquisas e desenvolvimento de práticas que promovam ex-
plicabilidade em sistemas de AM. Embora exista uma preocupação
da comunidade acadêmica em entender a relação da ER e sistemas
baseados em AM [3, 40, 43], até o momento desta pesquisa, não
encontramos estudos secundários como o mesmo objetivo que este.
Este artigo está assim organizado: a Seção 2 apresenta trabalhos
relacionados; a Seção 3 descreve o planejamento do protocolo do
MSL; a Seção 4 detalha a etapa de execução do MSL; a Seção 5
relaciona as respostas às questões de pesquisa, bem como sintetiza
os resultados do MSL; por fim, a Seção 6 sumariza as conclusões e
trabalhos futuros.
TRABALHOS RELACIONADOS
O processo da ER visa garantir que os requisitos devidamente eli-
citados, analisados, documentados e gerenciados atendam às ne-
cessidades dos stakeholders. No entanto, a natureza distinta dos
processos de desenvolvimento de software tradicionais e aqueles
baseados em IA gerou novas lacunas na ER para o desenvolvimento
de sistemas baseados em AM. A engenharia de software tradicio-
nal envolve, basicamente, as atividades de ER, o projeto detalhado
para implementação de um programa executável (principalmente
a escrita de código) e tarefas de gerenciamento. Por outro lado, o
desenvolvimento de um software baseado em AM inclui etapas
como a coleta de dados, a seleção de algoritmos de AM e o treina-
mento do modelo com base nos dados fornecidos, sendo a escrita
de código uma atividade menos central no processo. Amershi et al.
[5] compararam software tradicional com o software baseado em
AM, destacando que: (a) descobrir, gerenciar e versionar dados para
AM é mais complexo que em engenharia de software tradicional;
(b) customizar e reutilizar modelos requer habilidades diferentes
das encontradas em equipes de software; (c) componentes de IA
e AM são mais difíceis de modularizar que os componentes de
software convencionais. Nesse contexto, desenvolver software ba-
seado em AM sem conhecimento detalhado, ou com conhecimento
limitado do funcionamento interno do sistema, apresenta novos e
significativos desafios para a ER.
Estudos anteriores investigaram abordagens de ER para sistemas
de IA e AM, com ênfase em metodologias, ferramentas e técnicas
específicas para esses sistemas [2, 40]. Ahmad et al. [2] conduziram
uma MSL para identificar avaliações empíricas existentes, teorias
emergentes e desafios relacionados à ER para IA. Eles derivaram
uma lista de sinônimos para os termos “Engenharia de Requisitos”
e “Inteligência Artificial” e personalizaram uma string de busca
para cada base de dados, usando a seguinte sequência básica ((re-
quirements engineering OR requirements process OR requirements
elicitation OR requirements gathering OR requirements identification
OR requirements analysis OR requirements validation OR require-
ments verification OR requirements specification OR “requirements
development” OR “requirements documentation” OR “requirements
management” OR requirements testing OR functional requirements
OR requirements driven) AND (artificial intelligence OR machine
learning OR expert systems OR deep learning OR computer vision
OR natural language processing OR speech recognition OR machine
intelligence OR AI OR ML OR chatbot OR expert systems OR self
driving OR autonomous OR recommendation system OR robot)). A
busca automática em seis bases, combinada com snowballing para
frente e para trás, resultou na identificação de 43 estudos analisados.
Eles destacaram linguagens de modelagem, como SysML, ontoML,
diagrama de atividades e modelagem UML semi-formal, aplicadas
nos estudos, além de enfatizerem aspectos éticos. Villamizar et al.
[40] também conduziram uma MSL com o objetivo de delinear o
estado da arte em ER para sistemas baseados em AM. A string
de busca utilizada foi (Software OR Applications OR Systems) AND
(Machine Learning) AND (Requirements Engineering). Utilizando
uma estratégia de busca manual e snowballing para frente e para
trás, os autores obtiveram 35 estudos. As principais descobertas
indicaram que as etapas mais abordadas em ER foram elicitação
e análise, com o uso frequente da técnica de brainstorming. Além
disso, observou-se o emprego de métodos ad hoc para elicitar e
garantir o cumprimento de RNFs. Os autores mencionaram que
ainda não está claro quais ferramentas e técnicas tradicionais podem
ser aplicadas da ER para AM. Além disso, notou-se que a maioria
dos estudos sobre a intersecção entre ER e AM utiliza AM para
fins da ER. Ambos os estudos, [2, 40], destacaram que aspectos de
ética, confiança, justiça e explicabilidade são relevantes, mas são
abordados apenas teoricamente e sem avaliação.
O estudo de Alves et al. [4] conduziu um survey para obter in-
sights de profissionais sobre práticas e problemas enfrentados no
ciclo de vida de requisitos em projetos de sistemas baseados em AM.
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade em Sistemas Baseados em Aprendizado de Máquina
WRSL+’2024, Juiz de Fora/MG, Brazil
Foram coletadas 188 respostas de 25 países. Para análise, aplicaram-
se métodos quantitativos, como bootstrapping com intervalos de
confiança, e quantitativos, como codificação aberta e axial. Todos
os participantes eram experientes em projetos de AM, com perfis
predominantemente de cientistas de dados, seguidos por líderes de
projetos, desenvolvedores, arquitetos de soluções e engenheiros de
requisitos. A pesquisa indicou que líderes de projeto e cientistas
de dados estão assumindo a responsabilidade pelas atividades de
ER em sistemas baseados em AM, com notebooks sendo a principal
ferramente para documentação de requisitos. Os principais desafios
a compreensão do problema e do negócio, a gestão de expectativas,
requisitos pouco claros e a falta de disponibilidade e envolvimento
de especialistas no domínio. No que refere aos RNFs, os profissio-
nais destacaram preocupações específicas, como a qualidade dos
dados, a confiabilidade do modelo e a explicabilidade do modelo,
embora ainda falte aplicação prática sobre como lidar com esses
requisitos.
Por outro lado, a pesquisa de Theis et al. [38] realizou uma análise
estruturada da literatura e reportou os resultados de 48 artigos. Os
autores relacionaram a explicabilidade com perspectivas técnicas e
humanas no desenvolvimento de sistemas de IA, visando torná-los
compreensíveis, aceitáveis e confiáveis, especialmente em sistemas
críticos como tráfego aéreo. Os estudos analisados forneceram in-
sights sobre o que é necessário para que as pessoas percebam uma
IA como explicável, as informações necessárias para aceitação da
IA e os métodos de representação e interação que promovem a
confiança na IA. A pesquisa bibliográfica foi realizada de forma
abrangente nas bases de dados Web of Science e Google Scholar, no
repositório DLR eLib, no DLR Library Catalog, na NASA Technical
Report Server, no Ebook Portal Central e no banco de dados das
bibliotecas nacionais alemãs. Foram utilizados os seguintes termos
de pesquisa ((explicabilidade OR rastreabilidade OR aceitação) AND
(inteligência artificial) AND (raciocínio OR resolução de problemas
OR representação do conhecimento OR planejamento automático OR
programação automática OR percepção de máquina OR visão compu-
tacional OR robótica OR computação afetiva)). Os resultados indicam
que os dois principais grupos de usuários são desenvolvedores, que
necessitam de informações sobre as operações internas do modelo,
e usuários finais, que precisam de informações sobre os resultados
ou comportamento da IA. Além disso, os autores concluíram que
a aceitação dos sistemas de IA depende de informações sobre as
funções e o desempenho do sistema, de considerações éticas e de
privacidade, bem como de informações adaptadas às preferências
individuais para estabelecer confiança no sistema. Para atingir esses
objetivos, é fundamental identificar as necessidades dos usuários
e transformá-las em requisitos para o projeto do sistema de IA,
constituindo uma etapa inicial para a ER. Esses requisitos devem
ser validados e refinados para diferentes domínios de aplicação ,
servindo de base para as atividades de desenvolvimento.
Nosso estudo de MSL concentra-se particularmente no RNF de ex-
plicabilidade para sistemas baseados em AM. Exploramos aspectos
relacionados à ER, incluindo a identificação de metodologias, ferra-
mentas e notações de modelagem aplicadas a esses sistemas. Este
enfoque busca uma compreensão abrangente das práticas atuais e
a identificação de lacunas que possam orientar futuras pesquisas.
PROTOCOLO DE MAPEAMENTO
SISTEMÁTICO
Para este estudo, utilizamos um protocolo de pesquisa que nos
permitiu, por meio de MSL, averiguar os métodos, ferramentas,
técnicas, abordagens e processo da ER para alcançar a explicabili-
dade em sistemas baseado em AM. O estudo seguiu as diretrizes
propostas por Scannavino et al. [33]. Utilizamos a ferramenta Par-
sif.al4, uma ferramenta online de apoio à realização de revisões
sistemáticas da literatura. Para mais detalhes sobre o protocolo e a
extração dos dados, consulte o link disponível5.
A Figura 1 representa o MSL e inclui as fases de planejamento,
condução e publicação. Primeiramente, um protocolo é planejado
para que possa ser reproduzido posteriormente. Este protocolo
inclui questões de pesquisa, string de busca, estratégia de pesquisa,
fontes de estudos e critérios de seleção estudos e formulário de
extração.
Figura 1: Fases e atividades do MSL.
Inicialmente, verificamos a necessidade de um MSL para este
tema, dado que a literatura ainda é pouco explorada nessa área
específica. Em seguida definimos as Questões de Pesquisa (QPs)
fundamentais que guiaram nosso estudo, estabelecemos a string de
busca para encontrar estudos relevantes, e determinamos critérios
de inclusão e exclusão para seleção dos trabalhos.
3.1
Questões de Pesquisa
O principal objetivo desta pesquisa é investigar o estado da arte de
ER em relação ao requisito de explicabilidade em sistemas baseados
em AM. As questões de pesquisa formuladas derivaram deste ob-
jetivo, buscando esclarecer a proposta da ER para o requisito de
explicabilidade, e quais os desafios e limitações enfrentados na inte-
gração deste requisito em sistema de AM. As seguintes questões de
pesquisa (QPs) e seus objetivos foram definidas para este propósito:
4https://parsif.al/
5https://doi.org/10.5281/zenodo.13227993
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
QP1. Qual é o estado da arte da Engenharia de Requi-
sitos (ER) em relação ao requisito de explicabilidade
aplicado aos sistemas de AM?
O objetivo da QP1 é investigar frameworks, notações, lingua-
gens de modelagem e ferramentas da ER para o desenvolvimento e
definição de requisitos que garantam a explicabilidade em sistemas
baseados em AM. Além disso, procurou-se identificar domínios
mais preocupados em garantir a explicabilidade. Também foram
investigados quais aspectos estão associados à explicabilidade e
como os estudos têm conduzido suas pesquisas.
QP2. Quais são os desafios e lacunas apontados pela
literatura da ER para sistemas de AM em relação ao
requisito de explicabilidade?
O objetivo da QP2 é verificar os desafios e possíveis caminhos
para pesquisas futuras sobre ER em relação ao requisito de explica-
bilidade em sistemas baseados em AM.
3.2
String de Busca
Para limitar a busca dentro do escopo da ER, explicabilidade e AM,
elaboramos uma string de busca em três partes. A primeira parte
incluiu termos relacionados ao AM e seus sinônimos, a segunda
parte focou em ER e seus sinônimos, e a terceira parte abardou o
termo explicabilidade e seus sinônimos. Através de testes piloto,
verificamos quais palavras-chave eram mais citadas nos estudos que
tinham relação com os termos da nossa string de busca, conforme
segue:
• Aprendizado de Máquina, notamos que palavras como: ma-
chine learning, artificial intelligence, neural network, reinfor-
cement learning and deep learning, eram palavras que indica-
vam termos relacionados.
• Engenharia de Requisistos: os termos foram definidos de
acordo com o SWEBOK6, tais como: requirements enginee-
ring, requirements analysis, requirements elicitation, require-
ments management, requirements specification, requirements
validation, requirements modeling, requirements process.
• Explicabilidade: explainability, XAI, interpretability, explai-
nable, interpretable and trustworthy AI, faziam referência ao
termos explicabilidade.
Em seguida, realizamos diversas combinações em testes piloto
utilizando as strings e suas respectivas partes. Este processo foi es-
sencial para identificar a formulação mais eficaz da string de busca,
que fornecesse evidências em relação ao nosso objetivo de pesquisa.
Após várias iterações e refinamentos, definimos a string de busca
final. Esta string foi aplicada aos títulos, resumos e palavra-chaves
dos estudos selecionados, garantindo precisão na identificação de
trabalhos. A string definida foi: ((machine learning OR artificial in-
telligence OR neural network) AND (requirements engineering OR
requirements analysis OR requirements elicitation OR requirements
6https://www.computer.org/education/bodies-of-knowledge/software-engineering
management OR requirements specification OR requirements vali-
dation OR requirements modeling OR requirements process) AND
(explainability OR XAI)).
3.3
Estratégia de busca
A estratégia de busca adotada neste estudo foi a busca automática
em bases bibliográficas. De maneira geral, para que um MSL for-
neça uma visão ampla de um dado tópico de pesquisa, é necessário
realizar buscas automáticas em diferentes bases de dados relevantes
[25]. Dada a natureza emergente e ainda pouco explorada no con-
texto da ER, acreditamos que a utilização de buscas automáticas nos
permitirá explorar de forma eficaz a literatura disponível sobre ER e
explicabilidade em sistemas de AM. Este método é particularmente
adequado para identificar estudos primários, fornecendo uma base
sólida para análise e síntese dos dados.
Selecionamos seis bases bibliográficas e motores de busca. De
acordo com [25], essas bases podem ser úteis para obtenção de estu-
dos no domínio de Engenharia de Software, e consequentemente da
ER. São elas: Engineering Village, IEEE Digital Library, Science Direct,
Scopus, Springer Link e Web of Science. Demais bases de dados, como
a ACM Digital Library, não foram incluídas, pois, na data da realiza-
ção deste estudo (março de 2024), não estavam disponíveis através
do Portal de Periódicos da CAPES, que é a plataforma utilizada para
acessar estudos científicos.
3.4
Critérios de seleção
Definimos a busca dos estudos publicados entre os anos de 2019 a
2024. Formulamos critérios de seleção que consistiram em critério
de inclusão (CI) e critério de exclusão (CE). Os critério de seleção,
aplicados em todos os artigos que retornaram de acordo com a
string de busca, são apresentados na Tabela 1.
Tabela 1: Critérios de Inclusão (CI) e Exclusão (CE) do MSL.
Critério
Descrição
Estudos primários sobre ER que abordam
explicabilidade em sistemas de AM
CE1
Não se trata de estudo primário
CE2
Não se trata de um artigo (por exemplo, prefácio
ou resumo de periódicos ou anais de conferências
CE3
O texto completo do estudo não está em inglês
CE4
Texto completo indisponível gratuitamente na Web
ou na plataforma de periódico CAPES
CE5
É um versão preliminar ou resumida de outro
estudo já incluso
CE6
A pesquisa não aborda Engenharia de Requisitos
CE7
A pesquisa não faz uma relação da ER para
explicabilidade
CE8
O estudo não está no limite de publicação
entre 2019-2024
3.5
Formulário de extração
A atividade de extração de dados requer um formulário cujos cam-
pos devem ser mapeados para as questões de pesquisa. Esses campos
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade em Sistemas Baseados em Aprendizado de Máquina
WRSL+’2024, Juiz de Fora/MG, Brazil
são preenchidos durante a leitura do texto completo de cada artigo.
Elaboramos o formulário de extração de dados baseado nas QPs. A
Tabela 2 apresenta o mapeamento entre os campos do formulário e
as questões de pesquisa.
CONDUÇÃO DA SELEÇÃO DE ESTUDOS
Nesta seção descrevemos a etapa de seleção dos estudos primários.
A primeira etapa consistiu na busca de artigos utilizando a string
de busca, aplicadas aos títulos, resumos e palavras-chave nas bases
bibliográficas e motores de busca selecionados em março de 2024,
que retornou 200 artigos, como descreve a Tabela 3. Identificamos
e removemos 70 artigos duplicados (do grupo de 200 estudos) com
o apoio da ferramenta Parsif.al.
Tabela 3: Número de artigos por fonte incluindo duplicados.
Base bibliográfica
Artigos retornados
Artigos aceitos
Engineering Village
IEEE Digital Library
Science Direct
Scopus
Springer Link
Web of Science
Total
Na segunda etapa, com a exclusão dos artigos duplicados, lemos
o título, resumo e palavras-chave de cada um dos 130 artigos, sobre
os quais aplicamos CI e CE e eliminamos 91 artigos (ver Tabela 4).
Para decidir quais estudos seriam incluídos ou excluídos, aplicamos
os CI e CE. Neste processo, dois pesquisadores revisavam os estu-
dos de maneira independente e decidiam por incluir ou excluir o
artigo. Em casos de incerteza, o artigo recebia uma anotação com a
palavra “dúvida”. Nesses casos, um terceiro pesquisador, com maior
experiência, realizava uma verificação adicional para determinar a
inclusão ou a exclusão do artigo. Sempre que surgiam divergências
nas decisões, os pesquisadores se reuniam para discutir e chegar a
um consenso. Como resultado, selecionamos 39 artigos.
Durante a terceira etapa, no processo de extração de dados a
partir da leitura completa dos artigos, quatro estudos foram elimi-
nados pelo CE4, restando 35 estudos considerados como relevantes.
Além disso, os pesquisadores identificaram que havia estudos que
não se enquadravam no escopo definido. Esta discrepância ocorreu
porque na etapa de seleção os artigos pareceriam indicar propostas
adequadas à nossa pesquisa. Após relatar a situação ao pesquisador
mais experiente, outros oito artigos foram eliminados pelo CE7,
totalizando 103 estudos excluídos. Conforme descrito na Tabela 4,
o critério CE6 excluiu a maior parte dos artigos. Isso significa que
muitos estudos não focavam na ER para o requisito de explicabi-
lidade em sistemas baseados em AM. Esse resultado sugere uma
necessidade de maior clareza e detalhamento nas discussões sobre
explicabilidade dentro do contexto da ER. Além disso, 32 artigos
foram eliminados pelo CE6, pois não abordavam ER. Por último, 21
artigos foram excluídos por serem estudos secundários, mas que
não tinham os mesmos objetivos deste estudo, conforme CE1, e um
artigo pelo CE8, publicado em 2018.
No geral, extraímos informações de 27 artigos, estudos esses
identificados ao longo desta pesquisa como E1 a E27 (ver Tabela
5). A Figura 2 retrata todo o processo de condução do MSL com o
respectivo número de estudos primários escolhidos e removidos
em cada atividade.
RESULTADOS E DISCUSSÃO
Esta seção apresenta a análise e síntese dos dados extraídos dos 27
estudos para responder às QPs do MSL.
5.1
Sobre a questão de pesquisa 1
QP1. Qual é o estado da arte da Engenharia de Requisitos (ER) em
relação ao requisito de explicabilidade aplicado aos sistemas de AM?
A maioria dos artigos (20) é proveniente de conferências e workshops
internacionais de grande relevância para a comunidade da ER, como
Requirements Engineering Conference Workshops (REW), Internatio-
nal Requirements Engineering Conference (RE) e International Confe-
rence on Software Engineering. Isso sugere que o tema que relaciona
ER, explicabilidade e AM, é de significativo interesse para a co-
munidade acadêmica. Os demais artigos (7), foram publicados em
revistas da área de computação de modo geral. Além disso, a relação
entre ER, explicabilidade e AM tem atraído grande atenção na co-
munidade científica. Nos últimos anos, esse tema ganhou destaque
nas pesquisas (ver Figura 3).
Verificamos quais atividades da ER, incluindo elicitação, análise,
especificação, validação e gerenciamento, eram abordadas pelos
estudos analisados. Alguns desses estudos forneceram uma visão
geral da ER aplicada à explicabilidade em sistemas baseados em
AM e não contribuíram com técnicas específicas. Estes estudos
incluíam documento de visão (de negócio), artigos de opinião e
entrevistas tanto da indústria quanto da academia. Outros estudos
contribuíram com ontologias, arquiteturas, metamodelos, modelos
e frameworks concentrando-se mais nas primeiras atividades da
ER, particularmente na elicitação e análise. A Figura 4 apresenta
um gráfico que ilustra a quantidade de estudos focados em cada
atividade ou grupo de atividades da ER.
Elicitação. A elicitação foi a atividade mais destacada, conforme
demonstrado em: [6, 8, 9, 12, 24, 27, 34, 36, 41], correspondendo aos
artigos E2, E6, E15, E26, E20, E12, E1, E17 e E13. Embora nem todos
os estudos tenham apresentado técnicas de elicitação definidas,
eles forneceram considerações importantes sobre as preocupações
a serem abordadas ao elicitar requisitos de explicabilidade para
sistemas baseados em AM.
Os estudos E1, E6, E15 e E26, identificaram a técnica de entrevista
como uma possibilidade para elicitar requisito de explicabilidade.
Isso se deve ao fato de que esses estudos utilizaram a avaliação
empírica por meio de estudos de caso na indústria. Esses estudos
sugerem que entrevistas diretas com especialistas do domínio po-
dem ser uma abordagem eficaz para compreender os requisitos
de explicabilidade. No trabalho E1 de Schellingerhout et al. [34],
foram determinados os stakeholdersque devem ser envolvidos no
desenvolvimento de sistemas de IA, como Especialistas em Regula-
mentos e Engenheiros de IA, e suas relações. A explicabilidade foi
apresentada como um RNF que impacta diretamente o escopo do
projeto desses sistemas e deve ser uma preocupação desde o início
do projeto, ou seja, durante a elicitação de requisitos. A pesquisa E6
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
Tabela 2: Informações do formulário de extração relacionado às QPs.
Informação
QP
Descrição
Metadados
Informações como o título do artigo, autores, data e veículo de publicação
Atividade da ER
Atividades da ER que o estudo aborda, como elicitação, análise, especificação,
gerenciamento e validação
Técnicas e métodos associados
a cada atividade da ER
Contribuição dos estudos em relação às técnicas e métodos
Domínio da aplicação
Informação sobre o cenário que existia uma preocupação da explicabilidade em sistemas
baseados em AM
Tipos de requisitos abordados
com a explicabilidade
Informação sobre quais requisitos estavam associados à explicabilidade
Stakholders considerados no estudo
em relação à explicabilidade
Codificado em: engenheiros de requisitos, especialistas de domínio, profissional de AM e/ou
usuários finais
Tipo de contribuição
A contribuição foi codificado em: arquitetura, ferramenta, framework, metamodelo, modelo,
método, métrica, ontologia, processo ou taxonomia
Estratégia de pesquisa
Classificação de estratégia empírica, de acordo com \cite{scannavino2017revisao}, incluindo
estudos de casos, experimentos controlado, survey e simulação
Tipos de pesquisa
Classificação do tipo de pesquisa de acordo com \cite{scannavino2017revisao}, incluindo
artigo de opinião, documento de visão, pesquisa de avaliação, pesquisa de validação,
proposta de solução e relato de experiência
Limitação em relação a ER e a
explicabilidade
Informações sobre as limitações em relação a ER e a explicabilidade em sistemas de AM
Trabalho Futuro
Tópicos de ER que apontam para trabalhos futuros
Figura 2: Visão detalhada da fase de condução: estudos primários selecionados e removidos em cada atividade.
Tabela 4: Número de artigos excluídos por critério de exclusão
Leitura
metadados
Extração
de dados
Total
Critérios de
exclusão
CE1
CE2
CE3
CE4
CE5
CE6
CE7
CE8
Total
de Cabour el al. [8], propõe uma arquitetura que permite articular
sistematicamente as explicações necessárias para os usuários finais
em uma determinada tarefa, juntamente com os recursos XAI. Em
E26 de Cirqueira et al.[12], utilizaram as ciências cognitivas para
demonstrar um método de elicitação de requisitos baseado em ce-
nários. No estudo E15 de Chazette et al. [9], foi desenvolvido um
catálogo que abrange diversos aspectos de qualidade influenciados
positiva e negativamente pela explicabilidade, o qual foi validado
por especialistas.
Na pesquisa E2 de Aslam et al. [6], elaboraram uma proposta
de solução através de Modelos Mentais para elicitar requisitos de
explicabilidade e desenvolveram um Modelo Conceitual Orientado
à Ontologia para facilitar o processo de aprendizagem dos usuários
para uma melhor compreensão das explicações. Por outro lado, a
pesquisa E13 de Vogelsang [41], propõe um framework para soft-
ware autoexplicável, capaz de responder, em tempo de execução, a
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade em Sistemas Baseados em Aprendizado de Máquina
WRSL+’2024, Juiz de Fora/MG, Brazil
Tabela 5: A relação dos 27 estudos analisados neste MSL.
Estudo
Referência
E1
A Co-design Study for Multi-stakeholder Job Recommender System Explanations
[34]
E2
A Conceptual Model Framework for XAI Requirement Elicitation of Application Domain System
[6]
E3
A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)
[37]
E4
A Requirements Engineering Perspective to AI-Based Systems Development: A Vision Paper
[18]
E5
An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-Oriented Systems
[29]
E6
An explanation space to align user studies with the technical development of Explainable AI
[8]
E7
An extension of iStar for Machine Learning requirements by following the PRISE methodology
[7]
E8
An ontology-based approach to engineering ethicality requirements
[19]
E9
Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric
Approach for Explainability Requirements
[20]
E10
Dealing with Explainability Requirements for Machine Learning Systems
[28]
E11
Explainability as a Non-Functional Requirement
[26]
E12
Explainability Auditing for Intelligent Systems: A Rationale for Multi-Disciplinary Perspectives
[27]
E13
Explainable software systems
[41]
E14
Explainable software systems: from requirements analysis to system evaluation
[10]
E15
Exploring Explainability: A Definition, a Model, and a Knowledge Catalogue
[9]
E16
Holistic Explainability Requirements for End-to-End Machine Learning in IoT Cloud Systems
[31]
E17
How to Evaluate Explainability? - A Case for Three Criteria
[36]
E18
Human-centered XAI: Developing design patterns for explanations of clinical decision support
systems
[35]
E19
Non-functional requirements for machine learning: understanding current use and challenges
among practitioners
[21]
E20
On the Relation of Trust and Explainability: Why to Engineer for Trustworthiness
[24]
E21
Quality Characteristics of a Software Platform for Human-AI Teaming in Smart Manufacturing
[22]
E22
Quality-Driven Machine Learning-based Data Science Pipeline Realization: a software
engineering approach
[15]
E23
Requirements Engineering for Explainable AI
[17]
E24
Requirements engineering for machine learning: Perspectives from data scientists
[42]
E25
Revisiting the Performance-Explainability Trade-Off in Explainable Artificial
Intelligence (XAI)
[14]
E26
Scenario-Based Requirements Elicitation for User-Centric Explainable AI: A Case in
Fraud Detection
[12]
E27
XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine
Learning
[44]
perguntas sobre seu comportamento passado, presente e futuro. Em-
bora genérico, este estudo relaciona a ER na elaboração de técnicas
para a explicabilidade no contexto da elicitação.
Além disso, três dos estudos foram caracterizados como docu-
mentos de visão. Em [27], Langer et al., estudo E12, apresentaram
uma lista para prever como diferentes perspectivas podem se unir
para garantir a explicabilidade de sistemas de AM. Para cada pers-
pectiva, foram apresentados critérios que devem ser considerados
para realizar uma auditoria. Este estudo fornece insights sobre a
preocupação em elicitar requisitos técnicos, psicológicos, legais e
éticos de diferentes stakeholders para garantir uma análise abran-
gente da explicabilidade. Na pesquisa E17 de Speith [36], o autor
motiva e defende três critérios de qualidade da informação que
os sistemas devem fornecer para serem considerados explicáveis:
compreensibilidade, fidelidade e avaliabilidade. O autor examinou
algumas abordagens XAI, como LIME, e avaliou as informações que
elas produzem com bases nos critérios estabelecidos. Em relação
ao LIME, os critérios de compreensibilidade a avaliabilidade indica-
ram uma avaliação positiva da explicabilidade. No entanto, o autor
deixa claro que este estudo faz apenas um levantamento de ques-
tões teóricas, uma vez que há pouco ou nenhum acordo sobre os
métodos de avaliação em explicabilidade. Nesse sentido, avaliando
por uma perspectiva técnica, este estudo permite entender qual
abordagem XAI oferece os critérios citados para um determinado
contexto, auxiliando na elicitação do requisito de explicabiliade. Por
último, E20 de Kästner et al. [24] trás uma reflexão sobre a asso-
ciação entre confiança e explicabilidade. Os autores argumentam
que, para tornar um sistema baseado em AM confiável, garantir a
explicabilidade pode ser extremamente útil e, portanto, de grande
importância. Nesse sentido, deve-se avaliar como alcançar o critério
de confiança por meio da explicabilidade já no início do projeto, na
fase de elicitação.
Análise. Em relação a atividade de análise da ER, o trabalho E5 de
Metzger et al.[29], fornece uma arquitetura de chatbot (Chat4XAI)
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
Figura 3: Distribuição de publicação entre os anos de 2019-
2024 sobre ER, explicabilidade e AM.
que gera explicações em linguagem natural. Os autores se preocu-
param com a Lei Regulamentar da IA da União Europeia (UE) e
destacaram que a linguagem natural facilita uma melhor compreen-
são para usuários não técnicos. O Chat4XAI não exige a elicitação
antecipada de requisitos e a classificação refinada das questões que
os usuários possam fazer, pois aspectos mais específicos são tra-
tados naturalmente por um grande modelo de linguagem (LLM)
subjacente.
Embora os autores afirmem que a elicitação não seja uma exigên-
cia, os resultados demostraram que o Chat4XAI com engenharia de
prompt apresenta desempenho superior ao Chat4XAI com prompt
zero-shot (que depende exclusivamente da capacidade do modelo de
generalizar a partir de seu treinamento inicial). O estudo também
comparou a fidelidade do Chat4XAI com a eficácia dos engenheiros
de software, ou seja, o quão bem os engenheiros de software foram
capazes de entender a tomada de decisão de um determinado mo-
delo de AM comparado ao Chat4XAI. Os resultados demostraram
que Chat4XAI superou os engenheiros de software ao responder
corretamente às perguntas sobre as explicações da tomada de deci-
são. Este estudo demostra que, mesmo usando LLMs, é necessário
ter um corpus relacionado ao contexto do domínio para se ter uma
melhor eficácia nas explicações fornecidas. Ao avaliar o estudo,
percebemos que um especialista em engenharia de requisitos deve
participar do entendimento da necessidade de um corpus de do-
mínio específico, o que enfatiza a importância de incorporar esse
entendimento na atividade de análise de requisitos.
Em Li e Han [28], estudo E10, os autores propõem uma estrutura
de análise de requisitos de explicabilidade em sistema de AM , utili-
zando modelos de objetivos contextuais para derivar métodos XAI
de forma sistemática e automática. Os autores usaram a linguagem
de modelagem iStar7 para modelar um framework de explicabi-
lidade. Especificamente, pesquisaram e analisaram métodos XAI
existentes com o intuito de recomendá-los a desenvolvedores de
sistemas de AM. Este estudo utiliza uma abordagem baseada em
objetivos para modelar requisitos de explicabilidade, focando no
que as técnicas XAI visam oferecer. A investigação analisou a usa-
bilidade da estrutura para desenvolvedores da pós graduação com
vários níveis de conhecimento de explicabilidade. Neste estudo, a
7https://http://istarwiki.org/
explicabilidade focou exclusivamente em métodos XAI, abordando
apenas os aspectos técnicos do projeto, como os desenvolvedores.
Os demais stakeholders, que podem incluir usuários finais, especia-
listas de domínio e reguladores, não foram envolvidos no processo,
o que limita a abrangência e a aplicabilidade das soluções propostas.
Elicitação e análise. Essas foram atividades integradas nos estudos
[7, 10, 14, 19, 20, 26, 31, 44], correspondendo ao estudos E7, E14, E25,
E8, E9, E11, E16 e E27. Estes estudos contribuíram com metamodelos,
ontologias e frameworks.
No estudo E7 de Barrera et al. [7], os autores apresentaram um
extensão do iStar como um metamodelo para projetos de AM, per-
mitindo identificar RNF, selecionar algoritmos mais adequados e
analisar as fontes de dados disponíveis, alinhando-os aos objetivos
do projeto. Eles incluíram um conjunto de diretrizes para facilitar
sua aplicação, como um questionário que orienta especialistas em
AM em entrevistas com especialistas do domínio. Além disso, forne-
ceram um conjunto de tabelas que refletem como os modelos de AM
existentes contribuem para RNFs e quais métricas de AM podem
ser usadas para cada tipo de objetivo. Entre os RNF, destacaram
a explicabilidade como importante nesse contexto. No entanto, o
estudo não aborda todas as preocupações relacionadas à explicabili-
dade, concentrando em algumas tarefas de AM, como classificação,
regressão e clustering, que são modelos mais clássicos de AM, e não
incluíram áreas como Deep Learning. Além disso, os autores se pre-
ocuparam apenas com os métodos XAI que fornecem explicações
técnicas, não avaliando aspectos mais amplos da explicabilidade
que poderiam ser relevantes para diferentes stakeholders.
Na pesquisa E16 de Nguyen et al. [31], apresentaram uma abor-
dagem holística para a cobertura dos requisitos de explicabilidade, a
partir da definição dos stakeholders. Os autores mapearam as etapas
para elicitar requisitos de explicabilidade e destacaram a neces-
sidade de: i) identificar partes interessadas individuais concretas,
ii) coletar os requisitos por meio de inquérito ou entrevista, e iii)
continuar a atualizar os requisitos à medida que o desenvolvimento
avança, por exemplo, a cada sprint se o método de desenvolvimento
Ágil for empregado. Os autores ressaltam que ainda faltam técnicas
e ferramentas para gerenciar a explicabilidade em contexto de sis-
temas baseados em AM, como plataforma de gerenciamento para
acompanhar a documentação de requisitos, validar teste e entender
como explicar questões relevantes em termos de conjunto de dados
aos stakeholders. Tanto o estudo E7 quanto E16 realizaram uma
pesquisa de avaliação com estudo de caso na indústria.
No artigo E8 de Guizzardi et al. [19], os autores apresentaram a
Engenharia de Requisitos Baseada em Ontologia (ObRE) como um
método para elicitar e analisar requisitos éticos, focando em dois
importantes princípios de eticidade: explicabilidade e autonomia.
A ontologia proposta foi validada para verificar se atende aos ob-
jetivos estabelecidos pela UE para o desenvolvimento de sistemas
éticos. Utilizando um cenário de carro autônomo, os autores criaram
tabelas de requisitos para garantir a conformidade com os princí-
pios de explicabilidade e autonomia. Adicionalmente, empregaram
um modelo de metas baseado no iStar, que retrata a dependência
de cada um dos stakeholders e do carro autônomo. Embora o estudo
demonstre uma preocupação significativa com requisitos éticos
definidos pela UE, abordando a explicabilidadee propondo uma on-
tologia para elicitação e análise de requisitos de tais requisitos, os
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade em Sistemas Baseados em Aprendizado de Máquina
WRSL+’2024, Juiz de Fora/MG, Brazil
Figura 4: Atividades da ER apoiadas nos 27 artigos analisados neste MSL.
autores não implementou e nem validou a abordagem por meio de
estudos de caso reais no domínio dos sistemas éticos, nem avaliou
os resultados com especialistas.
Os autores Chazette et al.[10], estudo E14, realizaram uma pes-
quisa de validação por meio de estudo de caso acadêmico e desen-
volveram quatro artefatos: uma definição de explicabilidade, um
modelo conceitual, um catálogo de conhecimento e um modelo de
referência para sistemas explicáveis. Esses artefatos visam apoiar
engenheiros de software e de requisitos na compreensão da defi-
nição de explicabilidade e na sua interação com outros aspectos
de qualidade no desenvolvimento de sistemas baseados em AM.
No entanto, a correção dos modelos elaborados não foram avali-
ados e mais estudos são necessários para validar os modelos na
prática. O estudo de E27 de Zöller et al.[44], enfatizou a importân-
cia do engenheiro de requisitos no desenvolvimento de sistemas
baseados em AM, destacando a relação entre explicabilidade e de-
sempenho. Os autores validaram uma ferramenta de análise visual
intitulada eXplainable Automated Machine Learning (XAutoML),
que visa compreender e validar procedimentos do AutoML (fer-
ramenta que automatiza cada etapa do fluxo de trabalho de AM),
tornando as otimizações do AutoML transparentes e explicando os
modelos de AM produzidos.
O XAutoML permite a análise e compreensão de pipelines de
AM, auxiliando profissionais de AM e da ER na avaliação da relação
entre desempenho e explicabilidade. O estudo coletou e analisou
requisitos de explicabilidade de uma base diversificada de usuários,
incluindo 16 cientistas de dados, 11 especialistas de domínio e 9
pesquisadores de AutoML. Em relação à ER, a pesquisa revelou que,
mesmo dentro de apenas um único grupo de usuários, como os
especialistas de domínio, não há uma preferência clara sobre quais
explicações devem estar disponíveis. Isso pode expor os usuários a
informações indesejadas. Por outro lado, os pesquisadores de Au-
toML solicitaram explicações de componentes internos específicos
para depuração detalhada do sistema. Nesse sentido, um profissio-
nal da engenharia de requisitos poderia avaliar quais explicações
seriam relevantes para determinados stakeholders. No entanto, a
ferramenta precisa ser aprimorada para fornecer as interações co-
muns entre stakeholders e os sistemas AutoML em uma estrutura
padronizada que acople análise visual otimizada. Uma limitação sig-
nificativa é que o estudo não aborda completamente como balancear
a necessidade de explicações de diferentes grupos de usuários, o
que pode levar a dificuldades na implementação prática de soluções
de explicabilidade.
E por fim, três estudos desenvolveram proposta de solução. Em
[20], Habiba et al., artigo E9 desenvolveram um framework com
ênfase em uma abordagem centrada no usuário para requisitos de
explicabilidade em AM. O estudo E11 de Köhl et al. [26], fornece-
ram um catálogo baseado em diferentes noções de explicabilidade
e requisitos de alto nível que as pessoas têm em mente quando
exigem explicabilidade. Os autores enfatizaram a compreensão, uti-
lizando uma abordagem multidisciplinar que inclui resultados da
psicologia e das ciências cognitivas para avaliar se algo é realmente
uma explicação e como as pessoas reagem a diferentes tipos de
explicações. Os autores Crook et al. [14], do estudo E25 destacaram
que os métodos XAI devem ser considerados pelos engenheiros de
requisitos ao decidir sobre os modelos de AM na etapa de elicitação.
Eles ressaltam que vários pontos devem ser considerados, como re-
cursos de desenvolvimento, detalhes de domínio e riscos. Com base
nisso, desenvolveram um framework que explora o compromisso
entre desempenho, explicabilidade e tempo no contexto da IA a
partir da perspectiva da ER. Essas propostas de solução exploram
especialmente as atividades de elicitação e análise de requisitos de
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
explicabilidade. Embora apresentem soluções para explicabilidade
no contexto de sistemas de AM, são apenas propostas de solução e
carecem de experimentação em casos reais.
Elicitação, análise, especificação e validação. Estudos que trataram
este conjunto de atividades da ER, apresentaram algumas carac-
terísticas em comum. Os estudos E19 e E21 [21, 22] realizaram
entrevistas para entender características e desafios da ER para sis-
temas baseado em AM. Em E19 validaram o estado da prática em
relação aos RNFs de sistemas de AM, por meio de entrevistas com
profissionais da academia e da indústria. Diversos RNFs foram iden-
tificados, incluindo a explicabilidade, que se destacou como uma
preocupação nas principais atividades da ER. Como resultado, o
grupo profissional destacou o RNF de desempenho, enquanto o
grupo acadêmico enfatizou a explicabilidade. Na pesquisa E21 , os
autores entrevistaram profissionais da indústria para examinar a
relevância de 11 características de qualidade para a formação de
equipes humano-IA no desenvolvimento de sistemas de IA. As ca-
racterísticas de qualidade incluíam as 11 características do padrão
ISO 25010:2011 para qualidade de software (SQuaRE) e 3 caracterís-
ticas de qualidade específicas de IA: confiabilidade, explicabilidade e
auditabilidade. Neste estudo, a explicabilidade não foi um requisito
destacado, sendo o desempenho uma preocupação mais proemi-
nente. Estes estudos demonstram que a indústria ainda tem uma
preocupação maior com o requisito de desempenho em comparação
à explicabilidade. Isso sugere que os profissionais de desenvolvi-
mento tendem a priorizar o desempenho, possivelmente devido à
falta de profissionais de engenharia de requisitos que possam for-
necer um balanço adequado entre explicabilidade e desempenho. A
pesquisa E24 de Vogelsang e Borg [42], entrevistaram profissionais
da indústria (cientistas de dados) e os resultados mostraram que
esses profissionais tomam muitas decisões visando melhorar seus
modelos de AM, focando no desempenho. Para essa tarefa, utilizam
e referem-se a conceitos e medidas técnicas que muitas vezes não
são bem compreendidos pelos clientes. Os autores concluíram que
mudanças no paradigma de desenvolvimento, como no caso de
sistemas baseados em AM, também exigem mudanças na ER. Assim,
o desenvolvimento de sistemas de AM exige que os engenheiros
de requisitos estejam cientes dos novos requisitos de qualidade,
como a explicabilidade, e que extraiam esses requisitos do ponto de
vista do usuário. Este estudo corrobora com os estudos E19 e E21 ,
onde demonstram que os profissionais da indústria ainda têm uma
grande preocupação com o desempenho dos modelos. Portanto, há
uma demanda por engenheiros de requisitos que sejam capazes de
basear suas decisões e conceitos técnicos em uma compreensão e
análise completas das necessidades e do contexto dos clientes.
Em E18, Schoonderwoerd et al. [35], realizaram uma pesquisa
de avaliação com um estudo de caso na indústria e descreveram
uma abordagem de melhores práticas para design de um método
XAI centrado no ser humano chamado DoReMi. Avaliaram a ex-
plicabilidade por meio de interfaces de usuário (IU) e conduziram
um estudo de caso usando diagnósticos de um sistema de apoio à
decisão clínica. Usaram entrevistas e avaliaram documentos para
elicitar requisitos. Os autores estavam interessados em analisar a
explicabilidade para especialistas de domínio, especificamente no
campo médico. A abordagem DoReMi forneceu o primeiro con-
junto de requisitos de usuário e padrões de design de IU para um
sistema explicável de apoio à decisão em saúde infantil. As avalia-
ções com os médicos mostraram que eles realmente precisam de
explicações sobre os resultados da IA, especialmente para ajudar
a mitigar diagnósticos falsos positivos, evitando ao mesmo tempo
falsos negativos. As IUs devem, portanto, ser projetadas de forma
que as informações apresentadas sejam compreensíveis, ao mesmo
tempo que apoiam o aprendizado sobre o sistema. Por exemplo,
uma das IUs que apresentava a matriz de confusão mostrando a
sensibilidade e a especificidade (métricas utilizadas para avaliar a
qualidade do classificador) do modelo de AM para todos os casos
ou uma quantidade selecionada de casos que receberam o mesmo
diagnóstico, foi considerada como fornecendo a maior informa-
ção sobre a precisão do sistema. Este estudo focou basicamente
nas explicações para especialistas de domínio e demonstrou que
o conhecimento técnico é importante para esse grupo de usuário.
Portanto, é essencial identificar quais interfaces podem aumentar a
compreensão do sistema, garantindo que as explicações técnicas
sejam eficazes e acessíveis.
Os demais estudos desse grupo de atividades da ER são predo-
minantemente propostas de solução, focando em pesquisas que
sugerem investigações e planejamentos futuros. A pesquisa E22 de
d’Aloisio [15], apresenta uma proposta definindo um framework
baseado em modelos que orienta os cientistas de dados no desen-
volvimento de pipelines de AM garantindo requisitos de qualidade,
como a explicabilidade. Nessa mesma perspectiva o estudo E23 de
Umm-e-Habiba [17], propõe um modelo de processo de referência
que serve como um guia para os profissionais abordarem os requisi-
tos de explicabilidade relacionados a sistemas baseados em IA. Por
fim, o estudo E4 de Franch et al. [18], que é um documento de visão,
questiona o papel que a ER deve desempenhar no desenvolvimento
de sistemas baseados em IA, destacando o escopo dos RNFs. Os
autores argumentam que existem novos tipos de RNFs especifica-
mente relacionados a sistemas baseados em IA, cuja relevância se
destaca neste contexto, como confiança, ética e explicabilidade. Eles
enfatizam que a ER se tornará a pedra angular que coordena todas
as funções, atividades e artefatos envolvidos no desenvolvimento
de sistemas baseados em IA.
Validação. O estudo E3 de Speith e Langer [37] apresenta méto-
dos para avaliar abordagens de explicabilidade de acordo com os
aspectos do processo XAI. Com base em modelos que explicam
os principais processos XAI, a proposta é validar informações que
são realmente explicativas, que facilitam a compreensão e satis-
fazem desejos sociais. Os autores argumentam que, para validar
uma explicação, é necessário que ela apresente essas características.
Destacam que, para melhorar a qualidade das informações explica-
tivas, devem ser considerados critérios como compreensibilidade,
fidelidade e avaliabilidade. Este estudo é um documento de visão e,
portanto, como os demais estudos com essa característica, apresenta
apenas uma proposta de solução sem implementações práticas ou
validações empíricas.
Ainda sobre a QP1, apenas dez estudos realizaram pesquisa de
avaliação com estratégia empírica, incluindo estudos de casos na
indústria, workshop com especialistas e questionários aplicados a
profissionais, incluindo E1, E6, E7, E15, E16, E18, E19, E21, E26 e
E24. Outros 5 estudos, E5, E8, E10, E14 e E27 ,realizaram pesquisa
de validação, usando como estratégia estudo de caso acadêmico,
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade em Sistemas Baseados em Aprendizado de Máquina
WRSL+’2024, Juiz de Fora/MG, Brazil
simulação e prova de conceito. Os demais estudos, 12 num total,
focaram em apresentar proposta de solução e documento de visão:
E2, E3, E4, E9, E11, E12, E13, E17, E20, E22, E23 e E25. Além disso, os
estudos relatam preocupação com diferentes perfis de stakeholders
em relação a explicabilidade, isso inclui usuários finais, engenheiros
de requisitos, profissionais de AM e especialistas de domínios.
Em relação ao domínio de aplicação dos estudos, muitos focaram
suas propostas em um contexto geral, enquanto outros exploraram
cenários hipotéticos. Embora não tenhamos encontrado um estudo
específico sobre sistemas multimídia e web, entendemos que os
componentes de AM podem ser integrados a diversos tipos de
sistemas, uma vez que a explicabilidade é um requisito não funcional
(RNF) essencial nesses contextos.
Por exemplo, no estudo E5 de Metzger et al. [29], a adaptação
em sistemas orientados a serviços é discutida por meio de um caso
específico de um sistema de comércio eletrônico adaptativo cha-
mado SWIM, que simula uma loja virtual real. O SWIM permite
a adaptação do sistema para maximizar uma função de utilidade
específica diante de cargas de trabalho variáveis. Essa abordagem
é utilizada como um exemplo concreto para demonstrar a aplica-
ção do Chat4XAI na interpretação das decisões de algoritmos de
aprendizado por reforço profundo.
A necessidade de explicabilidade em sistemas multimídia e web é
crítica para garantir a confiança e a transparência das decisões toma-
das por algoritmos de AM. Quando esses algoritmos são aplicados
em contextos web, como recomendações de conteúdo ou persona-
lização da experiência do usuário em sistemas de e-commerce, é
fundamental que os stakeholders compreendam como e por que
determinadas decisões são tomadas. Isso não apenas aumenta a
confiança do usuário final, mas também facilita a manutenção e
a melhoria contínua dos sistemas, garantindo que eles operem de
maneira esperada e justa.
Como apresentado no estudos analisados, devemos considerar
que os métodos XAI tem uma grande importância no contexto da
explicabilidade. A Figura 5 ilustra os principais componentes da
XAI, proposto por [13]. Basicamente, a XAI é formada por dois
componentes principais: os modelos de AM e os métodos XAI. Na
figura 5, é apresentado um modelo de AM de previsão, onde o
modelo calcula as previsões com base nos dados de treinamento,
enquanto o método XAI é responsável por gerar explicações do
funcionamento interno e das previsões do modelo de AM [13].
Assim, a XAI incorpora dois resultados, previsões e explicações.
No entanto, alguns métodos de explicabilidade foram desenvol-
vidos especificamente para determinados tipos de dados, como
imagens ou dados tabulares, enquanto outros não dependem do
tipo de dados. Além disso, a XAI apresenta abordagens que focam
em explicações relacionadas à interpretabilidade dos modelos de
AM. Uma dessas abordagens é o post-hoc, que analisa e interpreta
o processo de tomada de decisão de um modelo de AM treinado
após ele ter feito previsões, fornecendo insights sobre como o mo-
delo chegou aos seus resultados [32]. Por outro lado, as abordagens
ante-hoc são chamados de explicabilidade intrínseca, modelos trans-
parentes ou de caixa de vidro, e são modelos de AM inerentemente
interpretáveis [32]. Esses métodos se concentram na incorporação
de técnicas de interpretabilidade diretamente na arquitetura do
modelo, como Árvores de Decisão.
Figura 5: Principais componentes da Inteligência Artificial
Explicável
Nesse sentido, é fundamental considerar a XAI e suas particula-
ridades ao derivar requisitos de explicabilidade. Além disso, definir
os stakeholders relevantes no processo de desenvolvimento de um
sistema baseado em AM é crucial, pois diferentes stakeldores re-
querem diferentes tipos de explicações e formas de visualização.
Por exemplo, um engenheiro de AM pode estar mais interessado
em identificar as variáveis mais importantes para entender melhor
o modelo. Por outro lado, um especialista de domínio no campo
médico, pode requerer compreender o caminho percorrido pelo mo-
delo de AM para chegar a um determinado diagnóstico, trazendo
exemplos de outros casos semelhantes.
Portanto, entendemos que não é apenas importante definir o que
explicar e a quem, mas também como explicar. Porém, a maioria
das técnicas e ferramentas existentes da XAI são compreensíveis
principalmente por stakeholders técnicos com experiência em AM,
enquanto abordagens que atendam a todos os stakeholders rele-
vantes são menos pesquisadas. Concluímos que os requisitos de
explicabilidade devem ser definidos considerando os aspectos men-
cionados.
Por fim, o estado da arte da ER em relação ao requisito de ex-
plicabilidade aplicado aos sistemas de AM, revela que é uma área
em crescimento. Nos últimos anos, especialmente em 2022 e 2023,
a área ganhou destaque nas pesquisas. As atividades da ER mais
exploradas foram a elicitação e análise, que são as fases iniciais do
processo de ER. No entanto, a área carece de estudos empíricos,
sendo a explicabilidade uma preocupação emergente em sistemas
que integram AM. A explicabilidade é um RNF que pode contribuir
com outros requisitos importantes em sistemas de AM, como con-
formidade legal, justiça e ética. De modo geral, a explicabilidade
na perspectiva da ER deve ser mais explorada em todas as suas
atividades, incluindo técnicas clássicas já consolidadas em software
tradicionais, para validar se são suficientes ou precisam de adapta-
ção neste contexto. A necessidade de entender como os sistemas
de AM podem ser explicados de maneira clara e compreensível é
urgente, mas a maioria dos estudos até agora tem se concentrado
em abordagens teóricas sem validação em ambientes reais.
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
5.2
Sobre a questão de pesquisa 2
QP2. Quais são os desafios e lacunas apontados pela literatura da ER
para sistemas de AM em relação ao requisito de explicabilidade?
Os desafios apresentados nos estudos estão principalmente re-
lacionados à falta de pesquisas que reflitam a explicabilidade em
ambientes reais. Dos 27 estudos analisados, 17 deles E2-E5, E8-
E14, E17, E22, E23, E24, E25, e E27, conduziram pesquisas como
documentos de visão, proposta de solução ou mesmo pesquisa de
validação. Essa falta de experimentação prática constitui uma la-
cuna significativa apontada pelos estudos. Isso pode estar associado
devido a crescente preocupação atual da explicabilidade em siste-
mas baseados em AM, que embora essencial em muitos contextos,
ainda está em um estágio inicial de pesquisa. No entanto, a própria
literatura sobre técnicas e abordagens de ER e explicabiliade em
sistemas de AM, ainda não é suficiente. O estudo E2 de Aslam et al.
[6], apresentou que as adoção de técnicas de modelagem conceitual
apresenta desafios de pesquisa devido à literatura limitada sobre as
abordagens interdisciplinares, além da falta de técnicas avançadas
de modelagem essenciais para o seu desenvolvimento. Isso corro-
bora com a falta de estudos que apresentem estratégias empíricas
na indústria.
A questão da generalização foi outro ponto abordado, pois os
resultados obtidos em cenários específicos nem sempre podem ser
aplicados a outros contextos. Por exemplo, no Brasil, é inaceitável
pela lei8, práticas de discriminação de gênero ou raça . Para um sis-
tema de AM utilizado em recrutamento de recursos humanos, essa
informação não deverá ser relevante, porém, gênero e raça podem
ser essenciais em um sistema de apoio à decisão baseado em AM
para aplicações médicas. Nesse sentido, conforme o estudo E25, um
engenheiro de requisitos deve identificar e excluir as características
protegidas que não devem ser utilizadas pelo algoritmo de AM para
discriminar amostras que dependem de cada contexto analisado
[42].
Além disso, uma outra limitação significativa é a falta de enge-
nheiro de requisitos especializados em sistemas de AM. De acordo
com E9, o engenheiro de requisitos desempenham um papel crucial
como mediadores entre os engenheiros de AM e os usuários finais
[20], facilitando a comunicação e a compreensão mútua das neces-
sidade e expectativas. Devido aos diversos stakeholders envolvidos
no desenvolvimento de sistemas de AM, as explicações devem ser
entregues de maneiras diferentes e adaptadas ao público-alvo.
Por exemplo, no cenário de recrutamento de recursos humanos,
os desenvolvedor de AM necessitam de explicações técnicas deta-
lhadas sobre como o modelo tomou suas decisões, permitindo-lhes
analisar o modelo treinado e identificar características importantes
nos dados de treino. Por outro lado, para os candidatos avaliados
pelo sistema, eles precisam de explicações claras e transparentes,
que forneçam informações compreensíveis sobre as razões por trás
da decisão de seleção ou rejeição. Neste cenário, a presença de um
engenheiro de requisitos é essencial para elicitar, analisar e espe-
cificar essas necessidades distintas, garantindo que as explicações
fornecidas sejam adequadas e eficazes para cada grupo de stakehol-
ders. O engenheiro de requisitos deve garantir que os requisitos
de explicabilidade sejam bem definidos e atendam às exigências
legais e éticas, bem como às expectativas dos usuários finais e dos
8https://www.planalto.gov.br/ccivil_03/_Ato2023-2026/2023/Lei/L14611.htm
desenvolvedores de AM. Essa mediação é fundamental para assegu-
rar a confiança e a aceitação dos sistemas de AM pelos diferentes
stakeholders envolvidos.
De modo geral, a QP2 revela que existe um problema de genera-
lização em relação aos contextos onde são empregados os sistemas
baseados em AM. Isto é, os resultados e métodos desenvolvidos
para um contexto específico muitas vezes não são diretamente apli-
cáveis a outros contextos, limitando a eficácia e a adaptabilidade
das soluções de AM. Além disso, destaca-se a falta de engenheiros
de requisitos especializados em AM, o que dificulta ainda mais a
correta elicitação, análise e implementação dos requisitos de expli-
cabilidade nesses sistemas. A ausência de profissionais qualificados
para mediar entre as necessidades dos usuários finais e as capacida-
des técnicas dos sistemas de AM resulta em soluções que podem ser
incompletas. Portanto, para avançar nesta área, é crucial investir
na formação de engenheiros de requisitos com expertise em AM
e desenvolver frameworks e metodologias que facilitem a gene-
ralização e adaptação de soluções de AM a diferentes contextos
operacionais. Essas medidas são essenciais para garantir que os
sistemas de AM possam ser amplamente aplicáveis, confiáveis e
transparentes, atendendo às expectativas e necessidades de diversos
stakeholders.
5.3
Síntese dos resultados
A Figura 6 apresenta o gráfico de bolhas que sintetiza as informações
mais relevantes que extraímos e analisamos dos artigos aceitos
neste MSL. Três eixos de informações compõem esses gráficos
bolhas que mapeiam as fases da ER, o tipo de contribuição e os
requisitos associados à explicabilidade. As atividades de elicitação e
análise concentraram a maior parte das pesquisas, especialmente no
que diz respeito a frameworks e modelos, que fornecem uma base
estruturada para a especificação e análise de requisitos. No entanto,
esses frameworks e modelos, bem como as demais técnicas avaliadas
nos estudos, precisam ser mais explorados em ambientes reais. A
ER oferece um arcabouço de técnicas, métodos e ferramentas para
a elicitação, análise, especificação, validação e gerenciamento de
requisitos. É necessário investigar se esse arcabouço é suficiente
para abordar o RNF de explicabilidade em sistemas baseados em
AM.
Além disso, os estudos demonstram que a explicabilidade pode
contribuir significativamente para aspectos como confiabilidade,
responsabilidade e transparência. Outros requisitos, como confor-
midade legal, justiça, satisfação e ética, também foram mencionados
nos estudos analisados. A relação entre a explicabilidade e esses
requisitos pode ser explorada para projetar a explicabilidade com
base nesses fatores. Essas descobertas confirmam que a explicabili-
dade é essencial para melhorar a confiança e o entendimento entre
o usuário e os sistemas baseados em AM.
CONCLUSÕES E TRABALHOS FUTUROS
De maneira geral, evidenciamos que a explicabilidade é um requisito
de qualidade emergente em sistemas baseado em AM, desafiando
os paradigmas clássicos da ER e levantando questões ainda sem
resposta. Sistemas de software, incluindo multimídia e Web, estão
cada vez mais integrando componentes de AM, impulsionados por
demandas mercadológicas que buscam atender às necessidades e
Estado da Arte sobre Engenharia de Requisitos e Explicabilidade em Sistemas Baseados em Aprendizado de Máquina
WRSL+’2024, Juiz de Fora/MG, Brazil
Figura 6: Gráfico de bolhas que mapeia as fases da ER, os requisitos associados à explicabilidade e o tipo de contribuição.
desejos dos seus clientes. Assim, a integração da explicabilidade
em sistemas baseados em AM não é apenas uma questão técnica,
abrangendo também questões éticas e sociais. Essa integração exige
atenção contínua dos pesquisadores e desenvolvedores para mitigar
os riscos associados e assegurar que a tecnologia beneficie a todos
com equidade.
Com base nos principais resultados do nosso estudo, apresenta-
mos uma síntese sobre os desafios e oportunidades para pesquisas
futuras em ER voltadas para a explicabilidade em sistemas baseados
em AM.
Atividades de ER. Nosso estudo revelou que as atividades de ER
mais abordadas para projetar a explicabilidade em sistemas basea-
dos em AM foram elicitação e análise. Isso sugere que a relação da
ER e explicabilidade ainda está em processo de amadurecimento,
com maior ênfase nas fases iniciais da ER. Por exemplo, vários
estudos contribuíram com frameworks que fornecem uma visão
geral para entender como elicitar componentes de AM, porém fre-
quentemente focando na parte técnica da explicabilidade através
de métodos XAI. Outros estudos abordaram soluções centrados
no ser humano, mas como propostas de solução. Isto indica uma
carência de estudos experimentais que validem essas abordagens
em contextos reais. Portanto, há uma necessidade de mais pesquisa
empírica para explorar a aplicabilidade prática dessas propostas,
de modo a validar sua eficiência em ambientes reais e em diversos
contextos de aplicação.
Aspectos relacionados à explicabilidade. Nosso estudo sinteti-
zou os principais aspectos associados à explicabilidade, incluindo
confiabilidade, responsabilidade, transparência e ética. A nossa aná-
lise sugere que, ao integrar a explicabilidade em sistemas baseados
em AM, é importante avaliar esses aspectos subjacentes de maneira
holística de acordo com o contexto.
Interdisciplinaridade. Trabalhar com explicabilidade exige uma
abordagem interdisciplinar que considera as especificidades do
domínio do problema, as necessidades dos stakeholders revelantes
e implicações, como a ética e leis regulamentadoras. É essencial
entender o contexto em que o sistema de AM opera para determinar
quais informações devem ser explicadas e como essa explicação deve
ser estruturada. Além disso, a explicabilidade deve ser projetada
de maneira a facilitar a auditabilidade e a verificação independente
das decisões tomadas pelo sistema, reforçando a confiança dos
usuários e a conformidade com regulamentos vigentes. Portanto, é
importante que, nesse contexto, as particularidades da XAI sejam
cuidadosamente consideradas para averiguar a contribuição nas
explicações que devem ser fornecidas.
Avaliação. A avaliação da explicabilidade é um ponto crucial em
sistemas baseados em AM. Nosso estudo, apontou a importância de
desenvolver explicações que sejam compreensível, fiéis aos resul-
tados gerados pelos modelos de AM e passíveis de avaliação pelos
stakeholders. Isso garante que as informações explicativas forne-
cidas por esses sistemas sejam de alta qualidade. Esses critérios
não apenas fornecem uma base estruturada para a avaliação da
explicabilidade, mas também oferecem um caminho para o aprimo-
ramento contínuo das capacidades explicativas dos sistemas de AM.
Ao aplicar esses critérios de maneira sistemática, os desenvolvedo-
res podem garantir que as explicações sejam claras, precisas e úteis
para os usuários finais.
Este trabalho contribuiu para o campos de ER e AM ao apresentar
os resultado de um MSL, e evidenciou importantes aspectos sobre
a necessidade da ER para explicabilidade em sistemas baseados em
AM, propondo avanços significativos na solução desse requisito.
A explicabilidade é um RNF importante para sistemas que têm im-
pactos relevantes, seja em um nível individual ou social, como por
exemplo, sistemas de seleção de candidatos para emprego, avalia-
ção de benefícios públicos, recomendação de conteúdo, e sistemas
médicos. Tais sistemas podem integrar plataformas multimídia e
Web, tornando relevante para a comunidade científica explorar a
explicabilidade sob a perspectiva da ER.
Uma das limitações deste trabalho foi a falta de acesso em bases
de dados como a ACM Digital Library, o que pode ter limitado a
abrangência da pesquisa. Essa decisão foi necessária devido às res-
trições de acesso impostas pelas portal da CAPES. Futuras pesquisas
poderiam considerar a inclusão dessas bases para uma análise mais
completa e abrangente.
WRSL+’2024, Juiz de Fora/MG, Brazil
Mancine et al.
Além disso, não utilizamos outras estratégias de busca, como o
snowballing. No entanto, dado que se trata de uma área de pesquisa
emergente, a estratégia de busca automática pode ter sido sufici-
ente para alcançar um número significativo de artigos relevantes,
fornecendo evidências importantes para a área de pesquisa.

--- FIM DO ARQUIVO: 30493-829-24941-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30497-829-24945-1-10-20241001.txt ---
Cursos Curtos Online Síncronos para Meninas do Ensino Médio e
Concluintes e para Professoras do Ensino Básico
Maria da Graca C. Pimentel, Juliana Eusébio, Rudinei Goularte, Uthant V. Leite, Helen S. Picoli
mgp@icmc.usp.br,julianaleoncio@usp.br,rudinei@icmc.usp.br,uthantvicentin@usp.br,hspicoli@gmail.com
ICMC-Universidade de São Paulo
2024. Since
February 2023, 119 elementary school teachers have completed
one of the courses for teachers. This report presents the evolution
of our courses, details their approach, reports quantitative results,
includes testimonials from graduates, and tracks the progress of
those who have reported pursuing higher education in the field.
W4E’2024, Juiz de Fora/MG, Brazil
Pimentel, Eusébio, Goularte, Leite e Picoli
jogos e história de mulheres na computação. Dos 91 alunos inscritos,
16 meninos e 14 meninas permaneceram até o final.
Sass et al. [20] registram o oferecimento (remoto durante a pan-
demia) de oficinas sobre mulheres cientistas, atividades de letra-
mento digital para crianças e adolescentes em abrigos, e cursos
de introdução à programação com Python para alunas da UFABC.
Oliveira et al. [14] construíram um curso remoto utilizando Python
e Moodle com público-alvo misto, e reportam que a primeira turma
teve 20 inscritos e 12 concluintes (41% meninas).
Nossa estratégia é complementar a essas: cursos curtos 100%
online combinando atividades síncronas (aulas e monitorias) e as-
síncronas (exercícios de programação). Com meninas do ensino
médio e concluintes, utilizamos Pyhton complementado por Scratch
para ilustrar conceitos de programação e de organização de com-
putadores. Com professoras do ensino básico, utilizamos Scratch.
HISTÓRICO E METODOLOGIA
3.1
Histórico
Inicialmente, uma proposta à Chamada CNPq/MCTIC Nº 31/2018
Meninas nas Ciências Exatas Engenharias e Computação não foi
contemplada.
Em maio de 2021, submetemos para nossa universidade o projeto
de extensão Meninas Programadoras que visava oferecer bolsas
para alunos de graduação atuarem como monitores de cursos remo-
tos introdutórios de Python para alunas do Ensino Médio. O projeto
foi contemplado e desde outubro de 2021 tem oferecido turmas
mensais de forma totalmente remota para alunas de todo o Brasil.
Essas turmas são constantemente avaliadas e aprimoradas. Inicial-
mente, foram disponibilizadas 40 vagas por turma, mas esse número
aumentou gradativamente para as atuais 150 vagas, acompanhado
pelo aumento no número de monitores, que passou de cinco para 12
e atualmente é seis. Quase a totalidade dos monitores se identifica
com o gênero feminino, o que proporciona uma motivação especial
e inspiradora para as alunas.
Em 2022, iniciamos o oferecimento de um curso de continuidade
para atender à demanda expressa por concluintes do curso original.
Assim, temos os cursos de Meninas Programadoras I e Meninas
Programadoras II [12]. Outra inovação implementada em 2022 foi
o oferecimento do Professoras Programadoras [18], um curso
remoto para introduzir o conceito de Pensamento Computacional
para docentes do ensino básico utilizando a plataforma Sctrach.
3.2
Tecnologias empregadas
Utilizamos a plataforma Google Classroom para disponibilizar mate-
rial didático, atividades e gravações das aulas. As aulas e monitorias,
síncronas, ocorrem via Google Meet, fornecendo um ambiente orga-
nizado e acessível para as alunas, permitindo o acesso ao conteúdo
a qualquer momento e facilitando a revisão dos materiais e o acom-
panhamento das aulas. Além disso, o Google Classroom proporciona
uma comunicação direta entre as alunas, monitores e a profes-
sora. Ainda, uma plataforma de comunicação instantânea agiliza
a interação entre as alunas, monitores e a professora, e facilita o
esclarecimento de dúvidas e a colaboração entre os participantes.
Nos cursos para meninas do ensino médio e concluintes, uti-
lizamos uma plataforma de correção automática (Beecrowd), gra-
tuita, na qual estão atualmente publicados cerca de 400 exercícios
criados para o curso. Os exercícios não são públicos visando permi-
tir sua reutilização no curso. Há a especificação de cada exercício,
solução de referência, casos de exemplo e casos de teste. Essa abor-
dagem permite um aprendizado prático e autônomo e com feedback
instantâneo. Criamos animações Scratch para ilustrar conceitos de
programação e de organização de computadores [10].
Nos cursos para professoras do ensino básico, utilizamos a
plataforma Scratch para desenvolvimento das aulas e para entrega
de trabalhos e de um projeto final produzido pelas alunas. Cada
entrega utiliza um estúdio na plataforma.
3.3
Divulgação, inscrição e matrícula
Desde as primeiras turmas criamos um site e um e-mail para os
projetos, mais tarde criamos uma conta no Instagram. As alunas
buscam informações em nosso site no qual incluímos, para cada
turma, o link para inscrição no sistema de gerenciamento de cursos
de extensão da universidade. Para realizar a inscrição, a candidata
deve enviar arquivos em formato PDF com cópias de documento de
identificação e, dependendo do caso, certificado de matrícula ou de
conclusão no Ensino Médio, certificado de conclusão do Meninas
Programadoras I ou comprovante de vínculo docente.
Um professor coordena as tarefas administrativas no sistema da
universidade (submissão e registro dos cursos de extensão no sis-
tema acadêmico, aprovação de inscrições, matrículas e resultados de
aprovação): tarefas desafiadoras devido ao número de inscrições e
à frequência das turmas. Uma professora é responsável pela análise
da documentação e seleção das alunas, com apoio de uma monitora.
Um script filtra as inscrições com documentação completa. Os doc-
umentos de cada inscrição são analisados manualmente. No caso
do Meninas Programadoras I, classificamos as alunas por por ano
cursado do Ensino Médio ou de conclusão. Egressas do Programa
Educação de Jovens e Adultos (EJA) têm prioridade.
Cada candidata selecionada recebe um e-mail confirmando sua
inscrição, enquanto as demais recebem um e-mail com notificação
de indeferimento e convite para a turma seguinte. O e-mail inclui as
regras do curso e reforça as datas das aulas, e um link para um Termo
de Compromisso que solicita concordância com as regras do curso,
além de solicitar informações de nome e CPF de um responsável no
caso de alunas menores de idade. Ao concordar com as regras do
Termo de Compromisso, as alunas têm acesso à opção de submissão
do formulário: é a tela de submissão que dá acesso ao link do Google
Classroom da turma. Esse mecanismo visa minimizar a entrada de
alunas pouco comprometidas com as regras do curso no Classroom.
Para aumentar a participação efetiva devido a ausências e de-
sistências, convocamos 30% mais alunas do que o número de vagas
disponíveis para cada turma. O e-mail de convocação e no termo de
compromisso informam que a matrícula será efetivada para as alu-
nas que comparecerem à primeira aula, considerada fundamental
para acompanhar o curso. Em poucas ocasiões houve mais alunas
presentes na primeira aula do que vagas disponíveis, e em nenhuma
ocasião a totalidade das vagas foi utilizada ao término do curso.
3.4
Organização dos cursos
Meninas Programadoras I
Nas três primeiras turmas, o curso tinha a duração de quatro sem-
anas, incluindo a prova final, e três listas de exercícios distribuídas
Cursos Curtos Online Síncronos para Meninas do Ensino Médio e Concluintes e para Professoras do Ensino Básico
W4E’2024, Juiz de Fora/MG, Brazil
entre as aulas. Na primeira aula, as alunas realizavam atividades
como a criação de uma conta no Beecrowd (antigo URI) e resolução
de um desafio na forma de um jogo do labirinto do blockly games [7].
Durante parte da aula, as alunas eram divididas em grupos, com um
monitor dedicado em salas temáticas no Google Meet, para realiza-
ção dessas atividades. Essa abordagem se mostrou pouco produtiva.
A partir da quarta turma, criamos uma Sessão Preparatória de
60 minutos na forma de uma monitoria na semana anterior ao in-
ício efetivo do curso. Nessa sessão, as alunas criam sua conta no
Beecrowd e submetem um programa, cujo código do programa faz
parte do enunciado, para utilizar o mecanismo de submissão de
exercícios na plataforma. Também nessa sessão, as alunas jogam
dois jogos no Blockly Games. O primeiro é um quebra-cabeça sim-
ples que demanda o encaixe de blocos com interação arrastar e
colar. O segundo emprega essa interação para encaixar de blocos de
comandos que comandam um boneco em um labirinto [7]. O jogo
tem dez fases: solicitamos às alunas jogarem até a sexta fase para
empregarem sequências de comandos, um comando condicional e
um comando de repetição.
Antes da primeira aula, as alunas respondem a questionários
(Google Forms) com conteúdo práticos sobre operadores lógicos e
aritméticos, e praticam inglês com termos relacionados a Python e
ao BeeCrowd. Elas também fornecem informações sobre sua origem
(estado), que computadores utilizam para estudar, horários de es-
tudo, se trabalham, tempo de sono, presença de animais de esti-
mação e motivação para o curso. Essas atividades são essenciais
para prepará-las para o conteúdo do curso e criar um ambiente de
aprendizado inclusivo.
Uma vez estabilizada a metodologia do curso, cada oferta é como
segue: Sessão Preparatória, em grupo de alunas, com monitores;
Atividades Preparatórias (listas de exercícios envolvendo inglês, op-
eradores lógicos, aritméticos e de comparação, e background das alu-
nas); quatro aulas ministradas ao vivo pela professora (sábados 14h-
17h, inclusive feriados); listas semanais de exercícios (plataforma
Beecrowd); prova e prova de recuperação (idem). A primeira aula de
uma turma coincide com a avaliação da turma anterior: provas são
acompanhadas por monitores. O número atual de seis monitores
bolsistas permite oferecer 40h horas de monitorias ao vivo por se-
mana (manhã, tarde e noite, sete dias/semana), número esse que é
sempre estendido bem como horas complementares oferecidas por
um número variável de monitores voluntários.
Para aprovação, exige-se presença mínima de 75% nas aulas sendo
a primeira obrigatória, presença em uma das sessões preparatórias
oferecidas na semana anterior ao início do curso, e de pelo menos
duas horas por semana de sessões de monitoria ao vivo. A presença
obrigatória nas monitorias permite um acompanhamento efetivo
das alunas e contribui para a criação de um ambiente coletivo de
aprendizado, promovendo a troca de conhecimentos e experiências
entre as alunas e os monitores.
O conteúdo do curso inclui conceitos de organização de computa-
dores, de programação e de resolução de problemas com recursos
da linguagem Python (e.g. condicionais, repetições e listas).
Os exercícios são formulados de modo a abordar questões rele-
vantes para a população feminina, como desigualdade e violência
de gênero, educação financeira e valorização do trabalho voluntário.
Temas comumente apresentados no ENEM também são utilizados.
Os exercícios das provas de cada turma são sempre originais.
Uma lista de problemas de programação (20 exercícios por lista nas
turmas mais recentes) é publicada semanalmente: os monitores
apoiam as alunas que devem resolver um número mínimo de
problemas (50%) para aprovação no curso.
Meninas Programadoras II
O Meninas Programadoras II foi criado para atender a demanda
colocada por concluintes do Meninas Programadoras I. Em duas
ofertas em 2022, concluíram 89 alunas. Nessas edições, o curso tinha
três meses de duração, uma aula semanal (sábados, das 13h às 14h)
e participação obrigatória semanal em uma hora de monitoria. Já
as listas semanais tinham entre cinco e dez exercícios. A terceira
edição, oferecida entre julho e agosto de 2023, foi reformulada para
seguir um cronograma similar ao do Meninas Programadoras I,
com cinco aulas das tardes de sábado e listas semanais de exercícios.
O conteúdo do Meninas Programadoras II inclui uma revisão
sobre o conteúdo do Meninas Programadoras I, dicionários e
tuplas, funções e recursividade, exemplos de busca e ordenação, e
de escalonamento de tarefas.
Professoras Programadoras
Apesar de Pensamento Computacional ser um dos conceitos pre-
sentes na Base Nacional Comum Curricular (BNCC), muitas profes-
soras do ensino básico não receberam treinamento nos conceitos,
competências e habilidades intrínsecas a ele. O Professoras Pro-
gramadoras foi criado com o objetivo de fornecer a essas professo-
ras uma oportunidade de desenvolver competências e habilidades
relacionadas ao Pensamento Computacional por meio de aulas e
exercícios que combinam teoria e prática na plataforma Scratch.
Em nossos esforços já utilizamos três modelos diferentes de
curso ao longo de cinco turmas. As duas primeiras edições tiveram
duração de seis semanas, com duas aulas semanais de 90 minutos
no período noturno com entrega de projeto individual na última
semana. As duas turmas seguintes ocorreram na forma de oficinas
de quatro horas em uma tarde de sábado, com entrega de projeto
individual na semana seguinte. A turma mais recente adotou o
cronograma de quatro aulas de quatro horas nas tardes de sábado,
com apresentação de projeto individual na última aula.
O projeto final de cada participante inclui uma descrição do
contexto do problema a ser resolvido e seu relacionamento com o
cotidiano docente em termos conteúdo, área da BNCC e série-alvo.
Todas as animações desenvolvidas para o curso e todos os projetos
apresentados pelas alunas estão disponíveis na plataforma Scratch.
No caso destes cursos, é aceita a inscrição de professores que
não se identificassem com o gênero feminino. Considerando todas
as turmas, apenas dois desses professores concluíram o curso.
RESULTADOS
Resultados do Meninas Programadoras I
O curso atrai participantes de todas as regiões do Brasil; cerca de
60% do Estado de São Paulo, seguido por Minas Gerais, Paraná e
Rio de Janeiro. Devido à demanda, várias turmas receberam exclu-
sivamente alunas do terceiro ano do Ensino Médio e do EJA.
Nas primeiras 27 turmas do Meninas Programadoras I, das
4027 inscrições completas recebidas, convocamos 3926 alunas para
W4E’2024, Juiz de Fora/MG, Brazil
Pimentel, Eusébio, Goularte, Leite e Picoli
Figura 1: Para as 27 primeiras turmas (outubro/2021 a maio/2024): Alunas Selecionadas/Que entraram no Classroom/Que
realizaram as tarefas preparatórias/Presentes na primeira aula/Concluintes aprovadas/Concluintes não aprovadas.
a primeira aula e 2871 ingressaram no Classroom. Compareceram à
primeira aula 2539 alunas, e ao final, 1397 concluíram o curso, com
1249 obtendo aprovação. Esses resultados estão sumarizados na
Figura 1 que apresenta, para as turmas concluídas, os números
de alunas chamadas para a primeira aula, as que efetivamente
ingressaram no Classroom, as que realizaram a principal tarefa
preparatória, as presentes na primeira aula, e as alunas concluintes
aprovadas e não aprovadas. A Turma 17 foi atípica: apenas 17 con-
cluintes em um mês com dois feriados no período das aulas.
No escopo do projeto, realizamos consultas regulares com as
alunas sobre suas trajetórias acadêmicas e profissionais por meio
de listas e grupos. Em uma consulta realizada na primeira sem-
ana de maio de 2024, recebemos 122 respostas espontâneas. Entre
as respostas, 55 alunas registraram seu ingresso em carreiras de
Computação ou Exatas. As alunas estão matriculadas em diversas
instituições de ensino superior, incluindo universidades públicas e
institutos federais. Entre as universidades públicas estão a UFABC,
UFAL, UFBA, UERJ, UFMG, UFMT, UFPE, UFPR, UFRGS, UFRJ,
UFSC, UFU, Unicamp, Univesp, UTFPR e USP. Entre os institutos
federais, temos egressas matriculadas no IFSP, IFBA e IFAM. Há
egressas na PUC Minas e outras instituições não públicas.
Entre as respondentes, 48 alunas estão matriculadas em vários
cursos relacionados à Computação e tecnologia. A maioria está
em Sistemas de Informação (9) e Ciência da Computação (9),
refletindo uma forte preferência por essas áreas. Outros cursos
incluem Engenharia de Software (4) e Engenharia de Computação
(4). Além disso, há um número significativo de alunas em Análise e
Desenvolvimento de Sistemas (13), o que demonstra um interesse
em programas voltados para o desenvolvimento de software.
Ciência de Dados também atrai um bom número, com quatro alunas
matriculadas. Tecnologia da Informação tem três alunas, e há
duas alunas em cursos específicos de Desenvolvimento de Software.
Resultados do Meninas Programadoras II
As duas turmas já concluídasregistraram um total de 89 aprovadas,
representando 58% das 154 alunas presentes na primeira aula.
Esperávamos uma maior retenção. Atribuímos esse resultado
principalmente ao fato de o curso ter menos horas-aula por semana
e maior duração. Por isso, a terceira oferta, atualmente em fase de
conclusão, teve calendário similar ao Meninas Programadoras I.
Resultados do Professoras Programadoras
As duas primeiras turmas do Professoras Programadoras
receberam mais de 200 inscrições para 40 vagas cada turma, e
as chamadas foram decididas por sorteio. Das 91 professoras
chamadas para a primeira aula, 86 compareceram. Concluíram
e foram aprovadas 61 alunas. As duas oficinas oferecidas em
dezembro de 2022 tiveram 44 professoras aprovadas. A turma de
fevereiro/2023, nas tardes de sábado, teve uma procura menor.
Das 17 professoras presentes na primeira aula, nove concluíram
sendo aprovadas. Planejamos oferecer outra turma aos sábados
e aumentar a divulgação, dado que esse horário é acessível para
professoras que lecionam no período noturno.
Monitores bolsistas e voluntários
O projeto contou com oito, doze e seis bolsas nos três anos de
projeto, respectivamente. Contamos também com monitores
voluntários. Por exemplo, no primeiro semestre de 2023 temos seis
monitoras voluntárias de projetos parceiros que oferecem duas
horas semanais cada. Com os bolsistas, esse grupo oferece 51 horas
de monitoria por semana. Outros cinco monitores voluntários de
nossa instituição que oferecem uma hora semanal para alunas que
participam de uma turma extra de treino contínuo.
Alguns depoimentos
Ao final de cada turma, ou esporadicamente durante o ano,
solicitamos às egressas que preencham um formulário no qual
respondem à pergunta Contribua com uma frase ou parágrafo para
atrairmos mais meninas para o curso. Essa atividade é voluntária e
não há identificação dos participantes. Reproduzimos alguns dos
depoimentos das discentes concluintes da Turma 24, disponíveis
anônima e publicamente no site [13].
Niterói-RJ: “Fazer o curso Meninas Programadoras me ajudou
a dar o pontapé inicial em programação que eu precisava, porque
eu não sabia quase nada mesmo, mas sempre tive afinidade com
tecnologia e computador. Ao iniciar o curso, percebi que a profes-
sora Maria da Graça é muito atenciosa e inspiradora, e o ambiente
é seguro e feito para meninas, o que me possibilitou concluir 100%
as atividades propostas do curso e gostar de programação. Além
disso, também tive mais certeza de que quero seguir na área de
computação! Obrigada por tudo!”
Cursos Curtos Online Síncronos para Meninas do Ensino Médio e Concluintes e para Professoras do Ensino Básico
W4E’2024, Juiz de Fora/MG, Brazil
Central-BA: “O curso foi muito acolhedor e didático, de fato
mudou meu pensamento sobre programação ser muito complexa a
ponto de não conseguir entendê-la. Hoje penso em fazer engenharia
da computação ou ciência da computação”
Fortaleza-CE: “Amei muito essa experiência e indico fortemente
que todas busquem esse aprendizado. É um curso bem introdutório
em programação, realmente para quem nunca teve contato, a
evolução é visível ao longo das semanas! Eu já tinha pelo menos
um contato com programação, mas foi nesse curso revendo a base
que consegui solidificar minha lógica para resolução de problemas,
percebi que foi essencial rever esses conceitos básicos e iniciais. A
professora e os monitores são atenciosos e prestativos, realmente
ajudaram muito! As aulas foram ótimas, super completas e interes-
santes! As monitorias sempre produtivas e em horários acessíveis
com monitores educados, gentis e inteligentes. Outra coisa que
achei importante foi a comunicação com as alunas, monitoras do
projeto, onde conversávamos sobre os cursos superiores, suas ex-
periências, indicações...Simplesmente incrível essa iniciativa, amei
cada momento e indico demais.”
Sorocaba-SP: “Esse curso me ajudou muito no entendimento
da área e a escolher uma direção profissional, grata desde já pela
começo do meu ensinamento profissional.”
São Paulo-SP: “O curso foi realmente incrível, e é uma ótima
oportunidade para qualquer menina que tenha algum interesse em
programação. Mesmo que você esteja intimidada pela complexidade
do assunto, no Meninas Programadoras você encontrará muito
apoio de professores e monitores para esclarecer dúvidas e atingir
o seu objetivo de aprender o básico da programação em python.”
Curitiba-PR: “Desde o primeiro dia de aula eu me sentia moti-
vada pela professora e o pessoal da monitoria a nunca desistir e
sempre pedir ajuda quando necessário. Não apenas eles como todas
as outras alunas ajudavam umas as outras, não deixando ninguém
para trás. O Meninas Programadoras não apenas vai te incentivar
a continuar no caminho da programação, como também irá ensinar
como não desistir, mesmo nos momentos mais apavorantes e desafi-
adores. Foi graças a todos que irei continuar a seguir com a cabeça
erguida até o fim. Obrigada professora e a todos da monitoria!!”
DISCUSSÃO
Fundamentação teórico-metodológica
A abordagem de cursos curtos online de programação para
meninas e professoras, que inclui aulas e monitorias síncronas,
atividades de programação individual trabalhadas nas monitorias e
problemas contextualizados, entre outros, em temas associados à
desigualdade de gênero e na promoção do trabalho voluntário, é
fundamentada em várias teorias educacionais. Piaget [16] destaca
a importância do aprendizado ativo e construtivo, onde discentes
constroem seu próprio conhecimento através da interação com o
ambiente e pela resolução de problemas. Vygotsky [22], enfatiza o
papel da interação social no desenvolvimento cognitivo, facilitada
nos cursos pelas aulas interativas e aprendizagem mediada por
monitorias com pequenos grupos de alunas. A ideia de inteligência
coletiva de Lévy [9] é especialmente relevante no contexto online
onde o conhecimento é construído coletivamente com o apoio
de tecnologias digitais. A pedagogia crítica de Freire [6], que
valoriza a conscientização e a transformação social através da
educação, é essencial para abordar e desafiar as desigualdades de
gênero dentro do ambiente educacional e na sociedade. Ainda,
aprender a programar permite às alunas dominarem o computador
para comandá-lo, o que está alinhado com a noção de Freire
[6] de “práxis” como sendo a reflexão e ação do ser humano
sobre o mundo para transformá-lo. Neste contexto, a “práxis” se
manifesta na capacidade das alunas de refletir criticamente sobre
seu aprendizado e, em um primeiro momento, se habilitarem como
programadoras e, em um segundo momento, usar essas habilidades
para transformar sua realidade e a sociedade ao seu redor.
Alinhamento com as metas globais da Agenda 2030 da ONU para o
Desenvolvimento Sustentável (ODS)
Nossos esforços estão alinhados, em particular, com dois objetivos.
O ODS-4, Educação de Qualidade, visa assegurar uma educação
inclusiva e equitativa de qualidade e promover oportunidades de
aprendizagem ao longo da vida para todos. Nosso projeto contribui
diretamente para essa meta ao oferecer cursos gratuitos e acessíveis
que introduzem conceitos de computação e de pensamento com-
putacional para meninas do ensino médio e concluintes e para
professoras do ensino básico em todo o Brasil. Ao capacitar alunas
e professoras, o projeto democratiza o acesso à educação de quali-
dade na área de Tecnologia da Informação, independentemente da
localização geográfica ou situação econômica das participantes.
Nossos esforços estão em consonância com o ODS-5, Igualdade
de Gênero, que promove a igualdade de gênero e o empoderamento
de todas as mulheres e meninas. O projeto enfrenta diretamente
os estereótipos de gênero prevalentes na área de Computação,
criando um ambiente acolhedor e representativo para alunas.
Depoimentos de egressas evidenciam a importância de iniciativas
que combatem a sub-representação feminina e incentivam a
participação ativa das mulheres em setores tradicionalmente
dominados por homens. Assim, o projeto promove a inclusão
e a diversidade, e também contribui para a construção de uma
sociedade mais justa e equitativa, em linha com os princípios e
metas da Agenda 2030 da ONU.
Uso de Tecnologias web e multimídia
Ao utilizar tecnologias web e multimídia integradamente ao con-
teúdo dos cursos, ampliam-se as oportunidades de aprendizagem
das alunas. O uso estratégico das plataformas web e multimídia,
juntamente com a plataforma de correção automática e o Scratch,
oferece às alunas um ambiente de aprendizagem diversificado e
desafiador, estimulando o desenvolvimento de habilidades de pro-
gramação e resolução de problemas. O uso de tecnologias de co-
municação instantânea facilita a interação e a colaboração entre as
participantes do curso.
O Google Classroom proporciona acesso fácil aos materiais didáti-
cos, atividades e gravações das aulas. Entretanto, dada a demanda
por comunicação instantânea pela natureza intensiva do curso, uti-
lizamos um aplicativo externo amplamente utilizado pelas alunas
em seu dia-a-dia. A plataforma de correção automática permite um
aprendizado prático e autônomo, com feedback instantâneo, aux-
iliando no desenvolvimento das habilidades de programação das
alunas. O uso do Scratch como plataforma de programação visual
estimula a criatividade, a experimentação e o pensamento lógico,
promovendo a compreensão dos fundamentos da programação.
W4E’2024, Juiz de Fora/MG, Brazil
Pimentel, Eusébio, Goularte, Leite e Picoli
No curso para professoras do ensino básico, o uso do Scratch
vai além da apresentação de animações. Ele também é utilizado
nos exercícios semanais e no desenvolvimento do projeto final
individual, proporcionando um ambiente prático e envolvente para
as alunas aplicarem os conceitos aprendidos e desenvolvam suas
habilidades de programação.
Em conjunto, essas tecnologias web e multimídia oferecem às
alunas um ambiente de aprendizagem interativo, prático e colabora-
tivo, permitindo que elas desenvolvam habilidades de programação
e resolução de problemas de forma criativa e estimulante. Além
disso, contribuem para a formação de educadoras capacitadas a
utilizar recursos digitais inovadores em sua prática pedagógica.
Problemas do uso tecnologias não integradas
O emprego de múltiplas plataformas web e multimídia nos cursos
sobrecarrega as alunas e a equipe por exigir um esforço adicional
que pode impactar negativamente a experiência de aprendizagem.
Além disso, muitos dados de interação e de participação estão
presos nas diferentes plataformas, o que dificulta o monitoramento
adequado do desempenho e da participação das alunas e prejudica
o suporte oferecido. Ainda, a execução simultânea de várias apps
em um dispositivo pode resultar em desempenho lento, consumo
excessivo de recursos do sistema e redução da vida útil da bateria.
Meninas Programadoras I e II: participação e engajamento
As estatísticas sumarizadas na Figura 1 evidenciam o interesse e
a busca das alunas em participar do programa. Ainda, ressaltam a
importância de adaptar as turmas às demandas ao longo do ano.
O impacto positivo do Meninas Programadoras é evidente. As
turmas do Meninas Programadoras II, composta principalmente
por alunas egressas do Meninas Programadoras I, refletem
o interesse contínuo e o engajamento dessas jovens em buscar
oportunidades de aprendizado e crescimento na área. O número
de inscrições e a taxa de retenção são evidências de que os cursos
proporcionam um ambiente propício para o desenvolvimento de
habilidades técnicas avançadas, criam uma rede de apoio mútuo e
ampliam as perspectivas profissionais das participantes.
Professoras Programadoras
Os oferecimentos registram interesse do público-alvo e taxa de
conclusão positivos. Depoimentos das participantes evidenciam
a eficácia da abordagem lúdica do curso de programação com o
Scratch, permitindo mudanças na prática docente, mesmo para
aquelas com pouco conhecimento prévio. Esses resultados destacam
a importância de continuar proporcionando oportunidades de
aprendizado em programação para professoras, promovendo
abordagens criativas que possam ser aplicadas nas salas de aula.
Desafios administrativos
Cada oferta é um curso de extensão: a Universidade emite
certificados para as aprovadas, o que atrai participantes. Mas os
sistemas administrativos apresentam problemas de usabilidade que
tornam o seu uso o aspecto mais desafiador do trabalho, inclusive
pelo volume de inscrições e à política de cursos mensais. Desde o
primeiro semestre de 2023, uma pessoa de apoio administrativo
tem aliviado essa carga.
Aprimoramento contínuo
Os cursos são avaliados constantemente para identificar pontos
fortes e pontos de aprimoramento. Essa prática é essencial para
garantir a qualidade do curso e proporcionar a melhor experiência
de aprendizado para as alunas. São considerados feedbacks das
participantes e dos monitores durante as aulas e nos canais de
comunicação assíncrona. Essas informações indicam ajustes e
melhorias no conteúdo, nas aulas e nas atividades propostas. Dessa
forma, o curso se mantém atualizado, relevante e alinhado às
demandas e expectativas das alunas e da equipe.
Intensidade: engajamento e retenção
A intensidade do curso é um fator a ser analisado. Ao con-
centrar as aulas e atividades em um curto período, como os
cursos mensais, é possível proporcionar uma imersão intensa
no conteúdo, favorecendo o aprendizado e a compreensão dos
conhecimentos. As participantes têm a oportunidade de se dedicar
intensivamente ao curso, concentrando seus esforços em sua
formação durante esse período específico, o que pode poten-
cializar os resultados e promover o engajamento até o final do curso.
Equidade e Experiência
As bolsas do projeto fazem parte do programa da instituição para
auxílio à permanência para alunos em situação de vulnerabilidade
financeira. Assim, ao obter bolsas para alunos de graduação atuarem
como monitores, promove-se a inclusão e a equidade também entre
os próprios monitores. A renovação das bolsas não é automática: a
equipe dedica esforço significativo para sua manutenção.
A maioria dos bolsistas ingressa no projeto ao concluírem o curso
de Introdução à Computação. Ao participarem do Meninas Pro-
gramadoras, eles aprimoraram suas habilidades de programação,
de comunicação, resolução de problemas, adaptação e motivação.
À medida que os bolsistas avançam em seus próprios cursos, se
tornam capacitados para atuar em projetos relacionados a eles.
CONSIDERAÇÕES FINAIS
Nossa comunidade investe em pesquisa e desenvolvimento cientí-
fico de tecnologias Web e Multimídia. Ao participar desse processo
de pesquisa e inovação, podemos presenciar os resultados concretos
e impactantes dessas tecnologias em nossa sociedade. Em nosso pro-
jeto, o uso estratégico dessas tecnologias proporciona um ambiente
que estimula o desenvolvimento de habilidades de programação
e de resolução de problemas de modo que muitas alunas decidem
seguir a carreira. Além disso, as experiências vivenciadas pelas
alunas, compartilhadas por meio de depoimentos voluntários e
anonimizados, motivam outras alunas.
A continuidade desses cursos depende da renovação das bolsas.
A participação dos monitores tem sido essencial para o acompan-
hamento efetivo das alunas e para a criação de um ambiente coletivo
de aprendizado. A manutenção dessas bolsas se faz necessária para
garantir a qualidade e a continuidade dessas experiências educa-
cionais, além de promover a formação dos bolsistas.
Agradecimentos: Agradecemos ao Programa PUB da USP e a
todos os monitores e voluntários pelo seu valioso apoio.
Cursos Curtos Online Síncronos para Meninas do Ensino Médio e Concluintes e para Professoras do Ensino Básico
W4E’2024, Juiz de Fora/MG, Brazil

--- FIM DO ARQUIVO: 30497-829-24945-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30498-829-24946-1-10-20241001.txt ---
GECET: Garotas nas Engenharias, Ciências Exatas e Tecnologias
Projeto de Extensão voltado para a participação feminina nas áreas STEM
Giovana A. Benvenuto
Analice C. Brandi
giovana.a.benvenuto@unesp.br
analice.brandi@unesp.br
Faculdade de Ciências e Tecnologia –
Universidade Estadual Paulista
(FCT/UNESP)
Presidente Prudente, São Paulo
Marilaine Colnago
Rafaella S. Ferreira
marilaine.colnago@unesp.br
rafaella.ferreira@unesp.br
Instituto de Biociências, Letras e
Ciências Exatas – Universidade
Estadual Paulista (IBILCE/UNESP)
São José do Rio Preto, São Paulo
Helen S. Picoli
h263551@dac.unicamp.br
Faculdade de Tecnologia -
Universidade Estadual de Campinas
(FT/UNICAMP)
Limeira, São Paulo
Figura 1: Logo e ações do projeto de extensão GECET
W4E’2024, Juiz de Fora/MG, Brazil
Benvenuto et al.
o enriquecimento cultural e a pluralidade de pensamento das en-
volvidas. Neste artigo, compartilharemos as experiências de nossos
principais projetos, discutindo seus impactos e resultados. Entre as
ações realizadas pelo projeto, destacam-se:
• Atividades de Formação e Divulgação: Promoção de eventos
e debates, divulgação de conhecimentos científicos e criação
de materiais educativos.
• Criação de Grupos e Canais: Formação do Pyladies Rosana e
estabelecimento de canais de comunicação.
• Cursos: Curso de linguagem Python para garotas, curso de
Formação para Quebra de Estereótipos de Gênero no ambi-
ente escolar e curso de Liderança Feminina nas STEM.
• Participação em Eventos e Parcerias: Participação na confer-
ência Meninas Digitais e Women in Information Technology,
além de parcerias com o AprovEnem e o grupo #unidaspela-
matemática para expandir nossos horizontes.
HISTÓRICO DE AÇÕES
Ao longo de sua trajetória, o projeto buscou fomentar a participação
feminina em diversas idades e níveis de escolaridade, promovendo
ações desde a educação básica. O objetivo, a médio e longo prazo,
é aumentar o ingresso de mulheres em áreas predominantemente
masculinas, reforçando a autoconfiança e a identidade das partici-
pantes, especialmente aquelas com poucas referências no tema ou
pertencentes a classes financeiramente desfavorecidas. Para isso,
entende-se a importância da formação e conscientização de profes-
sores da educação básica, bem como do incentivo à sua participação
no processo de redução da desigualdade de gênero e estímulo à
participação de futuras alunas nas carreiras STEM.
Para as mulheres que já iniciaram suas carreiras nessas áreas,
o projeto oferece formações e grupos de apoio, com o objetivo de
auxiliar na jornada de autoconhecimento das participantes e pro-
porcionar espaços onde se sintam seguras, ouvidas e reconhecidas.
Os encontros abordam temas voltados tanto para questões de saúde
mental quanto para a identificação de ambientes tóxicos, permitindo
a partilha de experiências.
A seguir, são descritas algumas das ações realizadas pelo projeto
de extensão GECET.
2.1
M++: Almanaque de ciências e curiosidades
A criação do almanaque "M++" (Figura 2) foi uma das ações do
projeto, visando disponibilizar materiais de apoio para professores
da educação básica. Disponibilizado gratuitamente para download
em 2021, o material de livre acesso sobre ciências destaca a partic-
ipação de mulheres na STEM. O almanaque ilustra mensalmente
uma cientista e suas contribuições para a área, além de apresen-
tar desafios e curiosidades. As atividades elucidam cientistas e a
ciência de forma divertida, com o objetivo de despertar o interesse
inicial de crianças e adolescentes nessas áreas de conhecimento,
além de trazer representatividade para que mais meninas possam
se identificar.
Figura 2: Capa do Almanaque M++ e exemplo de uma página
falando sobre a Katherine Johnson. Conteúdo disponível em:
https://www.viser.com.br/gecet/m
2.2
Programa Interdisciplinar Tecnológico -
InterTec
Com o objetivo de reunir mulheres de diversas áreas do conheci-
mento interessadas em desenvolver técnicas de resolução de prob-
lemas através de modelagem matemática e programação, o GECET
propôs, em 2021, o curso denominado InterTec. O curso, aplicado
quinzenalmente de forma totalmente online, contou com a partici-
pação de 11 mulheres que estavam cursando ou já haviam concluído
cursos técnicos e/ou superiores em diversas áreas do conhecimento.
Através dos encontros, as participantes puderam ter contato com
noções de lógica de programação e ciência de dados, aprofundando-
se no uso da linguagem de programação Python. Elas aplicaram o
conteúdo aprendido em problemas do dia a dia e em seus próprios
domínios. Ao final do projeto, as participantes demonstraram suas
aplicações, o que possibilitou a geração de uma publicação na qual
foi analisada a distribuição de bolsas de estudo do programa Uni-
versidade para Todos (PROUNI) [4].
2.3
Curso de programação SuperPython
Outra atividade, também direcionada ao ensino básico e realizada
durante o período em que foi necessário o remanejamento das ativi-
dades presenciais, foi o “SuperPython”. Realizado de forma remota,
o projeto atraiu 35 inscrições de meninas de diferentes regiões do
Brasil, com idades entre 11 e 18 anos. O objetivo do SuperPython
foi ensinar a linguagem de programação Python por meio do desen-
volvimento de jogos, proporcionando uma experiência educativa e
envolvente.
A proposta foi criada para aproveitar o talento e a criatividade
de crianças e jovens, permitindo-lhes produzir histórias interativas
baseadas em computador. Ao criar jogos, as participantes puderam
experimentar a satisfação de realizar seu mundo lúdico, enquanto
aprendiam e produziam material que poderia ser desfrutado e apre-
ciado por seus colegas. Considerando que muitas meninas nessa
faixa etária percebem os cursos de tecnologia como ambientes
hostis para mulheres, o que muitas vezes as desestimula a seguir
carreiras nessa área [11], o curso tinha, além do objetivo de ensinar,
a intenção de despertar o interesse das participantes por carreiras
em STEM. O projeto buscou oferecer uma nova perspectiva sobre
essas áreas, incentivando as meninas a enxergarem o potencial e as
oportunidades que elas podem oferecer.
GECET: Garotas nas Engenharias, Ciências Exatas e Tecnologias
W4E’2024, Juiz de Fora/MG, Brazil
A linguagem de programação utilizada no SuperPython é de fácil
aprendizado e permite a criação de programas de qualidade profis-
sional, adaptados à faixa etária dos alunos. Os participantes precis-
aram entregar resultados tangíveis rapidamente, motivados pela
aquisição de habilidades de programação cada vez mais complexas,
com orientação em aulas síncronas e suporte contínuo através de
canais de comunicação.
Dentro desta proposta, foram desenvolvidos jogos em Python que
contavam histórias de mulheres cientistas. O público-alvo foram alu-
nas do ensino fundamental II e médio interessadas em desenvolver
habilidades de programação e resolução de problemas. Ao término
do projeto, o jogo premiado contou a história de Ada Lovelace por
meio da Programação Orientada a Objetos, um exemplo pode ser
observado na Figura 3. Dois artigos relatando a experiência desse
projeto foram gerados, um apresentado no XIV Latin-American Con-
gress on Electricity Generation and Transmission (CLAGTEE) [5],
e o segundo no Congresso Nacional de Matemática Aplicada e
Computacional (CNMAC) [6].
Figura 3: Tela do jogo desenvolvido por uma das participantes
e trecho do código.
2.4
Formação quebra de estereótipos de gênero
no ambiente escolar
Na sociedade em que vivemos, somos constantemente confrontados
com conceitos de coisas tipicamente masculinas ou femininas, sejam
elas cores, brinquedos ou até profissões. Esses padrões nos são
apresentados desde a infância, perpetuando preconceitos e criando
barreiras para aqueles que desejam romper com esses padrões pré-
definidos pelo Estereótipo de Gênero.
Devido aos estereótipos de gênero, é comum que certos clichês
relacionados a profissões sejam reforçados na sociedade, o que
acaba limitando o desenvolvimento das habilidades e capacidades
dos indivíduos. Como consequência, apenas um terço das garotas
que chegam à universidade optam por cursos de ciências. Segundo
dados da Unesco, apenas 30% das universitárias escolhem carreiras
relacionadas a STEM [1].
Para combater esse problema, o GECET ofereceu um curso de for-
mação docente voltado para licenciandos em áreas de exatas, com o
objetivo de incentivar a quebra de estereótipos no ambiente escolar,
o convite para a participação pode ser visto na Figura 4. A formação
contou com encontros mensais de forma online, no período de junho
a outubro de 2022, com a presença de 15 licenciandos, abordando
temas como, mulheres nas ciências exatas, apresentando várias
biografias de mulheres que fizeram história, como as escolhas das
profissões são influenciadas pelos estereótipos de gênero, e como
quebrar este paradigma. Além desses temas, foram apresentadas
atividades sobre quebra de estereótipos aplicáveis em sala de aula.
No ambiente escolar, é essencial que o professor aborde temas
como reflexões sobre gêneros, promoção da equidade, estímulo à
empatia e fortalecimento do empoderamento feminino. É impor-
tante que algumas atividades sejam inseridas no plano de aula
do professor, tais como: destacar a importância das mulheres na
história; incentivar a leitura de livros escritos por mulheres; pro-
mover atividades físicas inclusivas; moderar debates sobre gênero
e feminismo; e fomentar a equidade em todas as disciplinas.
Figura 4: Convite para a inscrição dos encontros: Quebra de
estereótipos no ambiente escolar
2.5
Guia de profissões
Direcionado aos anos finais do ensino, o GECET produziu, em 2023,
um “Guia de Profissões”, vide Figura 5, focado em carreiras em
STEM. O material inclui mais de 20 cursos de graduação e licen-
ciatura, apresentando aspectos gerais sobre a formação, atribuições
e áreas de atuação, além de opções de onde cursar e perfis de mul-
heres da área, tanto pioneiras quanto profissionais da atualidade.
Esse conteúdo é disponibilizado de forma online e gratuita e pode
ser encontrado na página do projeto 1.
Além disso, nas redes sociais do projeto, foram realizadas entre-
vistas com profissionais de diversas áreas relacionadas à STEM no
formato de Lives com transmissão pelo Youtube ou pelo Instagram.
2.6
Divulgação cientifica através de Redes
Sociais
O trabalho de divulgação científica é uma ação recorrente do pro-
jeto. No Instagram2, buscamos ativamente apresentar e destacar a
participação feminina em áreas correlatas às ciências exatas, home-
nageando mulheres históricas e apresentando as mulheres que hoje
escrevem sua história em áreas majoritariamente dominadas por
homens. O perfil também informa sobre notícias atuais relacionadas
1https://www.viser.com.br/gecet
2@gecet.unesp
W4E’2024, Juiz de Fora/MG, Brazil
Benvenuto et al.
Figura 5: Capa do Guia de Profissões e exemplo da página
sobre Engenharia Ambiental. Conteúdo disponível em: https:
//www.viser.com.br/gecet
à área, além de compartilhar indicações de livros e filmes sobre o
tema.
Ademais, datas comemorativas são marcadas pela divulgação de
mulheres que conduzem pesquisas e contribuem significativamente
para a área, destacando a ciência que produzem. Interações, quizzes
e sorteios também foram algumas das ações promovidas nas redes
sociais, com o objetivo de engajar o público e ampliar o alcance do
conteúdo produzido pelo projeto.
CICLO ATUAL DE ATIVIDADES
Em 2023, o GECET iniciou um novo ciclo de atividades focado
na formação de lideranças em STEM, especialmente voltado para
estudantes mulheres de graduação e pós-graduação. Um estudo
realizado por Santos e Marczak [10] revelou que os principais fatores
de evasão de mulheres na área de computação incluem ansiedade,
estresse, falta de suporte e influência de terceiros.
Diante desse contexto, o projeto foi estruturado em duas frentes
principais: formação de lideranças e programação, ambas com a
missão comum de construir um futuro mais inclusivo em STEM.
Com 81 inscrições nas duas frentes, o projeto visa proporcionar a
essas mulheres uma rede de apoio, promovendo trocas de exper-
iências e oferecendo suporte para reafirmarem suas posições como
líderes, independentemente de suas áreas de atuação.
3.1
Formação de Lideranças em STEM
Na formação de lideranças, são realizados encontros remotos men-
sais com palestrantes convidados que promovem diálogos inspi-
radores e reflexões críticas. Na Figura 6 vemos dois exemplos de
convites dos encontros propostos onde foram tratados temas como
a síndrome de impostora e a saúde mental, com a participação de
palestrantes das áreas. Esses encontros são projetados para criar
uma rede de apoio entre as participantes, além de desenvolver ha-
bilidades interpessoais essenciais para a liderança.
Figura 6: Convites para os encontros de liderança expondo
os palestrantes convidados e temas trabalhados.
3.2
Curso de Programação em Python
No contexto de Programação, inspirado pela boa experiência do
InterTec mencionado anteriormente, são organizadas mensalmente
sessões de aprendizado técnico focadas na linguagem de progra-
mação Python (Figura 7). O objetivo é introduzir e aprimorar as
habilidades das participantes de maneira colaborativa, fortalecendo
seu domínio técnico em um ambiente de apoio mútuo. As aulas
abrangem diferentes domínios e gradualmente aplicam um olhar
crítico sobre a disparidade de gênero, utilizando os conceitos apren-
didos para análise de dados que revelam as barreiras ainda presentes
nos diversos setores da sociedade. Além disso, essa atividade pro-
porciona um espaço para a interação entre profissionais de áreas
diversas, buscando integrar outros campos do conhecimento às
ciências exatas.
Figura 7: Telas de encontros do curso de Python, mostrando
uma apresentação e um trecho de código em python.
IMPACTOS E RESULTADOS
O projeto buscou ter um impacto significativo no aumento do inter-
esse pelas áreas STEM. Somando as inscrições em todos os projetos
realizados pelo GECET, mais de 140 pessoas foram alcançadas. Du-
rante essas ações, as participantes manifestaram interesse crescente
em cursos de STEM, relatando motivação e encorajamento para
seguir essas carreiras.
Além disso, ações realizadas nas redes sociais, como a Semana
da Engenharia, lives de bate-papo, e posts de divulgação científica,
também tiveram um papel importante. Esses eventos geraram de-
poimentos de mulheres atuantes nas áreas abordadas, contribuindo
para reduzir a evasão e motivando alunas desmotivadas a continuar
seus cursos.
Houve uma reflexão importante sobre estereótipos de gênero,
com um aumento no engajamento de professores no curso de for-
mação de quebra de estereótipos. Esses educadores passaram a se
GECET: Garotas nas Engenharias, Ciências Exatas e Tecnologias
W4E’2024, Juiz de Fora/MG, Brazil
conscientizar mais sobre a importância de apoiar alunas e alunos a
seguirem as carreiras desejadas, independentemente do gênero.
O projeto também se destacou no desenvolvimento de materiais
educativos, como o almanaque “M++” e o “Guia de Profissões”.
Esses materiais, disponibilizados gratuitamente atingindo tanto o
público da edução básica como do ensino médio, contribui para o
enriquecimento cultural e a pluralidade de pensamento entre os
estudantes.
Vale ressaltar que muitas das ações do projeto resultaram em
produções acadêmicas e participações de congressos, como a já
citada participação no CLAGTEE[5] e também os seguintes resumos
apresentados no CNMAC:
• Análise da representação da diversidade de gênero nos cursos
de graduação da Universidade Estadual Paulista (Unesp) [7]
• Uma análise de gênero na premiação da OBMEP ao longo
dos anos [8]
• Análise de Gênero nas Diretorias de Sociedades Científi-
cas [3]
O artigo completo publicado, também apresentado no CNMAC
[4], foi reconhecido como o melhor trabalho da sessão técnica em
que foi apresentado. A seleção e certificação foram realizadas pelo
British Council e pelo Comitê Temático de Mulheres da Sociedade
Brasileira de Matemática Aplicada e Computacional (SBMAC).
No âmbito das redes sociais, o Instagram do projeto atualmente
conta com 1.486 seguidores e já realizou 986 publicações. A atuação
do projeto na divulgação científica pelas redes sociais foi premiada
duas vezes consecutivas, em 2022 e 2023, pela iniciativa Women in
Information Technology (WIT), promovida pela SBC para debater
questões de gênero nas áreas de tecnologia no Brasil.
CONCLUSÃO
O GECET surgiu como um projeto de extensão universitária local
em Rosana, no interior do estado de São Paulo, com o objetivo de
mudar a realidade escassa da participação feminina nos cursos das
áreas de STEM. Após um período de adaptação, encontrou no for-
mato online a oportunidade de atingir mulheres de diversas regiões
do Brasil, ultrapassando a barreira da distância física. Atualmente,
o projeto une integrantes de três diferentes instituições de ensino
superior: a Universidade Federal do Rio Grande (FURG), a Univer-
sidade Federal de Lavras (UFLA) e a Universidade Estadual Paulista
(UNESP), com integrantes em quatro campi diferentes: Araraquara-
SP, Bauru-SP, Presidente Prudente-SP, Rosana-SP e São José do Rio
Preto-SP. Além disso, o projeto conta com integrantes já formadas
que atuam em redes de ensino fundamental e médio, tendo contato
direto com crianças e adolescentes.
A transição para atividades online, inicialmente vista como um
desafio, revelou-se uma oportunidade para ampliar o alcance do
projeto. A utilização das redes sociais e outras plataformas digitais
permitiu uma participação mais diversificada e um impacto mais
profundo. No entanto, a necessidade de constante inovação e adap-
tação permanece crucial para manter o engajamento e a eficácia
das iniciativas.
Este projeto demonstrou que é possível promover a equidade de
gênero nas áreas STEM através de uma abordagem multifacetada
que inclui formação de professores, criação de materiais educativos
e uso estratégico das mídias sociais. A adaptação para o formato
online durante a pandemia não apenas garantiu a continuidade
das atividades, mas também ampliou o impacto, mostrando a im-
portância de flexibilidade e inovação em projetos educativos. Con-
tinuaremos a incentivar meninas e mulheres a buscar seus sonhos
nas áreas STEM, contribuindo para a construção de um futuro mais
equitativo.
AGRADECIMENTOS
As autoras agradecem à CAPES - Código de Financiamento 001,
pelo apoio para realização do presente trabalho, ao Comitê das
Mulheres da SBMAC e ao programa Meninas Digitais da SBC pelo
apoio.

--- FIM DO ARQUIVO: 30498-829-24946-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30499-829-24947-1-10-20241001.txt ---
Metagame Planeta Jovem
Transformando jovens em sua melhor versão
Daniel Gularte
Instituto Bojogá
Centro Universitário Unichristus
Fortaleza Ceará Brasil
danielgula@gmail.com
Mauro Oliveira
Iracema Digital
Instituto Federal do Ceará
Fortaleza Ceará Brasil
amauroboliveira@gmail.com
Sílvio Ramos
CRIATLO
Fortaleza Ceará Brasil
drsilvioramos@gmail.com
W4E’2024, Juiz de Fora/MG, Brazil
D. Gularte et al.
Todo funcionamento do framework do projeto é apresentado
neste relato, explicando os conceitos aplicados a fim de garantir
um produto divertido, de baixo custo, usando tecnologias
gratuitas e de fácil aplicação em múltiplos contextos educacionais.
Essa é uma experiencia que inspira pessoas a inovarem a educação
com os games – e se faz fundamental ser compartilhada, a fim de
auxiliar em soluções de impacto a nível nacional.
2 REFERENCIAL TEÓRICO
A seção a seguir apresenta a fundamentação teórica para a
construção da metodologia utilizada, fortemente baseada nas
referências das novas mídias, Game Studies e Objetivos de
Desenvolvimento Sustentável (ODS).
2.1 Signos Metavérsicos
O principal conceito que norteia o Planeta Jovem é a definição
proposta por Radoff, quando define camadas de valor dentro de
uma jornada no metaverso [11]. Sua origem surgiu no livro de
ficção científica, Snow Crash, no início dos anos 90, onde mundos
e redes virtuais existem mesmo antes da Internet [6]. O metaverso
tem como principal premissa a troca de elementos (objetos ou
sensações) entre universos, moldando novos significados para
eles.
Figura 1: As sete camadas do Metaverso, segundo Radoff
O mais relevante aspecto teórico encontrado está na camada
da experiência, onde se explora o conceito de desmaterialização
[11]. Aqui, o princípio repousa na desconstrução das coisas de um
mundo A para o mundo B.
Compreendendo a visão cognitiva da Santaella onde “[…] toda
mudança
no
modo
de
se
produzir
imagens
provoca
inevitavelmente mudanças no modo como percebemos o mundo
e, mais ainda, na imagem que temos do mundo. […]” [10], o
processo metavérsico adicionará possibilidades de ampliar as
percepções existentes no processo de codificação das imagens.
Dessa forma, enquanto ocorre essa troca entre universos em
uma realidade transitória, sobreposta ou mista, é possível
adicionar novos elementos à experiência, ampliando os formantes
semióticos e construindo um novo significado para as coisas [10].
2.2 Imersão em Jogos
O outro sustentáculo teórico do projeto se apropria
fortemente das técnicas imersivas dos games. De acordo com a
hipótese levantada neste artigo, os jogos possuem características
que se diferem de qualquer outra mídia: são, ao mesmo tempo,
experiências de alta imersão, com forte contato social e que
fornecem jornadas efetivas com tomadas de decisão.
Enquanto imersão, os jogos criam o que se denomina “círculo
Mágico”, um ambiente onde imaginação e a criatividade são postas
à prova, com desafios contendo regras e normas acordadas entre
os participantes em comum acordo. Nele é possível esquecer, por
um tempo, o mundo real e vivenciar outras possibilidades. [5]
Notadamente Huizinga se refere ao movimento do Círculo
Mágico como uma atividade de jogo. Assim, esse ambiente, que
pode ser um metaverso, vai garantir efetividade na realização das
tarefas propostas neste mundo lúdico, motivando seus jogadores
em jornadas desenhadas por quem criou o jogo.
Crawford tem sua definição clássica do que seria um jogo
eletrônico:
um
sistema
formal
fechado que
representa
subjetivamente uma extensão de uma realidade [2]. Essa definição
também aponta para sistemas de jogos que constroem mecânicas
e dinâmicas para uma experiência que amplia a percepção do que
se vive hoje em um mundo real, porém mais lúdica, sem se
desconectar dos signos e representações adquiridas pelos
indivíduos que experimentam os jogos.
O campo do Game Studies é repleto de contribuições no
sentido de afirmar que quando o jogo possui sistemas de
comandos e ações, sejam elas operacionais ou constitutivas - que
geram relacionamentos, como conflitos e decisões, está se
construindo uma efetividade de atitude que opera no jogador
como uma infinita fonte de complexidade de pensamentos [16].
Por isso, os jogos são ferramentas que estimulam, por suas ações
efetivas, o pensamento sistêmico, a criatividade e a inovação.
2.3 Narrativas Sustentáveis
Baseada nos Objetivos de Desenvolvimento Sustentável (ODS)
da ONU, as narrativas sustentáveis seriam aquelas que
incorporam os princípios e objetivos delineados pelos ODS em sua
trama e argumentos. Isso significa abordar questões, tais quais
erradicação da pobreza, igualdade de gênero, educação de
qualidade, ação climática, entre outros; de uma maneira que
promova a conscientização e a ação em prol do desenvolvimento
sustentável [13].
A narrativa pode destacar histórias inspiradoras de indivíduos
ou comunidades que estão trabalhando para alcançar esses
objetivos, bem como desafiar as estruturas e sistemas que
impedem o progresso sustentável. Em essência, uma narrativa
sustentável baseada nos ODS busca informar, engajar e mobilizar
o público para contribuir para um mundo mais justo, pacífico e
sustentável [13].
Por fim, é importante ressaltar que dentro das narrativas
propostas pelo Metagame existe um forte componente de
comunicação, usando conceitos de diegese, mediação e agência, de
forma a estimular que, dentro do conceito narrativo, os jogadores
possam ter as melhores escolhas, resolvendo problemas. A partir
Metagame Planeta Jovem – Transformando Jovens Em Sua
Melhor Versão
W4E’2024, Juiz de Fora/MG, Brazil
destes resultados e analisando indicadores de desempenho do
aprendizado [8], foi possível identificar que competências
socioemocionais foram desenvolvidas – o que torna esses jovens
mais acolhedores, generosos e resolutivos.
3 METODOLOGIA
Nesta seção, são exploradas as metodologias e ações operacionais
presentes no Planeta Jovem, onde serão aplicados os conceitos
teóricos em um modelo de produto viável para ser usado nas
mídias online.
O Planeta Jovem possui duas camadas: a vivência do mundo
real e a experiência online, implementando o conceito de
metaverso.
Figura 2: Design de Experiência do Planeta Jovem
Como é apresentado na Figura 2, no mundo real (chamado de
Planeta Terra - e que depois ganhou o apelido de Planeta Velho
pelos participantes do projeto), são debatidos os problemas
enfrentados nos territórios onde os jovens vivem. Essas dores são
discutidas em seus ambientes escolares, no caso, Institutos
Federais, escolas públicas e demais instituições formativas, a partir
de uma convocação de professores coordenadores de cada
unidade.
São promovidas ações de fruição durante esse período, para
que se possa construir ambientes livres e com menor pragmatismo
da escola tradicional, trazendo novamente os princípios da Escola
Nova de Anísio Teixeira e mudando o espaço físico do
conhecimento [9].
Finalmente, os alunos com melhores desempenhos nessas
atividades são chamados para participar da narrativa do
Metagame. Eles se tornarão tripulantes da Nave do Futuro,
embarcando em uma jornada para a exploração de um planeta
recém avistado, chamado de Planeta Jovem. Esta narrativa foi feita
através de um evento que contou com a participação da
Governadora do Estado do Ceará, o Reitor do IFCE e demais
autoridades [7].
Uma vez no mundo digital e na superfície do Planeta, os jovens
devem se organizar para terraformar o novo território e repará-lo
para a chegada de visitantes que se tornarão novos cidadãos do
Planeta Jovem. A narrativa ainda diz que no planeta existem
países habitados por populações humanas que vivem os mesmos
problemas do Planta Velho. Para ajudar a construir esse novo
planeta são delegadas atividades e seus entregáveis, como a
criação de um sistema de governo, constituição, bandeira, hino,
programa estratégico e outros modelos de gestão do país. Os
jovens têm total liberdade para criar seu modelo ideal, e são
responsáveis por suas escolhas.
No Planeta Jovem eles terão o desafio de preparar atividades
para os novos cidadãos, que são outros jovens que vão povoar o
planeta. Dessa forma, é estabelecida a relação de jovem ensinando
outro jovem, realizando, assim, as entregas a partir das novas
experiências vividas.
Finalmente, os jovens transformados retornam para o mundo
real e apresentam os resultados de sua jornada, munidos de
ensinamentos e mudanças que vão impactar o planeta Velho.
4 PLANEJAMENTO E EXECUÇÃO
Para se conseguir fazer a operacionalização do projeto, foi
necessário construir a jornada através de etapas acompanhadas
por uma equipe de professores e pesquisadores bolsistas.
Figura 3: Roadmap de execução do Planeta Jovem
O projeto foi dividido em cinco fases, tendo três meses de
duração:
1.
Passaporte: nesta fase ocorre a definição e criação dos polos
de estudantes para a inscrição e seleção dos jovens
participantes (2 semanas).
2.
Embarque: nesta fase acontecem eventos de fruição para
seleção dos jovens participantes para se tornarem tripulantes
(1 dia).
3.
Chek-in: nesta fase acontece um treinamento de cem jovens
selecionados para habitar os países do Planeta Jovem (1 mês).
4.
Metagame: nesta fase acontecem atividades de construção
do Planeta e interações jovem com jovem (1 mês).
5.
Desembarque: na fase final ocorre a geração dos relatórios
e preparação para evento final de apresentação dos
entregáveis do projeto (2 semanas).
Para organizar o fluxo de operação e controle, foi necessário
criar cargos para os atores envolvidos no Metagame, com suas
atribuições de trabalho definidas por bolsas de estudo:
W4E’2024, Juiz de Fora/MG, Brazil
D. Gularte et al.
6.
Cidadão: único cargo sem bolsa. Compreendem todos os
visitantes do Planeta Jovem que adotam um país para morar.
7.
Tripulante: jovem bolsista selecionado para habitar e
transformar o Planeta Jovem.
8.
Embaixador: pesquisador bolsista que acompanha o
desenvolvimento dos países e seus entregáveis.
9.
Diplomata: pesquisador, professor, empresário ou qualquer
outro visitante especial, convidado pelos tripulantes para
contribuir na construção do seu país.
10. Coordenador: administrador do game e do projeto em si.
Com o escopo do projeto definido, seus atores designados e a
metodologia pronta, iniciou-se a aplicação do Planeta Jovem,
como projeto piloto (MVP) a ser escalado nas outras edições do
projeto, adequando-se a condições específicas de contextos.
5 FERRAMENTAS E APLICAÇÃO
A seção a seguir detalha quais recursos e ferramentas foram
utilizados na execução do Planeta Jovem e no atingimento do seu
propósito.
5.1 Camada Narrativa do Metagame
Com a construção de uma ideia de game, onde o jogador se
coloca na posição de um explorador tripulante em uma viagem
para descobrir um novo planeta, foram criados países no Planeta
Jovem, onde cada um tem um problema semelhante ao Planeta
Velho.
Os tripulantes foram sorteados e ocuparam seus espaços em
cada país, que foi acompanhado por um embaixador. Uma vez
sabendo da temática envolvida no seu país, os tripulantes foram
estimulados a pesquisar projetos, produtos, trabalhos, pesquisas e
pessoas que pudessem dar subsídios na construção de soluções
para cada problema.
Figura 4: Temáticas sustentáveis para os países do Planeta
Jovem
A partir dos conceitos elaborados pelos objetivos do
desenvolvimento sustentável, as seguintes narrativas foram
desenvolvidas para cada país, alinhadas com as ODS a partir de
desafios:
1.
Galixandria: É o país mais antigo do Planeta Jovem e o mais
isolado. Essa região de prevalência tropical sofre com
doenças sazonais e a Saúde Digital pode diminuir riscos para
a população e melhorar se bem-estar.
2.
Midória: Capital do Planeta Jovem e o país mais
desenvolvido economicamente, que possui problemas
estruturantes sérios. A população mais rica habita os topos
dos edifícios, enquanto a população mais pobre enfrenta a
exclusão cidadã. A Transformação Digital pode superar esses
problemas e erradicar a pobreza.
3.
Lervúria: É o país responsável por quase toda a produção
industrial e manufatura. Ferrovias e portos dessa clássica
cidade encontram escassez de recursos, e ouviram dizer que
a
Transição
Energética
poderia
trazer
novamente
prosperidade para o lugar, com energia limpa e renovável.
4.
Astalânia: País insular e com sua população de maioria de
pescadores. Considerado um país pouco desenvolvido, ainda
não enxergou que a Economia do Mar pode trazer um futuro
maravilhoso para seus moradores, em uma agricultura
sustentável e diminuindo a fome no Planeta.
5.
Dolminn: País com diversos rios e chapadas, onde habitam
diversas tribos que usam sua Arte e Cultura como forma de
identidade, mas podem inspirar pessoas a pensarem sobre a
união dos povos, sua igualdade de gênero e a economia
criativa.
6.
Jakkar: Este país, rodeado por montanhas e vales, abriga
tribos selvagens que vivem em constante conflito. Acredita-
se que uma Escola do Futuro de qualidade trará educação
para se atingir a paz e a felicidade, com menos desigualdade.
7.
Jambor: É um país vizinho a Jakkar e de grandes florestas e
riquezas naturais, como cachoeiras e grutas. A invasão dessas
terras está deixando devastação. Pautas como Meio Ambiente
e Sustentabilidade são urgentes por aqui, evitando problemas
climáticos.
8.
Olímpia: A ilha mitológica onde vivem as vozes dos antigos
mestres e ocorre os encontros dos países no Conselho
Planetário.
5.2 Ferramentas Multimídia Utilizadas
Como o projeto não previu orçamento para a construção de
uma plataforma própria, foram utilizadas soluções e softwares
livres na aplicação do Planeta Jovem. A principal ferramenta é
uma plataforma colaborativa chamada Discord, que dispõe de uma
versão na web e para celulares de forma gratuita. Foi criado um
servidor próprio do Planeta Jovem nesta plataforma, para que
todos os tripulantes pudessem construir seu ambiente lúdico,
ativando o Círculo Mágico e imaginando o seu país, através da
interação com essa plataforma.
Dentro do servidor, a mecânica do Metagame usada foi
semelhante a um Role Playing Game (RPG): o usuário, como
jogador, viveu um papel fictício escolhido por ele, personalizável
e único. A partir da narrativa divulgada no Metagame, os
jogadores tiveram total liberdade para imaginar o mundo da forma
que quisessem. As imagens geradas por Inteligência Artificial
usadas por eles, ou ilustrações, designs e dados de conteúdos
pesquisados montavam uma espécie de portfólio de cada país,
podendo ser exibido e visto por outros participantes do servidor.
Para construir o Planeta Jovem, os tripulantes usaram
qualquer insumo ou recurso disponível por eles de forma online.
Foram disponibilizados espaços com computadores pelo IFCE e
Metagame Planeta Jovem – Transformando Jovens Em Sua
Melhor Versão
W4E’2024, Juiz de Fora/MG, Brazil
escolas afiliadas ao projeto, para que eles pudessem realizar as
tarefas diárias.
Os estudantes criaram artes da geografia de cada país, banners
promocionais e documentos diversos em programas digitais
escolhidos por eles, como Canva, Blender, Minecraft, ferramentas
de inteligência artificial generativa e até um aplicativo em
realidade aumentada para vender o turismo do seu país.
Figura 5: Publicação de atividades jovem com jovem através
do Discord
A vantagem do Discord é que a ferramenta pode ser usada
gratuitamente em qualquer dispositivo com Internet e consegue
receber arquivos multimídia, como hipertexto, vídeos e áudios -
inclusive gravando as atividades das reuniões. Dentre as
atividades, que aconteceram de forma planejada semanalmente,
eram propostos desafios para os países, como, por exemplo,
estabelecer relações comerciais entre um país vizinho, erradicar
uma doença global, trabalhar o adoecimento mental que gerou a
abstenção de outros membros do Planeta, dentre outros assuntos,
que foram trabalhados em uma narrativa dentro do jogo.
O Discord também possui interatividade, como salas de
conversa, palco para eventos virtuais e um sistema de divisão de
cargos por usuários capazes de organizar as informações das
contribuições dos tripulantes de forma categorizada por país.
Destaca-se, ainda, a utilização de recursos de linguagem da
Internet, como figuras, emojis, ícones e outros recursos de redes
sociais.
Figura 6: Ambiente Web com os entregáveis do Metagame
Ao final da jornada do Metagame, todos os entregáveis foram
compilados em um website [15]. Lá, foram colocados depoimentos,
downloads de arquivos PDF e demais artefatos para que pudessem
ser acessados por todos pela Internet.
Também foram feitas ações usando outra plataforma gratuita
chamada Mozilla Hubs. Neste espaço, era possível criar ambientes
3D imersivos, usados em eventos online importantes com
visitantes do Planeta Velho.
Todas as sextas-feiras aconteceram os encontros na Ilha de
Olímpia pelo Discord. Era o momento em que todos se
encontravam com seus personagens (inclusive embaixadores e
coordenadores do projeto), onde só poderiam se comunicar pelos
seus avatares. Os tripulantes, no papel de presidentes, primeiros-
ministros ou cientistas de seus respectivos países, compartilhavam
suas atividades semanais.
As atividades de jovens ensinando jovens também foram
planejadas e operadas pelo Discord na última etapa do projeto. Na
fase final do Metagame, o servidor foi aberto para o público em
geral e os jovens dos países criaram eventos presenciais e virtuais
como oficinas, cursos, debates e palestras, divulgados no canal de
cada país dentro do Discord. Nesses eventos, os jovens ensinavam
ferramentas digitais e matérias escolares, além de discutir sobre
pautas sustentáveis. Houve até um show musical.
Através da ferramenta, que permite organizar cargos, limitar
acessos e gerenciar conteúdos em diversos formatos multimídia,
foi possível gerenciar a programação de atividades de forma
prática e fácil. Os cidadãos de cada país usavam uma
representação (que chamamos de pin) para indicar qual país eles
tinham escolhido. Os países mais povoados ganharam vantagens
no Metagame. Essa saudável disputa pela atenção e qualidade das
ações engajou, na primeira edição do game, 3 mil pessoas online.
6 AVALIAÇÃO DO MODELO E IMPACTOS
GERADOS
A seção a seguir vai comentar os principais impactos a partir da
percepção avaliada pelos pesquisadores e professores bolsistas a
respeito da performance do Metagame e seus objetivos, através de
indicadores de TD&E.
Pelos indicadores do modelo CIRO [17], quanto à contexto e
entrada, as tecnologias de jogos ajudaram a engajar participantes
para o projeto, mesmo sendo em municípios distantes
(Guaramiranga, Juazeiro do Norte, Horizonte, Maracanaú,
Fortaleza, Limoeiro, Jaguaribe e Sobral).
As alternativas escolhidas dentro da narrativa de games
também alinharam propósitos das ODS, a partir de desafios
propostos dentro da ação lúdica do jogo com muita aderência,
como visto na construção de cada país e seus problemas de
sustentabilidade, no sentido de ampliar as possibilidades de uso
para outros problemas das ODS, gerando um bom índice de reação
e aprendizagem, definidos por Kirkpatrick e Philips [8] [14].
Nos indicadores de aplicação e comportamento, que avaliam o
entendimento dos objetivos e dos conteúdos desenvolvidos em seu
propósito [8], o Metagame acelerou o entendimento e interesse
dos participantes em resolver problemas e estabelecer conexões
W4E’2024, Juiz de Fora/MG, Brazil
D. Gularte et al.
com a realidade em que vivem, graças aos conceitos aplicados de
imersão através da narrativa metavérsica.
Nos indicadores de resultado [8][17], impacto e ROI [14], foi
necessário realizar análise de indicadores com as seguintes
métricas:
1.
Produtividade: relação entre a entrega e a meta estabelecida
2.
Satisfação: confiança e consequência positiva da atividade
3.
Propósito: entender o valor da atividade e seu impacto
4.
Turnover: taxa de desistência nas atividades propostas
5.
Taxa de conclusão: percentual de atividades completadas
6.
Net Promoter Score (NPS): recomendação de usuários nas
atividades desenvolvidas pelos jovens
7.
Proficiência Técnica: qualificação em ferramentas
8.
Proficiência comunicacional: capacidade de se comunicar
com outros jovens
Para realizar esta avaliação, foram feitas seções de escuta
dentro das atividades de saúde mental e de desenvolvimento
socioemocional durante a fase de Check-in. Durante o Metagame,
os embaixadores avaliaram os indicadores de desempenho. No
processo de geração dos relatórios finais na fase de Desembarque,
foram feitas novas escutas e aplicação de questionários para
avaliar a melhoria dos jovens tripulantes dentro da experiência.
A análise dos dados coletados sugere que o Planeta Jovem
oportunizou a mudança de perspectiva para muitos jovens. Os
depoimentos obtidos mostram que os estudantes bolsistas
encararam com responsabilidade pautas sustentáveis, gerando
para eles ganho pessoal. Muitos jovens retornaram a seus lares
disseminando seus aprendizados e buscando mais oportunidades
de crescimento profissional.
Figura 7: Reportagem indicando bolsista do Planeta Jovem
como representante da cidade de Fortaleza em evento
internacional [12].
Um exemplo é o caso da ex-tripulante Bianca dos Santos, de
23 anos. É graduanda em Engenharia Ambiental e Sanitária do
Instituto Federal de Educação, Ciência e Tecnologia do Ceará
(IFCE) e foi tripulante e embaixadora em duas edições do jogo. Ela
representará a cidade de Fortaleza-Ce na Cúpula Anual dos
Shapers, realizada na sede do Fórum Econômico Mundial, em
Genebra, na Suíça [12].
Essa representatividade, que empodera jovens periféricos dos
territórios cearenses, advém da construção coletiva e senso de
propósito, de companheirismo e gratidão pelas experiências
vividas no programa. Os indicadores de comunicação mostraram
o aumento do acolhimento dos próprios bolsistas entre si nas
tarefas passadas, cujos resultados indicam para o projeto a
construção de uma energia positiva que propagou na plataforma
durante os encontros de trabalho e atividades de jovens com
jovens.
7 CONCLUSÃO
O Planeta Jovem é um projeto que utiliza tecnologias multimídia,
em um design de Metagame, para transformar jovens durante três
meses em indivíduos com mais protagonismo, visão de futuro,
perspectiva e esperança no seu autodesenvolvimento, como
ferramenta de evolução dentro da sua própria realidade.
O projeto, hoje, é um programa permanente do Iracema
Digital - e que busca, todos os anos, atingir diferentes territórios e
ambientes formativos de jovens de escolas públicas. Através da
articulação junto ao setor de TICs do Ceará, o Planeta Jovem tem
conquistado mais espaço e escalabilidade.
A proposta desenvolvida se mostrou extremamente eficaz e
com um custo-benefício capaz de aplicar a solução em diversos
contextos inóspitos às tecnologias digitais, apropriando-se da
Gamecultura como um vetor de engajamento e resolução de
problemas. O planejamento de desenvolvimento socioemocional
despertou o modo criativo do Metagame, dando uma camada
humana às interações online.
As escolhas no uso das plataformas multimídia web gratuitas
não foram um problema e oportunizaram escalabilidade e inclusão
em diversos contextos no projeto de extensão, tendo em vista a
grande gama de aplicações online gratuitas e o poder da
ferramenta Discord como recurso multimidiático.
O uso da Internet como cadeia de publicação online permitiu
que as narrativas das ODS pudessem ser mais facilmente
pesquisadas e transformadas em ideias e projetos replicáveis, com
colaboração de dados e atualização em tempo real, dando mais
clareza ao contexto de game do produto, sua adequação na relação
social da temática com os jogadores e a perfeita aplicação das
teorias de semiose e comunicação no aprendizado proposto.
Em suma, acreditar que os jovens podem se desenvolver em
sua melhor versão, através das tecnologias multimídia, a partir de
problemas de universos e metaversos lúdicos, espelhados no
mundo real, podem sinalizar que o futuro das escolas brasileiras
pode ter um novo rumo: o da inovação disruptiva.

--- FIM DO ARQUIVO: 30499-829-24947-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30500-829-24948-1-10-20241001.txt ---
Mídias Digitais como Ferramentas para Promover o Pensamento
Computacional e Reduzir a Diferença de Gênero em Computação
Lara Oliveira Esteves
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brazil
lara.esteves@estudante.ufjf.br
Luiza Caldeira Daniel
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brazil
luiza.caldeira@estudante.ufjf.br
Sarah Cristina F. da Silva
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brazil
sarah.cristina@estudante.ufjf.br
Maria Luísa R. Guimarães
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brazil
maria.guimaraes@estudante.ufjf.br
Bárbara de Melo Quintela
Programa de Pós-graduação em
Modelagem Computacional
(PPGMC/UFJF)
Juiz de Fora, MG, Brazil
barbara.quintela@ufjf.br
Alessandreia Marta de Oliveira
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brazil
alessandreia.oliveira@ufjf.br
6. Para finalizar são apresentadas
as considerações finais na Seção 7.
PENSAMENTO COMPUTACIONAL E
INCLUSÃO
O Pensamento Computacional (PC) caracteriza-se como uma ha-
bilidade fundamental à formação do ser humano, assim como a
leitura, a escrita e a aritmética. Para além do simples ensino de
conceitos da Computação, o PC envolve a formulação de problemas
W4E’2024, Juiz de Fora/MG, Brazil
Esteves et al.
presentes no cotidiano, para que possam ser solucionados por pes-
soas e máquinas de maneira efetiva, utilizando ferramentas como a
abstração, o reconhecimento de padrões e a decomposição [9, 19].
No Brasil, o PC está incluído na Base Nacional Comum Curric-
ular (BNCC) [11] a partir das primeiras etapas do Ensino Básico.
Entretanto, 46% das escolas públicas do país ainda não implantaram
projetos ou programas relacionados à tecnologia e, embora o acesso
à Internet seja disponibilizado em grande parte dessas instituições,
em 78% delas há relatos de que a conexão apresenta instabilidade
ou para de funcionar com frequência [5].
Nesse contexto, e tendo em vista os diversos desafios educa-
cionais pós-pandemia, especialmente em locais de maior vulner-
abilidade socioeconômica [18], faz-se necessária a utilização de
abordagens acessíveis e democráticas. A Computação Desplugada
é, então, a alternativa ideal para o ensino do PC, por não depen-
der de infraestrutura tecnológica, como computadores ou conexão
com a Internet, e por possibilitar que as atividades sejam também
ministradas por tutores que não são especialistas na área [1], uti-
lizando jogos, contação de histórias, brinquedos e outras ferramen-
tas analógicas. Esta é, portanto, a base das ações desenvolvidas no
escopo do presente projeto, em que busca-se levar ao maior número
possível de escolas da rede pública de ensino as ferramentas para
que todas as crianças, mas principalmente as meninas, possam ter
contato com a Computação ainda no Ensino Fundamental.
WEBSITE INTERATIVO
Durante os dois anos de existência do projeto, aproximadamente
120 estudantes participaram das oficinas promovidas, entre suas
versões completas, aplicadas em escolas municipais, ou reduzidas,
apresentadas em eventos externos. O número de voluntárias e a
distância entre as escolas participantes e a UFJF são os principais
limitadores do impacto causado pelas ações presenciais do pro-
jeto, em especial, as oficinas, visto que a disseminação do conteúdo
abordado depende diretamente de recursos físicos e da disponibil-
idade e capacitação das voluntárias para aplicação das atividades
propostas. Desta forma, o website Meninas Digitais UFJF 1 foi de-
senvolvido visando não somente a disseminação de ideias, artigos
e eventos, como também o aumento do acesso de professores do
ensino básico a materiais pedagógicos, que são disponibilizados de
maneira pública e gratuita, viabilizando a aplicação das oficinas de
Algoritmos e PC em outras instituições de ensino. Para a primeira
versão foi escolhida a ferramenta Google Sites por já oferecer
uma série de opções que simplificam a criação de páginas Web de
forma gratuita.
Todo o conteúdo elaborado no contexto do projeto é disponibi-
lizado na página Web e categorizado entre publicações (artigos e
vídeos), oficinas, eventos e blog. Na seção “Oficinas”, as três aborda-
gens já desenvolvidas ou adaptadas pelo projeto são: “Aprendendo
Algoritmos com Cubos Mágicos”, “Turing Tumble” e “Números
Binários”.
1. Cubos Mágicos: Para esta oficina, disponibilizou-se um fôlder
para impressão que contém as instruções iniciais sobre o algoritmo
para resolver a face branca do cubo mágico, além de uma apresen-
tação inicial do cubo e um tutorial para resolução do mesmo.
1https://sites.google.com/ice.ufjf.br/meninasdigitaisufjf/
Figura 1: Captura de tela da página “Aprendendo Algoritmos
com Cubos Mágicos” no site do projeto.
2. Turing Tumble: Esta oficina utiliza o jogo educativo Tur-
ing Tumble [15] para introduzir conceitos de algoritmos e PC. O
material original do jogo inclui um tabuleiro mecânico, as peças
necessárias e um livro com as instruções e desafios em língua inglesa.
Para a aplicação das oficinas, as voluntárias do projeto traduziram
e adaptaram todo o material para o contexto das salas de aula,
disponibilizado na página Web como uma apresentação de slides.
Além disso, um tabuleiro de papel para impressão foi elaborado e
está disponível para download em formato PDF. As instruções de
aplicação estão separadas por encontros, do primeiro ao décimo, e
detalham quais peças serão introduzidas em cada dia, além de um
passo a passo sobre como conduzir a oficina e quais slides utilizar.
Dessa forma, mesmo que a instituição não possua o jogo original, é
possível conduzir a oficina com simuladores online associados ao
material produzido pelo projeto.
3. Números Binários: Já para a oficina de apresentação de
números binários com cartões numerados, também são oferecidos
materiais divididos por faixa etária e instruções para aplicação.
Além da descrição das oficinas, o website também é utilizado para
a divulgação e registro dos eventos organizados pelo projeto na UFJF
e a seção Blog contém textos mais longos, focados na disseminação
de diversos conteúdos relacionados à Computação e ao projeto.
Dessa forma, utilizando a página Web para expandir o acesso de
professores e estudantes a esses materiais, a replicação das oficinas
nas escolas é facilitada, incentivando o engajamento, a entrada
e permanência de meninas na área STEM (Ciência, Tecnologia,
Engenharia e Matemática).
REDES SOCIAIS DO PROJETO
Um dos grandes objetivos do projeto é utilizar as redes sociais como
ferramentas fundamentais para divulgar a presença feminina na
área de Computação no intuito de fortalecer a comunidade trazendo
mais diversidade para a área. Com um foco em conteúdo acessível
Mídias Digitais como Ferramentas para Promover o Pensamento Computacional e Reduzir a Diferença de Gênero em Computação
W4E’2024, Juiz de Fora/MG, Brazil
Figura 2: Capturas de tela da página “Turing Tumble”.
e lúdico, busca-se alcançar mais meninas e atraí-las para a área de
Computação. O conteúdo compartilhado inclui dicas, cobertura de
eventos de tecnologia, divulgação das oficinas realizadas por profes-
soras e tutoras de graduação, conteúdo sobre mulheres relevantes
na área de Computação, publicações interativas, todas pensadas
para tornar o aprendizado mais divertido e engajador.
No momento são utilizadas várias plataformas considerando que
cada uma possui um público diferente, com o objetivo de alcançar
a maior quantidade de pessoas possível. O Instagram, por ter uma
grande presença do público jovem que busca entretenimento e
informação, é essencial para promover as oficinas, compartilhar
conquistas de alunas, e criar um espaço onde as meninas possam se
sentir apoiadas. Essa rede auxilia no alcance de um público amplo
e também facilita a interação com as seguidoras, promovendo um
diálogo aberto e incentivando a troca de experiências.
As postagens são compartilhadas também na rede Facebook e
os vídeos no YouTube e TikTok, como ferramentas estratégicas
para ampliar o alcance da iniciativa. No Facebook, com o objetivo
de alcançar pais e educadores que possam apoiar e incentivar as
meninas a se interessarem por tecnologia. Já no TikTok, o formato
dinâmico e envolvente da plataforma é aproveitado para criar vídeos
curtos e criativos, que despertem a curiosidade e incentivem o
aprendizado. O YouTube permite a postagem de vídeos de maior
duração e, portanto atualmente conta com vídeos que apresentam
o projeto além dos vídeos no formato shorts.
Embora seja sabido que a presença de crianças em redes sociais
não seja ideal, a realidade é que muitas estão ativas nessas platafor-
mas [4]. Diante disso, utilizar essas redes para disseminar conteúdo
educativo e inspirador nos permite não apenas lidar com a realidade
do uso precoce de redes sociais, mas também transformá-la em uma
oportunidade para promover o ensino.
O LinkedIn é utilizado para divulgar o projeto e aumentar a
possibilidade de ampliar o alcance e estabelecer parcerias. Dessa
forma, as iniciativas e resultados são frequentemente compartil-
hadas, buscando apoio de empresas e instituições que possam unir
forças na missão de fomentar a inclusão de mulheres na tecnologia.
Essa rede é importante para construir credibilidade e atrair novos
interessados para o projeto.
TRABALHOS RELACIONADOS
Existem diversos trabalhos relacionados à proposta deste projeto
de extensão. A revisão sistemática apresentada em Chen et al. [2]
mostra como as atividades desplugadas promovem o Pensamento
Computacional em contextos educacionais (jardim de infância ao
ensino médio). Os autores concluem que atividades desplugadas
são uma ferramenta poderosa para a educação em PC, oferecendo
uma introdução acessível e motivadora para alunos de várias idades.
As atividades desplugadas não somente facilitam o entendimento
de conceitos computacionais complexos, mas também são eficazes
para envolver meninas na área de computação, promovendo uma
educação mais inclusiva e equitativa.
O trabalho apresentado em Lin et al. [10] examina a eficácia das
atividades plugadas e desplugadas no desenvolvimento de habili-
dades de Pensamento Computacional em crianças. O estudo mostra
que ambas as abordagens são eficazes, com atividades plugadas
oferecendo uma compreensão mais rápida e profunda de conceitos
complexos, enquanto atividades desplugadas são mais acessíveis,
fáceis de implementar e particularmente eficazes em aumentar a
motivação e o interesse das crianças, especialmente entre meninas.
Os autores recomendam a integração de ambas as abordagens para
maximizar os benefícios educacionais e motivacionais. Os autores
concluem ainda que atividades desplugadas tendem a ser mais in-
clusivas e motivadoras, especialmente para meninas, promovendo
um ambiente de aprendizado colaborativo e prático.
O trabalho de Sartori et al. [16] ressalta a desigualdade de gênero
na área da Computação e traz o relato da aplicação de duas oficinas
de desenvolvimento de jogos utilizando a ferramenta Scratch, com 8
meninas em situação de vulnerabilidade social. Como resultado, os
autores mostram que as oficinas trouxeram empoderamento para
as meninas, que se sentiram motivadas com o assunto proposto de
Computação e desenvolvimento de jogos, demonstrando interesse
em aprender mais sobre a área de desenvolvimento.
O projeto de extensão Meninas Digitais - Regional Sergipe [6],
trouxe uma reflexão sobre a importância do ensino do PC e progra-
mação para meninas no ensino básico. O projeto aplicou o curso
“App Inventor2” com 13 alunas do ensino fundamental e médio de
duas escolas públicas da cidade de Lagarto. No curso, as alunas
utilizaram as ferramentas Scratch e App Inventor2 para desenvolver
programação em blocos. Ao final do curso, apesar das dificuldades
de infraestrutura e de desenvolvimento enfrentadas por algumas
alunas, foi constatado pelo projeto que as estudantes tiveram mel-
hora no raciocínio lógico para resolução de problemas, além disso,
a maioria das alunas se sentiu motivada a continuar os estudos na
área e se inscreveu para o Ensino Médio integrado do IFS na área
de redes de computadores e em outros cursos da instituição.
Fiori et al. [7] apresentam as estratégias utilizadas para adaptar
as ações do projeto Meninas Digitais do Vale a um contexto digital,
devido às limitações ocasionadas pela pandemia do COVID-19.
Foram promovidos cursos e eventos online para a comunidade
W4E’2024, Juiz de Fora/MG, Brazil
Esteves et al.
acadêmica e o público geral, além da produção de conteúdo para
as redes sociais. Como resultado, o projeto conseguiu manter a
qualidade de suas ações, mesmo em um cenário adverso, e pôde
impactar um público maior e mais diverso.
A abordagem do projeto de extensão Meninas Digitais UFJF
assemelha-se aos trabalhos apresentados por utilizar de meios dig-
itais para a disseminação de iniciativas que visam o aumento da
participação feminina em STEM. O enfoque em Computação De-
splugada para ensino de PC se dá pela facilidade de implementação
do método e pela promoção de um ambiente mais inclusivo e, con-
sequentemente, motivador.
RESULTADOS
Nesta seção são abordados os impactos das redes sociais e resultados
de pesquisa realizada com as integrantes do projeto.
6.1
Impactos das Redes Sociais
O perfil do projeto no Instagram conta com mais de 750 seguidores,
majoritariamente entre 18 e 44 anos (66, 5% identificam-se como
mulheres), e quase 100 publicações. Em 2024 percebe-se um au-
mento no número de novos seguidores em março, que acompanha
o início do semestre letivo, e em maio, quando as integrantes do
projeto marcaram presença em um evento de tecnologia local e
outro nacional (Figura 3).
Os vídeos curtos, ou reels, são o formato mais popular dentre
os conteúdos compartilhados no perfil. Segundo dados extraídos
no dia 18 de julho de 2024 e apresentados na Tabela 1, publicações
relacionadas diretamente às ações do projeto ou à participação em
eventos externos lideram em número de visualizações. Em alguns
casos, os vídeos sobre eventos foram publicados de forma colabo-
rativa com outros perfis, ou compartilhados pelos mesmos, o que
auxilia no alcance. Entre os vídeos informativos, a série “Guia de
Sobrevivência”, com dicas gerais sobre a UFJF para calouros, destaca-
se por somar mais de quatro mil visualizações. Da mesma forma,
outros conteúdos com grande potencial de compartilhamento apre-
sentam bons resultados, como os registros estratégicos de insights
ou falas empoderadoras, conhecidos popularmente como cortes,
feitos em eventos organizados pelo projeto.
número de seguidores Instagram
Janeiro
Fevereiro
Março
Ab̶il
Maio
Junho
Julho
Figura 3: Número de novos seguidores no Instagram por mês
para o ano de 2024 de 01/01 até 16/07.
Com relação ao alcance das publicações nas redes sociais uti-
lizadas, as postagens do Instagram apresentam um alcance muito
Tabela 1: Visualizações totais e média por categoria de vídeo
postado no Instagram.
Categoria
Total
Média
Vídeos sobre participação em eventos externos
Vídeos sobre ações do projeto
1486.2
Vídeos informativos sobre a UFJF
Falas em eventos organizados pelo projeto
Outros
654.6
maior do que as do Facebook, comparando o mesmo conteúdo com-
partilhado nas duas redes de janeiro a julho de 2024, como pode ser
observado nas Figuras 4 e 5, respectivamente.
Alcance Instagram
01/01/2024
01/02/2024
01/03/2024
01/04/2024
01/05/2024
01/06/2024
01/07/2024
Figura 4: Alcance do perfil no Instagram para o ano de 2024
de 01/01 até 16/07.
Alcance Facebook
01/01/2024
01/02/2024
01/03/2024
01/04/2024
01/05/2024
01/06/2024
01/07/2024
Figura 5: Alcance das postagens no Facebook para o ano de
2024 de 01/01 até 16/07.
O número de visitas ao perfil do Instagram acompanha a tendên-
cia do alcance tendo um número maior de visitas ocorrendo próximo
ao início do semestre letivo. Mesmo com a greve nacional de pro-
fessores ocorrendo em maio e junho o perfil do Instagram manteve
um número razoável de visitas, enquanto a página do Facebook
teve um aumento no final de junho e início de julho conforme pode
ser observado nas Figuras 6 e 7, respectivamente.
A página do projeto no LinkedIn foi criada em 2023, com posta-
gens sendo realizadas de forma mais frequente acompanhando as
demais redes a partir de Fevereiro de 2024. No início de maio e
Mídias Digitais como Ferramentas para Promover o Pensamento Computacional e Reduzir a Diferença de Gênero em Computação
W4E’2024, Juiz de Fora/MG, Brazil
Visitas ao perfil do Instagram
01/01/2024
01/02/2024
01/03/2024
01/04/2024
01/05/2024
01/06/2024
01/07/2024
Figura 6: Visitas ao perfil do Instagram para o ano de 2024 de
01/01 até 16/07.
Visitas ao perfil no Facebook
01/01/2024
01/02/2024
01/03/2024
01/04/2024
01/05/2024
01/06/2024
01/07/2024
Figura 7: Visitas à página do Facebook para o ano de 2024 de
01/01 até 16/07.
início de junho observa-se picos nas impressões das publicações
relacionadas a eventos dos quais as integrantes do projeto partici-
param (Figura 8).
Impressões (orgânicas) LinkedIn
01/01/2024
01/02/2024
01/03/2024
01/04/2024
01/05/2024
01/06/2024
01/07/2024
Figura 8: Impressões das publicações do LinkedIn para o ano
de 2024 de 01/01 até 15/07.
O projeto possui também perfil no YouTube desde janeiro de
2023, contando com 13 inscritos, 9 vídeos e 273 visualizações. O
perfil do TikTok até o presente momento contém apenas dois vídeos
que não atingiram visualizações significativas para permitir uso de
ferramentas para exportar os dados. Pretende-se compartilhar mais
conteúdo nesse formato futuramente para disseminar ainda mais
os conceitos de Computação e apresentar mais mulheres relevantes
para o público jovem que utiliza esta plataforma.
Em conjunto, Instagram, Facebook, YouTube, TikTok e LinkedIn
são importantes ferramentas de divulgação para este projeto. Elas
permitem alcançar mais meninas através da divulgação de conteúdo
de forma lúdica e fortalecem a questão da representatividade ao
compartilhar várias ações realizadas pelas participantes e também
mulheres relevantes na área de Computação. Espera-se que com a
disseminação realizada pelo projeto, pessoas de todos os gêneros e
idades de forma geral possam se informar mais sobre a presença
feminina na área de Computação e passem a contribuir para que
mais meninas se sintam pertencentes a essa comunidade.
Além disso, é possível, por meio dessas plataformas, estabelecer
colaborações com escolas, empresas de tecnologia que compartil-
ham nossos objetivos. Assim, as redes sociais facilitam a construção
de parcerias com outras organizações e profissionais, amplificando
a mensagem do projeto e fortalecendo as iniciativas.
6.2
Pesquisa com as integrantes
No primeiro ano do projeto uma pesquisa foi conduzida com as
integrantes, alunas de graduação, para avaliar se a elaboração e
aplicação de oficinas de Pensamento Computacional no Ensino Fun-
damental voltado para meninas, junto da divulgação de conteúdo
relacionado em redes sociais, poderiam colaborar para a melhoria
da experiência acadêmica dessas alunas e aumentar o interesse e
a participação feminina nas áreas de tecnologia e Computação a
longo prazo.
Das 6 integrantes iniciais apenas 3 participaram da pesquisa. Um
questionário foi aplicado assim que elas foram selecionadas para
participar do projeto, antes do desenvolvimento de qualquer ativi-
dade, e um segundo questionário foi aplicado ao final do primeiro
ano de participação após terem atuado aplicando oficinas presenci-
ais, elaborando conteúdos e postagens em redes sociais. Nenhum
dado pessoal foi armazenado no questionário, não sendo permitida
a identificação das participantes. O projeto, incluindo os dois instru-
mentos utilizados, foi aprovado pelo Comitê de Ética em Pesquisa
em Seres Humanos (CEP) CAAE 58881822.9.0000.5147.
As integrantes do projeto que responderam à pesquisa eram,
no momento da resposta, alunas de graduação matriculadas em
cursos relacionados à Computação como Ciência da Computação
e Sistemas de Informação. Não foram coletados esses dados es-
pecificamente junto das respostas para minimizar possibilidade
de identificação e para que as participantes se sentissem mais à
vontade ao responder o questionário.
6.3
Questionário Inicial
Entre as três participantes, uma indicou ter tido contato com labo-
ratório de informática no Ensino Fundamental, outra indicou ter
tido contato no Ensino Médio e a terceira indicou não ter tido con-
tato na escola antes de ingressar no Ensino Superior. Ao questionar
mais especificamente se tiveram contato com uma disciplina de
Informática ou Computação na escola, apenas uma participação
respondeu de forma afirmativa. Todas as participantes afirmaram
ter tido acesso a um computador em casa ou na casa de algum
parente antes dos 14 anos. E em uma pergunta relacionada, todas
W4E’2024, Juiz de Fora/MG, Brazil
Esteves et al.
afirmaram que esse contato pode ter influenciado na escolha por
um curso superior na área de Computação.
Ao serem questionados se já haviam vivenciado ou presenci-
ado alguma situação que as tenham deixado desconfortáveis com
relação ao gênero durante o curso superior no ambiente da UFJF,
infelizmente todas responderam que sim. E em uma pergunta mais
específica com relação à participação em projetos, estágios ou no
caso de já terem trabalhado na área de tecnologia, duas responderam
já terem vivenciado ou presenciado alguma situação desconfortável
em relação ao gênero. Entre as três participantes, duas responderam
ter pensado em desistir por se sentir minoria ou pensar que exis-
tem mais oportunidades para quem se identifica como cisgênero
masculino. E 100% das participantes responderam que gostariam de
mudar algumas coisas no ambiente acadêmico nos relacionamentos
com professores e colegas. Em um campo aberto para comentários
e sugestões, uma resposta foi registrada:
"O meu contato com o Pensamento Computacional du-
rante o ensino médio foi crucial para minha decisão de
seguir a área durante a graduação. Com certeza se eu
tivesse tido o contato anteriormente seria muito mais
segura e confiante em seguir o caminho da tecnolo-
gia como um todo. A questão da representatividade
também é extremamente importante, e me incentivou
muito quando pensei em desistir durante a gradu-
ação. Por isso, e por acreditar que pode-se diminuir
essa desigualdade de gênero na área que entendo a
importância do projeto Meninas Digitais UFJF."
6.4
Questionário Final
Ao responderem o questionário final, apenas uma participante in-
dicou ter vivenciado ou presenciou alguma situação que a tenha
deixado desconfortável com seu gênero durante a duração do pro-
jeto no ambiente da UFJF, duas indicaram terem presenciado ou
vivenciado alguma situação fora da universidade. Novamente, to-
das as participantes indicaram que mudariam algumas coisas no
relacionamento com professores e colegas.
Com relação aos impactos da realização do projeto, todas respon-
deram de forma afirmativa sobre a sua participação no projeto de
extensão ter influenciado a forma que se veem como profissionais na
área de Computação. Com relação ao conteúdo, todas responderam
também de forma afirmativa acreditarem que abordar conceitos de
Pensamento Computacional no ensino infantil pode colaborar para
diminuir os estereótipos de que Computação é para meninos. E,
por fim, todas responderam que recomendariam a participação no
projeto a outras colegas de graduação.
No campo de comentários e sugestões, obteve-se uma resposta:
"A participação no projeto foi de extrema importância
para que eu me sentisse acolhida e me identificasse
com mais mulheres na área. Perceber que o ambiente
do projeto sempre foi confortável para que eu pudesse
agir e pensar sem ser questionada sobre o meu gênero
ou vista como inferior pelo mesmo motivo tornou
minha jornada acadêmica muito mais leve. Projetos
como esses são essenciais para a permanência estu-
dantil de meninas/mulheres na área das exatas e atuar
cada vez mais cedo (ensino médio e fundamental) co-
labora para que as meninas vejam como possibilidade
seguir na área como carreira, fortalecendo a represen-
tatividade feminina no meio e inspirando cada vez
mais. Aproveito para agradecer pela oportunidade de
ter feito parte do projeto e torço para que ele continue
e cresça cada vez mais."
As participantes do projeto atuaram tanto na elaboração de
material para as oficinas quanto de mídias digitais para dissem-
inação de conteúdo e divulgação do projeto. Com isso, os resul-
tados da pesquisa podem indicar que essas ações colaboram para
reforçar a permanência das mulheres na área de Computação sendo
necessárias pesquisas com maior número de participantes para
obter resultados mais conclusivos.
CONSIDERAÇÕES FINAIS
Este artigo apresentou um projeto para incentivar meninas no En-
sino Fundamental a se interessarem por Computação através de
oficinas e disseminação de conteúdo nas redes sociais. Além disso,
foram descritas as diferentes formas que o projeto se beneficia com
o uso de sistemas multimídia. Primeiro, na apresentação de con-
teúdos, a criação de vídeos educativos que explicam conceitos de
Computação de forma visual e interativa facilitou o entendimento
de conceitos complexos. O uso de materiais interativos permitiu que
os alunos experimentassem conceitos de Computação de maneira
prática e visual. Para a plataforma online, um website interativo
que disponibilizou recursos educativos, vídeos e tutoriais. Na co-
municação e divulgação, o uso de vídeos, imagens e infográficos
nas redes sociais atraiu a atenção da comunidade e contribuiu para
disseminar não somente as ações do projeto mas também conteúdo
desenvolvido por mulheres relevantes na área de computação. Além
disso, uma pesquisa realizada entre as participantes do projeto in-
dicou que as ações poder ser uma ferramenta auxiliar para per-
manência na área. Concluindo, o uso de sistemas multimídia para
analisar o engajamento e a eficácia das atividades permitiu ajustes
e melhorias contínuas no projeto.
A realização de transmissões ao vivo para explicar conceitos,
responder perguntas e interagir com as alunas em tempo real tam-
bém pode ser uma boa prática e está prevista como trabalho futuro.
Além disso, vídeos tutoriais, passo a passo, podem mostrar como
realizar atividades desplugadas de forma clara e didática e como
foram desenvolvidas. Outra sugestão de trabalho futuro é a utiliza-
ção de formulários e questionários interativos que permitam coletar
feedback das alunas sobre as oficinas e atividades.
AGRADECIMENTOS
À Universidade Federal de Juiz de Fora (UFJF) e ao Programa Meni-
nas Digitais (PMD) pelo apoio à realização deste projeto de extensão.

--- FIM DO ARQUIVO: 30500-829-24948-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30501-829-24949-1-10-20241001.txt ---
Nutrilac: Aplicativo para Elaboração de Dietas de Bovinos de Leite
Deivid Duarte Nascimento
deivid.nascimento@engenharia.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Marcos Paulo Priamo Ferreira
marcos.priamo@estudante.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Brian Luís Coimbra Maia
brianluis@ice.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Maria Carolina da Silva Vita
mariacarolina.vita@estudante.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Alexandra Calda Cézar de Faria
alexandra.faria@estudante.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Almira Biazon Franca
almira.biazon@ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Amália Saturnino Chaves
amalia.chaves@ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Luiz Maurílio da Silva Maciel
luiz.maurilio@ice.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
Priscila Vanessa Zabala
Capriles Goliatt
capriles@ice.ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Minas Gerais, Brasil
W4E’2024, Juiz de Fora/MG, Brazil
Nascimento et al.
Figura 1: Tela inicial do aplicativo (à esquerda) e menu prin-
cipal (à direita).
sem a necessidade de conexão com a internet. Portanto, o aplicativo
tem o objetivo de trazer uma maior acessibilidade aos pequenos
e médios produtores, na formulação de dietas personalizadas de
bovinos leiteiros, a fim de maximizar suas produções, tendo um
suporte nutricional e de auxílio na gestão.
METODOLOGIA
Nesta seção será descrito o processo de desenvolvimento do aplica-
tivo. A Subseção 2.1 descreve a coleta de dados para compor a base
de alimentos. A Subseção 2.2 descreve o modelo de otimização em-
pregado. Finalmente, a Subseção 2.3 descreve o desenvolvimento
da aplicação final.
2.1
Construção da base de alimentos
Inicialmente foi realizado um levantamento dos principais alimen-
tos utilizados pelos produtores da região sudeste do Brasil, onde
se localiza o público alvo da aplicação. Foram escolhidos 103 al-
imentos e as informações nutricionais (energia líquida, proteína
bruta, cálcio e fósforo) de cada um foram obtidas do CQBAL 4.0
[2]. Os alimentos foram divididos nas seguintes categorias, visando
facilitar a localização pelo produtor: capins, fenos, silagens, con-
centrados energéticos, concentrados proteicos e minerais. Também
foi realizada uma pesquisa no mercado para obter o custo de cada
alimento para o produtor na região da Zona da Mata de Minas
Gerais. Essas informações foram incluídas no banco de dados do
aplicativo.
2.2
Modelo de otimização
A partir do NRC 1989 [7], referência na literatura em nutrição de
bovinos de leite, foram definidas equações para a exigência nutri-
cional de cada um dos nutrientes considerados no aplicativo. Essas
equações tomam como base informações de entrada do usuário ref-
erentes a cada lote de animais, como peso médio, produção média,
dias de lactação e gestação e número de lactações. Ressalta-se que
optou-se pela utilização da versão 1989 do NRC, pois essa versão se
adapta melhor às exigências nutricionais do rebanho de pequenos
e médios produtores, que compõem o principal público alvo do
aplicativo.
Uma vez definidas as restrições, foi construído um modelo de
programação linear cuja função objetivo consiste em determinar
a quantidade ideal de cada alimento escolhido pelo usuário para
compor a dieta, de modo a obter o menor custo total. Dessa forma,
a função objetivo do modelo é definida como:
min𝑧=
𝑛
∑︁
𝑖=1
100(𝑐𝑖𝑥𝑖)
𝑝𝑖
,
(1)
onde 𝑥𝑖representa a quantidade in kg de matéria seca do alimento
𝑖, 𝑐𝑖indica o preço por kg de matéria natural, 𝑝𝑖é o percentual de
matéria seca que compõe o alimento e 𝑛corresponde ao número
total de alimentos na dieta.
Em relação às restrições do modelo, considerando 𝑏1, 𝑏2, e 𝑏3
os valores exigidos de matéria seca, energia e proteína, respec-
tivamente, e sendo 𝑛1,𝑖, 𝑛2,𝑖e 𝑛3,𝑖a quantidade dos respectivos
nutrientes no alimento 𝑖, definiu-se as seguintes restrições:
𝑏𝑗≤
𝑛
∑︁
𝑖=1
𝑥𝑖𝑛𝑗,𝑖≤1.1𝑏𝑗,
∀𝑗∈{1, 2, 3}.
(2)
Dessa forma, permitiu-se uma certa tolerância para o valor exigido,
prática comum na formulação de dietas, que facilita a convergên-
cia do modelo. Em relação ao cálcio e fósforo, estabeleceu-se uma
restrição que relaciona esses nutrientes:
𝑏4 ≤
𝑛
∑︁
𝑖=1
𝑥𝑖𝑛4,𝑖≤2𝑏5
(3)
𝑛
∑︁
𝑖=1
𝑥𝑖𝑛5,𝑖≥𝑏5,
(4)
onde 𝑛4,𝑖e 𝑛5,𝑖representam a quantidade de cálcio e fósforo no
alimento, respectivamente, 𝑏4 e 𝑏5 as respectivas exigências.
Na formulação de dietas para bovinos de leite, também é impor-
tante relacionar a proporção de alimentos volumosos e concentra-
dos. Para isso, o modelo conta com a restrição:
𝑟𝑒𝑙=
 Í𝑛
𝑖=1 0.55𝑥𝑖𝑛1,𝑖(1 −𝑦𝑖) −0.45𝑥𝑖𝑛1,𝑖𝑦𝑖≥0
Í𝑛
𝑖=1 0.7𝑥𝑖𝑛1,𝑖(1 −𝑦𝑖) −0.3𝑥𝑖𝑛1,𝑖𝑦𝑖≤0

,
(5)
onde 𝑦𝑖∈{0, 1} assume o valor 1 para alimento volumoso e 0 para
alimento concentrado.
Finalmente, o modelo também restringe o consumo de ureia,
ingrediente que pode prejudicar a saúde dos animais, se consumido
em excesso. Seguindo as recomendações da literatura, restringiu-se
a inclusão de ureia na dieta a 50 g para cada 100 kg de peso corporal.
2.3
Desenvolvimento do aplicativo
O aplicativo foi desenvolvido para dispositivos Android, utilizando
a linguagem de programação Java. O modelo de programação linear
foi implementado utilizando a biblioteca SSC [8]. Uma vez que a
proposta era desenvolver uma aplicação para atender a pequenos
e médios produtores rurais, tomou-se por base duas premissas
importantes, o aplicativo deveria ter uma interface simples, de
Nutrilac: Aplicativo para Elaboração de Dietas de Bovinos de Leite
W4E’2024, Juiz de Fora/MG, Brazil
Figura 2: Tela de cadastro de alimentos (à esquerda) e listagem
de alimentos (à direita).
modo que a dieta possa ser formulada em poucos passos e deve ser
possível utilizar a ferramenta mesmo sem conexão com a internet.
A Figura 1 mostra a tela inicial do aplicativo e o menu princi-
pal. O aplicativo se divide em 3 módulos principais. O módulo de
alimentos tem como objetivo a alteração dos dados dos alimentos
existentes e a inclusão de novos alimentos. Essa funcionalidade é
importante para permitir que o produtor possa alterar o custo ou
as informações nutricionais de algum alimento de acordo com a
realidade da região onde se encontra, assim como incluir algum
alimento que não esteja previamente cadastrado. A Figura 2 mostra
as telas de listagem e cadastro de alimentos. O módulo de animais
permite ao produtor incluir, editar e excluir lotes de animais. A
inserção dessas informações é essencial para que as dietas possam
ser calculadas, uma vez que as informações do lote definem as re-
strições de nutrientes que precisam ser satisfeitas pela dieta criada.
A Figura 3 mostra as telas de listagem e cadastro de lotes.
O módulo de dietas é o principal módulo do aplicativo e tem como
objetivo a criação, visualização e exportação de dietas. A interface
é simples, após a definição do nome e do lote para o qual a dieta
será criada, o usuário seleciona, a partir de uma lista, os alimentos
que gostaria de incluir na dieta. Após a confirmação dos alimentos
selecionados, o modelo de programação linear é executado. Caso
o modelo tenha conseguido obter uma solução viável, o aplicativo
exibe para o produtor uma tela simples indicando a quantidade e
custos de cada alimento que deve ser oferecido aos animais. Caso
não tenha sido possível obter uma dieta, o aplicativo informa ao
usuário a necessidade de incluir novos alimentos na seleção, para
que as exigências nutricionais sejam atendidas. Adicionalmente, o
aplicativo também permite que o usuário exporte o relatório técnico
da dieta em PDF ou planilha. Esse relatório se destina a usuários
mais avançados, e apresenta, além das quantidades e custos de
cada alimento, as quantidades fornecida e ideal de cada nutriente,
assim como um balanço entre essas quantidades. A Figura 4 mostra
as telas de revisão dos alimentos selecionados e de saída da dieta
criada.
Figura 3: Tela de listagem de lotes (à esquerda) e cadastro de
lotes (à direita).
A equipe do projeto possui especialistas em nutrição animal,
os quais realizaram testes no aplicativo, testando a elaboração de
dietas combinando diferentes alimentos e animais/lotes com car-
acterísticas diferentes, com o objetivo de verificar a capacidade do
modelo funcionar em diversos cenários. Os resultados obtidos se
mostraram coerentes com os esperados na literatura e validaram o
modelo implementado no aplicativo.
AÇÕES DE EXTENSÃO REALIZADAS
A proposta de desenvolvimento do aplicativo Nutrilac surgiu no ano
de 2019 no desafio de programação Vacathon, dentro da iniciativa
Ideas for Milk, promovida pela Embrapa Gado de Leite. Tornou-se
projeto de extensão em 2020 e desde então está em desenvolvi-
mento. O primeiro protótipo obteve registro do INPI em 2021 sob
o número BR512022003100-0. Em junho de 2024 o aplicativo foi
oficialmente lançado ao público na Play Store. Ressalta-se ainda
que o projeto possui aprovação no comitê de ética sob o número
CAAE 34339520.4.0000.5147, parecer número 6.667.688.
Visando a divulgação da ferramenta ao público alvo, algumas
ações vêm sendo realizadas nos últimos anos. Durante os anos de
2022 e 2023 foram realizadas visitas técnicas a 4 fazendas. Nessas
visitas, membros da equipe do projeto apresentaram aos produtores
o aplicativo a fim de mostrar o funcionamento e familiarizá-los
com a tecnologia. Também foi possível conhecer a realidade das
propriedades e levantar novas demandas para atualização da ferra-
menta.
Durante as visitas técnicas, os integrantes do projeto tiveram a
oportunidade de acompanhar o dia-a-dia das propriedades, desde
a formulação da ração dos animais até os meios de produção e
manejo da fazenda. Dessa forma, eles presenciaram momentos como
a ordenha, buscando entender o sistema de produção, formas de
armazenamento do leite em cada propriedade, o sistema de criação
do gado de leite, o fornecimento da ração e visitas às capineiras e
pastos. Além disso, em algumas das fazendas, a ração dos animais
era produzida na propriedade, permitindo o acompanhamento do
W4E’2024, Juiz de Fora/MG, Brazil
Nascimento et al.
Figura 4: Tela de revisão dos alimentos selecionados (à es-
querda) e saída da dieta criada (à direita).
processo de picagem das capineiras, mistura do volumoso com o
concentrados, o fornecimento desses alimentos para os animais,
além de buscar entender o acesso dos produtores a esses alimentos
e como foram feitas as formulações das rações até esse momento.
Por fim, nas visitas, após a compreensão do sistema como um
todo, já que todos os fatores são importantes para a produção, os
participantes do projeto buscavam mostrar o aplicativo, orientar so-
bre a formulação da ração e também sobre a utilização do programa,
evidenciando a acessibilidade e integração dos elementos que inter-
ferem na dieta dos bovinos de leite. Durante as visitas, foram levados
questionários para nortear as conversas com os produtores, a fim de
direcionar as perguntas sobre o sistema de produção, destacando-se
a parte de alimentação dos animais.
Recentemente, após a publicação do aplicativo na loja da Google,
foi realizado um evento de lançamento com produtores rurais da
cidade de Liberdade - MG. O evento contou com a participação
de produtores rurais da cidade e região, além de estudantes do
curso Técnico em Agropecuária. Nesse evento a equipe do projeto
realizou uma exposição sobre nutrição animal, ressaltando a im-
portância de uma nutrição adequada para que os animais tenham
melhores condições de saúde e produção de leite. A apresentação
mostrou como dividir os animais em lotes e os tipos de alimentos
que precisam ser incluídos na dieta para garantir uma nutrição
adequada.
Na segunda parte do evento, a equipe do projeto apresentou
a ferramenta desenvolvida, mostrando o passo a passo de como
criar uma dieta, começando pela criação de um lote, passando pela
seleção dos alimentos e formulação da dieta. O público também foi
instruído sobre como incluir e editar os alimentos no aplicativo e
realizar a exportação de relatórios técnicos das dietas criadas.
Ao final, abriu-se espaço para que o público pudesse tirar dúvi-
das diretamente com a equipe sobre os assuntos abordados. Esse
momento foi bastante enriquecedor, com perguntas relacionadas
à alimentação e dúvidas sobre como utilizar o aplicativo para for-
mular uma dieta. O público presente recebeu um material impresso
sobre o aplicativo, com QR codes que permitiam a instalação do
aplicativo e assistir um vídeo tutorial sobre como utilizá-lo. De
modo geral, o evento foi um marco importante para a divulgação
do aplicativo, onde a equipe pode entrar em contato direto com os
beneficiários e apresentar a ferramenta desenvolvida.
CONSIDERAÇÕES FINAIS
A alimentação dos animais é um fator crucial para o sucesso da
produção de leite e, para isso, o produtor precisa de uma assistência
para auxiliá-lo a não cometer erros nesse processo. Nesse contexto,
este trabalho apresentou o Nutrilac, um aplicativo móvel gratuito,
intuitivo e que pode ser muito útil e vantajoso para produtores
rurais e profissionais da área de Medicina Veterinária na elaboração
de dietas de custo mínimo para os animais.
A equipe efetuou visitas em fazendas, com o objetivo de testar
e apresentar a ferramenta para produtores rurais e, dessa forma,
avaliar o funcionamento do aplicativo no próprio ambiente das
propriedades e colher opiniões de produtores rurais. Foi realizado
também um evento de divulgação, permitindo o contato direto da
equipe com os beneficiários da proposta.
Além do seu importante papel junto a produtores rurais, destaca-
se ainda a importância do projeto na experiência acadêmica dos
professores e alunos envolvidos. Por se tratar de uma proposta
multidisciplinar, o projeto permite uma importante troca de ex-
periências entre profissionais das áreas de Medicina Veterinária,
Ciência da Computação e Engenharia. Essa experiência se mostra
extremamente enriquecedora para a formação acadêmica dos en-
volvidos.
Como trabalhos futuros, a equipe buscará realizar aprimoramen-
tos na interface do aplicativo e permitir que ele seja usado em
aparelhos com diferentes sistemas operacionais, o que vai facilitar a
difusão da ferramenta. Os resultados do aplicativo se mostraram sat-
isfatórios a partir das validações realizadas, mas realizar testes adi-
cionais para verificar o funcionamento da aplicação em diferentes
ambientes e obter sugestões dos usuários da ferramenta também
será importante para entender os pontos que podem ser melhorados
em versões futuras.
AGRADECIMENTOS
Os autores agradecem a Universidade Federal de Juiz de Fora (UFJF)
pelo suporte financeiro ao projeto.

--- FIM DO ARQUIVO: 30501-829-24949-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30502-829-24950-2-10-20241024.txt ---
Práticas com smartphones para idosos - Um projeto
de extensão do ICMC/USP
Kamila R. H. Rodrigues
ICMC/USP, São Carlos
kamila.rios@icmc.usp.br
Suzane S. dos Santos
ICMC/USP, São Carlos
suzanesantos@usp.br
Daniele Gallego
ICMC/USP, São Carlos
danielegallego@usp.br
Ketlen Martins
ICMC/USP, São Carlos
ketlen.martins@usp.br
Katherin F. C.
Malpartida
ICMC/USP, São Carlos
katherincm@usp.br
Aline E. C. Verhalen
ICMC/USP, São Carlos
alineverhalen@usp.br
João Pedro de Deus
ICMC/USP, São Carlos
joao.deus@usp.br
1.
De acordo com Zaine et al. [11], a alfabetização digital se refere
ao estágio inicial de aprender a utilizar dispositivos computacio-
nais. Já o letramento digital engloba habilidades mais avançadas,
como compreender, assimilar, reelaborar e adquirir conhecimento
por meio de práticas de leitura, releitura de informações e escrita,
visando utilizar as tecnologias para benefício pessoal e coletivo [1].
1https://prceu.usp.br/usp60/
In: III WebMedia for Everyone (W4E 2024) (W4E 2024). Anais Estendidos do XXX
Simpósio Brasileiro de Sistemas Multimídia e Web (W4E’2024). Juiz de Fora/MG, Brazil.
Porto Alegre: Brazilian Computer Society, 2024.
© 2024 SBC – Sociedade Brasileira de Computação.
ISSN 2596-1683
Para que os idosos possam usufruir de maneira mais ampla dos
benefícios tecnológicos, Zaine et al. [11] sugerem que a exposição
aos dispositivos digitais evolua para uma análise crítica dos conteú-
dos disponíveis. Para Silva e Behar [8], o letramento digital deve
consistir “na capacidade de usar e compreender informações de vá-
rios formatos e fontes, incluindo a apropriação da nova tecnologia
e a prática de leitura e escrita em tela”.
Este trabalho descreve um curso de letramento digital voltado
para o público idoso, que faz parte do programa USP 60+, e que,
ao longo de quase 10 anos, tem ajudado a dar autonomia a uma
parcela desse grupo de usuários da comunidade local, no entorno
da universidade.
O curso é um projeto de extensão que tem acontecido desde
2015, e que semestralmente oferece em torno de 30 vagas para que
idosos da cidade de São Carlos e região possam ter um espaço de
aprendizado sobre smartphones, suas configurações e principais
aplicativos. O curso é ainda um espaço de convívio social e tem
sido palco de diversos relatos de apoio na superação de depressão,
ansiedade, opressão e violência doméstica.
Ao longo desses anos, a metodologia do curso passou por diversas
transformações e, atualmente, representa uma forma consolidada
de realizar o letramento com o referido público, tendo tido sucesso
inclusive durante a pandemia, em seu formato online [5].
Este artigo descreve o formato e metodologia do curso, bem como
traz relatos dos alunos idosos e dos monitores sobre o mesmo.
O documento está dividido como segue: a Seção 2 descreve a
estrutura do curso de letramento digital, a Seção 3 discorre sobre a
metodologia híbrida empregada, a Seção 4 descreve os relatos de
idosos e monitores do curso, e a Seção 5, por fim, discorre sobre as
considerações finais.
ESTRUTURA DO CURSO DE LETRAMENTO
O curso de extensão universitária intitulado “Práticas com smartpho-
nes para idosos” tem ocorrido semestralmente, ao longo de 15 se-
manas, com duas horas aulas por semana, na USP de São Carlos,
no Instituto de Ciências Matemáticas e de Computação (ICMC). No
primeiro semestre é oferecido o módulo básico, com aulas sobre as
principais configurações dos celulares, bem como aulas sobre os
aplicativos mais comumente usados, como WhatsApp, YouTube e
Google [11]. No segundo semestre é oferecido o módulo avançado,
em que uma revisão do módulo básico é inicialmente feita, e, na
sequência, os idosos aprendem a interagir com aplicativos como
W4E’2024, Juiz de Fora/MG, Brazil
Rodrigues et al.
iFood, Uber, Instagram, entre outros. Houve semestres em que os
dois módulos foram oferecidos simultaneamente.
Os idosos se inscrevem por meio do sistema Apolo2, da própria
universidade. São oferecidas semestralmente 30 vagas. Após o pe-
ríodo de inscrição, a coordenação inicia um processo de análise
documental para a seleção dos alunos.
No primeiro dia de aula os idosos são convidados a responder
a um questionário em que descrevem seu conhecimento prévio
sobre tecnologias como tablets, smartphones, rede Wi-Fi, entre ou-
tras, bem como informam sobre seus interesses no uso do celular e
expectativas sobre o curso. No semestre do módulo avançado, os
alunos passam por uma atividade de avaliação do conhecimento,
em que são solicitados a realizar 10 tarefas relacionadas ao con-
teúdo ministrado no módulo básico, como: realizar ligações, salvar
um número na agenda, configurar itens do celular como senha ou
tamanho da fonte, enviar uma imagem no WhatsApp, compartilhar
a localização no WhatsApp, entre outras. Após a avaliação, a co-
ordenação do curso informa sobre o cronograma de aulas e sobre
os conteúdos a serem ministrados, bem como sobre a dinâmica das
aulas e as regras básicas de convivência em sala e no grupo do
WhatsApp da turma.
As treze aulas seguintes são de teoria e práticas relacionadas
aos conteúdos do cronograma. Em cada aula a equipe elabora um
conjunto de slides que são capturas das telas do celular e dos apli-
cativos, ensinando o passo a passo das tarefas foco do ensino em
cada aula. O curso é ministrado apenas para usuários de celulares
da plataforma Android. A Figura 1 ilustra um dos slides sobre o
conteúdo relacionado a como curtir uma publicação no aplicativo
Instagram.
Figura 1: Exemplo de slide de um conteúdo ministrado.
Nota: A imagem dos artistas é de domínio público e extraída
do Instagram de um deles, cujo acesso é aberto.
Após cada sequência de instruções ensinando as funcionalidades
do celular ou do aplicativo em contexto, há um slide resumo para
guiar os idosos na execução da funcionalidade. Em seguida, uma
atividade prática é proposta sobre o mesmo assunto. A Figura 2
ilustra um exemplo de resumo no contexto da aula do Instagram.
O curso conta com o apoio de monitores, que são alunos de
graduação e de pós-graduação da referida universidade. Parte dos
alunos de graduação são bolsistas de Iniciação Científica, outra parte
atua de forma voluntária. Os alunos de pós são todos voluntários.
Antes de ministrar o conteúdo das aulas, os idosos participam
de um Kahoot3, um jogo de perguntas e respostas relacionadas ao
2https://uspdigital.usp.br/apolo/
3https://kahoot.com/
Figura 2: Exemplo de slide resumindo como comentar publi-
cações no Instagram. Fonte: Autoria Própria.
conteúdo da aula ministrada na semana anterior. A Figura 3 ilustra
um dos Kahoots realizados.
Figura 3: Exemplo de Kahoot com perguntas sobre uma das
aulas. Fonte: Autoria Própria.
Essa parte da aula é aguardada e, os idosos costumam ser com-
petitivos. A brincadeira permite reforçar o conteúdo ministrado
de forma lúdica e divertida. Habilidades cognitivas e de destreza
também são trabalhadas nos idosos nesse momento, uma vez que
para alcançar o pódio, além de responder de forma correta, é preciso
responder rápido e tocar com precisão na tela.
Na última aula do curso os idosos respondem novamente ao
mesmo questionário da aula 1, para que se possa comparar o co-
nhecimento no início e no fim do curso. Também passam por uma
avaliação prática, em que são convidados a realizar 10 tarefas sobre
práticas conduzidas durante o curso. Eles são avaliados individual-
mente por um dos monitores. Nesse último dia também há sempre
uma confraternização e o momento da fotografia do grupo.
Cada idoso recebe um crachá com seu nome para ser usado
durante as aulas, e recebem um cartão de acesso ao campus. É
comum relatos de sentimento de pertencimento ao receberem o
cartão de acesso. Os idosos recebem ainda a impressão dos slides em
cada aula, de modo que possam acompanhar e realizar anotações. A
impressão e entrega de forma gratuita dos crachás, bem como dos
cartões de acesso e dos slides das aulas só tem sido possível porque
a coordenação tem submetido o projeto em editais internos da
Universidade de São Paulo, no contexto da Pró-Reitoria de Cultura
e Extensão (PRCEU) e da Comissão de Cultura e Extensão (CCEx) do
ICMC, e recebido aporte financeiro de tais instâncias para o projeto.
Nos anos iniciais do curso, cada aluno pagava pela impressão do
material na gráfica do instituto, caso desejasse tê-lo impresso.
Práticas com smartphones para idosos - Um projeto
de extensão do ICMC/USP
W4E’2024, Juiz de Fora/MG, Brazil
É preciso destacar que a sala de aula do curso é sempre em
andar térreo para permitir acessibilidade, e tal sala é equipada com
Internet dedicada apenas ao curso.
Antes de iniciar o curso os monitores são treinados sobre como
deve ser a postura durante as aulas. Reforça-se que eles não de-
vem fazer as atividades pelos idosos durante as práticas, devem
apenas guiar os alunos. Caso precisem ensinar, devem pedir que o
aluno faça novamente sozinho na sequência. Os monitores também
são convidados a ministrar as aulas sempre em voz alta, clara e
cadenciada.
Os slides são elaborados com pouco texto, contendo mais ima-
gens, em fonte grande e com bom contraste de cores, visando mitigar
problemas de leitura e compreensão por parte dos idosos. Além dos
slides, o monitor deve também propor 5 atividades sobre o tema da
aula.
A sétima aula e a décima quarta aula são revisões do conteúdo.
Todos os monitores ministram ao menos uma aula no curso e de-
vem preparar os slides, que passam na sequência pela revisão da
coordenação do curso. A frequência é semanalmente conferida e
cada aluno deve ter o mínimo de 75% de presença para receber o
certificado ao fim do curso.
Nos dois módulos os alunos têm uma aula dedicada ao ensino
sobre fake news e responsabilidade no compartilhamento, uma vez
que esse público é considerado aquele que mais dissemina esse tipo
de notícia no Brasil [7].
A Tabela 1 descreve o conteúdo que costuma ser ministrado no
curso básico e no curso avançado.
Semanalmente os alunos recebem as 5 atividades para serem rea-
lizadas em suas casas, de modo a reforçar o conteúdo. Os monitores
são responsáveis por acompanhar a realização das tarefas e fazer
as devidas correções com os alunos.
A seção a seguir descreve a metodologia híbrida adotada, em
que os alunos têm as aulas em sala na universidade com atividades
práticas e, em casa, respondem a tarefas relacionadas durante a
semana, por meio de aplicativo chamado de Ajudante Digital.
METODOLOGIA HÍBRIDA ADOTADA
Ao longo dos anos de curso a coordenação observou que apenas as
atividades práticas em sala de aula não conduzia a uma autonomia
no uso do celular por parte de alguns idosos, sobretudo aqueles que
iniciavam o curso sem conhecimento prévio. Destaca-se aqui, que
esse perfil é mais raro atualmente, após a pandemia. Os idosos do
curso chegam atualmente com alguma experiência, em razão das
necessidades impostas pelo período da COVID-19. Antes da pande-
mia, era comum receber idosos com o celular na caixa ou fornecer
apoio para que pudessem comprar seu primeiro smartphone.
Tendo em vista que os idosos realizavam as atividades em sala
e tinham dificuldades de reter o conteúdo se não treinassem, foi
adotado (desde 2017) um aplicativo, chamado de Ajudante Digital,
responsável por enviar tarefas aos idosos de forma remota, ao longo
da semana, com o intuito de relembrar o conteúdo e estimular o
exercício [2, 3].
A metodologia híbrida então é composta de duas etapas: 1) ex-
posição em sala de aula, com atividades práticas de reforço; 2)
realização de atividades em casa, por meio do Ajudante Digital,
com o objetivo de exercitar o conteúdo e reter o conhecimento do
idoso [2, 3]. Os idosos costumam consultar o material impresso e
também tiram dúvidas com os monitores por meio do grupo do
WhatsApp da turma.
Destaca-se que o objetivo deste trabalho não é descrever deta-
lhes complementares da construção da metodologia, uma vez que
a mesma já está disponível em trabalhos prévios [2–4]. O objetivo
deste artigo é trazer um relato da importância do curso de extensão
para idosos e monitores, com destaque para a fala dos mesmos, bem
como trazer o cronograma de conteúdo do curso avançado, dispo-
nível somente a partir do segundo semestre de 2018 no ICMC/USP.
Em cada aula, as 5 atividades planejadas pelo monitor sobre o
tema ministrado em aula são criadas no Ajudante Digital. Os idosos
podem responder a todas as atividades de uma vez, ou uma a cada
dia da semana. Um alarme do aplicativo dispara todos os dias, em
horário acordado com a turma, lembrando sobre a tarefa. A Figura
4 ilustra a sequência de uma das atividades no Ajudante Digital,
relacionada ao aplicativo Instagram.
Na primeira aula do curso os idosos instalam o aplicativo em
seus celulares, com o apoio dos monitores e, são treinados no uso
do Ajudante Digital. Eles então têm a possibilidade de interagir com
os diferentes formatos de respostas que o aplicativo pode pedir ao
longo do curso.
A seção a seguir descreve mais detalhes sobre o aplicativo no-
meando de Ajudante Digital.
3.1
Ajudante Digital
O Ajudante Digital em questão é o sistema ESPIM4 [4, 10], criado
pelo grupo de pesquisa da coordenação do curso de letramento, que
disponibiliza duas interfaces de usuário, uma Web para especialistas
criarem programas de coleta e intervenção (podem ser profissionais
da Saúde e Educação, por exemplo) e outra para dispositivos móveis
– apenas plataforma Android, usada pelo público alvo desses espe-
cialistas para interagir com as intervenções [4]. O sistema permite
a autoria de intervenções baseadas em dados coletados de maneira
explícita e pervasiva. Por meio do uso do sistema é possível ampliar
o alcance da coleta de dados e intervenções realizadas de maneira
remota, no ambiente natural dos participantes de interesse, e sem a
presença direta do especialista [4].
A interface Web do sistema permite a criação de planos individu-
alizados de intervenções programadas, o cadastro de participantes
e a visualização dos resultados. Por meio da interface Web podem
ser criados programas interventivos compostos por questões do
tipo aberta, múltipla escolha com uma ou mais possibilidades de
resposta, mensagens instrucionais/informativas, envio de imagens,
vídeos e áudios, respostas a escalas Likert, escalas de diferencial
semântico, entre outros formatos (ver Figura 5). O fluxo entre uma
questão e outra pode ser diferente, conforme a resposta enviada
pelo participante. Na interface Web, o especialista também define
um intervalo em que as intervenções ou a coleta de dados deverão
ficar ativas. Um aplicativo móvel é disponibilizado pelo sistema na
loja do Google Play5 e esse é um player mobile que replica as tarefas
configuradas na interface Web pelo especialista (ver Figura 4). O
sistema também permite que os participantes recebam notificações
4https://espim.icmc.usp.br/
5https://play.google.com/store/apps/details?id=br.usp.icmc.intermidia.sensem&hl=
pt_BR
W4E’2024, Juiz de Fora/MG, Brazil
Rodrigues et al.
Tabela 1: Cronograma dos dois módulos do curso de letramento digital para idosos (módulo básico adaptado de Zaine et al. [11]).
Aula
Conteúdo
Módulo Básico
Aula 1
Boas-vindas e Apresentações. Apresentação do Ajudante Digital. Questionários iniciais
Aula 2
Linguagem da Internet/Fake News e a Internet (responsabilidades)
Aula 3
Gestos / Botões / Telas / Wi-fi / Organização de Apps
Aula 4
Ligação / agenda e contatos / câmera do dispositivo(fotos) / alarme
Aula 5
Configurações telefone: acessibilidade e segurança
Aula 6
Google Play e Google Fotos
Aula 7
Revisão
Aula 8
WhatsApp - parte 1
Aula 9
WhatsApp - parte 2
Aula 10
WhatsApp - parte 3
Aula 11
Facebook - parte 1
Aula 12
Facebook - parte 2
Aula 13
Segurança nas compras de Internet
Aula 14
Revisão
Aula 15
Questionários + Avaliação. Confraternização
Módulo Avançado
Aula 1
Boas-vindas e Apresentações. Apresentação do Ajudante Digital. Questionários iniciais
Aula 2
Revisão do módulo básico
Aula 3
Linguagem da Internet/Fake News e a Internet (responsabilidades)
Aula 4
YouTube
Aula 5
Gmail
Aula 6
Drive
Aula 7
Revisão
Aula 8
iFood
Aula 9
Spotify
Aula 10
Instagram - parte 1
Aula 11
Instagram - parte 2
Aula 12
Uber
Aula 13
Segurança nas compras de Internet
Aula 14
Revisão
Aula 15
Questionários + Avaliação. Confraternização
Figura 4: Exemplo de tarefa no Ajudante Digital - interface móvel. Fonte: Autoria Própria.
sonoras sempre que houver alguma intervenção programada no
sistema. Deste modo, o especialista pode configurar o horário e o
tipo de alarme que poderá ser disparado [4, 10].
No contexto do curso de letramento, as intervenções são deno-
minadas de tarefas e essas tarefas ficam disponíveis por 7 dias. O
uso do ajudante digital, em complemento às aulas, ocorre desde o
primeiro semestre de 2017, fazendo parte das pesquisas do projeto
Práticas com smartphones para idosos - Um projeto
de extensão do ICMC/USP
W4E’2024, Juiz de Fora/MG, Brazil
FAPESP (processo 2017/19915-0) coordenado pela Profa. Dra. Meire
Cachioni (EACH/USP), em parceria com o ICMC/USP [2, 3].
A Figura 5 ilustra um exemplo de telas na interface Web do
sistema, referente às tarefas ilustradas na interface do aplicativo
Android da Figura 4.
A seção a seguir descreve alguns dos relatos dos idosos e dos
monitores à respeito da última oferta do curso, realizada no primeiro
semestre de 2024.
RELATOS DOS ALUNOS E MONITORES
Ao fim do curso, idosos e monitores são convidados a darem feed-
back sobre a experiência vivida e sobre as suas percepções à respeito
do curso. Algumas das falas dos alunos idosos (I) no último dia de
aula são transcritas abaixo:
I1: “Nós estamos encerrando mais um curso, que pena que passou
tão rápido. Foi maravilhoso esse curso. Deu uma base boa para nós,
além do material que nós levamos para casa. Tenho todos os materiais
guardados na gaveta da escrivaninha, todas as apostilas. Alguma
dúvida que eu tiver vou procurar nelas. E a gente aprendeu bastante
coisa, foi muito muito bom. A equipe é muito atenciosa, os monitores,
todos eles vêm nos auxiliando direto. ”.
I2: “Eu gostei muito do curso e pra mim foi de grande aprendizado.
Espero que não termine por aqui, quero aprender mais.”.
I3: “Foi muito proveitoso, eu aprendi muito [...] cada vez que eu
venho aqui eu aprendo muito mais. Nós idosos 60+, precisamos muito
dessa informação que eles passam para a gente, porque nós, na nossa
ingenuidade, somos muito vítimas de golpe [...] se a gente não estiver
a par do que está acontecendo na internet, no nosso celular, a gente
pode até ser muito prejudicado, e se a gente tiver algum conhecimento,
a gente vai se ver livre das fraudes [...]”.
I4: “Quero dizer que gostei muito do curso, para mim foi muito
produtivo o aprendizado, me sinto motivado e agradeço muito por
esta oportunidade. E que vocês continuem aprimorando o aprendizado
dos idosos [...]”.
Há um grupo de monitores que atuam no curso, alguns há mais
de 3 anos. Alguns são alunos de graduação, da Computação em sua
maioria, que têm a oportunidade de exercitar soft skills, como habi-
lidades como gestão, empatia, e a importância do desenvolvimento
de soluções mais inclusivas e socialmente responsáveis. Ocorre de
forma semelhante para os alunos de pós, com a adição de que a
maioria deles atua com o público idoso em suas pesquisas. Abaixo,
são destacadas as falas de alguns dos monitores (M) da última oferta
do curso em 2024/1.
M1: “É fundamental para o público da terceira idade se conectar
no mundo tecnológico e não apenas isso como também proporciona
um ambiente de interação para esse público que não vive isso há
muito tempo. Além disso, também existem vários benefícios para
os monitores, como o aprendizado enorme em didática e criação de
materiais intuitivos, tirando toda a parte humana que é gerada pela
interação com o público mais velho”.
M2: “O curso não apenas proporcionou conhecimentos práticos
sobre o uso de dispositivos móveis e aplicativos populares, mas também
ajudou os participantes a desenvolverem habilidades importantes,
como identificar fake news e prevenir golpes online. A metodologia
adotada, com aulas teóricas e práticas, permitiu uma aprendizagem
efetiva e personalizada. Além disso, foi gratificante para mim observar
de perto a evolução dos alunos não apenas em termos técnicos, mas
também no aumento da autoestima. Muitos idosos expressaram um
sentimento de realização ao dominar novas habilidades digitais e ao
se tornarem mais confiantes na utilização de seus smartphones para
diversas tarefas do dia a dia”.
M3: “A interação direta com os idosos durante as atividades prá-
ticas mostrou a importância de adaptar o ensino às características
e ritmos individuais de aprendizado, garantindo uma experiência
enriquecedora para todos os envolvidos”.
M4: “O curso representa uma oportunidade única tanto para os
alunos quanto para os monitores. Para os alunos, é uma oportunidade
de aprendizado, bem como um espaço social de convívio e um espaço
para os idosos exporem sentimentos e problemas. Para os monitores é
uma oportunidade de exercitar as atividades relacionadas ao ensino e
de compartilhar com o público externo parte do conhecimento adqui-
rido dentro da universidade, além de exercitar a empatia e aprender
com o público idoso, que tem muito a oferecer”.
M5: “O curso é extremamente essencial como ferramenta social,
possibilitando ao público 60+ adquirir autonomia sobre os recursos
digitais, e aos estudantes, aprendizados e troca de experiências, favo-
recendo o desenvolvimento da empatia, do trabalho em grupo e da
prática docente. ”.
CONSIDERAÇÕES FINAIS
Em tempos de curricularização da extensão, projetos como este
representam um espaço para alunos da graduação atuarem e rece-
berem parte de suas horas necessárias para a conclusão do curso.
Para além de receberem tais horas, a interação com outra geração
permite uma troca genuína de experiências entre os alunos e ensina
sobre respeito, superação e autonomia.
Como produtores de tecnologia, o curso ensina aos alunos da
universidade sobre a importância do desenvolvimento responsável
e inclusivo de soluções computacionais.
Cursos de extensão são uma forma de envolver a comunidade do
entorno da universidade, torná-la parte do ambiente e é um espaço
para compartilhar o conhecimento ali produzido.
Para os idosos do curso de letramento, em especial, o curso é
um espaço de aprendizado, de superação de medos e receios, de
encontro com pessoas amigas, um momento de fuga dos problemas
e dificuldades do cotidiano, um local de troca de cuidados e carinho,
e um espaço de fala e escuta ativa.
Durante as aulas, a equipe tem a missão de ensinar os alunos
idosos a entenderem sobre a iconografia dos aplicativos, para torná-
los mais autônomos durante a interação. Diversos dos alunos idosos
chegam com o relato de impaciência de seus familiares no suporte
ao uso da tecnologia e o receio pessoal de realizar alguma ação
irreparável. Após passarem pelo curso, a maioria dos idosos se
torna menos reticente e compreende melhor as ações que podem
ser realizadas em cada tela. Passam a observar melhor. A metologia
híbrida usada neste curso apoia nessa missão, uma vez que há
diversas atividades práticas, incluindo aquelas em que os monitores
não estão presentes para ajudar e, que ao tentarem fazer sozinhos,
podem testar seu conhecimento e levar dúvidas para a aula seguinte.
W4E’2024, Juiz de Fora/MG, Brazil
Rodrigues et al.
Figura 5: Exemplo de tarefa criada na interface Web do sistema. Fonte: Autoria Própria.
Apesar da existência de um cronograma, é importante destacar
que as aulas são cadenciadas pelo ritmo da turma e não é raro
que parte do conteúdo deixe de ser ministrado. Nesses casos, há
um acordo com a turma sobre quais dos conteúdos são de maior
interesse.
Ao longo dos quase 10 anos de curso, quatro docentes da USP já
coordenaram o mesmo, diversos alunos de graduação, pós-graduação
e pós-doutorado já atuaram como monitores. Profissionais da Geron-
tologia e Psicologia atuaram no projeto e ajudaram na elaboração e
aperfeiçoamento da metodologia usada. O curso foi ainda replicado
em outra unidade da USP, na Escola de Artes, Ciências e Humani-
dades (EACH) [2, 3], com um grupo parceiro e que moldou com o
grupo do ICMC parte da metodologia hoje adotada, uma vez que
na EACH também era oferecido o curso de letramento para idosos,
no contexto do programa USP 60+. Ao longo dos anos, tal curso
ofereceu 31 turmas e letrou digitalmente aproximadamente 900
alunos idosos. Cerca de 150 alunos já participaram como monitores.
Alguns alunos de pós-graduação, membros do grupo de pesquisa da
coordenação deste curso, realizaram pesquisa com idosos e usaram
o projeto de extensão como espaço para interagir com os alunos e
conhecê-los. Alguns dos idosos foram convidados a participar das
pesquisas de tais alunos, mas em contextos externos ao projeto de
extensão, cada pesquisa com sua respectiva aprovação por Comitê
de Ética em Pesquisa (CEP). Durante o processo de construção da
metodologia aqui citada, que não é o foco deste artigo, conforme o
supracitado, houve atividades de pesquisa que podem ser encon-
tradas em mais detalhes nos trabalhos de Cunha et al. [4], Cliquet
et al. [3], Zaine et al. [11] e Cachioni et al. [2], por exemplo. Essas
atividades de pesquisa foram previstas e aprovadas pelo comitê de
ética da EACH/USP, na ocasião.
Destaca-se ainda que projeto de extensão aqui citado está em
consonância com uma das metas da agenda da Organização das
Nações Unidas (ONU) para 2023: Redução das Desigualdades.
Os autores deste trabalho esperam que as experiências e con-
teúdos aqui compartilhados possam inspirar outros pesquisadores,
docentes e alunos a replicarem tal curso em outras universidades
e cidades do Brasil, podendo ajudar a reduzir as desigualdades e o
etarismo, bem como permitir que outras pessoas possam desfrutar
do retorno imediato de uma abraço de gratidão pela autonomia
conquistada e o empoderamento sentido.
AGRADECIMENTOS
Agradecemos a todos os idosos que foram alunos do curso e ajuda-
ram a consolidar o mesmo. Agradecemos ainda a todos os alunos, de
graduação e pós-graduação, que participaram como monitores, vo-
luntários ou bolsistas do projeto. Deixamos registrado aqui também
nosso agradecimento especial às professoras Maria da Graça Cam-
pos Pimentel e Renata Pontin de Mattos Fortes, criadoras do projeto
no ICMC e coordenadoras do mesmo até 2019. Agradecemos tam-
bém à Comissão de Cultura e Extensão (CCEx) do ICMC, e à PRCEU
pelo apoio financeiro fornecido ao longo dos anos, e que viabilizou
a realização dos cursos de letramento aqui relatados. Destacamos,
por fim, que o grupo de autores deste trabalho representa um pe-
queno número de alunos monitores, voluntários e profissionais que
passaram pelo curso ao longo desses anos. Registramos também que
parte da metodologia aqui descrita foi construída em conjunto com
a EACH/USP, em projeto FAPESP com processo número 2017/19915-
0, coordenado pela professora Dra. Meire Cachioni (EACH/USP).
Nota: A ferramenta Grammarly foi usada neste texto para ajustes
ortográficos e o ChatGPT para reestruturação de trechos da Seção 1.
Práticas com smartphones para idosos - Um projeto
de extensão do ICMC/USP
W4E’2024, Juiz de Fora/MG, Brazil

--- FIM DO ARQUIVO: 30502-829-24950-2-10-20241024.txt ---

--- INÍCIO DO ARQUIVO: 30503-829-24951-1-10-20241001.txt ---
Promovendo a Inclusão Feminina na Computação: Cursos de
Programação em Python
Rhara Ianna Barcelos Costa
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brasil
rhara.ianna@estudante.ufjf.br
Vanessa Trajano
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brasil
vanessa.trajano@estudante.ufjf.br
Nina Aguiar
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brasil
nina.aguiar@estudante.ufjf.br
Lorenza Leao Oliveira Moreno
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brasil
lorenza@ice.ufjf.br
Bárbara de Melo Quintela
Programa de Pós-Graduação em
Modelagem Computacional
(PPGMC/UFJF)
Juiz de Fora, MG, Brazil
barbara.quintela@ufjf.br
Alessandreia Marta de Oliveira
Departamento de Ciência da
Computação (UFJF)
Juiz de Fora, MG, Brasil
alessandreia.oliveira@ufjf.br
20301. O ODS 5 - Igualdade de
Gênero - busca eliminar as disparidades de gênero e empoderar to-
das as mulheres e meninas, ressaltando a importância de iniciativas
que promovam a participação feminina em todas as áreas, incluindo
a Tecnologia da Informação.
Diante disso, o projeto Meninas Programadoras JF, parceiro do
Programa Meninas Digitais2 da Sociedade Brasileira de Computação
- SBC, surgiu em setembro de 2023, inspirado pelo projeto Meninas
Programadoras USP, fundado pela professora Maria da Graça em
2021, para atender a comunidade de Juiz de Fora e região. O projeto,
voltado, principalmente, para o desenvolvimento de habilidades
de programação em Python por meninas, estudantes do Ensino
Médio (EM), com prioridade para escolas públicas. Com ênfase em
aulas e monitorias remotas, diferentes plataformas Web são uti-
lizadas para facilitar o acesso ao material didático multimídia, o
acompanhamento das atividades práticas e a interação entre alu-
nas e monitoras. O objetivo principal é proporcionar um primeiro
contato das meninas com a programação, visando capacitação e
promovendo aumento de confiança ao ingressarem em cursos de
Computação e afins ou no mercado de trabalho. Além disso, para
ampliar o seu alcance, o projeto mantém uma postura ativa nas
redes sociais, com o objetivo de promover continuamente a partic-
ipação feminina na área de Tecnologia da Informação (TI). Além
disso, organiza e promove eventos que são divulgados por meio
dessas plataformas digitais.
Para tanto, este artigo está organizado como a seguir. A Seção 2
apresenta alguns conceitos importantes no contexto deste trabalho.
A Seção 3 apresenta alguns trabalhos relacionados a essa proposta,
1https://brasil.un.org/pt-br/sdgs
2https://meninas.sbc.org.br/
W4E’2024, Juiz de Fora/MG, Brazil
Costa et al.
enquanto a Seção 4 lista os métodos utilizados. A Seção 5 apresenta
os resultados alcançados pelo projeto neste primeiro ano de exe-
cução e a Seção 6 apresenta as considerações finais e sugestões de
trabalho futuro.
FUNDAMENTAÇÃO TEÓRICA
A Educação em Computação é uma área interdisciplinar que abrange
todos os níveis formais do sistema educacional, desde a educação in-
fantil até a pós-graduação. Além disso, inclui a educação informal,
não se limitando apenas ao conteúdo técnico, mas também bus-
cando empoderar os aprendizes e causar um impacto positivo em
suas vidas e nas comunidades onde estão inseridos. Esta abordagem
está diretamente relacionada ao projeto Meninas Programadoras JF,
que tem como objetivos impactar a formação de meninas do ensino
médio e promover uma mudança na atual configuração de gênero
na comunidade de Computação e áreas afins [1].
O ensino on-line utiliza tecnologias Web e multimídia como
ferramentas fundamentais para ensino e aprendizagem. Diversas
plataformas e recursos potencializam a experiência educacional e
promovem a interação entre os participantes. As atividades podem
ser síncronas ou assíncronas, proporcionando maior flexibilidade
no processo de aprendizagem de cada indivíduo. A escolha das
plataformas permite um caráter personalizado para o ensino [7].
TRABALHOS RELACIONADOS
Na literatura, existem vários trabalhos para ampliar a inserção e
a permanência de mulheres em áreas relacionadas à tecnologia.
Alguns deles são apresentados a seguir.
No projeto Meninas Programadoras USP é implementado um
curso de introdução à programação curto, on-line e síncrono. O
curso combina sessões ao vivo e tarefas individuais via tecnolo-
gias Web e multimídia, permitindo que alunas acessem recursos
relevantes, colaborem com colegas e desenvolvam habilidades por
meio de experiência prática. Em vinte turmas do Meninas Progra-
madoras I, 1.543 alunas compareceram à primeira aula e, ao final,
849 concluíram o curso, com 725 obtendo aprovação [7]. O Meni-
nas Programadoras USP recebeu, no Congresso da SBC de 2023,
o prêmio de Projeto mais Engajado na Comunidade, com o maior
número de escolas atendidas no ano de 2022/2023 no escopo do
Programa Meninas Digitais.
O projeto UFABC para MiN@s tem o intuito de aproximar meni-
nas do final do Ensino Fundamental II e Ensino Médio das áreas
STEM (Science, Technology, Engineering and Mathematics), dis-
seminando a figura da mulher cientista, mostrando os caminhos
traçados por diversas mulheres e as possibilidades para meninas em
seu primeiro contato com as áreas STEM. Por meio de um ambiente
acolhedor, a equipe busca interagir com as participantes para que,
através desse contato mais próximo, seja possível potencializar e
incentivar as descobertas e aprendizados [10].
O curso Moodle de Lovelace visa ensinar conceitos, técnicas
e práticas de resolução de problemas do Pensamento Computa-
cional que podem ser aplicadas em outras áreas do conhecimento,
abrangendo públicos além da Educação Básica e considerando es-
pecificidades de gênero e de acessibilidade. Com uma aprendizagem
de programação por meio de metodologias ativas e de tecnologias
digitais, o Moodle de Lovelace desenvolveu um curso a distância
de Programação Python Essencial, Ativa e Prática, direcionado à
formação de programadoras [4].
O grupo Meninas Digitais Rio Pomba, atuante na região da Zona
da Mata mineira, visa levar mais representatividade feminina dentro
do Departamento Acadêmico de Ciência da Computação (DACC).
O projeto surgiu a partir de conversas entre as estudantes, onde foi
observada a dificuldade das mesmas em se sentirem confiantes e
confortáveis em um curso predominantemente masculino, sendo
oficialmente registrado na instituição em agosto de 20193.
O programa de extensão MinasCoders, da Universidade Federal
de Viçosa - campus Florestal (UFV/Florestal), objetiva promover
ações para atrair, motivar e fixar meninas da comunidade de Flo-
restal nos cursos de Computação e afins, visando aproximar a tec-
nologia do universo feminino, reduzir as desigualdades de gênero
no mercado de trabalho e potencializar a participação feminina
nestas áreas. Com a participação de alunas bolsistas e voluntárias,
provenientes do curso Técnico em Informática da CEDAF-UFV e do
curso de Ciência da Computação, esses projetos passaram a integrar
o programa MinasCoders em 2017, com o intuito de formar, dentro
da universidade, um grupo consolidado de professoras e alunas que,
juntas, elaboram mecanismos eficientes para apresentar às alunas
da comunidade a realidade sobre os cursos correlatos às áreas de
Computação e afins e sobre a baixa atuação da mulher no mercado
de trabalho4.
O projeto Meninas Programadoras JF é uma iniciativa dedicada
a promover a igualdade de gênero na área de tecnologia, com foco
especial na região da Zona da Mata. Inspirado pelo sucesso de
projetos como Meninas Programadoras USP, UFABC para MiN@s,
Moodle de Lovelace, Meninas Digitais Rio Pomba e MinasCoders,
entre outros, o projeto visa capacitar meninas em programação,
aumentando sua confiança e motivação para ingressar em cursos
de Computação e áreas afins, bem como no mercado de trabalho.
MÉTODOS
O projeto atua em diferentes frentes de trabalho, sendo a primeira
delas relacionada às monitorias, outra às redes sociais, uma terceira
relacionada à elaboração de cursos próprios e, por fim, atua também
coorganizando eventos voltados para o público feminino. Nesta
seção são apresentadas as diferentes abordagens.
4.1
Monitorias
Este projeto atua replicando ações de outro projeto relacionado.
A seguir é apresentada uma descrição do projeto base e as suas
atividades.
O projeto Meninas Programadoras JF está associado ao Meninas
Programadoras USP, oferecendo suporte em monitorias. O curso
proposto pelo projeto Meninas Programadoras USP ensina Python
para meninas do Ensino Médio e concluintes por meio da Web, per-
mitindo atingir um público mais amplo. Novas turmas são abertas
periodicamente e divulgadas pelas mídias sociais. O conteúdo é
apresentado por meio de aulas semanais on-line e síncronas. Ao
longo da semana, são disponibilizadas listas de exercícios. As alunas
com dúvidas podem recorrer às monitoras, que ficam disponíveis
em horários pré determinados, durante toda a semana.
3https://sistemas.riopomba.ifsudestemg.edu.br/meninasdigitais/?page_id=186
4https://minascoders.caf.ufv.br/
Promovendo a Inclusão Feminina na Computação: Cursos de Programação em Python
W4E’2024, Juiz de Fora/MG, Brazil
Por ser um curso on-line, utilizam-se diversas ferramentas para
tornar aulas e contato com as alunas mais dinâmicos. As aulas e as
monitorias são realizadas pelo Google Meet, visto que a ferramenta
possibilita vasta interatividade além de trabalhar bem com outros
ambientes da Google, como o Google Sala de Aula (utilizado como
quadro de mensagens para o curso), Google Forms (utilizado para
introduzir conceitos básicos de programação no curso) e Google
Colaboratory (utilizado para testar códigos).
Para atividades de cunho avaliativo, utiliza-se o Beecrowd5 (Figura
1), que permite às alunas aplicar o que aprenderam nas aulas de
forma prática, promovendo um aprendizado mais efetivo. A fer-
ramenta oferece recurso de teste automatizado dos exercícios e
permite também que professores e monitores tenham acesso às
soluções formuladas pelas alunas. A independência proporcionada
pela plataforma também é um ponto positivo, visto que permite
ritmo de aprendizado personalizado [2]. Para a comunicação mais
cotidiana, é utilizado o WhatsApp, repassando links de aulas e
tirando dúvidas sobre o programa.
Para que o projeto Meninas Programadoras JF possa fornecer
um apoio efetivo para o projeto base Meninas Programadoras USP,
foi realizada a capacitação das alunas voluntárias e da bolsista. Uma
das tarefas realizadas constitui em monitorias que são realizadas
todas via Google Meet.
4.2
Curso Autoral
Conforme o mapeamento realizado por [8], são fatores de atração
e de permanência de mulheres na Computação, respectivamente:
"Aprimoramento de conhecimento básico previamente obtido" e
"Inspiração em outras mulheres do curso". Visa-se cobrir esses dois
fatores por meio dos minicursos desenvolvidos pelo projeto. Os
cursos cobrem o fator de atração, visto que, ao estabelecer contato
com programação em idade precoce, gera-se interesse em seguir a
área. Os cursos voltados para o contexto da universidade cobrem o
fator de permanência ao serem ensinados por mulheres, inspirando
meninas a programarem e criando um ambiente de pertencimento
e autoridade inspiracional para as mulheres.
Utiliza-se o Google Colaboratory para executar os códigos em
Python, já que a plataforma facilita a prática de programação por ser
toda on-line e dispensar a instalação de quaisquer compiladores ou
bibliotecas [3]. Possui vantagens como armazenamento dos códigos
no drive, comentários nos códigos por meio de blocos separados,
compilação de código modularizada e recursos de colaboração.
Para elaboração do material didático, utiliza-se o Canva, que per-
mite trabalho colaborativo simultâneo e oferece diversos recursos
gráficos para aulas mais interessantes.
Para cada curso planejado, inicialmente o formato (on-line ou
presencial), público-alvo e objetivo final são definidos. É impor-
tante estabelecer um objetivo final, como o projeto de um jogo ou
programa interessante, pois segundo [5], as metodologias precisam
acompanhar os objetivos pretendidos. Um objetivo mais complexo
no final do curso serve como estímulo para os estudantes e, com
base neste, a grade de ensino e a carga horária são previstas. O
material é preparado no Canva e a divulgação feita a partir das
redes sociais do projeto.
5https://judge.beecrowd.com/pt/login
4.3
Coorganização de Eventos
O mapeamento realizado em [8] traz ainda outros fatores de per-
manência de mulheres na Computação: a influência de mulheres da
área como modelos e uma rede de apoio de mulheres da área. Para
estimular a retenção de mulheres, foram elaborados diferentes tipos
de eventos sociais e acadêmicos, em conjunto com outros projetos
da universidade, que visam fortalecer a convivência feminina:
• mesas redondas voltadas para um público majoritariamente
feminino, em que mulheres inspiradoras da tecnologia são
convidadas para compartilhar suas experiências de vida;
• momentos de descontração, em que um lugar é reservado
para a convivência das estudantes das áreas de exatas, em
meio a jogos, música e comida;
• workshops ministrados também por mulheres inspiradoras
da área de tecnologia.
Para a organização das mesas redondas e workshops, elabora-se
o tema e mulheres associadas são convidadas para participar. Uma
aluna da graduação é selecionada para fazer a mediação. Com tema,
convidadas e mediadora definidos, o evento é divulgado nas redes
sociais. A mediadora elabora o roteiro do evento e perguntas para
as convidadas. Próximo ao evento, um coffee break é montado com
auxílio das professoras coordenadoras do projeto.
Esses eventos também ocorrem de forma remota, o que permite
atingir um novo público, trazendo maior visibilidade e possibili-
tando a presença de convidadas distantes geograficamente. Nessa
modalidade, o Google Meet é utilizado.
Para os eventos mais descontraídos, espaços como o do diretório
acadêmico são utilizados, e são preparadas: músicas, comidas e
atividades para interação das alunas. O evento também é divulgado
com antecedência nas redes sociais.
4.4
Redes Sociais
Um estudo demonstrou que 62 de 100 dos alunos do Ensino Médio
utilizam seus smartphones prioritariamente para acessar as redes so-
ciais [6], tornando as mídias sociais meios eficazes de comunicação
com jovens. Como o público-alvo do projeto são mulheres de Ensino
Médio ou universitárias, as redes sociais são um meio efetivo de
comunicação. O Instagram foi escolhido como mídia principal por
estar mais inserido no cotidiano das professoras e alunas envolvi-
das no projeto. No início de cada mês, é preparado um calendário
editorial, organizando as postagens mensais e discutindo ideias de
novos quadros. Durante o mês, as alunas preparam as postagens no
Canva, que passam pela aprovação das professoras antes de serem
publicadas. Os quadros regulares incluem:
• quadro semanal de Python: ensina novos conteúdos de Python;
• Code Like a Girl: divulga mulheres inspiradoras da atualidade
que trabalham com tecnologia;
• divulgação de eventos: informa sobre novas turmas do Meni-
nas Programadoras USP, minicursos e outros eventos do
projeto;
• compartilhamento de conhecimentos: tem cunho educativo
e informativo.
O projeto também faz uso do LinkedIn como uma ferramenta
estratégica para alcançar um público mais amplo e profissional, bem
W4E’2024, Juiz de Fora/MG, Brazil
Costa et al.
Figura 1: Plataforma beecrowd utilizada para submissão de exercícios no curso.
como promovendo a conexão com potenciais colaboradores, em-
presas parceiras e participantes interessados. Essa presença ativa
no LinkedIn não somente permite compartilhar atualizações so-
bre cursos, eventos e sucessos do projeto, mas também reforça a
relevância das nossas ações e valorizam a integração de tecnologias
de comunicação e mídia digital. Através do LinkedIn, amplia-se
a divulgação do projeto, destaca a importância de iniciativas que
promovem a igualdade de gênero na tecnologia e cria um ponto
de ligação essencial entre a academia e o mercado de trabalho,
fortalecendo o impacto social do nosso projeto.
RESULTADOS
Nesta seção, são apresentados os processos de planejamento e de
execução dos cursos, bem como os resultados obtidos. Ao compartil-
har tais experiências e resultados, espera-se inspirar outras pessoas
e contribuir para promover a diversidade de gênero e a inclusão
na área. Considerando as diferentes formas de atuação do projeto,
percebe-se uma grande abrangência de resultados alcançados.
5.1
Monitoria
Cada uma das quatro alunas envolvidas no projeto disponibilizou
duas horas semanais para monitoria. Três turmas distintas foram
atendidas, totalizando 96 horas de monitoria. Na primeira turma,
com 280 inscritas, 140 alunas foram atendidas. Na segunda turma,
também com 280 inscritas, 143 alunas procuraram atendimento
nos horários de monitoria (Figura 2). Na terceira turma, com 73
inscritas, 30 alunas pediram apoio às monitoras. Além disso, várias
dúvidas foram respondidas via Whatsapp, aproveitando a dinâmica
on-line do curso para uma maior abrangência do projeto.
Alunas Inscritas
Alunas na Monitoria
Turma 25
Turma 26
Turma 27
Figura 2: Alunas inscritas e alunas nas monitorias.
5.2
Minicurso de Python ministrado
Foi elaborado um curso básico de Python de 2 horas voltado para
alunos da Universidade Federal de Juiz de Fora. O objetivo do curso
foi incentivar a montagem de um jogo de pedra, papel e tesoura,
assim como um jogo da forca, na linguagem de programação men-
cionada. Participaram 16 pessoas e foram ensinados os conceitos
básicos da linguagem para a programação final do jogo. O mini-
curso foi planejado em conjunto com um Grupo de Educação Tu-
torial da Engenharia Computacional, que tem tradição em realizar
treinamentos em Python na universidade, resultando em uma maior
integração do projeto com a comunidade acadêmica.
5.3
Coorganização eventos
Foram realizados 3 encontros informais (Figura 3), com caráter in-
tegrador do evento, promovido principalmente por jogos coletivos.
Promovendo a Inclusão Feminina na Computação: Cursos de Programação em Python
W4E’2024, Juiz de Fora/MG, Brazil
Figura 3: Momentos de integração entre as meninas.
Também ocorreu um workshop, ilustrado na Figura 4, que mostra
uma especialista em estratégias de integração ao mercado de tra-
balho transmitindo seu conhecimento às meninas.
Figura 4: Oportunidades no mercado de trabalho.
Além disso, foram organizadas 5 mesas-redondas, destacando
mulheres experientes da área de Computação que compartilharam
suas perspectivas com as meninas participantes (Figura 5).
Figura 5: Mesa redonda sobre Mulheres na Tecnologia.
Nos encontros informais, participaram 34, 20, 39 alunas; nas
mesas redondas, 30, 42, 30, 27, 37 alunas; e no workshop, 29 alunas
(Figura 6). É interessante notar a recorrência das alunas nos even-
tos, evidenciando um bom índice de adesão às atividades propostas.
Esses eventos permitem que as alunas criem laços com outras meni-
nas do curso e contatos com mulheres inspiradoras da área. Além
disso, são oportunidades para divulgar o projeto. Outra observação
importante é que parte dos eventos ocorreu em períodos de férias,
e a Web permitiu a manutenção dos laços criados pelas alunas
e a conexão com outras mulheres, como na mesa redonda com
convidadas que moram no exterior.
Figura 6: Participantes dos Eventos Promovidos.
5.4
Seguidores no Instagram
No primeiro ano do projeto, o Instagram alcançou mais de 500
seguidores. O público-alvo desejado foi alcançado, sendo 39,8%
entre 18 e 24 anos e 63,8%, mulheres (Figura 7).
Figura 7: Faixa etária e gênero do público do Instagram.
As principais cidades alcançadas são Juiz de Fora (43,7%) e São
Paulo (8,9%). Em relação ao alcance, percebe-se um panorama posi-
tivo, que pode ser observado na Figura 8, considerando um total de
87 postagens no perfil até julho de 2024.
5.5
Participação em eventos de tecnologia
O projeto participou de seis eventos, identificados na Tabela 1. A
participação incluiu tanto o lado organizacional quanto operacional
para maior visibilidade do projeto. Esses eventos são importantes
para que as alunas conheçam novas áreas de atuação e ganhem
mais confiança no mercado de trabalho. Os principais benefícios
dessa participação incluem a divulgação do projeto para um público
mais amplo, o aprendizado e o networking.
W4E’2024, Juiz de Fora/MG, Brazil
Costa et al.
Figura 8: Evolução cumulativa do alcance do Instagram.
Nome
Link
Tec Hub
https://techhubjf.org/
SBSI 2024
https://sbsi2024.ufjf.br/index.php
BARTS 2024
https://www2.ufjf.br/barts/
CSBC
https://csbc.sbc.org.br/2024/#sobre
Semana da Computação
UFJF
https://semana-computacao-
ufjf.github.io/site/
DevOpsDays
https://devopsdays.org/
Tabela 1: Eventos com participação do projeto.
5.6
Impacto na comunidade acadêmica
feminina
Embora seja mais difícil de mensurar, é importante ressaltar um úl-
timo resultado alcançado pelo projeto: o impacto que este conjunto
de iniciativas propicia às graduandasde Computação e áreas afins.
Os eventos e a presença constante nas redes sociais reforçam a
presença feminina nos cursos de tecnologia e trazem uma sensação
de acolhimento e pertencimento. Ao mesmo tempo, contribuem
para a redução do estereótipo de ser uma área predominantemente
masculina, não somente entre as meninas, mas também entre os
alunos do curso, que, como futuros colegas de trabalho, também
precisam valorizar e reconhecer a presença feminina na área.
De forma mais pontual, ressalta-se também a contribuição do
projeto no crescimento acadêmico e profissional das alunas par-
ticipantes, bolsista e voluntárias. O projeto criou um ambiente de
aprendizado interativo que facilita a participação ativa e as experiên-
cias de aprendizado personalizadas. Ao conduzirem pessoalmente
as iniciativas do projeto, acompanhadas pelas professoras orien-
tadoras, as alunas não somente melhoram seus conhecimentos de
programação, como também:
• aprendem sobre didática, na elaboração de cursos e na atu-
ação como monitoras;
• aprendem sobre comunicação e divulgação científica, ao tra-
balharem na criação e redação de posts para redes sociais,
biografias e textos acadêmicos e científicos;
• falam em público, nos cursos por elas lecionados, apresen-
tando o projeto em eventos, mediando mesas redondas;
• organizam eventos, atuando desde o planejamento até a exe-
cução;
• expandem sua rede de contatos, ao trabalharem com orga-
nizadores de eventos parceiros e contactarem e receberem
palestrantes para nossos eventos.
Observa-se que essas alunas, além de melhorarem seus conhec-
imentos acadêmicos, têm a oportunidade de exercitarem e desen-
volverem diversas soft skills, importantes em seu futuro profissional.
CONSIDERAÇÕES FINAIS E TRABALHOS
FUTUROS
As integrantes do projeto pretendem manter a parceria com o pro-
jeto base Meninas Programadoras USP, aplicando monitoria às suas
turmas futuras. Além disso, planejam organizar mais cursos de
Python, incluindo cursos para alunos da Universidade Federal de
Juiz de Fora, cursos virtuais para estudantes de Ensino Médio e
cursos presenciais em escolas públicas da cidade de Juiz de Fora e
regiões próximas. Dessa forma, o projeto pretende, de forma con-
tínua, incentivar a inserção de meninas nos cursos de Computação,
proporcionando-lhes um primeiro contato e uma base de conheci-
mento em programação.
Futuramente, o projeto deve também explorar novas edições dos
eventos promovidos Café das Minas e da Sinuca das Minas, testando
novos modelos de interação das alunas. Esta iniciativa fortalece
a comunidade feminina dos cursos de Computação e, assim, tem
potencial para incentivar a permanência das meninas nestes cursos.
O comparecimento em eventos e a participação ativa em redes
sociais também são atividades importantes na manutenção do pro-
jeto, uma vez que possibilitam oportunidades de networking para
fortalecer conexões profissionais.
Os principais benefícios esperados a longo prazo incluem uma
mudança cultural na forma de enxergar a Computação e áreas afins,
como um ambiente masculino, resultando no aumento da procura
por cursos desta área por meninas.
AGRADECIMENTOS
À Universidade Federal de Juiz de Fora, ao Programa Meninas Digi-
tais e ao Projeto Meninas Programadoras USP pelo apoio à realiza-
ção deste projeto de extensão institucional.

--- FIM DO ARQUIVO: 30503-829-24951-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30504-829-24952-1-10-20241001.txt ---
Avaliação da Experiência do Telespectador na Transição para a TV
Orientada a Aplicativos
Li-Chang Shuen
li.chang@ufma.br
Universidade Federal do Maranhão
Carlos de Salles Soares Neto
carlos.salles@ufma.br
Universidade Federal do Maranhão
Marcelo Ferreira Moreno
marcelo@ice.ufjf.br
Universidade Federal de Juiz de Fora
Zefinha Bentivi
zefinha.bentivi@ufma.br
Universidade Federal do Maranhão
João Victor Cruz Gonçalves
jvc.goncalves@discente.ufma.br
Universidade Federal do Maranhão
Iago Victor Silva Costa
iago.vsc@discente.ufma.br
Universidade Federal do Maranhão
2007. A tecnologia brasileira foi reconhecida inter-
nacionalmente pela ITU-T em 2009 [4] [7] e hoje é adotada em 18
países. O Ginga continua a ser um componente de referência no
desenvolvimento da TV 3.0, possibilitando a integração de novas
funcionalidades. A pesquisa e o desenvolvimento dessa nova gera-
ção de TV digital envolvem uma colaboração entre universidades, o
governo e o setor privado, destacando a importância de um esforço
conjunto para avançar as capacidades tecnológicas e para a manu-
tenção da posição de vanguarda do Brasil no desenvolvimento da
televisão digital terrestre.
Este position paper traz, além da introdução e das considerações
finais, uma seção de materiais e métodos que descreve a meto-
dologia empregada; a seção de resultados apresenta os principais
achados, destacando tanto a recepção positiva quanto dificulda-
des encontradas por diferentes grupos, com ênfase nas populações
vulneráveis; a discussão analisa as implicações desses resultados, su-
gerindo intervenções de políticas públicas necessárias para garantir
uma transição inclusiva.
MATERIAIS E MÉTODOS
Foram realizadas quatro sessões de grupo focal e oito sessões de
avaliação individual de um protótipo que replica a jornada espe-
rada para o telespectador da TV 3.0 como plataforma orientada
a aplicativos. As três primeiras sessões coletivas e as avaliações
individuais foram feitas com o grupo inicialmente selecionado para
a fase de levantamento de requisitos, com encontros no laboratório
TeleMídia da Universidade Federal do Maranhão. A quarta sessão
de grupo focal foi realizada em uma comunidade remanescente de
quilombo, a partir da necessidade de empreender um estudo quali-
tativo baseado nos dados obtidos com a pesquisa de opinião pública,
que apontou a parcela da população com maior vulnerabilidade
tecnológica e consequente dificuldade para adoção do novo para-
digma de TV aberta: pessoas idosas, de baixa escolaridade, menor
renda e menor índice de televisores conectados. A metodologia com
grupos focais ensejou a necessidade de submissão do projeto de
pesquisa à avaliação do comitê de ética da Universidade Federal
de Juiz de Fora, por meio da Plataforma Brasil, registrada sob o
número 77032223.7.1001.5147.
O objetivo da realização de grupos focais foi tanto fornecer sub-
sídios para o desenvolvimento do protótipo de aplicação da nova
WTVDI’2024, Juiz de Fora/MG, Brazil
Shuen et al.
jornada do telespectador quanto avaliar a recepção e a compreen-
são dos telespectadores sobre a mudança de paradigma para a TV
orientada a aplicativos. A pesquisa teve dois tipos de grupo: o grupo
1, formado por pessoas residentes em São Luís, recrutadas a partir
da definição de perfis de telespectadores, com idades entre 15 e 73
anos; e o grupo 2, formado por pessoas idosas residentes em uma
comunidade quilombola de município distante 22 km da capital,
recrutadas a partir da necessidade de avaliação qualitativa de um
conjunto de dados obtidos em pesquisa quantitativa. Neste processo,
foram realizadas três sessões de grupo e oito sessões individuais
com os integrantes do grupo 1 e uma sessão com o grupo 2.
O grupo 1 foi composto por oito perfis de telespectadores, defi-
nidos de acordo com experiências de consumo tanto de conteúdo
quanto de tecnologia. São quatro pessoas do sexo feminino e quatro
do sexo masculino, com idades que variam dos 15 aos 78 anos. As
experiências de consumo de televisão incluem aqueles que assistem
e têm acesso somente à TV aberta, sem consumo de streaming até
pessoas que apenas consomem streaming. Entre os participantes, há
um profissional de Rádio e TV, uma pessoa com deficiência visual
(baixa visão), além de estudantes, profissionais da educação e de
serviços gerais e um aposentado, representando variados níveis de
escolaridade e de renda.
Primeira sessão, grupo 1. O grupo focal avaliou a usabilidade e a
experiência da TV aberta como ela se apresenta hoje nos aparelhos
padrão do mercado. As pessoas responderam a questões qualitativas
sobre como elas se relacionam com o conteúdo e a forma de acesso
a esse conteúdo na televisão e sobre modificações que imaginam
que melhorariam a experiência do telespectador. Os requisitos le-
vantados, durante a discussão, foram encaminhados para a equipe
de P&D e considerados para a implementação do protótipo da nova
jornada do telespectador da televisão orientada a aplicativos.
Segunda sessão, grupo 1. O grupo avaliou a primeira versão do
protótipo, respondendo a questões comparativas entre a experiência
que eles têm hoje e a experiência que o protótipo proporcionou.
Ajustes foram apontados e encaminhados para a equipe de P&D.
Terceira sessão, grupo 1. Após os ajustes feitos pela equipe de
desenvolvimento, a partir das avaliações e recomendações da sessão
2, os participantes do grupo focal repetiram a dinâmica da sessão
anterior, agora avaliando uma versão mais avançada do protótipo.
Avaliações individuais. Após novos ajustes, os integrantes do
grupo focal foram convidados para sessões individuais de avaliação
da nova jornada do telespectador e puderam usar, eles mesmos, o
controle remoto, simulando todos os passos, desde a conexão do
televisor à tomada, até o consumo efetivo de conteúdo, com zapea-
mento de canais e escolha de programas nos aplicativos iniciais e
secundários disponíveis no protótipo.
Sessão única, grupo 2. O grupo 2 foi formado por sete idosos,
seis homens e uma mulher, com idades entre 54 e 75 anos, todos
residentes em uma agrovila situada em território étnico-quilombola.
Os homens têm idades entre 54, 55, 62, 69, 73 e 75 anos. A mulher
tem 72 anos. Seis dos participantes têm renda de até 2 salários
mínimos e um declarou renda de até 4 salários mínimos. Todos
assistem à televisão diariamente, sem acesso a serviços de streaming
pelo televisor. Quatro declararam acessar conteúdo de streaming
por celular quando conectados a redes disponíveis na comunidade.
Apenas um idoso possui nível superior de escolaridade. Os demais
informaram serem alfabetizados com escolaridades que variam
entre ensino fundamental incompleto a ensino médio completo.
Pesquisa quantitativa. A pesquisa de opinião pública foi aplicada,
de forma presencial, durante sete semanas na região metropolitana
de São Luís/MA, composta por quatro municípios e totalizando
1.442.952 habitantes. Para esta população, a amostra calculada foi de
385 questionários, com margem de erro de 5% e índice de confiança
de 95%, aplicados ponderadamente da seguinte forma: São Luís:
1,109 milhão de habitantes, 295 questionários; São José de Ribamar:
179,028 mil habitantes, 48 questionários; Paço do Lumiar: 123,747
mil habitantes, 33 questionários; Raposa: 31,177 mil habitantes, 9
questionários. Os resultados foram organizados em dashboards no
software Microsoft Power BI1 e estão disponíveis em sua totalidade
no link http://tinyurl.com/TVDI30.
Protótipo. O protótipo da nova jornada do telespectador da TV
orientada a aplicativos foi criado utilizando HTML5, executado
tanto em um hardware Raspberry Pi 4 modelo B quanto em um
notebook com sistema operacional Windows 11 padrão de mercado,
incorporando metodologias de design centrado no usuário e de-
sign thinking, com várias iterações de prototipagem e refinamento
baseadas em feedbacks quali e quantitativos.
A TELEVISÃO DE PRÓXIMA GERAÇÃO E O
TELESPECTADOR MÉDIO
A proposta de nova jornada do telespectador de TV aberta orientada
a aplicativos é composta por várias etapas, com objetivo de tornar a
experiência de consumo audiovisual mais acessível e personalizada.
Ao ligar a smart TV pela primeira vez, o telespectador seleciona
um idioma de configuração e define opções de acessibilidade, como
audiodescrição e tradução para linguagem de sinais. Em seguida,
há a opção de criar um perfil pessoal ou de grupo, configurando
controles parentais e preferências de acessibilidade. A criação do
perfil permite a integração de dados pessoais, utilizados pelos apli-
cativos das emissoras para fornecer uma experiência personalizada
(Figura 1).
Figura 1: Criação de perfis.
Após a configuração inicial, a TV realiza uma varredura dos
sinais das emissoras disponíveis e instala, automaticamente, os
aplicativos iniciais para cada emissora identificada (Figura 2). Esse
1https://www.microsoft.com/pt-br/power-platform/products/power-bi
Avaliação da Experiência do Telespectador na Transição para a TV Orientada a Aplicativos
WTVDI’2024, Juiz de Fora/MG, Brazil
processo assegura que todas as emissoras de TV aberta estejam
acessíveis, organizadas por ícones em vez de números de canais
e permite a personalização da ordem de preferência em que cada
aplicativo aparecerá para o telespectador (essa personalização será
refletida na ordem de zapeamento).
Figura 2: Varredura de sinais.
No catálogo de apps de TV aberta, o telespectador navega por
um ambiente dedicado às emissoras de TV aberta, podendo selecio-
nar os aplicativos iniciais das emissoras para acessar o conteúdo
disponível, seja via transmissão over-the-air (OTA) ou over-the-top
(OTT), dependendo da conectividade do aparelho e da disponibiliza-
ção, por parte dos radiodifusores, de conteúdos adicionais em seus
aplicativos secundários. A integração broadcast-broadband permite
o acesso a uma variedade de conteúdo, garante a transparência da
origem desse conteúdo e amplia as possibilidades de interação e
personalização da experiência de assistir à TV (Figura 3).
Figura 3: Catálogo de Aplicativos de TV aberta.
A realização dos grupos focais forneceu uma base qualitativa
para identificar e categorizar os principais critérios de avaliação do
protótipo da nova jornada do telespectador da TV aberta orientada
a aplicativos. Com base nas sessões coletivas e individuais, foram
definidos quatro critérios principais: usabilidade, acessibilidade,
personalização e integração de tecnologia.
A usabilidade foi o critério mais discutido, destacando-se a im-
portância de uma navegação fácil e intuitiva pelos menus e opções
da TV. Os participantes apontaram a clareza e a organização do
menu como fatores cruciais para uma boa experiência do telespec-
tador. Enfatizaram, também, a necessidade de o uso do controle
remoto ser fácil, com uma preferência clara pelo modelo tradicional
com teclas numéricas, que proporciona uma navegação mais rápida
e inclusiva pela presença de teclas com marcações táteis.
Já a acessibilidade emergiu como um critério fundamental, especi-
almente após as sessões que incluíram participantes com deficiência
visual. A presença de audiodescrição e tradução em língua de sinais
foi valorizada mesmo pelos telespectadores que não necessitam
delas, assim como a inclusão de marcadores táteis no controle re-
moto para auxiliar na navegação. O contraste e o tamanho da fonte
também foram destacados como elementos que merecem atenção
para garantir que a interface seja legível e utilizável por todos os
telespectadores, independentemente de suas limitações visuais ou
tamanho de suas telas.
A possibilidade de personalização foi vista como um avanço
desejável, permitindo que cada telespectador configure a TV de
acordo com suas preferências individuais. A configuração de perfis
personalizados foi bem recebida, especialmente pela capacidade de
adaptar a interface e o conteúdo às necessidades específicas de cada
telespectador. Embora a identificação das emissoras por ícones, em
vez de números, tenha gerado estranhamento e muito debate, foi
aceita como uma mudança positiva que pode melhorar a navegação
com o tempo.
Outro eixo de discussão é a integração de tecnologia, com ên-
fase na conectividade e na facilidade de uso da internet pela TV.
A possibilidade de usar dispositivos móveis como segunda tela e
como controle remoto foi vista como uma funcionalidade desejada,
embora tenha sido ressaltada a necessidade de incluir recursos de
acessibilidade, como comandos por voz. Além disso, funcionali-
dades adicionais, como acesso a serviços governamentais, foram
mencionadas como desejáveis, evidenciando a TV como um portal
para oferta de serviços além do entretenimento.
A Tabela 1 a seguir sintetiza a avaliação qualitativa.
Tabela 1: Síntese da Avaliação Qualitativa
WTVDI’2024, Juiz de Fora/MG, Brazil
Shuen et al.
Após a fase de prototipação e avaliação qualitativa com grupos
focais de telespectadores, tornou-se necessário ampliar o escopo da
avaliação para entender melhor a aceitação e os desafios da nova
tecnologia em um público mais amplo. A fase seguinte envolveu a
apresentação do conceito de TV orientada a aplicativos para uma
amostra aleatória da população que iria ter contato pela primeira
vez com as novas interfaces fora de um ambiente controlado de la-
boratório. Essa etapa contribuiu para avaliar como diferentes perfis
de telespectadores, especialmente aqueles com menor familiaridade
tecnológica, reagiriam à nova proposta em seus próprios contextos
de consumo diário da televisão aberta.
Apesar do crescimento das plataformas de streaming e outras
formas de entretenimento personalizado, a TV aberta ainda mantém
uma forte relação com o público. Entre os entrevistados em nosso
levantamento, 89,35% relataram consumir conteúdo da TV aberta,
e destes, 26,75% assistem a ela diariamente. A adesão é maior entre
aqueles com menor renda e escolaridade (ensino fundamental e
renda de até um salário mínimo), faixa em que 100% da amostra
consome TV aberta e 56,25% assistem, diariamente.
Um dos aspectos mais característicos da televisão aberta é a
grade linear de programação. Este formato posiciona o telespecta-
dor como um receptor passivo, que espera o horário determinado
de seu programa de interesse, funcionando como um marcador cul-
tural de tempo, organizando o dia a dia das pessoas de acordo com
os horários dos programas e eventos transmitidos. A grade passou
a ter um caráter nacional a partir da criação da Empresa Brasileira
de Telecomunicações (Embratel) em 1969, quando foi possível esta-
belecer uma infraestrutura que permitiu a transmissão simultânea
de conteúdo para todo o país, solidificando a importância cultural,
social e política da grade de programação [1].
A pesquisa indica que a grade ainda é valorizada: 51,17% dos
entrevistados afirmaram que horários fixos são uma característica
positiva. Entre os entrevistados de menor escolaridade e renda, o
percentual sobe para 81,25%.
Outro elemento enraizado na prática de consumo televisivo bra-
sileiro é a identificação numérica dos canais. Mesmo quando apre-
sentados a uma interface estilo catálogo com ícones das emissoras,
30,39% dos entrevistados relataram sentir falta dos números dos
canais na tela. Esse sentimento é ainda mais forte entre aqueles com
renda de até três salários mínimos e ensino fundamental: 58,33%
afirmaram sentir falta dessa informação, apesar de conseguirem
identificar suas emissoras favoritas pelos ícones.
Os dados quantitativos obtidos na pesquisa de opinião pública in-
dicaram que a população com menor escolaridade, renda mais baixa,
menor acesso a televisores conectados e de idade mais avançada
demonstrou um maior apego à grade linear de programação e à
identificação numérica dos canais. Esses achados sugerem que essa
faixa da população pode enfrentar dificuldades na compreensão e
adoção do conceito de TV orientada a aplicativos. Neste sentido,
a equipe de P&D expandiu a avaliação qualitativa para um novo
grupo focal, entendendo que possíveis resistências à mudança de
paradigma na TV aberta não é apenas uma questão de preferência
ou de hábitos culturais arraigados, mas também de vulnerabilidade
técnico-informacional, expressa pela exclusão digital. A avaliação
qualitativa com uma amostra dessa população específica comple-
menta os dados quantitativos para identificar os pontos de fricção e
as barreiras que podem dificultar a adoção da televisão de próxima
geração.
O TELESPECTADOR IDOSO E OS IMPACTOS
DA VULNERABILIDADE
TECNO-INFORMACIONAL
A avaliação qualitativa conduzida com o grupo de idosos de baixa
renda e escolaridade, sem acesso a televisores conectados à internet,
revelou uma vulnerabilidade tecnológica significativa. Este grupo
enfrenta desafios decorrentes de fatores culturais e socioeconô-
micos, caracterizados como vulnerabilidade por exclusão ou baixo
letramento digital. No contexto da exclusão digital [5][6][9][8][3], o
projeto TV 3.0 possibilitou aos pesquisadores a definição da catego-
ria vulnerabilidade tecno-informacional. Esta se refere à dificuldade
no acesso, uso e compreensão das tecnologias da informação e
comunicação (TICs). Esse fenômeno é influenciado por diversos
fatores, como a falta de acesso a dispositivos, conexões de internet li-
mitadas, baixo letramento digital e ausência de habilidades técnicas,
com consequências sociais, políticas, culturais e econômicas.
Semelhante à vulnerabilidade social, que se relaciona com a
exposição a riscos e desvantagens sociais, a vulnerabilidade tecno-
informacional tem implicações significativas para o exercício da
cidadania. Indivíduos tecnologicamente vulneráveis encontram-se
em desvantagem em termos de oportunidades, participação cívica,
acesso a serviços públicos, educação e emprego, afetando a quali-
dade de vida e o exercício da cidadania, podendo agravar a exclusão
social, dada a crescente dependência das tecnologias na vida mo-
derna [5][8].
Este tipo de vulnerabilidade não pode ser desconsiderado na
fase de prototipação de novas tecnologias. Fatores como idade,
nível de escolaridade, renda e acesso a inovações influenciam a
capacidade de adaptação [6][8]. Os idosos quilombolas são sujeitos
a essa vulnerabilidade devido a barreiras materiais, motivacionais,
de habilidades e de uso. Para entender a resposta dos telespectadores
vulneráveis à proposta de TV orientada a aplicativos, esta etapa de
avaliação qualitativa foi acrescentada à pesquisa a partir da análise
dos dados quantitativos apurados no levantamento, que sugeriu
maior dificuldade dessa parcela da amostra em compreender o
conceito de televisão orientada a aplicativos e maior resistência em
abandonar o paradigma de televisão orientada a canais, vigente há
mais de sete décadas.
A sessão foi realizada em uma agrovila distante 14 km da sede
do município de Alcântara/MA que, por sua vez, está distante 22
km da capital São Luís. Trata-se de uma comunidade remanescente
de quilombo, deslocada durante o processo de desapropriação ter-
ritorial das comunidades tradicionais que habitavam a área hoje
ocupada pelo Centro de Lançamento de Alcântara. Não existe sinal
de telefonia celular. Algumas casas possuem internet cabeada por
fibra óptica. A comunidade é formada por 87 famílias, cerca de 250
pessoas.
Os participantes demonstraram interesse e disposição para ado-
tar a nova tecnologia, desde que as interfaces sejam amigáveis e a
transição não seja abrupta. A simplicidade e a clareza das interfaces
apresentadas foram destacadas como fatores positivos para a ado-
ção do novo paradigma. No entanto, a avaliação revelou que a falta
de acesso a dispositivos adequados e à infraestrutura tecnológica
Avaliação da Experiência do Telespectador na Transição para a TV Orientada a Aplicativos
WTVDI’2024, Juiz de Fora/MG, Brazil
necessária são barreiras que podem impactar negativamente a ex-
periência desses telespectadores com a nova geração de TV aberta
brasileira, além do letramento digital limitado dos indivíduos.
O grupo não expressou desconfiança em relação à nova tecno-
logia, mas preocupação com a complexidade das novas funciona-
lidades. Os idosos insistiram, em todas as suas intervenções, na
necessidade de interfaces intuitivas e de suporte técnico acessível
para facilitar a transição para aplicativos, como o desenvolvimento
de programas de letramento digital e a oferta de dispositivos a
preços acessíveis para grupos economicamente vulneráveis.
A avaliação da nova jornada do telespectador, das funcionalida-
des do catálogo de apps de TV aberta e do acesso ao ambiente de
aplicativos/canais das emissoras mostrou que os participantes apre-
sentaram um entendimento variado da proposta, de razoável a bom.
Eles aprovaram a disposição dos ícones das emissoras, compreen-
deram e endossaram a necessidade de haver um ícone padronizado
para acesso ao catálogo de TV aberta, mas demonstraram apego ao
uso da expressão "canal"em vez de "aplicativo". A identificação das
emissoras por seus logotipos, e não por números, causou estranha-
mento para todos os participantes, que expressaram preferência
pela inclusão dessa informação no catálogo de aplicativos. Quando
questionados sobre a exclusão definitiva dos números, o grupo focal
respondeu que o estranhamento seria inevitável, pois é arraigado
o costume de se referir às emissoras pelos números, e não pelos
nomes. A pesquisa de opinião já indicava essa preferência nessa
parcela dos entrevistados, revelando que dois terços dos responden-
tes de menor escolaridade e renda sentiram falta dos números dos
canais.
A proposta de utilizar cores diferenciadas nos botões do controle
remoto (vermelho, verde, azul e amarelo) para interatividade foi bem
recebida pelos participantes como um mecanismo de redução do
sentimento de exclusão e dependência tecnológica. Eles acreditam
que a associação de cores facilita a navegação, especialmente para
aqueles com dificuldades visuais ou cognitivas. Os participantes
ressaltaram que o uso das teclas pode promover a inclusão daqueles
menos familiarizados com tecnologia, pois é simples associar uma
cor a um comando sem esforço, sem leitura e de forma a não se
perder o interesse no programa assistido. Eles desconheciam a
existência de teclas coloridas virtuais, presentes em controles com
proposta mais minimalista.
Nenhum dos participantes tem experiência com uso de internet
pela televisão e todos relataram dificuldades em usar a internet em
celulares, com relatos de dependência de familiares, especialmente
netos, para realizar tarefas online, expressando temerem repetir essa
dependência em relação à TV aberta de próxima geração. Houve
consenso de que a TV orientada a aplicativos pode servir como
uma ferramenta para acessar conteúdos e serviços de forma mais
intuitiva, desde que os comandos sejam simples, contribuindo para
a redução da vulnerabilidade tecno-informacional de pessoas idosas.
Houve manifestação de interesse em utilizar a TV para telemedicina
e educação a distância, com a sugestão de criação de aplicativos que
permitam consultas médicas e participação em cursos diretamente
pela TV.
Os participantes discutiram sobre como interagir com a TV e
aplicativos, e como esses aspectos impactam a experiência do teles-
pectador idoso. Destacaram, ainda, suas experiências individuais e
preocupações com a usabilidade e funcionalidade da tecnologias,
mostrando-se dispostos a aprender a usar novas tecnologias, es-
pecialmente se houver suporte adequado e interfaces amigáveis.
A facilidade de navegação, incluindo ícones grandes, comando de
voz e opções de audiodescrição, foi apontada como essencial para
melhorar a experiência de uso.
Nenhum idoso expressou preocupação com o uso de dados pes-
soais e houve o entendimento de que a sua coleta pode melhorar a
oferta de conteúdo desejado. No entanto, a vulnerabilidade tecno-
informacional emerge como uma variável crucial que pode afetar
a adoção da TV orientada a aplicativos por parte desta parcela da
população. A dependência de familiares para o uso de tecnologias
digitais e a falta de intimidade com comandos digitais são barreiras
que precisam ser endereçadas para garantir que a transição para
o modelo de televisão digital de próxima geração seja inclusiva e
acessível.
CONSIDERAÇÕES FINAIS
A avaliação da proposta de nova jornada do telespectador da TV
orientada a aplicativos revelou diferenças e semelhanças sobre a
experiência do telespectador médio e daqueles que sofrem de ex-
clusão digital, exclusão dimensionada pela vulnerabilidade tecno-
informacional. Embora os telespectadores tenham demonstrado
interesse e aceitação pela nova tecnologia, a pesquisa evidenciou
que populações vulneráveis enfrentam barreiras adicionais rela-
cionadas à usabilidade, acessibilidade e familiaridade tecnológica.
Essas descobertas sublinham a importância do desenvolvimento
de interfaces intuitivas, com oferta de letramento digital adequado
para garantir uma transição inclusiva, permitindo que todos os
telespectadores se beneficiem plenamente das vantagens da TV
digital de próxima geração.
A pesquisa quantitativa mostrou que a familiaridade com a tec-
nologia e o acesso a dispositivos conectados são fatores críticos
para a aceitação e uso eficiente da nova TV aberta orientada a apli-
cativos. Os dados indicaram que há uma lacuna significativa na
adaptação tecnológica entre diferentes grupos socioeconômicos. Os
grupos focais e as avaliações individuais revelaram que os telespec-
tadores mais jovens e tecnologicamente proficientes adaptaram-se
rapidamente ao novo formato, apreciando a personalização e a in-
teratividade oferecidas pela proposta da TV orientada a aplicativos.
Já os idosos e pessoas de baixa escolaridade apresentaram algumas
dificuldades em compreender e utilizar as novas funcionalidades.
Os resultados reforçam a necessidade de intervenções educacionais
e tecnológicas para mitigar essas dificuldades, como a promoção
contínua do letramento digital.
AGRADECIMENTOS
O projeto TV 3.0 é uma iniciativa do Fórum SBTVD, financiado
pelo Ministério das Comunicações através da Rede Nacional de
Ensino e Pesquisa (RNP). A Superintendência de Tecnologia da
Informação da Universidade Federal do Maranhão forneceu suporte
técnico e equipamentos necessários para o desenvolvimento e teste
do protótipo em laboratório.

--- FIM DO ARQUIVO: 30504-829-24952-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30506-829-24954-1-10-20241001.txt ---
Planejamento e Estratégia de Comunicação para a TV UFMA na
transição para a TV 3.0
Miguel Bruno Alves Chaves
miguel.chaves@discente.ufma.br
Universidade Federal do Maranhão
São Luís, Maranhão
Li-Chang Shuen
li.chang@ufma.br
Universidade Federal do Maranhão
São Luís, Maranhão
WTVDI’2024, Juiz de Fora/MG, Brazil
Miguel Bruno Alves Chaves and Li-Chang Shuen
c) Você sabe quando a nova geração TV 3.0 vai iniciar sua
operação no Brasil?
d) Quais as suas expectativas para a inclusão da TV UFMA na
próxima geração de TV digital brasileira?
O objetivo principal desta pesquisa foi compreender a profun-
didade do debate sobre a transição para a TV de próxima geração
dentro da emissora, identificando o comprometimento da equipe
com este processo e as preocupações emergentes. Procurou-se tam-
bém investigar como cada membro da equipe está se preparando
para essa migração em suas respectivas áreas de atuação e quais
limitações têm sido encontradas durante este período de transição.
Além do questionário principal, um questionário separado foi
enviado ao diretor de engenharia e ao técnico operacional, com as
seguintes questões específicas:
a) O que a TV UFMA precisa fazer para se adequar à TV 3.0
orientada a apps?
b) Como está o parque tecnológico da emissora?
Participaram da pesquisa um total de 13 indivíduos, incluindo es-
tagiários, editores, gerentes, jornalistas, diretores e o departamento
de engenharia. A diversidade dos participantes permitiu a obtenção
de uma ampla gama de perspectivas, enriquecendo a análise dos
dados coletados. Através desta investigação, espera-se não apenas
mapear o atual estado de preparação da TV UFMA para a imple-
mentação da TV do Futuro, mas também fornecer subsídios para a
formulação de políticas e práticas que possam melhorar a eficiência
deste processo de transição.
A EVOLUÇÃO TECNOLÓGICA DA
TELEVISÃO BRASILEIRA
A televisão, ao longo de sua história, passou por transformações sig-
nificativas que moldaram não apenas sua tecnologia, mas também
sua influência na sociedade [7]. Desde os primeiros dias em que o
áudio mono dominava as transmissões em preto e branco até os
avanços mais recentes em alta definição e interatividade, a televisão
tem sido um reflexo das inovações tecnológicas e das demandas do
público.
Foram necessários 22 anos desde o início das transmissões tele-
visivas no Brasil para a primeira grande mudança significativa: a
transição do áudio mono para o estéreo. Este avanço permitiu uma
experiência sonora mais imersiva para os telespectadores, enrique-
cendo ainda mais o conteúdo transmitido.
Além disso, a introdução das cores no lugar do tradicional preto
e branco revolucionou a forma como as pessoas viam televisão,
proporcionando uma experiência mais vibrante e realista. Na década
de 1980, o surgimento do closed caption trouxe acessibilidade para
pessoas com deficiência auditiva, permitindo que elas pudessem
fruir melhor os programas televisivos.
Enquanto os radiodifusores privados exploravam essas funcionali-
dades com fins comerciais, é crucial reconhecer a atuação das tele-
visões públicas, especialmente as educativas e universitárias em
desempenhar um papel fundamental em dar visibilidade àqueles que
tradicionalmente têm menos representação na mídia. Sua responsa-
bilidade vai além do entretenimento, estendendo-se à educação e à
promoção da diversidade cultural.
Com o advento da TV 3.0, novas possibilidades se apresentam.
Desde a transmissão em alta definição até a integração de funcionali-
dades como multitelas, interatividade e acessibilidade avançada, a
televisão ficará mais dinâmica ([5]; [6]; [3]; [2]; [1]). No entanto,
surge a questão de como esse processo se dará dentro das televisões
universitárias já em operação.
Em São Luís, a TV UFMA, pioneira na transmissão digital, está à
frente desse processo. Através de seu departamento de engenharia,
a TV UFMA está acompanhando de perto as tendências da TV
3.0 e buscando integrar essas inovações em sua programação. À
medida que nos aproximamos da era da TV 3.0, é essencial que todos
os setores, incluindo governamentais, acadêmicos e empresariais,
estejam alinhados com as tendências e inovações que estão por vir.
A comunicação institucional é um elemento vital para qualquer
organização, incluindo instituições de ensino superior. A Univer-
sidade Federal do Maranhão (UFMA) não é uma exceção e possui
regimentos específicos que direcionam suas práticas comunica-
cionais. Com o advento da próxima geração de TV digital terrestre,
que introduz novas formas de interação entre emissora e público, é
imperativo analisar como essas mudanças tecnológicas podem ser
integradas às estratégias de comunicação da UFMA.
A evolução da tecnologia televisiva está levando as emissoras a
se adaptarem a novas plataformas e formatos, como a substituição
do paradigma de orientação a canais para a orientação a aplica-
tivos. Para que a TV UFMA se adapte adequadamente a essa nova
realidade, é essencial que a equipe compreenda plenamente as lin-
guagens de programação e as ferramentas que serão utilizadas na
TV 3.0. Essa compreensão permitirá à emissora identificar e adquirir
os equipamentos e softwares necessários para o desenvolvimento
de aplicações compatíveis com essa tecnologia emergente, além
do investimento em qualificação para as equipes de produção e
edição de conteúdo. A antecipação dessas mudanças tecnológicas é
crucial para garantir que a TV UFMA mantenha sua relevância e
capacidade de inovação no cenário televisivo atual.
O quadro de pessoal da emissora é composto por uma equipe
técnica e de produção que desempenha papéis fundamentais na
operação da TV. No âmbito técnico, a equipe inclui um engenheiro
de telecomunicações, responsável por garantir a integridade técnica
das transmissões, um coordenador técnico que supervisiona as
operações diárias, um técnico que executa as tarefas de manutenção
e suporte técnico, um profissional de Descrição Técnica de Vídeo
(DTV) que assegura a acessibilidade dos conteúdos, e um diretor
de estúdio que coordena as atividades de gravação e transmissão.
Essa estrutura de pessoal é essencial para o funcionamento eficaz
da emissora e para a implementação de novas tecnologias.
A TV UFMA, embora tenha sido concebida como uma emis-
sora digital, enfrenta o desafio de adaptar e transformar todo o
seu parque tecnológico para a transição completa para a TV 3.0.
Atualmente, a emissora opera com um sistema digital HD, que
atende aos requisitos das redes sociais e seus respectivos protoco-
los. No entanto, para se alinhar às demandas da nova geração de TV
digital terrestre, será necessária a atualização das ilhas de edição,
câmeras de externa e estúdio, switcher, master e por fim, os recep-
tores domésticos, garantindo que os conteúdos gerados possam ser
adequadamente recebidos e exibidos pelos telespectadores.
Planejamento e Estratégia de Comunicação para a TV UFMA na transição para a TV 3.0
WTVDI’2024, Juiz de Fora/MG, Brazil
TV UNIVERSITÁRIA COMO LABORATÓRIO
DE INOVAÇÃO
No dia 7 de outubro de 2015, entrou no ar a primeira televisão
universitária do Maranhão: TV UFMA. Disponível no canal aberto
16.1, na NET/Claro TV canal 17, na Sky canal 316 e na TVN canal 16.
Possui um site oficial, um canal no YouTube e perfis nas redes sociais
Instagram e Facebook. Além disso, está disponível na plataforma
Eduplay, mantida pela Rede Nacional de Ensino e Pesquisa (RNP),
que oferece canais universitários online.
No site da TV UFMA, é possível assistir à programação ao vivo
e acessar as redes sociais do canal. No entanto, é importante notar
que a página do Facebook não recebe atualizações desde maio de
2023. O YouTube da TV UFMA funciona como um repositório de
programas. Na programação, destaca-se o JTV UFMA, um telejornal
diário transmitido às 12h30 com interpretação simultânea em Libras.
Atualmente, o setor de mídias da TV UFMA concentra seus esforços
de divulgação no Instagram, com postagens diárias e conteúdo nos
stories, incluindo bastidores. Além disso, listas de transmissão no
WhatsApp são usadas para divulgar as manchetes do jornal.
A TV UFMA implementou uma estratégia de transmissão que
tem mostrado resultados significativos. Seu conteúdo é disponi-
bilizado tanto em seu canal no YouTube quanto na plataforma
Eduplay, com transmissão ao vivo da programação e a opção de
acessar os conteúdos por meio de playlists na plataforma. No Edu-
play, a TV UFMA se consolidou como a terceira TV universitária
mais assistida do país, com mais de 500 matérias produzidas em
2023 e 20 programas autorais.
A interação entre a TV aberta, a internet e as redes sociais digitais
sugere um futuro promissor para a TV UFMA, especialmente com
as novas oportunidades oferecidas pela TV 3.0. Esse cenário já
tem se refletido positivamente, posicionando a TV UFMA como a
maior fornecedora de conteúdo para os protótipos de demonstração
desenvolvidos pela equipe de P&D UFMA/UFJF/PUC-Rio no âmbito
do projeto TV 3.0. As figuras a seguir ilustram a participação da
emissora como laboratório de conteúdos.
Figure 1: Catálogo de apps de tv aberta
A próxima geração da TV brasileira representa uma revolução
na forma como o conteúdo televisivo é produzido, distribuído e con-
sumido. Ela promete uma interação mais dinâmica e personalizada,
algo que vai além da veiculação tradicional de reportagens e pro-
gramas locais. Com a TV 3.0, novas possibilidades de interatividade
Figure 2: Tela da sugestão de aplicativo principal com con-
teúdo da TV UFMA
Figure 3: Tela com sugestão de recomendações/aplicativo
secundário com conteúdo da TV UFMA. As recomendações
de conteúdo extra podem vir pelo ar (OTA) ou por broadband
(OTT)
se abrem, permitindo uma participação mais ativa dos telespecta-
dores. Isso inclui a possibilidade de personalização de conteúdo,
maior integração com outras plataformas digitais e a utilização de
tecnologias emergentes como realidade aumentada e virtual.
Essas mudanças trazem consigo uma série de desafios tecnológi-
cos. A transição para a TV 3.0 exige uma atualização significativa ou
até mesmo a total substituição de tecnologias e equipamentos. Isso
inclui a adoção de novos padrões de transmissão, a implementação
de infraestrutura compatível e a formação de pessoal capacitado
para operar e manter essas novas tecnologias. A TV UFMA, assim
como outras emissoras, terá que investir em capacitação técnica e
em pesquisa para acompanhar essa evolução e explorar plenamente
as potencialidades oferecidas pela TV 3.0.
Além dos desafios técnicos, há também um desafio estratégico. A
emissora maranhense precisa redefinir seu papel e suas estratégias
de conteúdo para se manter relevante e cumprir sua missão educa-
tiva e cultural no novo cenário digital. Isso implica em desenvolver
novos formatos de programas que aproveitem as funcionalidades
interativas da TV 3.0 e estabelecer parcerias que potencializam sua
capacidade de produção e distribuição de conteúdo. Dessa forma,
WTVDI’2024, Juiz de Fora/MG, Brazil
Miguel Bruno Alves Chaves and Li-Chang Shuen
a TV UFMA poderá fortalecer seu vínculo com a sociedade e con-
tribuir para o desenvolvimento de uma televisão mais participativa
e inclusiva, refletindo os avanços tecnológicos e as mudanças nos
hábitos de consumo de mídia, sem deixar de lado seu papel como
emissora que faz parte de um ecossistema de comunicação pública
[4].
Um levantamento foi conduzido com profissionais de comuni-
cação que prestam serviços à TV UFMA, incluindo servidores, co-
laboradores terceirizados e estagiários, com o objetivo de avaliar
percepções e expectativas em relação à transição para a TV de pró-
xima geração. Três perguntas objetivas foram formuladas para obter
uma visão abrangente sobre o processo de adaptação tecnológica e
operacional da emissora.
A maioria dos profissionais já têm conhecimento do processo
de desenvolvimento da TV 3.0: 69,2%. No entanto, 61,5% dos entre-
vistados afirmam não saber o prazo para a próxima geração de TV
entrar em operação no Brasil. Com relação às características que os
colaboradores da emissora associam à TV 3.0, a interatividade e a
personalização de conteúdos são aquelas mais lembradas, conforme
gráfico a seguir.
Figure 4: Gráfico das características mais associadas à TV 3.0
pelos colaboradores da TV UFMA
A análise das respostas revela que, embora a maioria dos en-
trevistados ainda não consiga determinar com precisão o início
das atividades da nova geração de televisão, há uma expectativa
considerável em torno da TV 3.0. Esta tecnologia emergente pro-
mete transformar tanto a produção quanto o consumo de conteúdo
audiovisual. As inovações associadas à TV 3.0 incluem a integração
de funcionalidades interativas e a oferta de conteúdo personalizado,
o que representa um avanço significativo em relação aos modelos
tradicionais de transmissão.
Para os produtores de conteúdo, a TV 3.0 possibilita novas formas
de engajamento com o público, permitindo a criação de narrativas
mais dinâmicas e interativas. Esta evolução tecnológica oferece um
campo fértil para a experimentação criativa e a inovação, possibili-
tando o desenvolvimento de conteúdos que atendam às demandas
de uma audiência cada vez mais diversificada e exigente. Além
disso, as ferramentas avançadas de coleta de dados, medição de
audiência e feedback em tempo real proporcionam aos produtores
a capacidade de ajustar rapidamente suas estratégias de conteúdo
para otimizar a experiência do telespectador.
Do ponto de vista dos telespectadores, a TV 3.0 oferece uma ex-
periência de visualização mais rica e personalizada. A possibilidade
de acessar conteúdo sob demanda, combinado com recursos intera-
tivos, transforma a televisão em uma plataforma mais atraente.
Isso é particularmente relevante no contexto da TV UFMA, cujo
foco principal é a produção de conteúdo educativo. A TV 3.0 não
apenas amplia o alcance desse conteúdo, mas também enriquece a
experiência de aprendizagem dos telespectadores.
Além disso, a introdução da TV 3.0 na Universidade Federal do
Maranhão oferece uma oportunidade valiosa para os estudantes do
curso de Comunicação Social em suas três habilitações (Jornalismo,
Rádio e TV e Relações Públicas). O envolvimento direto com a nova
tecnologia em um ambiente de estágio proporciona uma experiên-
cia prática única, permitindo que os alunos adquiram habilidades
essenciais e se familiarizem com as últimas tendências do setor.
Este aspecto educacional tem muita importância para preparar a
próxima geração de profissionais de comunicação, que serão res-
ponsáveis por explorar e expandir as possibilidades oferecidas por
essa tecnologia emergente.
Vale destacar também que a grade curricular do curso de Rádio e
TV, oferecido pela UFMA, passou por uma atualização significativa
em 2023. Este bacharelado, com carga horária de 2.910 horas, agora
tem como foco principal a formação de novos profissionais que terão
contato direto com as inovações da próxima geração de televisão.
As disciplinas incluem Roteiro para Audiovisual, Direção de Arte,
Edição e Pós-Produção, Mídia Digital e Dinâmicas Emergentes, en-
tre outras. Essa atualização curricular reflete a preocupação do curso
em se alinhar com os novos processos comunicacionais, garantindo
que os alunos estejam preparados para enfrentar os desafios e opor-
tunidades do setor.
CONSIDERAÇÕES FINAIS
As emissoras universitárias em operação enfrentarão desafios téc-
nicos, tecnológicos, econômicos e políticos na transição para TV
3.0, pois são radiodifusores dependentes de orçamentos limitados e
com pouca autonomia decisória dentro da estrutura das instituições
que as abrigam. Mesmo assim, as possibilidades de ampliação da
interação com o público e de personalização do conteúdo oferecido
faz com que essas pequenas emissoras públicas já se movimentem e
se preparem, na medida do possível, para avançar rumo à próxima
geração de TV aberta brasileira.
Em relação à TV UFMA, os colaboradores da emissora possuem
um nível razoável de conhecimento e alto de expectativa em relação
à nova tecnologia, apesar das incertezas quanto ao cronograma
de implementação e do acesso aos insumos necessários para ade-
quação do canal. Os principais desafios identificados incluem a
atualização tecnológica e a necessidade de qualificação profissional,
ambos essenciais para uma adaptação bem-sucedida. Como emis-
sora universitária, a TV UFMA desempenha um papel pedagógico
na experimentação e no desenvolvimento de novas práticas comuni-
cacionais, precisando estar apta para preparar futuros profissionais
para as demandas da TV digital.
Conforme citado neste position paper, o curso de Comunicação
Social da UFMA, que dá suporte à emissora, começou o processo
de atualização curricular pela habilitação Rádio e TV, inserindo
disciplinas e atividades extensionistas com foco em formar profis-
sionais preparados para a nova realidade tecnológica vindoura, mas
os atuais colaboradores do canal universitário também demandam
Planejamento e Estratégia de Comunicação para a TV UFMA na transição para a TV 3.0
WTVDI’2024, Juiz de Fora/MG, Brazil
capacitação para não apenas transmitirem um sinal digital mais
avançado mas, sobretudo, ofertarem conteúdo compatível com as
expectativas criadas com as possibilidades da próxima geração de
TV digital terrestre.
Este cenário não é exclusivo da emissora maranhense, posto
que os canais universitários fazem parte de um ecossistema de
comunicação pública que enfrenta desafios semelhantes em todos
os estados – sejam as TV’s universitárias, legislativas, educativas
estaduais, do judiciário ou do executivo federal. A academia, como
parte integrante do processo de Pesquisa & Desenvolvimento do
Projeto TV 3.0 tem uma demanda a ser enfrentada: a de desenvolver
meios que promovam a equidade entre as emissoras comerciais e
as emissoras públicas para que o advento de TV 3.0 não aumente a
disparidade e nem promova a exclusão da comunicação pública do
espectro da radiodifusão digital.

--- FIM DO ARQUIVO: 30506-829-24954-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30507-829-24955-1-10-20241001.txt ---
Uma análise de plataformas de streaming e as possibilidades de
interface para conteúdos sob demanda na TV 3.0
Cristiane Turnes Montezano∗
cristiane.turnes@estudante.ufjf.br
Universidade Federal de Juiz de Fora
(UFJF)
Juiz de Fora, Minas Gerais
Carlos Pernisa Júnior†
carlos.pernisa@ufjf.br
Universidade Federal de Juiz de Fora
(UFJF)
Juiz de Fora, Minas Gerais
Marcelo F. Moreno‡
marcelo.moreno@ufjf.br
Universidade Federal de Juiz de Fora
(UFJF)
Juiz de Fora, Minas Gerais
0.
WTVDI’2024, Juiz de Fora/MG, Brazil
Montezano et al.
Além das plataformas mais conhecidas de conglomerados norte-
americanos, como Netflix 1, Max2, Prime Video3 e Disney+4, tam-
bém exploramos plataformas de outras regiões do mundo, às quais
tivemos acesso, como a chinesa Tencent Video (proprietária da
WeTV5), a indiana Eros Now6, a russa iviTV7 e a árabe Shahid8.
A maioria dessas plataformas de streaming é voltada para con-
teúdos sob demanda e apresenta interfaces semelhantes. Após esse
levantamento inicial, realizamos uma filtragem, selecionando aque-
las que possuíam aspectos e características mais relevantes para
nossa pesquisa. Focamos, em particular, nas plataformas que se des-
tacavam no mercado por mesclarem conteúdos sob demanda com
transmissões lineares9 simultâneas, incluindo simulcasting com a
TV (transmissões de canais de TV aberta ou fechada via Internet),
canais próprios para o ambiente digital, como os canais FAST (Free
Ad-Supported Streaming Television), que transmitem exclusivamente
online com programação pré-definida, intervalos comerciais e te-
máticos (esporte, gastronomia, novela, etc.), ou ainda transmissões
esporádicas de eventos ao vivo, como partidas esportivas e shows.
Dessa forma, selecionamos as plataformas Globoplay10, PlutoTV11
e RTVEplay12 para uma análise mais detalhada.
A primeira plataforma analisada é um caso nacional. Desenvol-
vida pelo Grupo Globo, proprietário de uma das mais tradicionais
emissoras de TV do Brasil, foi lançada em 2015 e é atualmente a
maior em número de assinantes no país. Incluímos essa plataforma
em nossa análise por dois diferenciais: foi criada por uma empresa
com forte tradição nas mídias tradicionais e oferece dois formatos
de conteúdo: "sob demanda"e "ao vivo". A Figura 1 ilustra a tela
inicial da plataforma Globoplay.
A segunda plataforma é a RTVEplay, da Televisión Española
(TVE), uma emissora pública da Espanha. Lançada em 2008 com o
nome RTVE a la Carta, a plataforma foi atualizada em 2021, pas-
sando por mudanças não apenas no nome, mas também em sua
interface. Assim como no caso anterior, a escolha dessa plataforma
para nossa análise se deve ao fato de oferecer conteúdos sob de-
manda juntamente com transmissões ao vivo, além de pertencer a
uma emissora de TV tradicional e pública. A Figura 2 ilustra a tela
inicial da plataforma RTVEplay.
A terceira plataforma é a Pluto TV, da Paramount Global. Lan-
çada em 2013, chegou ao Brasil em 2020, destaca-se por priorizar
o conteúdo ao vivo, promovendo-o como seu principal atrativo.
Sua interface se assemelha à de TVs a cabo, com ênfase no guia de
programação.
Destacamos alguns pontos que consideramos importantes. Nas
duas primeiras plataformas, as páginas iniciais apresentam várias
semelhanças. Há conteúdos destacados em formato de carrossel,
1https://www.netflix.com/
2https://www.max.com/br/pt
3https://www.primevideo.com
4https://www.disneyplus.com/pt-br
5https://wetv.vip/pt
6plataforma indisponível no Brasil
7https://www.ivi.ru/
8https://shahid.mbc.net/en
9Modelo de distribuição de conteúdo da TV tradicional, com grade de programação
pré-definida e horários fixos para cada atração.
10https://globoplay.globo.com/
11https://pluto.tv/br
12O
acesso
foi
feito
através
da
versão
internacional
https://www.rtve.es/play/internacional/portada/
Figura 1: Tela inicial da Plataforma Globoplay.
Figura 2: Tela inicial da Plataforma RTVEplay.
ocupando uma parte significativa da tela. Nesse carrossel, são exibi-
dos uma imagem, o título da produção e um breve texto descritivo,
acompanhado de um botão clicável que direciona o espectador para
assistir ao conteúdo. Ao rolar a página, o material é organizado em
fileiras horizontais, com demarcações de identificação. Essas fileiras
normalmente misturam conteúdos de gêneros diferentes, mas que
possuem algo em comum, seja por serem os mais consumidos na
plataforma, seja por serem os lançamentos mais recentes.
A divisão por classificação de tipo de conteúdo ou gênero pode
ser acessada em outra instância, geralmente através de um atalho.
Na plataforma Globoplay, essa funcionalidade está disponível na
barra de atalhos, localizada na parte superior, inferior ou lateral da
Uma análise de plataformas de streaming e as possibilidades de interface para conteúdos sob demanda na TV 3.0
WTVDI’2024, Juiz de Fora/MG, Brazil
tela, dependendo do dispositivo utilizado para o acesso. Essa barra
permite catalogar o conteúdo oferecido pela plataforma, dividindo-
o entre canais disponíveis e por tipos de formatos, como ilustrado
pela Figura 3, incluindo jornalismo, entretenimento, séries, filmes,
novelas, além de classificações por gênero, como drama, romance,
comédia, entre outros.
Figura 3: Catálogo da Plataforma Globoplay.
Já na RTVEplay, para acessar a classificação por tema, é necessá-
rio utilizar uma barra de atalhos semelhante. Primeiro, o usuário
deve clicar no ícone de menu e, em seguida, selecionar a opção
“Temáticas”, onde é possível navegar pelos produtos da plataforma
organizados por tema.
Na Globoplay, o conteúdo “ao vivo” aparece mais abaixo na
rolagem da página inicial, mas também possui um ícone específico
na barra de atalhos, conforme ilustrado na Figura 4. Ao clicar nesse
ícone, o usuário é direcionado a uma segunda tela, onde pode acessar
diretamente o serviço linear.
Figura 4: Barra de atalho Globoplay, onde se observa o “Agora
na TV” usado para acessar as transmissões ao vivo.
Na RTVEPlay, o serviço linear aparece mais acima na rolagem
da primeira tela; no entanto, para acessar a aba específica desse
conteúdo, é necessário clicar no ícone de menu na barra de atalhos
e, em seguida, selecionar “Directos”. Ou seja, são necessários dois
cliques para acessar o serviço linear. Cada uma dessas plataformas, à
sua maneira, parece dar um destaque menor a esse tipo de conteúdo.
É nesse aspecto que a Pluto TV se diferencia, pois nela o conteúdo
“ao vivo” recebe pleno destaque.
Na Pluto TV, a primeira tela é inteiramente dedicada aos ca-
nais “ao vivo”, como pode ser observado na Figura 5. Assim que a
plataforma é acessada, um conteúdo já começa a ser reproduzido
automaticamente, ocupando uma posição semelhante à do carrossel
de conteúdos sob demanda nas outras duas plataformas. Ao rolar
essa primeira tela, encontram-se algumas divisões de classificação
de conteúdo dispostas horizontalmente em pequenos rótulos, que se
assemelham a botões clicáveis e ocupam pouco espaço na tela. Logo
abaixo, está o guia completo da programação, organizado por canais
e conteúdos que podem ser assistidos “ao vivo”. Na barra de atalhos,
que é a mais simples entre as três plataformas analisadas, há um
ícone dedicado às produções sob demanda, que, nessa plataforma,
estão localizadas em uma segunda tela. Essa forma de organização
de conteúdo não só contrasta com as outras duas plataformas, como
também com todas as demais levantadas inicialmente na pesquisa.
Figura 5: Tela inicial Pluto TV.
Todas as três plataformas oferecem a função de criação de perfis;
no entanto, na RTVEplay e na Pluto TV, é possível acessar conteúdos
sem a necessidade de criar um perfil, enquanto na Globoplay esse
cadastro é obrigatório, mesmo para acessar os conteúdos gratuitos.
As plataformas de streaming inicialmente funcionavam como
repositórios de produções, atuando como agregadores de conteúdo
audiovisual. Eram semelhantes a locadoras online, onde os consu-
midores podiam encontrar filmes e séries para assistir, como foi o
caso literal de um dos primeiros grandes sucessos desse modelo, a
Netflix, que começou suas operações em 1998 como uma locadora
que enviava DVDs pelo correio [3] e, em 2007, lançou-se como uma
plataforma de streaming online. Essa origem se reflete no design de
interface e na classificação dos conteúdos, com divisões por gêneros
tradicionalmente utilizados em produções cinematográficas, como
terror, drama, romance e comédia. Essa herança ainda persiste, e, em
geral, essas plataformas mantêm estruturas organizacionais basea-
das no conteúdo oferecido, com classificações fortemente centradas
em gêneros, mas que também incorporam as evoluções tecnológicas
da Internet e as mudanças no serviço oferecido, como a inclusão de
conteúdos originais e a mescla de diferentes formatos.
As plataformas de streaming adotaram barras de rolagem no
acesso através de PC, destaques para as produções mais consumi-
das semanalmente e ênfase em produções originais. Com a mescla
de formatos, a barra de atalhos — quase sempre localizada na parte
superior, inferior ou lateral da tela, dependendo do dispositivo uti-
lizado — apresenta, em alguns casos, um direcionamento específico
para produções em simulcasting, conteúdos ao vivo e transmissões
lineares simultâneas. Em relação ao atalho para a divisão em “ca-
tálogo”, ele se aproxima das estruturas de gênero, mas atualmente
inclui alguns formatos e/ou gêneros mais associados às produções
televisivas, como jornalismo, esportes, novelas, séries, entre outros.
É importante destacar que, nas três plataformas de streaming, há
Guias Eletrônicos de Programação (EPG - Electronic Program Guide).
Nesses guias, a programação predefinida dos conteúdos dos serviços
lineares é organizada. Essa ferramenta, comumente encontrada na
televisão, serve como um recurso de consulta e informação sobre
os programas, indicando os horários e os dias da semana em que
serão exibidos.
WTVDI’2024, Juiz de Fora/MG, Brazil
Montezano et al.
A ORGANIZAÇÃO DE CONTEÚDO NA TV 3.0
A TV 3.0 representa uma evolução significativa em termos de or-
ganização e acesso a conteúdos audiovisuais, integrando de forma
mais eficiente as transmissões OTA com o acesso OTT. Uma das
inovações cruciais para essa integração é a possibilidade de aquisi-
ção de metadados tanto via OTA quanto via OTT. Esses metadados,
que podem incluir informações detalhadas sobre o conteúdo, como
descrição, gênero, elenco, horários de exibição e disponibilidade,
são fundamentais para criar uma experiência de navegação mais
fluida e personalizada. Os metadados descrevem tanto conteúdos
dos serviços lineares quanto conteúdos sob demanda,
Na TV 3.0, a plataforma orientada a aplicativos se torna um ver-
dadeiro agregador de conteúdos de diferentes radiodifusores. Essa
centralização de informações e conteúdos oferece aos telespecta-
dores uma interface unificada, onde é possível acessar uma vasta
gama de programas de diversas emissoras, sejam eles transmitidos
em uma grade de programação linear ou disponibilizados sob de-
manda. No entanto, essa agregação apresenta desafios significativos,
como garantir uma usabilidade intuitiva e fluida, além de manter a
isonomia no acesso a todos os conteúdos, independentemente da
emissora que os provê.
Outra necessidade é a correta identificação da emissora para
cada conteúdo oferecido. Com a diversidade de fontes de conteúdo
disponíveis na TV 3.0, é essencial que o sistema identifique cla-
ramente a origem de cada programa ou conteúdo, preservando a
identidade das emissoras enquanto promove uma experiência coesa
e integrada ao telespectador.
Acreditamos que a organização da TV 3.0 deve se basear em duas
abordagens principais: o Guia Eletrônico de Programação (EPG) e o
Guia Eletrônico de Conteúdo (ECG - Electronic Content Guide). Tais
conceitos estão presentes na padronização de plataformas de aplica-
ções IPTV da União Internacional de Telecomunicações desde 2009
[2]. O EPG seria responsável pela organização do conteúdo OTA,
que possui programação predefinida, onde o horário e o dia de exi-
bição são, a priori, centrais para orientar o telespectador/interator.
Por outro lado, o ECG seria essencial para organizar os conteú-
dos sob demanda disponibilizados via Internet, oferecendo uma
navegação mais intuitiva e eficiente para o usuário.
O ECG seria um guia projetado para organizar o conteúdo de
maneira semelhante às estruturas encontradas nas plataformas de
streaming. Ele catalogaria o conteúdo por gêneros, como jornalismo,
esportes, novelas, séries, entre outros, conforme identificado em
nossa pesquisa. Essa divisão seria aplicada tanto aos conteúdos sob
demanda quanto aos conteúdos OTA, funcionando como rótulos
que facilitam a navegação.
A proposta do ECG é ampliar as possibilidades de organização em
resposta às inovações de conteúdo que o novo modelo de TV trará,
mas utilizando estruturas familiares aos telespectadores/interatores.
Dessa forma, o ECG poderia se tornar uma ferramenta facilitadora,
proporcionando uma compreensão e identificação mais rápida.
Entendemos que, com a profunda integração da TV 3.0 à Inter-
net, haverá uma proliferação ainda maior de conteúdos e formas de
acessá-los. Nesse cenário, os telespectadores/interatores poderão
precisar de mais caminhos para encontrar o que desejam assistir.
Quanto mais familiares esses caminhos forem, melhor será a experi-
ência de interação promovida por eles. Além disso, ressaltamos que
o ECG deve oferecer o máximo de interatividade possível, aprovei-
tando plenamente as potencialidades que o ambiente digital pode
trazer para a televisão.
Para proporcionar uma experiência de interatividade mais com-
pleta, é fundamental que o ECG inclua uma ferramenta de busca
abrangente, capaz de explorar todo o conjunto agregado de meta-
dados. Essa ferramenta deve permitir que os telespectadores encon-
trem rapidamente o conteúdo desejado, independentemente de sua
fonte ou tipo. Além disso, o sistema deve oferecer opções de intera-
ção flexíveis, como um teclado virtual para entradas manuais e a
possibilidade de comandos de voz. Essas funcionalidades garantirão
que os telespectadores possam fornecer suas palavras de busca de
forma rápida e conveniente, aprimorando ainda mais a usabilidade
e a acessibilidade da TV 3.0.
A listagem de conteúdos no ECG deve ser organizada em uma
grade de pequenas imagens que exibem a arte de capa do respectivo
conteúdo, facilitando a visualização e a identificação. Além disso,
acompanhando cada imagem deve-se incluir o título do conteúdo
em texto, o que contribui para a acessibilidade, ao permitir a leitura
automatizada das interfaces gráficas. Cada imagem de capa deve
também ter sobreposto um rótulo com o nome da emissora que
provê o conteúdo. Essa identificação é essencial desde a listagem
principal do ECG, algo que geralmente não se observa prontamente
em plataformas agregadoras de streaming. Isso assegura que o te-
lespectador/interator tenha clareza sobre a origem do conteúdo,
fortalecendo a transparência e a confiança no serviço oferecido. A
Figura 6 ilustra os projetos de interface elaborados no âmbito do
Projeto TV 3.0 para a prototipação do ECG, como uma interface que
se utiliza da estrutura típica de plataformas de streaming, porém
agregando as identificações que consideramos importantes.
Não vemos a possibilidade de introduzir carrosséis de conteú-
dos recomendados em telas iniciais do ECG, pois, para manter a
isonomia no acesso, não deve-se promover os conteúdos de certos
radiodifusores em detrimento de outros. A promoção desigual de
conteúdos poderia comprometer a equidade entre as emissoras e
afetar a experiência do telespectador. No entanto, essas recomenda-
ções poderiam ser viáveis em uma tela específica do ECG dedicada
à listagem de emissoras favoritas, conforme a seleção do telespec-
tador. Nesse contexto, as recomendações seriam personalizadas e
alinhadas com as preferências individuais, sem prejudicar a isono-
mia geral.
A Tabela 1 sintetiza uma comparação entre as plataformas de
streaming analisadas e as possibilidades identificadas para a TV 3.0.
CONSIDERAÇÕES FINAIS
Nesta pesquisa, buscamos explorar como a interface e a organiza-
ção de conteúdo da TV 3.0 serão moldadas com a introdução de
materiais sob demanda e a navegação por meio de aplicativos. Para
considerar as possíveis mudanças e adaptações nesse novo modelo
de televisão, realizamos um estudo das interfaces de plataformas
de streaming. Essas plataformas, além de representarem o princi-
pal modelo de distribuição de conteúdo audiovisual no ambiente
digital, apresentam características alinhadas aos requisitos da nova
geração de TV. A partir de uma pesquisa exploratória, selecionamos
as plataformas Globoplay, RTVEplay e Pluto TV para nossa aná-
lise. Essas plataformas oferecem conteúdo misto: além do modelo
Uma análise de plataformas de streaming e as possibilidades de interface para conteúdos sob demanda na TV 3.0
WTVDI’2024, Juiz de Fora/MG, Brazil
Figura 6: Projeto de interface para prototipação do ECG.
Tabela 1: Resumo comparativo plataformas de streaming e
ECG TV 3.0
Característica
Globoplay
RTVEplay
Pluto TV
TV 3.0
Recomendações
na tela inicial
Sim
Sim
Não
Não
Metadados OTT
Sim
Sim
Sim
Sim
Conteúdos OTT
Sim
Sim
Sim
Sim
Filtro por
Gênero
Sim
Sim
Sim
Sim
Função de busca
Sim
Sim
Sim
Sim
Isonomia entre
emissoras
Não
Não
Sim
Sim
Metadados OTA
Não
Não
Não
Sim
Conteúdos OTA
Não
Não
Não
Sim
Identificação de
emissoras
Não
Não
Não
Sim
tradicional sob demanda, típico do ambiente digital, elas também
disponibilizam serviço linear, “ao vivo”, trazendo, assim, aspectos
relevantes para este estudo.
Observamos que as plataformas de streaming utilizam divisões
por gênero originadas do cinema, como terror, drama, romance
e comédia, uma herança de seu histórico vinculado às locadoras
digitais. No entanto, ao longo dos anos, essas plataformas foram se
adaptando e introduzindo outras categorias, como produções mais
acessadas, conteúdos originais, entre outras.
Nas plataformas de streaming analisadas, encontramos EPGs
(guias de programação) que organizam os conteúdos em serviços
lineares e “ao vivo”.
Para a TV 3.0, acreditamos que será necessário implementar
mais de uma forma de organização, combinando o EPG, já pre-
sente na televisão, com uma nova abordagem semelhante à divisão
por gênero encontrada nas plataformas de streaming. Essa nova
abordagem, que chamamos de Guia Eletrônico de Conteúdo (ECG -
Electronic Content Guide), pode ser usada tanto para conteúdos sob
demanda quanto como rótulos para conteúdos OTA. Dessa forma, a
TV 3.0 pode oferecer uma organização que seja familiar ao telespec-
tador/interator, aproveitando associações intuitivas que ele já faz
a partir de outras experiências com audiovisuais na Internet. Isso
promoverá uma interação convidativa e fácil, preservando um dos
principais pontos fortes da televisão atual: a maneira descompli-
cada e confortável de acessar seu conteúdo, que não exige muitos
comandos.
Destacamos ainda que ambos os modelos de organização, tanto
o EPG quanto o ECG, devem oferecer o máximo de interatividade
possível, explorando os recursos e potenciais que as inovações do
novo modelo de TV podem proporcionar à experiência do telespec-
tador/interator. Acreditamos que essa nova tecnologia representa
uma oportunidade para aprimorar as formas de uso do EPG e do
ECG já existentes nas plataformas de streaming.
Além disso, o foco na isonomia e na identificação clara das emis-
soras no ECG reforça a importância de manter a diversidade e a
transparência no acesso ao conteúdo. A TV 3.0 pode assimilar o
melhor das plataformas de streaming, mas também deve garantir
que a pluralidade de fontes e a integridade da programação sejam
mantidas. Isso não só preserva a confiança do público na televi-
são, como também promove uma experiência mais democrática,
onde todos os radiodifusores têm a oportunidade de apresentar seus
conteúdos de forma justa e equilibrada.
Acreditamos que a TV 3.0, com essas inovações, tem o potencial
de enriquecer ainda mais a relação entre o público e o conteúdo
audiovisual, oferecendo uma plataforma moderna e adaptada às
necessidades e expectativas do telespectador contemporâneo. A
combinação de tecnologia avançada com uma interface amigável
e acessível pode estabelecer um novo padrão na maneira como
interagimos com a televisão.
AGRADECIMENTOS
Os autores agradecem a toda a equipe de P&D do Projeto TV 3.0,
pelas discussões e esforços de pesquisa em conjunto. Agradecem
também à RNP e MCom pela gestão e financiamento, assim como
ao Fórum SBTVD e seus membros pela iniciativa, acompanhamento
e contribuições constantes.

--- FIM DO ARQUIVO: 30507-829-24955-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30508-829-24956-1-10-20241001.txt ---
Uma análise de plataformas de streaming e as possibilidades para
um guia de programação na TV 3.0
Cristiane Turnes Montezano∗
cristiane.turnes@estudante.ufjf.br
Universidade Federal de Juiz de Fora
(UFJF)
Juiz de Fora, Minas Gerais
Carlos Pernisa Júnior†
carlos.pernisa@ufjf.br
Universidade Federal de Juiz de Fora
(UFJF)
Juiz de Fora, Minas Gerais
Marcelo F. Moreno‡
marcelo.moreno@ufjf.br
Universidade Federal de Juiz de Fora
(UFJF)
Juiz de Fora, Minas Gerais
WTVDI’2024, Juiz de Fora/MG, Brazil
Montezano et al.
A ORGANIZAÇÃO DE CONTEÚDO NA TV
Na televisão, a estrutura de organização do conteúdo se diferencia
significativamente de outros meios, apresentando uma forma única
de ordenar e distribuir informações. Ao contrário de mídias como o
cinema ou a internet, onde os conteúdos são geralmente acessados
de maneira isolada e sob demanda, a televisão tradicionalmente
adota uma abordagem distinta, caracterizada por uma sequência
contínua ou fluxo.
Como observou Williams (2016) [3], a organização característica
da radiodifusão, e consequentemente a experiência que ela propor-
ciona, é a de um fluxo planejado. Esse fluxo é, portanto, uma marca
distintiva que define a radiodifusão tanto como uma tecnologia
quanto como uma forma cultural. Nos sistemas de comunicação
anteriores à radiodifusão, os elementos essenciais da mensagem
eram apresentados de forma separada e isolada. Na televisão, po-
rém, esses elementos são integrados em um fluxo contínuo, que
guia o telespectador de uma programação a outra, criando uma
experiência de consumo única e linear.
Devido a essa característica de fluxo contínuo e à existência de
uma grade de programação pré-definida, a organização de con-
teúdos na televisão é predominantemente baseada no tempo e no
horário das transmissões. Cada programa ou conteúdo é inserido
em uma estrutura temporal que guia o espectador ao longo do dia,
criando uma experiência linear e previsível.
Essa organização temporal é mediada pelo EPG, visto como ferra-
menta essencial, especialmente no ambiente digital, onde o número
de canais e opções de conteúdo é vasto e diversificado. Ele oferece
uma maneira eficiente para os telespectadores navegarem por essa
vasta gama de opções, permitindo a localização rápida e organizada
dos programas de interesse.
Entre as diversas aplicações desenvolvidas para o ambiente da
TV Digital, o EPG destaca-se como uma das mais importantes.
Como observado por Tavares (2013) [2], os telespectadores utilizam
os guias com o propósito de encontrar, de maneira organizada
e rápida, os conteúdos que desejam assistir. Essa funcionalidade
é fundamental em um cenário onde a diversidade de conteúdos
e a multiplicidade de canais podem facilmente sobrecarregar o
espectador.
Com o aumento progressivo do número de canais ao longo do
tempo, o guia tornou-se ainda mais indispensável para que o teles-
pectador, ou interator, pudesse localizar facilmente os programas
de seu interesse. À medida que a diversidade de opções cresceu,
a necessidade de uma ferramenta que organizasse e apresentasse
essas opções de forma clara e acessível se tornou fundamental para
melhorar a experiência do telespectador.
A ferramenta, anteriormente comum nas TVs a cabo, tornou-
se ainda mais presente com a digitalização da televisão. Hoje, há
diversos aplicativos para dispositivos móveis que agregam a pro-
gramação de inúmeros canais de TV, permitindo consultas e o
agendamento do que assistir. Com a evolução tecnológica, o guia
de programação, que antes era disponibilizado em revistas, migrou
para a tela da TV. Inicialmente, ele surgiu como uma simples lista
de horários dos programas nos canais, mas, ao longo do tempo,
foi incorporando novos recursos e interatividade. Agora, além de
consultar os horários, o telespectador pode programar a gravação
de conteúdos, acessar informações detalhadas sobre cada programa
e realizar buscas precisas para encontrar um programa específico
entre diferentes canais.
O EPG geralmente segue uma estrutura padronizada, exibindo a
programação, geralmente semanal, com variações na quantidade de
dias anteriores e futuros disponibilizados. Na interface, os horários
são dispostos na parte superior da tela, uma barra lateral vertical
exibe os canais, e os programas são apresentados em barras hori-
zontais, cujo tamanho corresponde à duração de cada um. Algumas
variações nessa estrutura podem ocorrer, dependendo da interface
específica.
Cientes da relevância dessa ferramenta, também procuramos
observar a presença de EPGs em plataformas de streaming. É im-
portante destacar que essa funcionalidade não é comum nesses
ambientes, pois, conforme discutido anteriormente, o EPG está tra-
dicionalmente associado à televisão linear, voltada para conteúdos
em fluxo contínuo. Como esses conteúdos estão apenas começando
a ser incorporados às plataformas de streaming, o EPG ainda não
está presente na maioria delas.
AVALIANDO AS PLATAFORMAS DIGITAIS:
GLOBOPLAY, RTVEPLAY E PLUTO TV
Para esta pesquisa, conduzimos inicialmente uma investigação ex-
ploratória com o objetivo de mapear diferentes plataformas de
streaming em diversos países. Nossa análise considerou aquelas
plataformas cujas características se aproximam do modelo que a
nova geração de TV, a TV 3.0, deve adotar.
Examinamos plataformas de streaming com grande número de
assinantes em diferentes regiões, incluindo aquelas pertencentes
a conglomerados norte-americanos, como Netflix1, Max2, Prime
Video3 e Disney+4, que têm presença global. Além disso, voltamos
nossa atenção para plataformas que se destacam em mercados es-
pecíficos, como a chinesa Tencent Video (responsável pela WeTV5),
a indiana Eros Now6, a russa iviTV7 e a árabe Shahid8.
No entanto, priorizamos plataformas que apresentassem caracte-
rísticas que as aproximassem do modelo da TV 3.0, especificamente
aquelas que oferecessem diferentes serviços de TV linear simulta-
neamente, ao vivo e em fluxo contínuo, além de possuírem EPGs
integrados. Com essa filtragem, identificamos as plataformas Glo-
boplay9, Pluto TV10 e RTVEplay11 como os objetos de estudo com
maior potencial para a nossa análise.
A Globoplay é uma plataforma brasileira, lançada em 2015 e
pertencente ao Grupo Globo, uma das empresas mais tradicionais
no cenário televisivo brasileiro. Atualmente, é o serviço de streaming
com o maior número de assinantes no Brasil, consolidando-se como
uma das principais alternativas de conteúdo digital no país.
1https://www.netflix.com
2https://www.max.com/br/pt
3https://www.primevideo.com
4https://www.disneyplus.com/pt-br
5https://wetv.vip/pt
6plataforma indisponível no Brasil
7https://www.ivi.ru/
8https://shahid.mbc.net/en
9https://globoplay.globo.com/
10https://pluto.tv/br
11O
acesso
foi
feito
através
da
versão
internacional
https://www.rtve.es/play/internacional/portada/
Uma análise de plataformas de streaming e as possibilidades para um guia de programação na TV 3.0
WTVDI’2024, Juiz de Fora/MG, Brazil
A segunda plataforma analisada é a RTVEplay, pertencente à
emissora pública espanhola Televisión Española (TVE). Lançada em
2008 sob o nome de RTVE a la carta, a plataforma passou por signi-
ficativas atualizações em 2021, adaptando-se às novas demandas
do mercado digital e ampliando sua oferta de conteúdos.
A terceira plataforma é a Pluto TV, criada em 2013 e pertencente
ao conglomerado norte-americano Paramount Global. A Pluto TV
se destaca por sua oferta de conteúdo gratuito, baseada em um
modelo de transmissão linear que combina grades de programação
com uma vasta biblioteca de conteúdos sob demanda, tornando-se
um exemplo interessante de integração de diferentes formatos de
mídia digital.
Cada uma dessas plataformas traz aspectos relevantes que contri-
buem significativamente para nossa pesquisa, seja por pertencerem
a empresas ligadas à mídia tradicional, seja por serem públicas ou
por destacarem conteúdos “ao vivo” em suas interfaces. Essas ca-
racterísticas são essenciais para analisar como o Guia Eletrônico de
Programação (EPG) se integra e funciona em diferentes contextos
de streaming.
Todas as três plataformas – Globoplay, RTVEplay e Pluto TV
– possuem um guia de programação que organiza o conteúdo em
fluxo contínuo disponibilizado por elas. Realizamos uma análise de-
talhada dos EPGs de cada uma para entender como são estruturados
e como facilitam a navegação dos telespectadores.
Na Globoplay, o acesso ao guia de programação é feito através da
aba dedicada às transmissões simultâneas. Ao iniciar a visualização
de qualquer uma dessas transmissões, aparece abaixo do vídeo um
retângulo branco clicável com o texto “programação” – no celular –
ou “programação completa” – no computador – em preto. Ao clicar,
uma nova página é aberta, onde o EPG é exibido. Seu formato segue
de perto o modelo tradicional: os canais são listados em uma barra
vertical no lado esquerdo da tela, os programas são apresentados
em barras horizontais que variam conforme a duração de cada um,
o horário é exibido na parte superior da tela, e, acima, uma lista
com os dias da semana permite a navegação entre diferentes dias.
A Figura 1 ilustra a tela de EPG da plataforma Globoplay.
Figura 1: EPG Globoplay.
A funcionalidade do EPG na Globoplay não apresenta grandes
inovações em relação aos guias de programação já disponíveis nas
TVs atuais. Seu principal objetivo é atuar como um auxiliar para
o telespectador/interator, informando em quais dias da semana
e horários os conteúdos transmitidos estão disponíveis. O guia
abrange a data de acesso, além de permitir a visualização de até
três dias anteriores e dois dias posteriores, totalizando seis dias de
programação disponíveis no EPG.
No entanto, o guia é relativamente pouco interativo. O telespec-
tador/interator é conduzido ao conteúdo apenas quando o material
selecionado está sendo exibido “ao vivo” naquele momento. Mesmo
assim, o acesso não é imediato; primeiro, é exibida uma tela com
informações sobre o programa e um botão “assista agora”, que, ao
ser clicado, direciona o telespectador ao conteúdo. Para programas
que não estão em exibição no momento, o EPG oferece apenas in-
formações básicas, como o nome, o horário de transmissão e uma
breve sinopse, como observado na Figura 2.
Figura 2: Informações que aparecem sobre os programas ao
clicar no EPG Globoplay – apenas na primeira imagem há
um botão que leva ao conteúdo.
Na Pluto TV, o EPG é a principal ferramenta de navegação e
descoberta de conteúdo na plataforma. O guia de programação
recebe grande destaque e é exibido logo na tela inicial. Para acessá-
lo, basta clicar no retângulo branco localizado no canto da tela, onde
está escrito “Guia de canais de TV” em letras pretas.
Em termos de disposição, o EPG da Pluto TV segue o formato
tradicional, com pequenas variações dependendo do dispositivo uti-
lizado, seja no computador ou no aplicativo para celular. No caso do
celular, a navegação pelo guia é simplificada, permitindo que o teles-
pectador apenas role a tela para explorar o EPG, sem a necessidade
de clicar em ícones adicionais para acessar as informações.
Na Pluto TV, os canais são organizados em uma barra lateral
vertical no canto esquerdo da tela, enquanto os programas são dis-
postos em barras horizontais, com a hora de exibição localizada
acima dessas barras. Esse layout é projetado para facilitar a visuali-
zação rápida e a navegação entre os diferentes conteúdos. A Figura
3 ilustra o EPG da Plataforma Pluto.
Ao acessar a plataforma pelo computador, inicialmente, as barras
que representam os programas são exibidas com o mesmo tamanho,
WTVDI’2024, Juiz de Fora/MG, Brazil
Montezano et al.
Figura 3: EPG Pluto TV.
independentemente da duração de cada um. No entanto, existe
uma opção no canto superior direito da tela chamada “Linha do
tempo”. Ao clicar nessa opção, o EPG é ajustado para o formato
mais comum, no qual as barras horizontais variam em tamanho,
refletindo a duração específica de cada programa, ocupando assim a
tela de forma mais intuitiva e prática, conforme ilustrado na Figura
4.
Figura 4: EPG Pluto TV.
É interessante notar que, no guia de programação da Pluto TV,
há uma barra lateral adicional que organiza os conteúdos por temas
e/ou formatos, como filmes, séries, animes, notícias, esportes, entre
outros. Embora existam pequenas variações na disposição do EPG
no celular, as diferenças são sutis. No celular, por exemplo, a barra
de temas está localizada na parte superior da tela, em vez de na
lateral.
O EPG da Pluto TV transmite uma sensação de maior interativi-
dade, respondendo de forma rápida aos comandos do telespectador.
Quando se clica em um programa, a transmissão começa imedia-
tamente, sem a necessidade de um segundo clique, como ocorre
na Globoplay. No entanto, o EPG da Pluto TV só permite o acesso
ao conteúdo que está sendo exibido “ao vivo” naquele momento.
Embora a plataforma também ofereça conteúdo sob demanda, o
telespectador/interator não é direcionado para esse conteúdo dire-
tamente a partir do EPG. Para acessar o conteúdo sob demanda, é
necessário navegar até a aba “Sob Demanda”, onde apenas o con-
teúdo disponível no dia em questão é exibido na tela.
Na plataforma RTVE, o EPG pode ser acessado clicando na aba
Menu, onde ele é listado como o nono item, sob o nome “Guía
TV”. Esse guia permite visualizar a programação do dia de acesso,
além de oferecer informações sobre sete dias anteriores e dois dias
futuros, totalizando 11 dias de programação disponíveis no EPG.
É importante notar que a disposição do guia varia conforme o
dispositivo utilizado. No computador, o formato do EPG é seme-
lhante ao modelo tradicional, com a diferença de que ele agrega
a programação de mais de uma emissora, como mostra a Figura
5. Na parte superior da tela, aparecem três opções de canais: TVE,
TVE Catalunya e TVE Canarias. Abaixo dessas opções, em uma
lista horizontal, estão os dias da semana, permitindo que o telespec-
tador/interator selecione o dia específico cuja programação deseja
visualizar. No restante, a disposição é bastante similar aos outros
EPGs descritos anteriormente: os canais da emissora selecionada
são listados em uma barra lateral vertical à esquerda, enquanto os
programas são apresentados em barras horizontais, que ocupam o
espaço correspondente ao horário de transmissão.
Figura 5: EPG RTVEplay.
No entanto, no aplicativo para celular, a disposição do EPG é
diferente. Logo abaixo da barra horizontal superior, onde estão
listadas as opções de TVs (TVE, TVE Catalunya e TVE Canarias),
encontram-se outras duas barras horizontais: uma para os canais e
outra para os dias da semana. A seguir, os programas são dispostos
verticalmente em barras horizontais de tamanho uniforme. Além
disso, há uma barra vertical do lado esquerdo da tela que exibe os
horários de cada programa, facilitando a navegação e a visualização
da programação no dispositivo móvel. A Figura 6 mostra a interface
da RTVEplay em celular.
Uma característica distinta que chama a atenção no EPG da
RTVE é a sensação ampliada de interatividade. Embora a resposta
aos comandos seja um pouco mais lenta em comparação com a
Pluto TV, essa plataforma oferece uma funcionalidade adicional:
além de direcionar o telespectador diretamente ao conteúdo em
exibição ao vivo quando clicado no guia, ela também redireciona o
telespectador a conteúdos, independentemente de estarem sendo
transmitidos simultaneamente ou não.
Diferente dos guias anteriores, o que determina se o telespecta-
dor/interator será direcionado ao conteúdo é se caso a plataforma
possua os direitos de exibição do programa no formato sob demanda.
Isso significa que alguns filmes, por exemplo, podem não estar dis-
poníveis, mas, no caso de programas produzidos pela própria RTVE
ou daqueles cujo streaming é licenciado pela emissora, ao clicar no
programa, o telespectador é levado para uma nova página dedicada
a esse conteúdo, onde pode o assistir no formato sob demanda.
Uma análise de plataformas de streaming e as possibilidades para um guia de programação na TV 3.0
WTVDI’2024, Juiz de Fora/MG, Brazil
Figura 6: EPG RTVEplay no celular.
Essa característica torna o EPG da RTVE o mais interativo entre
os analisados. Praticamente tudo nele é clicável e leva o telespecta-
dor/interator diretamente ao conteúdo, em vez de apenas fornecer
informações sobre ele. Essa funcionalidade melhora significativa-
mente a experiência do telespectador, tornando a navegação mais
fluida e integrada. Tais telas estão ilustradas na Figura 7.
No entanto, a disponibilidade do que pode ser assistido em cada
programa dependerá da sua disponibilidade na plataforma. No caso
de um jornal ou de uma série em andamento, o usuário poderá
assistir às edições ou episódios já exibidos. Se o programa for um
produto já finalizado e a plataforma tiver os direitos de exibição, será
possível acessar e assistir a qualquer episódio a qualquer momento.
EPG NA TV 3.0
Acreditamos que a TV 3.0 pode se beneficiar ao adotar e adaptar as
estruturas de EPG já conhecidas e existentes, como as que analisa-
mos, facilitando a transição e a adaptação do telespectador/interator
à nova interface. Essa abordagem pode ajudar a suavizar a curva
de aprendizado, garantindo uma experiência de uso familiar, mas
aprimorada, para o público.
No entanto, também entendemos que a TV 3.0 representa uma
oportunidade de avançar ainda mais nos potenciais tecnológicos
oferecidos pelo ambiente digital, que, em nossa visão, ainda são
subutilizados, mesmo nas estruturas e modelos atuais presentes em
plataformas de streaming ou em agregadores de conteúdo, como
as TVs por assinatura. Exemplos internacionais, como o Freeview
Play12, um aplicativo que integra várias TVs do Reino Unido e pode
12https://www.freeview.co.uk
Figura 7: Informações e telas que aparecem ao clicar nos
conteúdos do EPG RTVEplay.
ser acessado via celular, TV e computador, ilustram as possibilidades
de inovação nesse campo13.
“Novas tecnologias de informação abrem as portas para o cres-
cimento exponencial de conteúdo, o que cria uma necessidade de
inovação na organização” (Rosenfeld e Morville [1], tradução nossa).
A TV 3.0, portanto, se apresenta como uma oportunidade para de-
senvolver novas formas de organização e interfaces que sejam mais
envolventes e que possam estabelecer novos vínculos entre as pes-
soas e a televisão. Afinal, a televisão, além de ser uma “tecnologia,
é uma forma cultural” (Williams, 2016 [3]). Nesse sentido, a pro-
posta para a TV 3.0 visa proporcionar mais interatividade em sua
13O Freeview Play é um aplicativo que pode ser baixado em diferentes dispositivos,
similarmente aos outros serviços de streaming analisados neste estudo. No entanto, ele
não é compatível com todas as marcas de Smart TVs, uma limitação que provavelmente
persistirá na nova versão planejada especificamente para Smart TVs, denominada
Freely.
WTVDI’2024, Juiz de Fora/MG, Brazil
Montezano et al.
interface, especialmente em seu guia de programação de TV aberta
(EPG).
A recomendação é que a interface do EPG na TV 3.0 seja ampla-
mente clicável, permitindo ao telespectador/interator acessar tanto
os conteúdos já transmitidos quanto aqueles que ainda irão ao ar
– conforme as permissões estabelecidas pelas emissoras de TV. O
guia deve fornecer o máximo possível de informações sobre cada
programa, para que o usuário compreenda plenamente o status
atual do que está assistindo. A Figura 8 ilustra o projeto de interface
para o EPG da TV 3.0, no qual se procurou adicionar vários rótulos
com identificações sobre o conteúdo e também sobre seu status,
como por exemplo se está sendo transmitido naquele momento "ON
AIR"ou não.
Figura 8: Projeto de interface do EPG em modelo de teste da
TV 3.0.
Essas informações devem incluir a classificação indicativa por
idade, a indicação de se o conteúdo está sendo transmitido ao vivo
ou não, e se ele estará disponível para visualização após a trans-
missão inicial. Além disso, o EPG deve indicar se o programa pode
ser gravado para ser assistido em outro momento. A proposta é
ampliar a sensação de controle do telespectador, que na TV 3.0 terá
a oportunidade de se tornar mais interativo do que nas gerações
anteriores da televisão brasileira. A Figura 9 mostra o projeto de
interface em fromato linha do tempo para o EPG da TV 3.0.
CONSIDERAÇÕES FINAIS
Neste artigo, exploramos as formas de organização de conteúdo
para a TV 3.0, com foco específico no Guia Eletrônico de Progra-
mação (EPG). A partir de estudos de plataformas de streaming que
combinam conteúdos sob demanda e “ao vivo” – uma caracterís-
tica esperada na nova geração de TV – analisamos os EPGs das
plataformas Globoplay, RTVEplay e Pluto TV.
Observamos que, embora todas as três plataformas possuam
EPGs com estruturas bastante semelhantes, existem pequenas vari-
ações entre elas, especialmente na interface quando acessadas por
dispositivos móveis. Na Globoplay, o EPG se mostrou o menos inte-
rativo, funcionando principalmente como um espaço de consulta
da programação, permitindo a interação apenas com o conteúdo
que está sendo transmitido ao vivo no momento do acesso.
Figura 9: Projeto de interface do EPG em modelo de teste da
TV 3.0 (formato linha do tempo).
Por outro lado, na Pluto TV, o EPG é uma ferramenta central
para a navegação, proporcionando uma sensação de maior intera-
tividade devido às rápidas respostas aos comandos e ao destaque
dado ao EPG na interface. No entanto, assim como na Globoplay, o
acesso está restrito aos conteúdos que estão em exibição ao vivo no
momento.
A RTVEplay, embora exija mais cliques para acessar o EPG e
apresente uma resposta um pouco mais lenta aos comandos, destaca-
se como a plataforma que oferece a maior interatividade. Nela, o
telespectador/interator é conduzido diretamente aos conteúdos,
mesmo que não estejam sendo exibidos ao vivo, exceto nos casos
em que a RTVE não possui os direitos de exibição sob demanda.
Com base nos resultados de nossa pesquisa e nos desenvolvimen-
tos em interfaces realizados no Projeto TV 3.0, acreditamos que o
EPG da nova geração de TV deve buscar um equilíbrio. Ele deve
aproveitar os exemplos de guias já existentes para garantir uma
boa resposta dos telespectadores/interatores, utilizando a familia-
ridade com estruturas conhecidas para não descaracterizar a TV.
Ao mesmo tempo, deve inaugurar novos formatos que sejam mais
interativos, explorando plenamente os recursos que o ambiente
digital pode agregar à televisão. Essa abordagem pode criar uma
experiência mais rica e engajadora, que alinhe a tradição da TV
com as possibilidades oferecidas pela tecnologia digital.
AGRADECIMENTOS
Os autores agradecem a toda a equipe de P&D do Projeto TV 3.0,
pelas discussões e esforços de pesquisa em conjunto. Agradecem
também à RNP e MCom pela gestão e financiamento, assim como
ao Fórum SBTVD e seus membros pela iniciativa, acompanhamento
e contribuições constantes.

--- FIM DO ARQUIVO: 30508-829-24956-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30509-829-24957-1-10-20241001.txt ---
Controle remoto e segunda tela para uma TV 3.0 orientada a
aplicativos
Carlos Pernisa Júnior∗
carlos.pernisa@ufjf.br
Universidade Federal de Juiz de Fora (UFJF)
Juiz de Fora, Minas Gerais
Marcelo F. Moreno†
marcelo.moreno@ufjf.br
Universidade Federal de Juiz de Fora (UFJF)
Juiz de Fora, Minas Gerais
Stanley Cunha Teixeira‡
stanley.teixeira@ufjf.br
Universidade Federal de Juiz de Fora (UFJF)
Juiz de Fora, Minas Gerais
Cristiane Turnes Montezano§
cristiane.turnes@estudante.ufjf.br
Universidade Federal de Juiz de Fora (UFJF)
Juiz de Fora, Minas Gerais
0.
Contudo, é importante destacar que as propostas aqui discutidas
são resultados preliminares das pesquisas em andamento, desenvol-
vidas pelos grupos de trabalho mencionados. Estas inovações ainda
passarão por revisões e validações em outras instâncias antes de
serem implementadas em larga escala. Portanto, o presente estudo
deve ser entendido como uma análise prospectiva, idealizada com
embasamento científico e de viabilidade tecnológica, e não como
uma solução já pronta para aplicação prática.
CONTROLE REMOTO DE REFERÊNCIA
Um dos dispositivos mais utilizados em conjunto com o aparelho de
televisão é o controle remoto, cujo uso remonta a 1955, quando foi
inventado por Eugene Polley, engenheiro da Zenith Electronics [2].
Ao longo das décadas, os modelos desse dispositivo passaram por
diversas modificações e adaptações, acompanhando as inovações
tecnológicas e as funcionalidades adicionadas aos televisores. Atu-
almente, há uma vasta gama de controles remotos disponíveis no
mercado, com designs e funções variados, adaptados às necessidades
de diferentes fabricantes2 e usuários.
1No contexto deste artigo, “segunda tela” se refere à capacidade de sincronização entre
a programação exibida em um dispositivo principal, geralmente uma televisão, e outro
dispositivo, tipicamente um smartphone. Esta definição difere do conceito de "Social
TV", no qual o uso de uma segunda tela não necessariamente envolve tal sincronização
e está frequentemente associado ao uso de plataformas digitais sociais.
2Para esta pesquisa, foram realizados levantamentos sobre controles remotos de di-
versas marcas, incluindo Philco (sistema Roku), LG (sistema WebOS), TCL (sistema
Android TV) e Samsung (sistema Tizen).
WTVDI’2024, Juiz de Fora/MG, Brazil
Pernisa Jr et al.
No entanto, com os desenvolvimentos da TV 3.0, torna-se neces-
sário repensar e adaptar o controle remoto para atender às novas
exigências dessa tecnologia emergente. A equipe de pesquisa e de-
senvolvimento envolvida no projeto do novo padrão televisivo vem
investigando as demandas específicas identificadas nas fases iniciais
dos estudos e, com base nessas observações, propõe um modelo de
referência de controle remoto que atenda às necessidades da TV
3.0. A Figura 1 ilustra o design do controle remoto de referência,
para o seguinte detalhamento de suas funções.
Figura 1: Controle remoto de referência para a plataforma
de TV 3.0 orientada a aplicativos. Fonte: Autores.
É importante destacar que não há uma padronização ou norma
específica do Fórum SBTVD, ou mesmo proposta pela equipe de
Pesquisa e Desenvolvimento (P&D), sobre o design do controle
remoto. A escolha do layout e das funcionalidades propostas visa
atender às necessidades específicas identificadas para a TV 3.0, mas
deixa espaço para que fabricantes possam adaptar o dispositivo
conforme suas próprias diretrizes e inovações tecnológicas.
No desenvolvimento do controle remoto de referência, algumas
funções existentes são mantidas, com o objetivo de minimizar obs-
táculos para os usuários já familiarizados com controles remotos
atuais. A intenção é garantir uma transição suave, preservando
elementos reconhecíveis enquanto se introduzem novas funcionali-
dades que acompanham as inovações tecnológicas da TV 3.0.
O botão de liga/desliga, localizado no alto à esquerda, desempe-
nha a função tradicional de ligar e desligar o aparelho. Ao lado deste,
o botão de configurações mantém sua função de acesso direto ao
menu de configurações do aparelho. Ainda na primeira fila horizon-
tal, há o botão de “input”, já presente em modelos contemporâneos.
Este botão permite ao telespectador selecionar a fonte de entrada
do sinal, como HDMI ou AV. Nota-se que o acesso à TV 3.0 deve
ser uma das opções na lista de entradas de sinal.
As funções inovadoras começam a aparecer na segunda fila do
controle remoto. A função “home”, amplamente reconhecida em
smart TVs continuará resevada para acessar a interface principal do
dispositivo. Ao lado dessa função, uma novidade é introduzida: o
botão de controle de privacidade. Esta função permitirá ao usuário
acessar diretamente as configurações de privacidade, onde poderá
gerenciar suas preferências sobre o uso e compartilhamento de
dados com um radiodifusor, garantindo assim controle sobre suas
informações pessoais. Por fim, será introduzida a função “TV 3.0”,
localizada próximo ao botão de controle de privacidade. Esta função
oferece acesso direto ao catálogo de aplicativos da TV 3.0, onde
todos os sinais de radiodifusores se tornam aplicativos de TV. O
catálogo é uma interface organizada que permite a navegação in-
tuitiva entre as emissoras de televisão disponíveis, tanto as que
transmitem sinais de TV 3.0, quanto as que transmitem sinais em
conformidade com a atual geração.
A região logo abaixo da segunda fila é composta pelas funções
alfanuméricas, semelhantes às encontradas em modelos atuais de
controle remoto. A inclusão não apenas de números, mas também
de letras, visa facilitar a jornada, oferecendo mais opções para
o preenchimento de formulários ou similares, sem a necessidade
de navegação extensiva em teclados virtuais. Além disso, foram
adicionadas as funções de favoritar (representada por uma estrela)
programas para fácil identificação e acesso futuro, e a de apagar,
que auxilia na edição de textos e dados.
Na plataforma TV 3.0, as letras e números no controle remoto de
referência não são funções utilizadas para a seleção de canais, mas
sim como funções de entrada de texto ou outra específica, determi-
nada pelo aplicativo ou módulo que está ativo em primeiro plano.
Nos ambientes de apresentação (Ginga-NCL e Ginga-HTML5), os
valores de entrada por teclas são mapeados para cada uma das
funções que compõem o grupo de informações alfanuméricas, per-
mitindo seu uso flexível e adaptado ao contexto de cada aplicativo.
Todas as funções do grupo alfanumérico estão disponíveis e dedica-
das exclusivamente às ações de entrada definidas pelo aplicativo
ou módulo atualmente ativo na plataforma TV 3.0.
Logo abaixo da segunda fila de funções alfanuméricas, na parte
mais próxima da seção circular do controle remoto, estão localiza-
das as funções de voltar, pesquisa/busca, controle por voz, e menu.
O botão de voltar, essencial para a navegação intuitiva, permite ao
telespectador retornar à tela anterior. A função de pesquisa/busca é
indicada pelo ícone de uma lupa, facilitando a localização rápida de
conteúdo. O controle por voz, identificado por um ícone de micro-
fone, já presente em alguns dispositivos atuais, oferece a funciona-
lidade de comandos de voz, permitindo uma interação mais direta
e eficiente com a TV 3.0. Por fim, a função de menu, representada
por três linhas horizontais, é outra função amplamente difundida
em controles remotos contemporâneos, oferecendo acesso rápido
às opções do aplicativo ou módulo ativo no momento.
A parte redonda do controle remoto mantém as mesmas caracte-
rísticas das versões anteriores, com a possibilidade de movimentos
direcionais para cima, para baixo e para os lados, além da função “ok”
para confirmação de ações. Esta seção continua desempenhando
suas funções tradicionais, como navegação por menus, ícones e
confirmação de seleções, dependentes, novamente, da lógica de
interatividade definida pelo aplicativo ou módulo ativo.
Logo abaixo da área circular, encontram-se os botões coloridos,
uma funcionalidade que foi pouco explorada nas gerações atuais de
TV, que possuíam interatividade subutilizada. Com a proposta da
TV 3.0, que enfatiza uma maior interação entre o telespectador e o
aparelho, essas funções ganham uma nova relevância. Elas serão
possivelmente muito mais utilizadas para permitir escolhas e indicar
Controle remoto e segunda tela para uma TV 3.0 orientada a aplicativos
WTVDI’2024, Juiz de Fora/MG, Brazil
opções em diversos contextos de uso, aumentando a facilidade do
telespectador em personalizar sua experiência televisiva.
Seguindo na próxima fila, ficam as funções destinadas ao acesso
rápido ao guia de programação (EPG) e ao catálogo de vídeos sob
demanda (ECG), que são novidades na TV 3.0. Ao lado deles, um
outro botão, já conhecido, é o de informações sobre a programação
ou algo relevante no contexto do aplicativo ou módulo ativo.
Na parte inferior do controle remoto, as funções de volume e
de troca rápida de aplicativos, estão dispostas nas laterais, com a
função “mudo” e a de configurações de áudio, ao centro. Este último
tem a função de ajustar opções de áudio que só vão fazer sentido na
TV 3.0. Elas abarcam o áudio imersivo, mas também determinadas
funções de troca de idiomas, de narração, entre outras novidades
na experiência sonora.
Abaixo desta região, tem-se os controles de acessibilidade, com
as funções de audiodescrição, de aprimoramento de diálogos, de
legendas em closed caption e de língua de sinais. Alguns destes tam-
bém são novidades, já que a acessibilidade nem sempre fez parte de
controles antigos. Para a TV 3.0, este aspecto é essencial, no sentido
de se ter uma televisão que abrange o máximo de telespectadores.
Por fim, o controle remoto apresenta as funções já conhecidos desde
o antigo videocassete, de retrocesso, pausa e retomada/reprodução
e de avanço. É bom lembrar que esta nova televisão vai contar com
opções de parar e avançar a programação, o que não é ainda algo
tão difundido nos modelos dos aparelhos atuais.
Uma proposta adicional sugere que o controle remoto também
seja levado para uma segunda tela, tipicamente um smartphone,
a fim de proporcionar mais opções e comodidade ao telespecta-
dor/interator. Essa segunda tela poderia ser usada para diversas
outras funções, conforme detalhado a seguir.
SEGUNDA TELA E SINCRONISMO NA TV 3.0
A segunda tela, frequentemente representada pelo uso de smartpho-
nes, pode funcionar como um controle remoto adicional [1], permi-
tindo a interação com a tela principal, normalmente uma TV. Mas
o conceito de “segunda tela” surge justamente da possibilidade de
utilizar o smartphone como uma extensão do televisor, criando um
desafio técnico importante: a adaptação do conteúdo televisivo para
uma tela menor, mantendo sua funcionalidade e usabilidade.
O uso de dispositivos como smartphones, computadores de mesa,
notebooks ou tablets como segunda tela para auxiliar o telespecta-
dor/interator não é uma novidade; essa prática já ocorre há algum
tempo, tanto de maneira síncrona quanto assíncrona3. O público
já está acostumado a assistir à TV com o smartphone em mãos,
complementando sua experiência de visualização.
No contexto da TV 3.0, a segunda tela ganha ainda mais versati-
lidade. Ela não só pode replicar as funções de um controle remoto
padrão, permitindo o controle direto da primeira tela, como tam-
bém pode incorporar novas formas de interação. Antes mesmo do
lançamento oficial da TV 3.0, já existem métodos para reproduzir
o layout e as funcionalidades do controle remoto em dispositivos
móveis. Serviços de streaming, por exemplo, frequentemente utili-
zam smartphones como segunda tela, o que sublinha a importância
3Sobre este aspecto, tem-se estudos sobre Social TV [6], que se dedicam a observar
como as pessoas utilizam-se dos seus smartphones em conexão com determinadas
plataformas digitais sociais, tais como X (antigo Twitter), Instagram ou Facebook.
dessa funcionalidade no ecossistema da TV 3.0. Além disso, o uso
de menus, gráficos, réplicas de botões e outras funcionalidades pro-
jetadas para facilitar a interação com o conteúdo exibido é feito de
maneira intuitiva, dispensando manuais explicativos ou tutoriais.
Contudo, a proposta para a TV 3.0 vai além da simples replica-
ção do controle remoto em uma segunda tela. O design precisa ser
adaptado para que o telespectador/interator possa agir diretamente
sobre o conteúdo exibido na tela principal. Isso envolve necessaria-
mente a sincronização entre os dispositivos e a noção de “agência”,
conforme definido por Murray [4]. Por “agência”, entende-se a ca-
pacidade de um usuário realizar uma ação em um dispositivo –
como um smartphone – e ver essa ação refletida em uma mudança
de estado no conteúdo exibido na TV. A sincronização é, portanto,
uma parte fundamental desse processo. Um simples espelhamento
do controle remoto oferece um nível básico de comando, mas a
TV 3.0 exige uma interação mais profunda, onde nem todos os co-
mandos tradicionais serão suficientes ou adequados para explorar
plenamente as capacidades desta nova plataforma.
Assim, o Laboratório de Mídia Digital (LMD/UFJF) propõe uma
inversão do que ocorre atualmente nos serviços de streaming de
vídeo. Hoje, a tela da TV recebe informações enviadas pela segunda
tela, para proporcionar uma experiência audiovisual aprimorada.
Nessa dinâmica, o usuário vê ações realizadas no dispositivo móvel
serem refletidas na televisão. A proposta para a TV 3.0 é inverter
esse fluxo, permitindo que ações realizadas na TV sejam transferidas
para a segunda tela. Essa ideia vai além de uma simples transposição
das funcionalidades do controle remoto, propondo, em vez disso,
uma interface e um layout específicos para maximizar o potencial da
TV 3.0. Através do uso ampliado da interatividade, a capacidade de
agência com uma sincronização precisa proporcionará ao usuário
uma sensação mais profunda de imersão na programação. Janet
Murray também explora essa relação entre agência e imersão em
seu livro Hamlet no holodeck [4].
A ideia de uma segunda tela, sincronizada e integrada ao televisor,
que receba conteúdo específico com interface e layout adaptados
ao que está sendo exibido na TV, promete transformar significati-
vamente a forma como as pessoas consomem a programação das
emissoras. Essa inversão de papéis é particularmente impactante,
pois coloca o conteúdo nas mãos de um telespectador que se torna,
efetivamente, um interator. Ele assume um nível de controle que
vai além do oferecido por um controle remoto tradicional, permi-
tindo que ações interativas sejam realizadas diretamente no dispo-
sitivo móvel. Isso abre novas possibilidades de interação, como a
capacidade de um grupo de pessoas controlar ações de diferentes
personagens em uma trama, semelhante ao que ocorre em certos
videogames, mas com um foco mais dramático, como em jogos de
interpretação de papéis (RPGs)4.
É importante destacar que há uma diferença significativa entre
possuir um controle remoto e utilizar um dispositivo móvel como
segunda tela quando este último espelha a TV em sua tela. A se-
gunda tela oferece acesso a mais recursos, permitindo ações que o
controle remoto tradicional não consegue executar, ou que o faz
com maior dificuldade. Torna-se possível a entrega específica de
conteúdo para uma pessoa dentro de um grupo de telespectadores.
4RPG (Role Playing Game) é um tipo de jogo em que as pessoas interpretam personagens
e criam narrativas em torno de um enredo. No caso de um RPG de mesa, cada uma
dessas histórias é criada por uma pessoa nomeada o “mestre do jogo” [3].
WTVDI’2024, Juiz de Fora/MG, Brazil
Pernisa Jr et al.
Além disso, a proposta não é criar um substituto para um controle
de videogame, mas sim utilizar aplicativos que podem ser baixados
e adaptados conforme as necessidades do conteúdo transmitido,
eliminando a necessidade de adquirir hardware adicional.
Outro aspecto a ser considerado é o fato de que muitas pessoas já
estão familiarizadas com o uso de smartphones, o que tornaria seu
uso como segunda tela ainda mais intuitivo, de fácil decodificação
e fluido. A combinação da audiovisualidade com a interatividade
tátil dos smartphones oferece uma experiência mais acessível e
conveniente para o telespectador/interator, explorando as carac-
terísticas específicas da tela sensível ao toque desses dispositivos
para melhorar a interação com a TV 3.0.
3.1
Casos de uso da segunda tela
Ações como arrastar para cima ou para baixo, utilizando o touchs-
creen, ou ampliar uma imagem com um gesto de pinça são interações
comuns nos smartphones e que podem ser adaptadas para a inte-
ração na TV 3.0. As possibilidades de uso são vastas, dependendo
apenas da criatividade e da necessidade de cada conteúdo audio-
visual. Por exemplo, durante a transmissão de jogos esportivos, o
telespectador poderia ter acesso instantâneo, na tela de seu celular,
a informações detalhadas sobre escalações, bem como estatísticas
dos times e jogadores em campo.
A inclusão de ferramentas de acessibilidade, como o direciona-
mento de audiodescrição para os celulares de pessoas com deficiên-
cia visual, é outra possibilidade promissora. Essa abordagem pode
transformar a experiência de acessibilidade, tornando os comandos
de voz e gestos mais responsivos e adequados quando executados a
partir de segundas telas.
A interação com um teclado alfanumérico, que costuma ser com-
plicada em um controle remoto tradicional, pode ser simplificada
e tornar-se mais intuitiva na tela de um smartphone, graças ao
touchscreen. Além disso, o uso de cores para controlar ações, que
nos controles remotos se limita a quatro botões coloridos, pode
ser expandido. A funcionalidade de toque pode ser estendida para
qualquer área da tela do smartphone, ampliando significativamente
o raio de ação do usuário.
Não se deve ignorar a preferência crescente entre as gerações
mais jovens por assistir a vídeos em telas de celulares. Em vez de
representar um desafio, essa tendência pode ser vista como uma
oportunidade para atrair esse público para a TV aberta, aprovei-
tando a individualização crescente da visualização de conteúdo
entre os jovens. Ao integrar a TV com o smartphone, essa vincula-
ção se torna menos disruptiva, facilitando a experiência para todos
os usuários, desde os mais jovens até os idosos.
Além disso, a possibilidade de elementos da tela da TV apare-
cerem no smartphone pode proporcionar uma experiência mais
intuitiva. É comum, por exemplo, observar crianças pequenas des-
lizando os dedos em uma tela de TV grande na expectativa de
modificar a imagem, acreditando tratar-se de um dispositivo sensí-
vel ao toque, como um tablet gigante. A sensação de toque é sutil,
mas pode ser mais eficaz do que o uso de um controle remoto, que
exige uma ação mais deliberada e consciente.
3.1.1
Controle remoto na segunda tela. A maioria das funções do
controle remoto de referência pode ser replicada em um dispositivo
de segunda tela, conforme ilustrado na Figura 2.
Figura 2: Caso de uso: controle remoto na segunda tela, com
teclado virtual para números e letras.
A principal vantagem dessa abordagem é a possibilidade de in-
corporar um teclado virtual muito mais funcional e intuitivo, que
pode ser utilizado em qualquer situação que exija a inserção de
texto ou números. Isso oferece ao telespectador a conveniência de
acessar rapidamente elementos alfanuméricos diretamente em suas
mãos, melhorando significativamente a experiência de interação.
Vale destacar as funções gerais da segunda tela mostradas na
parte inferior da figura, incluindo entradas para aplicativos de TV
3.0, guia de programação, controle remoto virtual, perfis e priva-
cidade, modo de segunda tela e opções de acessibilidade. Essas
funcionalidades ampliam a utilidade do dispositivo de segunda tela,
tornando-o um complemento versátil e poderoso para a TV 3.0.
3.1.2
Uso de touchpad virtual para ações de digitação, controle
e navegação. O touchpad virtual, uma ferramenta já familiar aos
usuários de notebooks, adapta-se perfeitamente às necessidades de
digitação, navegação e controle na TV 3.0. Este recurso permite
não apenas o uso de funções como clicar e arrastar, mas também
a simples ação de clicar em um local ou região específica da tela.
Qualquer toque realizado no touchpad virtual da segunda tela terá
uma ação correspondente na TV, proporcionando uma experiência
de controle fluida e intuitiva. Por exemplo, ao clicar duas vezes
e selecionar uma região, o usuário pode destacá-la e transportar
elementos específicos para a segunda tela, como campos de texto
para preenchimento, listas de opções ou informações adicionais. A
rolagem do conteúdo na tela principal também pode ser controlada
pelo movimento de dois dedos no touchpad, facilitando a navegação
e melhorando a interação com o conteúdo exibido. Essas ações são
ilustradas na Figura 3.
Figura 3: Caso de uso: touchpad virtual para ações de digita-
ção, controle e navegação.
Controle remoto e segunda tela para uma TV 3.0 orientada a aplicativos
WTVDI’2024, Juiz de Fora/MG, Brazil
Um caso de uso já pensado para a segunda tela envolve a capaci-
dade de tocar em uma região específica exibida na tela principal,
“marcando-a” para que possa ser ampliada ou reduzida. Para isso,
o telespectador precisa acessar a função de touchpad em seu dis-
positivo de segunda tela. No contexto de transmissões esportivas,
por exemplo, essa funcionalidade pode ser aplicada a um campo de
futebol, uma quadra de basquete, uma piscina olímpica, ou qualquer
outro ambiente de competição, como vôlei, tênis, rugby, entre ou-
tros. Ao definir a área desejada com dois toques no local escolhido, o
telespectador pode usar o movimento de pinça com dois dedos para
ampliar ou reduzir a imagem daquela área específica. Dois novos
toques na tela indicam que a imagem deve ser fixada, impedindo
alterações posteriores e mantendo o enquadramento definido pelo
usuário. A Figura 4 ilustra tal variação do caso de uso.
Figura 4: Caso de uso: touchpad virtual contextualizado para
ações de controle e navegação.
Além disso, o uso de som ambiente do estádio pode ser integrado
a essa experiência, oferecendo áudio imersivo e direcionado para
áreas específicas do ambiente sonoro, criadas pela disposição es-
tratégica de microfones. Isso permite que o telespectador/interator
destaque o som de uma torcida específica ou, alternativamente, si-
lencie determinadas áreas, conforme sua preferência, aumentando
a personalização da experiência auditiva durante a transmissão.
3.1.3
Segunda tela como extensão sincronizada. Esta função encap-
sula a proposta central de inverter o fluxo de controle tradicional,
permitindo que ações realizadas na TV sejam transferidas para a
segunda tela do dispositivo. Esse modo permite que a segunda tela
se adapte dinamicamente às opções oferecidas pelo aplicativo em
execução na TV, de forma síncrona e perfeitamente alinhada ao
conteúdo exibido na tela principal. A extensão da tela principal
para a segunda tela pode incluir funcionalidades como a listagem
de recomendações de programas, a exibição de anúncios personali-
zados ou outras informações contextuais. Além disso, este modo
oferece a capacidade de direcionar informações específicas a um
telespectador/interator, mesmo em um ambiente com múltiplos
espectadores, fornecendo conteúdos exclusivos ou personalizados
de acordo com o perfil individual do usuário. Essa personalização
pode incluir sugestões de programas, informações adicionais ou
experiências interativas que aprimorem a imersão e o envolvimento
do telespectador. A Figura 5 ilustra o caso de uso.
3.1.4
Perfil e privacidade em segunda tela. Ao selecionar o modo
de perfil e privacidade na parte inferior da tela, o telespectador pode
Figura 5: Caso de uso: segunda tela, extensão sincronizada.
realizar todas as suas configurações diretamente em seu smartphone,
evitando a leitura prolongada e cansativa na tela principal. Esse
modo facilita a personalização das preferências de perfil e priva-
cidade, permitindo que o usuário utilize o teclado virtual e faça
escolhas rapidamente através de toques na segunda tela. Todas
as configurações previstas para perfil e privacidade na TV 3.0, in-
cluindo a criação e a gestão de novos perfis, podem ser facilmente
adaptadas para essa interface, oferecendo uma experiência de con-
figuração mais intuitiva e acessível, como ilustrado na Figura 6.
Figura 6: Caso de uso: perfil e privacidade em segunda tela.
3.1.5
Guias de programação e de conteúdos de TV 3.0 na segunda
tela. Ao clicar na seção de guia de programação, o telespectador
é levado a um ambiente na segunda tela onde pode explorar as
programações das emissoras disponíveis. Nesse ambiente, o usuário
tem a opção de selecionar ao que deseja assistir, incluindo progra-
mas que não estão sendo transmitidos ao vivo naquele momento,
mas que estão disponíveis para visualização sob demanda. Além
disso, o telespectador pode decidir se quer continuar assistindo ao
conteúdo na tela do seu dispositivo móvel ou transferir a exibição
para a tela da TV, proporcionando flexibilidade e controle total
sobre a experiência de visualização, conforme ilustrado na Figura 7.
3.1.6
Catálogo de aplicativos de TV 3.0 na segunda tela. Conforme
ilustrado na Figura 8, é possível replicar a interação com o catálogo
de aplicativos da TV 3.0 disponíveis na região onde o telespecta-
dor/interator se encontra, utilizando uma segunda tela. A partir
desse catálogo, o usuário pode escolher a emissora a que deseja
WTVDI’2024, Juiz de Fora/MG, Brazil
Pernisa Jr et al.
Figura 7: Caso de uso: guias de programação e de conteúdos
de TV 3.0 na segunda tela.
assistir e, assim como no exemplo anterior, decidir se prefere con-
tinuar assistindo ao conteúdo em seu smartphone ou transferir a
exibição para a tela da TV. Essa flexibilidade permite ao usuário
controlar sua experiência de visualização de maneira intuitiva, seja
no dispositivo móvel ou no televisor.
Figura 8: Caso de uso: Catálogo de aplicativos de TV 3.0 na
segunda tela.
3.1.7
Acessibilidade na segunda tela. A tecla de acessibilidade é
uma das funções que estarão disponíveis na lista de controles da
segunda tela. Através dela, o telespectador pode acessar diretamente
opções como língua de sinais, aprimoramento de diálogos, legendas
em closed caption e audiodescrição. Essas funcionalidades foram
concebidas para a TV 3.0 desde o início, com a intenção de serem
utilizadas também na segunda tela. Cada uma dessas opções foi
adaptada para uso em smartphones, oferecendo flexibilidade para
que o conteúdo seja acessado diretamente no dispositivo móvel ou
na TV, conforme a preferência do telespectador, de maneira similar
aos exemplos anteriores. A Figura 9 ilustra o caso de uso.
CONSIDERAÇÕES FINAIS
Embora ainda não exista uma norma estabelecida para o uso do con-
trole remoto ou da segunda tela na TV 3.0, as sugestões apresentadas
neste estudo visam oferecer diretrizes práticas que possam propor-
cionar ao telespectador/interator uma experiência mais confortável
e intuitiva ao utilizar esses dispositivos. O objetivo é garantir que,
em sua jornada, ele encontre facilidades em vez de dificuldades ao
navegar por este novo ambiente televisivo.
É esperado que ajustes e correções de rota sejam realizados antes
que o Projeto TV 3.0 seja concluído e comece a fase de implantação
Figura 9: Caso de uso: acessibilidade na segunda tela.
do novo padrão no país. No entanto, isso não diminui a relevân-
cia do material apresentado aqui. Pelo contrário, as contribuições
das instituições de pesquisa e desenvolvimento desempenham um
papel crucial e devem ser reconhecidas como parte integral do
processo, assegurando que o trabalho desenvolvido tenha impacto
significativo na fase final da implantação do projeto.
Também é importante reconhecer que o telespectador/interator
não busca apenas “novidades” tecnológicas na TV. A qualidade dos
programas transmitidos continuará sendo o fator de maior relevân-
cia nesta nova televisão. Um grande projeto, por mais inovador que
seja, deve estar alinhado com as expectativas do público em relação
à programação. Portanto, qualquer conteúdo transmitido pela TV
3.0 deve estar em sintonia com o que o telespectador/interator es-
pera, garantindo que a inovação tecnológica venha acompanhada
de uma programação que atenda às suas necessidades e interesses.
AGRADECIMENTOS
Os autores agradecem a toda a equipe de P&D do Projeto TV 3.0,
pelas discussões e esforços de pesquisa em conjunto. Agradecem
também à RNP e ao MCom pela gestão e financiamento, assim como
ao Fórum SBTVD e seus membros pela iniciativa, acompanhamento
e contribuições constantes.

--- FIM DO ARQUIVO: 30509-829-24957-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30510-829-24958-1-10-20241001.txt ---
TV 3.0: Integração e Controle de Renderizadores de Efeitos
Sensoriais
Marina Ivanov
marinaivanov@midiacom.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, Rio de Janeiro, Brasil
Rômulo Vieira
romulo_vieira@midiacom.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, Rio de Janeiro, Brasil
Joel A. F. dos Santos
jsantos@eic.cefet-rj.br
MultiSenS, Centro Federal de Educação Tecnológica
Celso Suckow da Fonseca
Rio de Janeiro, Rio de Janeiro, Brasil
Débora C. Muchaluat-Saade
debora@midiacom.uff.br
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, Rio de Janeiro, Brasil
0.
Entretanto, a recepção na TV digital pode se dar por diferentes
equipamentos, como televisores ou set-top boxes. O mesmo ocorre
para os atuadores dos efeitos sensoriais, que são dispositivos he-
torogêneos, desenvolvidos por diversos fabricantes e com as mais
variadas funcionalidades, protocolos e tecnologias funcionais, o que
dificulta uma comunicação padronizada com outros equipamentos
presentes na mesma rede local. Somado a isso, em um ambiente de
TV imersiva, os atuadores de efeitos podem ser controlados tanto
por aplicações executadas diretamente na TV, quanto por aplicações
das emissoras que são executadas em dispositivos de segunda tela.
A fim de viabilizar essas novas funcionalidades, especialmente o
suporte a efeitos sensoriais, o presente trabalho propõe mudanças
arquiteturais no receptor de TV 3.0, bem como um protocolo de
comunicação entre o mesmo e dispositivos remotos que renderizam
efeitos sensoriais e uma API para permitir o controle desses efeitos
e sua sincronização com conteúdo audiovisual. Esta pesquisa segue
a API genérica proposta em [16], que estabeleceu as diretrizes para
comunicação entre dispositivos remotos hetoregêneos e o receptor
de TV 3.0, aqui aplicadas para inclusão e controle de renderizadores
de efeitos sensoriais.
O restante do artigo está estruturado da seguinte maneira. A
Seção 2 examina o estado da arte na integração entre conteúdo mul-
timídia e efeitos sensoriais, abordando os métodos predominantes
para combinar esses diferentes tipos de mídia e concentrando-se
na aplicação desses parâmetros em ambientes de TV imersiva. A
Seção 3 discute as modificações arquiteturais no receptor de TV
WTVDI’2024, Juiz de Fora/MG, Brazil
Josué et al.
3.0 necessárias para suportar essas novas funcionalidades, além do
protocolo de comunicação necessário para a descoberta e registro
de dispositivos remotos, o que culmina na API capaz de controlar
os efeitos sensoriais. A Seção 4 ilustra um caso de uso de conteúdo
televisivo que incorpora efeitos de luz, aroma e vento, desenvolvido
com base nos conceitos expostos neste estudo. Por fim, a Seção
5 apresenta as conclusões gerais decorrentes da realização deste
trabalho.
TRABALHOS RELACIONADOS
A apresentação de conteúdo audiovisual acompanhada de efeitos
que estimulam múltiplos sentidos humanos é conhecida como mul-
semídia (do inglês, Multiple Sensorial Media) [2]. Essa abordagem
permite a integração de efeitos sensoriais, tais como vento, aroma
e temperatura, sincronizados com mídias tradicionais, como texto,
vídeo, imagem e áudio. A renderização desses efeitos pode ser reali-
zada por meio de dispositivos atuadores, como ventiladores, disper-
sores de aroma, lâmpadas e aquecedores [14].
Dada a ubiquidade das aplicações multimídia no cotidiano, di-
versos estudos foram conduzidos para integrar conteúdo de mídia
tradicional com efeitos sensoriais em uma aplicação web. Um tipo
recorrente de trabalho nesta área baseia-se no uso de plug-ins. Por
exemplo, a proposta de Rainer et al. [12] apresenta uma extensão
para navegadores web e uma biblioteca denominada Ambient Li-
brary (AmbientLib), que são responsáveis por extrair quadros de
vídeo e informações temporais para sincronizar o conteúdo mul-
timídia com efeitos sensoriais relacionados ao conteúdo do vídeo
em questão. De forma similar, o trabalho de Waltl et al. [18] faci-
lita a comunicação entre um navegador web executando conteúdo
multimídia e o sistema amBX1, formado por dois ventiladores, uma
pulseira de vibração, duas luzes frontais e aparelhagem sonora. Este
sistema também recupera quadros de vídeo e exibe uma cor cor-
respondente no ambiente amBX, além de solicitar descrições de
metadados dos efeitos sensoriais para ativar os ventiladores e a
pulseira conforme o conteúdo exibido.
Além disso, há uma série de estudos focados em desenvolver
frameworks para capturar, medir, quantificar, julgar e explicar a
experiência do usuário no consumo de conteúdo mulsemídia. O
trabalho de Waltl et al. [19] contribui ao realizar testes utilizando o
padrão ITU-T P.911 e a técnica de (Degradation Category Rating–
DCR) para avaliar e categorizar o estado de deteriorização dos ren-
derizadores de efeitos sensoriais. Saleme et al. [15] propõem uma
estrutura mulsemídia interoperável que acomoda diferentes perfis
de comunicação e conectividade, além dos mais distintos padrões de
metadados de efeitos sensoriais conforme as necessidades dos apli-
cativos e dispositivos disponíveis no ambiente do usuário. Guedes
et al. [4], por sua vez, apresentam uma estrutura de programação
de alto nível que suporta interfaces de usuário multimodais em
aplicações multimídia interativas, integrando elementos de entrada
e saída, como gestos, reconhecimento de voz, conteúdo audiovi-
sual, sintetizadores de fala e atuadores para implementar diferentes
efeitos sensoriais.
Por outro lado, a proposta de Josué et al. [8] foca na modelagem
abstrata de efeitos sensoriais em aplicações multimídia, utilizando
a linguagem NCL (ITU-T H.761/ABNT NBR 15606) [11, 13] para
1http://www.ambx.com
tratar os efeitos sensoriais como entidades de primeira classe, permi-
tindo sua definição independente da instalação física que executará
a aplicação. Dessa forma, os efeitos sensoriais são modelados como
entidades multimídia, análogas à representação abstrata de con-
teúdo multimídia tradicional.
Especificamente no contexto de sistemas de TV, as aplicações
de Jalal et al. [5–7] utilizam a pervasividade da Internet das Coisas
(IoT) para habilitar um cenário de casa inteligente com dispositivos
reais e consolidados no mercado. Para viabilizar esse cenário, a
arquitetura do ambiente é dividida em quatro camadas, a saber:
camada física, implementada na nuvem e composta por objetos
capazes de acessarem a Internet; camada de virtualização, que desa-
copla a parte de hardware da representação de software baseada em
nuvem a partir da criação de uma contraparte digital de qualquer
entidade no mundo real; camada de agregação, responsável pela
junção dos dados vindos de diferentes fontes para garantir um alto
nível de reutilização; e camada de aplicação, onde os aplicativos do
usuário são responsáveis pelo processamento e apresentação do
conteúdo final.
AMPLIANDO AS FUNCIONALIDADES DO
GINGA PARA RENDERIZAÇÃO DE EFEITOS
SENSORIAIS
Esta seção oferece uma visão geral sobre a inserção de efeitos sen-
soriais no Ginga, abrangendo desde seu comportamento no docu-
mento de autoria da aplicação até as modificações arquiteturais
e o protocolo de comunicação necessário para suportar dispositi-
vos atuadores heterogêneos. Este panorama culmina na API que
controla um conjunto de funções nesses dispositivos.
3.1
Inserindo Efeitos Sensoriais no Ginga
O middleware Ginga [17] foi desenvolvido para permitir que ser-
viços de TV operem em diversos modelos de set-top boxes, apre-
sentando uma arquitetura composta por subsistemas que desem-
penham funções específicas voltadas para a exibição de aplicações
declarativas. Tais aplicações são criadas utilizando a linguagem
NCL, adotada como padrão na autoria de aplicação para o sistema
de TV brasileiro, capaz de descrever o comportamento espacial
e temporal entre seus objetos, habilitar interação dos usuários e
descrever o layout da apresentação em múltiplos dispositivos. Para
prover suporte a aplicações mulsemídia, a linguagem foi recente-
mente estendida para a versão 4.0, de modo a permitir interação
multimodal do usuário por meio de comandos de voz, gestos ou
movimentos oculares [1], além de representar estímulos sensoriais
(como luzes RGB/estroboscópicas, estímulos táteis, olfativos e etc.)
como entidades de primeira ordem [8, 9], independentemente dos
dispositivos físicos utilizados para sua renderização.
Dessa forma, o autor da aplicação pode usar os mesmos con-
ceitos já utilizados para manipular nós de mídia para os nós de
efeitos sensoriais. O elemento <effect> possui um conjunto de atri-
butos para identificar e caracterizar a ocorrência do efeito sensorial,
como o id, que identifica univocamente cada elemento sensorial
dentro do documento NCL, o type, que especifica o tipo de efeito
(podendo assumir os valores de efeito de luz, flash, temperatura,
vento, vibração, spray, aroma e névoa), e o descritor, que dispõe de
TV 3.0: Integração e Controle de Renderizadores de Efeitos Sensoriais
WTVDI’2024, Juiz de Fora/MG, Brazil
propriedades adicionais, como uma especificação de início ou fim
da apresentação, por exemplo.
No que tange à localização de tais efeitos, ela pode ser especifi-
cada de duas maneiras. Primeiro, o autor da aplicação pode usar um
sistema de coordenadas esféricas, no qual a região dos efeitos co-
meça no ponto indicado pelos atributos azimutal e polar, enquanto
que no segundo método são utilizados os atributos de largura e
altura para indicar o tamanho da área a ser usada para renderizar o
efeito.
Por fim, cabe destacar que a unidade de medida para o efeito
luminoso é definida em lux, enquanto a intensidade de um efeito
térmico é expressa em graus Celsius. No que concerne ao efeito do
vento, a mensuração é feita de acordo com a escala Beaufort. Para
a medição do efeito de vibração, utiliza-se a escala Hertz (Hz). A
intensidade dos efeitos de spray, aroma e névoa pode ser descrita
em termos de mililitros por hora (ml/h). Para evitar que o autor de
uma aplicação NCL tenha que lidar com tantas unidades distintas,
a intensidade de um efeito sensorial é especificada em uma escala
relativa de 0 a 10.
3.2
Arquitetura do Receptor TV 3.0
Renderizadores de efeitos sensoriais podem ser desenvolvidos por
diferentes fabricantes e implementar diferentes protocolos de con-
trole. Deste modo, torna-se necessário a definição de uma interface
de comunicação, a fim de permitir que estes diferentes dispositivos
se comuniquem com o receptor de TV para a execução de aplicações
mulsemídia.
A comunicação dos renderizadores de efeito com o middleware
Ginga pode se dar através de uma API específica para cada renderi-
zador, permitindo que o mesmo se comunique diretamente com o
Ginga Common Core, conforme proposto por Josué et al. [9]. Neste
cenário, os implementadores de middleware devem desenvolver
suas soluções já acopladas aos fabricantes de renderizador de efeito.
A fim de possibilitar uma maior independência entre o mid-
dleware e os fabricantes de renderizadores, este trabalho propõe um
novo componente, externo ao middleware, que se comunica com o
Ginga através do Ginga CCWS, denominado Sensory Effect (SE) Pre-
sentation Engine, e especifica seu próprio protocolo de comunicação
com os renderizadores de efeitos sensoriais, conforme apresentado
na Figura 1.
O middleware Ginga deve manter um registro dos renderizadores
de efeitos cadastrados, incluindo as configurações necessárias para
controlá-los, seja via API do dispositivo ou através do Ginga CC
WebServices. Isso pode ser realizado por meio de um arquivo de
configuração que contenha informações como o endereço IP de um
dispositivo conectado diretamente ao Ginga ou o identificador de
um SE Presentation Engine que controla um conjunto de dispositivos.
3.3
Comunicação do SE Presentation Engine com
o Ginga
Conforme citado anteriormente, a comunicação entre o SE Presen-
tation Engine e o middleware Ginga é estabelecida através do Ginga
CC Webservices. Como o SE Presentation Engine é um elemento
externo ao receptor, ele deve utilizar o protocolo SSDP para deter-
minar a existência de um terminal receptor TV 3.0 com suporte ao
Ginga CC WebServices e obter o ponto de acesso às API fornecidas.
Figura 1: Arquitetura do middleware Ginga para suporte a
controle de renderizadores de efeitos via Ginga CCWS .
Figura 2: Processo de descoberta e registro de dispositivo
remoto capaz de renderizar efeitos sensoriais [16].
O Simple Service Discovery Protocol (SSDP) [3] fornece um meca-
nismo para dispositivos em rede se comunicarem e se descobrirem.
Para isso, o protocolo SSDP fornece suporte à descoberta multi-
cast, bem como notificação baseada em servidor e roteamento de
descoberta. Este processo de descoberta e registro de dispositivos
remotos junto ao Ginga é descrito por Santos et al. [16] e ilustrado
na Figura 2.
Após a descoberta, a comunicação avança para a fase de Auto-
rização, onde o Remote Device WebService (RDWS), componente
responsável por intermediar a comunicação entre o Ginga e os dis-
positivos remotos, obtém o token de acesso do dispositivo que está
tentando se conectar e estabelece um vínculo com o receptor do
sinal de TV. Em vista da necessidade de autorização do usuário, a
etapa de Falha pode ser alcançada caso nenhuma autorização seja
concedida.
Por outro lado, se a operação for autorizada, a fase de Registro
é iniciada com o RDWS fazendo uma solicitação POST à API de
registro para, em seguida, conectar-se ao ponto de entrada Web-
Socket, iniciando a fase de Conexão. Se algum erro for detectado,
WTVDI’2024, Juiz de Fora/MG, Brazil
Josué et al.
a comunicação move-se para a fase de Cancelamento de Registro.
Caso contrário, avança para a fase de Execução.
Para o registro do SE Presentation Engine no Ginga, o corpo
da mensagem deve conter um campo que identifica a classe do
dispositivo que está se registrando como renderizador de efeitos
sensoriais e os tipos de efeitos suportados por este equipamento,
que devem estar em conformidade com os efeitos suportados pelo
middleware. A Listagem 1 apresenta um exemplo de mensagem
para registro da SE Presentation Engine.
1 {
" d e v i c e C l a s s "
:
" sensory −e f f e c t " ,
" supportedTypes "
:
[ " LightType " ,
" ScentType " ,
"
WindType " ,
" TemperatureType " ,
" VibrationType " ]
4 }
Listagem 1: Corpo da mensagem de registro da SE
Presentation Engine.
Na fase de Cancelamento de Registro, o RDWS cancela a ope-
ração no CCWS, liberando o WebSocket previamente criado. Este
procedimento também pode ser acionado quando o mecanismo
de apresentação do dispositivo remoto é interrompido. É possível
mover-se desta fase para a etapa de Pausa, onde o registro pode ser
reiniciado quando o dispositivo remoto é acionado pelo usuário, ou
completamente encerrado, caso o mecanismo de apresentação do
dispositivo remoto pare.
Na fase de Execução, o middleware pode enviar comandos para
controlar a apresentação de efeitos sensoriais. A partir deste ponto, é
possível retornar à fase de Conexão se a comunicação for perdida ou
alcançar a fase de Pausa se o dispositivo entrar em modo de espera.
O RDWS deve permanecer nesta fase durante toda a apresentação
do nó no dispositivo remoto. Além disso, durante este período,
o RDWS deve trocar mensagens com o Ginga (via WebSocket)
para que os metadados auxiliem no controle dessa apresentação
e notifiquem transições de eventos. Se ocorrerem alterações nas
capacidades do dispositivo remoto durante esse processo, o RDWS
deve transitar para a fase de Atualização.
A SE Presentation Engine pode ser invocada pelo Ginga para a
reprodução de efeitos sensoriais de uma aplicação NCL 4. Neste caso,
ao iniciar a aplicação NCL4, o Ginga deve enviar uma mensagem
conforme apresentada na Listagem 2, contendo o identificador do nó
de efeito sensorial na aplicação NCL, suas propriedades e também
o identificador da aplicação.
1 {
" nodeId "
:
< nodeId > ,
" type "
:
" < LightType
|
ScentType
|
WindType
|
TemperatureType
|
VibrationType >"
" appId "
:
< appId > ,
" p r o p e r t i e s "
:
[
{
" name "
:
<propName > ,
" value "
:
< propValue > }
]
8 }
Listagem 2: Mensagem com metadados de efeito sensorial a
ser renderizado pela SE Presentation Engine.
Quando a SE Presentation Engine recebe uma solicitação do
Ginga para realizar uma ação sobre determinado efeito sensorial,
ela deve enviar uma mensagem de resposta indicando se a ação
foi bem-sucedida (retornando o código 200), ou se ocorreu alguma
falha, seja pelo fato do tipo de efeito sensorial não ser suportado
ou devido a alguma limitação do SE Presentation Engine. Caso
a renderização do efeito sensorial seja interrompida por alguma
interação externa ao Ginga, a SE Presentation Engine deve notificar
o middleware sobre uma ação de abort no evento de apresentação
do efeito em questão.
Como os dispositivos de renderização podem possuir capacidades
e características diferentes, este trabalho também propõe uma API
para que o Ginga possa solicitar tais informações à SE Presentation
Engine, conforme descrito na Listagem 3
1 {
" type "
:
< e f f e c t T y p e > ,
" c a p a b i l i t i e s "
:
[ {
" name "
:
< capName >
} ]
4 }
Listagem 3: Mensagem de solicitação de capacidades da SE
Presentation Engine realizada pelo Ginga.
3.4
API CCWS para Suporte a Efeitos Sensoriais
via Ginga-HTML5
O middleware Ginga oferece suporte à execução de aplicações es-
pecificadas tanto na linguagem NCL quanto em HTML5. Embora
NCL4 contemple, de forma nativa, a especificação de efeitos sen-
soriais, o HTML5 permite a especificação apenas de nós de mídia
tradicionais, como vídeo e áudio. Assim, com o intuito de viabilizar a
especificação de aplicações multimídia também para esta linguagem,
este trabalho propõe uma API, denominada sensory-effect-renderers,
para o controle de renderizadores de efeitos sensoriais por meio do
Ginga CCWS.
A aplicação HTML5 pode consultar os renderizadores de efei-
tos disponíveis no receptor por meio de uma requisição GET na rota
http(s)://<host>/dtv/sensory-effect-renderers. Em caso de
sucesso, o Ginga CCWS responde à requisição com informações
sobre os renderizadores disponíveis, como localização, efeitos supor-
tados e também seu estado (isto é, se estão em uso por alguma apli-
cação, preparados, ou disponíveis, por exemplo). Caso a aplicação
queira obter a informação de um renderizador específico, ela deve
utilizar a requisição GET na rota http(s)://<host>/dtv/sensory
-effect-renderers/<renderer-id>, onde renderer-id define o
identificador do renderizador desejado.
O controle de um renderizador pela aplicação HTML5 se dá pela
requisição POST na rota http(s)://<host>/dtv/sensory-effect
-renderers/<renderer-id> com o corpo da mensagem especifi-
cando o tipo de ação a ser aplicada sobre o efeito sensorial, conforme
descrito na Listagem 4.
1 {
" e f f e c t T y p e " :
" < e f f e c t T y p e > " ,
" a c t i o n " :
< p r e p a r e | s t a r t | pause | resume | s t o p | s e t
> ,
" p r o p e r t i e s " :
[
{ " name " :
<propName > ,
" value " :
< propValue > }
]
7 }
Listagem 4: Mensagem de controle de um renderizador de
efeito específico.
TV 3.0: Integração e Controle de Renderizadores de Efeitos Sensoriais
WTVDI’2024, Juiz de Fora/MG, Brazil
CASO DE USO
Esta seção apresenta um novo caso de uso viabilizado pelas APIs
propostas neste trabalho: uma aplicação multimídia em HTML5
que abrange os requisitos da TV 3.0, especificamente relacionados
ao suporte para TV imersiva (AP-req-14.1, 14.4 e 14.6).
Considere uma aplicação mulsemídia composta de três conteúdos
de vídeo, efeito de luz e efeito de aroma. Inicialmente, é apresentado
um efeito de luz laranja, refletindo a cor predominante no conteúdo
audiovisual sendo exibido na tela. Durante a execução da aplicação,
o efeito de luz deve ter a sua propriedade “color” alterada, sincroni-
zada com a troca entre os conteúdos de vídeo que são apresentados
na tela. Além do efeito de luz, a aplicação apresenta um efeito de
aroma sincronizado com o terceiro vídeo. O efeito de aroma é rende-
rizado pelo dispositivo no momento em que este vídeo começa a ser
exibido. A Figura 3 exibe a visão temporal da aplicação, ilustrando
a sincronização entre os vídeos e os efeitos de luz e aroma.
Figura 3: Linha do tempo correspondente ao plano de apre-
sentação da aplicação de exemplo.
Para este caso de uso, será utilizada a rota de controle de rende-
rizador de efeito apresentado na Seção 3.4 para disparar os efeitos
sensoriais sincronizados com a apresentação dos elementos de vídeo.
A Listagem 5 descreve a aplicação proposta.
1 < !DOCTYPE html>
2 <html lang= " pt −BR" >
3 <head>
. . .
5 < / head>
6 <body>
<video
id= " myVideo "
c o n t r o l s >
<source
src= " media / video1 . mp4"
type= " video
/mp4" >
< / video >
< script >
const
videos = [
' media / video1 . mp4 ' ,
' media / video2 . mp4 ' ,
' media / video3 . mp4 '
] ;
l e t
currentVideoIndex = 0 ;
const
videoElement = document .
getElementById ( ' myVideo ' ) ;
videoElement . addEventListener ( ' ended ' ,
f u n c ti o n ( )
{
currentVideoIndex ++;
i f
( currentVideoIndex < videos . length )
{
videoElement . src = videos [
currentVideoIndex ] ;
videoElement . play ( ) ;
}
} ) ;
videoElement . addEventListener ( ' play ' ,
f u n c ti o n ( )
{
switch
( currentVideoIndex ) {
case
0 :
s t a r t L i g h t E f f e c t ( " orange " ) ;
break ;
case
1 :
s e t L i g h t C o l o r ( " green " ) ;
break ;
case
2 :
s e t L i g h t C o l o r ( " blue " ) ;
s t a r t S c e n t E f f e c t ( " sea " ) ;
break ;
}
} ) ;
videoElement . src = videos [
currentVideoIndex ] ;
videoElement . play ( ) ;
</ script >
48 < / body>
49 < / html>
Listagem 5: Aplicação HTML5 mulsemídia.
A função startLightEffect recebe a cor do efeito de luz como
parâmetro, e implementa a requisição do tipo POST na rota
https://<host>/dtv/sensory-effect-renderers/<renderer
-id> com a mensagem de corpo descrita na Listagem 6.
1 {
" e f f e c t T y p e " :
" LightType " ,
" a c t i o n " :
" s t a r t " ,
" p r o p e r t i e s " :
[
{ " name " :
" c o l o r " ,
" value " :
" orange " }
]
7 }
Listagem 6: Mensagem enviada pela função startLightEffect.
Por fim, as funções setLightColor e startScentEffect tam-
bém utilizam a mesma rota da função startLightEffect porém
com o corpo da mensagem contendo informações diferentes, con-
forme descrito nas Listagens 7 e 8, respectivamente.
1 {
" e f f e c t T y p e " :
" LightType " ,
" a c t i o n " :
" s e t " ,
" p r o p e r t i e s " :
[
{ " name " :
" c o l o r " ,
" value " :
" green " }
]
7 }
WTVDI’2024, Juiz de Fora/MG, Brazil
Josué et al.
Listagem 7: Mensagem enviada pela função setLightColor.
1 {
" e f f e c t T y p e " :
" ScentType " ,
" a c t i o n " :
" s t a r t " ,
" p r o p e r t i e s " :
[
{ " name " :
" s c e n t " ,
" value " :
" sea " }
]
7 }
Listagem 8: Mensagem enviada pela função startScentEffect.
CONSIDERAÇÕES FINAIS
A evolução constante do setor televisivo brasileiro, impulsionada
por inovações tecnológicas, continua a moldar a forma como o
conteúdo é recebido pelos telespectadores. Este artigo apresentou
uma proposta de adaptação do middleware Ginga para suportar
a próxima geração de TV digital no Brasil, conhecida como TV
3.0, com foco na integração de renderizadores de efeitos sensori-
ais. Através de mudanças arquiteturais e a implementação de uma
API específica, demonstramos como o Ginga pode se comunicar e
controlar dispositivos remotos responsáveis pela renderização de
efeitos como luz, aroma e vento, proporcionando uma experiência
mais imersiva e interativa.
Os resultados obtidos a partir do caso de uso validaram a apli-
cabilidade das modificações propostas, evidenciando a viabilidade
técnica e os benefícios proporcionados por uma experiência de TV
sensorial, que além de aprimorar a imersão, abre novas possibilida-
des para o desenvolvimento de aplicações inovadoras no contexto
da TV digital, como conteúdos publicitários mais impactantes e
ambientes domésticos transformados em experiências 4D.
No entanto, a implementação completa e a adoção massiva desse
novo padrão dependem de um ecossistema robusto que inclua fa-
bricantes de dispositivos, emissoras e desenvolvedores de conteúdo.
Futuras pesquisas irão focar em otimizações de desempenho, segu-
rança da comunicação entre dispositivos e a criação de ferramentas
que facilitem o desenvolvimento de aplicações sensoriais.
A TV 3.0 representa um salto significativo na experiência televi-
siva, trazendo novas formas de interatividade e imersão. Outrossim,
este trabalho contribui para esse avanço, estabelecendo as bases
tecnológicas para a integração de efeitos sensoriais no ecossistema
de TV digital do Brasil.
AGRADECIMENTOS
Os autores agradecem o suporte das agências CAPES, Capes-Print,
RNP, CNPq e FAPERJ, bem como ao Ministério das Comunicações
(MCOM) e ao Fórum SBTVD.
REFERÊNCIAS
[1] Fábio Barreto, Raphael Abreu, Marina Josué, Eyre Montevecchi, Pedro Valentim,
and Débora Muchaluat-Saade. 2023. Providing multimodal and multi-user inte-
ractions for digital tv applications. Multimedia Tools and Applications 82 (2023),
4821–4846. https://doi.org/10.1007/s11042-021-11847-3
[2] Gheorghita Ghinea, Christian Timmerer, Weisi Lin, and Stephen R Gulliver. 2014.
Mulsemedia: State of the art, perspectives, and challenges. ACM Transactions on
Multimedia Computing, Communications, and Applications (TOMM) 11, 1s (2014),
1–23.
[3] Y. Y. Goland, T. Cai, P. Leach, and Y. Gu. 1999. Simple service discovery proto-
col/1.0 operating without on arbiter. IETF INTERNET-DRAFT draft-cai-ssdp-v1-03.
txt (1999). https://datatracker.ietf.org/doc/html/draft-cai-ssdp-v1-03
[4] Álan Lívio Vasconcelos Guedes, Roberto Gerson de Albuquerque Azevedo, and
Simone Diniz Junqueira Barbosa. 2017. Extending multimedia languages to
support multimodal user interactions. Multimedia tools and applications 76
(2017), 5691–5720.
[5] Lana Jalal, Matteo Anedda, Vlad Popescu, and Maurizio Murroni. 2018. Internet
of Things for enabling multi sensorial TV in smart home. In 2018 IEEE Broadcast
Symposium (BTS). IEEE, 1–5.
[6] Lana Jalal, Matteo Anedda, Vlad Popescu, and Maurizio Murroni. 2018. Qoe
assessment for broadcasting multi sensorial media in smart home scenario. In 2018
IEEE International Symposium on Broadband Multimedia Systems and Broadcasting
(BMSB). IEEE, 1–5.
[7] Lana Jalal, Matteo Anedda, Vlad Popescu, and Maurizio Murroni. 2018. QoE
assessment for IoT-based multi sensorial media broadcasting. IEEE Transactions
on Broadcasting 64, 2 (2018), 552–560.
[8] Marina Josué, Raphael Abreu, Fábio Barreto, Douglas Mattos, Glauco Amorim,
Joel dos Santos, and Débora Muchaluat-Saade. 2018. Modeling sensory effects
as first-class entities in multimedia applications. In Proceedings of the 9th ACM
Multimedia Systems Conference. 225–236.
[9] Marina Josué, Marcelo F. Moreno, and Débora Muchaluat-Saade. 2024. Automatic
Preparation of Sensory Effects. In Proceedings of MMSys ’24: ACM Multimedia
Systems Conference 2024 (Bari). 35–40.
[10] ABNT NBR. 2023. ABNT NBR 15606-1, Codificação de dados e especificações
de transmissão para radiodifusão digital, Parte 1: Codificação de dados. ABNT
(2023).
[11] ABNT NBR. 2023. ABNT NBR 15606-2. Televisão digital terrestre–Codificação de
dados e especificações de transmissão para radiodifusão digital–Parte 2: Ginga-
NCL para receptores fixos e móveis–Linguagem de aplicação XML para codifica-
ção de aplicações. ABNT (2023).
[12] Benjamin Rainer, Markus Waltl, Eva Cheng, Muawiyath Shujau, Christian Tim-
merer, Stephen Davis, Ian Burnett, Christian Ritz, and Hermann Hellwagner.
2012. Investigating the impact of sensory effects on the quality of experience
and emotional response in web videos. In 2012 Fourth International Workshop on
Quality of multimedia experience. IEEE, 278–283.
[13] ITU-T Recommendation. 2014. Nested Context Language (NCL) and Ginga-NCL.
(2014). https://www.itu.int/itu-t/recommendations/rec.aspx?rec=H.761
[14] Renato O Rodrigues, Marina IP Josué, Raphael S Abreu, Glauco F Amorim, Dé-
bora C Muchaluat-Saade, and Joel AF dos Santos. 2019. A proposal for supporting
sensory effect rendering in ginga-ncl. In Proceedings of the 25th Brazillian Sym-
posium on Multimedia and the Web. 273–280.
[15] Estêvão Bissoli Saleme, Celso AS Santos, and Gheorghita Ghinea. 2019. A mul-
semedia framework for delivering sensory effects to heterogeneous systems.
Multimedia Systems 25 (2019), 421–447.
[16] Joel Santos, Rômulo Vieira, , Marina Josué, Karen Oliveira, and Débora C.
Muchaluat-Saade. 2024. Multidevice Support in the Next Generation of the
Brazilian Terrestrial TV System. In Proceedings of the 2024 ACM Internatio-
nal Conference on Interactive Media Experiences (Stockholm, Sweden) (IMX
’24). Association for Computing Machinery, New York, NY, USA, 1–9.
https:
//doi.org/10.1145/3639701.3656304
[17] Luiz Fernando Gomes Soares, Marcio Ferreira Moreno, Carlos de Salles Soares
Neto, and Marcelo Ferreira Moreno. 2010. Ginga-NCL: Declarative middleware
for multimedia IPTV services. IEEE Communications Magazine 48, 6 (2010),
74–81.
[18] Markus Waltl, Benjamin Rainer, Christian Timmerer, and Hermann Hellwagner.
2011. Sensory Experience for Videos on the Web. In 2011 Workshop on Multimedia
on the Web. 49–51. https://doi.org/10.1109/MMWeb.2011.12
[19] Markus Waltl, Christian Timmerer, and Hermann Hellwagner. 2010. Increasing
the user experience of multimedia presentations with sensory effects. In 11th
International Workshop on Image Analysis for Multimedia Interactive Services
WIAMIS 10. IEEE, 1–4.

--- FIM DO ARQUIVO: 30510-829-24958-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30511-829-24959-1-10-20241001.txt ---
Suporte a Alertas de Emergência na TV 3.0 Brasileira
Richelieu R. A. Costa *
richelieu.costa@lavid.ufpb.br
Derzu Omaia *
derzu@lavid.ufpb.br
Tiago M. U. Araújo *
tiagomaritan@lavid.ufpb.br
Jóison O. Pereira *
joison.pereira@lavid.ufpb.br
Miguel P. S. Cruz *
miguel.cruz@lavid.ufpb.br
Matheus M. Barbosa *
matheus.mendonca@lavid.ufpb.br
Rafael Toscano *
rafael.toscano@lavid.ufpb.br
Guido L. S. Filho *
guido@lavid.ufpb.br
* Univ. Federal da Paraíba
João Pessoa, PB, Brasil
WTVDI’2024, Juiz de Fora/MG, Brazil
Costa et al.
mensagens urgentes sejam recebidas, compreendidas e atendidas
rapidamente.
REFERENCIAL TEÓRICO
Nesta seção são apresentados alguns dos conceitos teóricos que
fundamentam o presente trabalho.
2.1
Coordenação de alertas de emergência no
Brasil
A Secretaria Nacional de Proteção e Defesa Civil (SEDEC) é um
órgão central do Sistema Nacional de Proteção e Defesa Civil (SIN-
PDEC), e faz parte da estrutura do Ministério da Integração e do
Desenvolvimento Regional (MIDR). A SEDEC é responsável por
coordenar as ações de proteção e defesa civil em todo o território na-
cional. Sua atuação tem o objetivo de reduzir os riscos de desastres.
Também compreende ações de prevenção, mitigação, preparação,
resposta e recuperação, e se dá de forma multissetorial e nos três
níveis de governo federal, estadual e municipal [6].
O Centro Nacional de Gerenciamento de Riscos e Desastres (CE-
NAD) é um departamento da SEDEC, e se dedica a elaboração de
alertas e avisos de emergência, possibilitando à população tomar
decisões antecipadas na possível ocorrência de desastres. A partir
dessas informações, é possível adiantar-se aos eventos adversos,
e tomar medidas de preparação. Quando o desastre acontece, é
possível organizar e articular vários órgãos do governo federal no
esforço conjunto e integrado para fazer frente ao desafio e atender
as populações [6].
O Centro Nacional de Monitoramento e Alerta de Desastres Nat-
urais (CEMADEN) é um órgão vinculado ao Ministério da Ciência,
Tecnologia, Inovações e Comunicações (MCTIC). Ele também real-
iza o monitoramento e emite alertas de desastres naturais. O Órgão
opera ininterruptamente em todo o território nacional, monitorando
as áreas de risco de 1038 municípios classificados como vulneráveis
a desastres naturais. Entre outras competências, envia os alertas de
desastres naturais ao (CENAD), para que este o distribua. [8].
Os alertas gerados, pelas entidades supracitadas, possuem in-
formações detalhadas sobre o evento, entre elas estão o nível de
prioridade do alerta, seu tipo, o título, a descrição, arquivos de
mídias relacionadas (vídeos ou imagens), localização geográfica,
entre diversos outros campos. O tipo do alerta deve obedecer a
Codificação Brasileira de Desastres (COBRADE) e podem ser classi-
ficados nas categorias Naturais ou Tecnológicos. Entre os Naturais,
tem-se 5 possíveis grupos: Geológico, Hidrológico, Meteorológico,
Climatológico, Biológico. Já entre os Tecnológicos tem outros 5
grupos: Relacionados a Substâncias radioativas, Relacionados a Pro-
dutos Perigosos, Relacionados a Incêndios Urbanos, Relacionados a
obras civis, Relacionados a transporte de passageiros e cargas não
perigosas [6].
Essas informações são essenciais para a prevenção, preparação e
mitigação dos desastres. Desta forma, deve ser propagado o mais
rápido possível para os mais diversos sistemas e redes de alerta.
Esse alerta geralmente é armazenado em formato de arquivo XML
seguindo o Protocolo de Alerta Comum (CAP).
2.2
Protocolo de Alerta Comum (CAP)
O Protocolo de Alerta Comum (Common Alert Protocol - CAP) [5] é
um padrão internacional normatizado pelo ITU e amplamente uti-
lizado como protocolo de troca de mensagens de alerta de emergên-
cia em diversos tipos de redes. Ele define como mensagens de alerta
devem ser estruturadas e transmitidas para garantir a interoper-
abilidade entre diferentes sistemas de alerta. O CAP permite que
uma mensagem de alerta seja disseminada simultaneamente por
muitos sistemas de alerta diferentes, aumentando assim a eficácia
de distribuição [5]. Ele é distribuído em formato XML e é utilizado
em diversos sistemas, entre eles o Sistema de Alertas públicos do
Google [3] e também pelo Centro Nacional de Gerenciamento de
Riscos e Desastres (CENAD) [8].
2.3
AEA - ATSC 3.0
No padrão ATSC 3.0, os alertas de emergência, no padrão ATSC
3.0 seguem o formato Informações Avançadas de Emergência (Ad-
vanced Emergency Information - AEA) [1]. Nesse formato, o alerta
de emergência é configurado através de um arquivo XML no qual
as características do alerta são definidas como elementos (tags) e
atributos do XML. Esse arquivo é comprimido no formato gzip [2]
e transmitido dentro da tabela Low Level Signaling (LLS), a qual é
transmitida no payload de pacotes IP no ATSC 3.0 [1].
O AEA é compatível com o protocolo CAP, sendo possível con-
verter o CAP para o formato AEA. Desta forma, as emissoras podem
receber um alerta no formato CAP e transmiti-lo no formato AEA
[5].
Existem 5 possíveis níveis de prioridade nos alertas. Eles podem
ser: mínimo, baixo, moderado, alto ou máximo, que possuem difer-
entes interações com o usuário, desde oferecer a ele uma informação
útil de forma sutil, até uma informação crítica que necessite de uma
maior atenção.
2.4
Alertas de Emergência TV 3.0 Brasileira
Na TV 3.0, após um CfP (Call for Proposals), foi definido que o
formato de alertas de emergência a ser utilizado será o AEA do
ATSC 3.0. Experimentos estão sendo realizados visando investigar a
funcionalidade do sistema em situações de emergência como desas-
tres naturais, inundações, ruptura de barragens entre outros. Para
atingir o maior público possível esses alertas podem ser transmiti-
dos com acessibilidade sendo disponibilizados em formato textual,
gráfico (imagens e vídeos), áudio descrição e em língua de sinais.
METODOLOGIA
Este artigo foca na metodologia, propostas e solução desenvolvida
pelo grupo de pesquisa da Universidade Federal da Paraíba (UFPB)
em relação aos requisitos do desenvolvimento de aplicações na área
de alertas de emergência e acessibilidade da TV 3.0. A equipe é com-
posta por 9 pesquisadores, entre doutores, mestres e graduandos.
O trabalho, nesta fase 3, começou em Abril de 2023.
Como forma de colaborar ativamente com a metodologia de in-
vestigação, os Módulos Técnicos e de Mercado do Fórum SBTVD
decidiram conjuntamente uma priorização de requisitos para de-
terminar a sequência de estudos para a equipe de pesquisa e de-
senvolvimento (P&D). Além disso, o Grupo de Trabalho (GT) de
Suporte a Alertas de Emergência na TV 3.0 Brasileira
WTVDI’2024, Juiz de Fora/MG, Brazil
Codificação de Aplicações do Fórum especificou as diretrizes ini-
ciais sobre como abordar cada requisito, com base nos resultados
da avaliação da Fase 2 e na experiência do GT na padronização do
middleware DTV. Finalmente, o GT especificou um total de 7 casos
de uso a serem prototipados, com o objetivo de validar as soluções
de P&D e demonstrar publicamente as novas funcionalidades da
TV 3.0.
A equipe de P&D incorporou todas as contribuições do Fórum
SBTVD na sua metodologia, permitindo um progresso consistente
em determinados requisitos e apresentando os resultados iniciais
dos estudos.
O trabalho desenvolvido na UFPB, atualmente, foca no caso de
uso 5 (AP-uc-5) especificado pelo GT, o qual trata sobre o geoloca-
lização e alertas de emergência.
Desta forma, estão sendo investigadas e propostas soluções para a
transmissão e recepção de alertas de emergência de forma acessível,
e, também, formatos de apresentação desses alertas na televisão.
Para isso, é proposto uma API CCWS onde aplicações podem se
cadastrar como ouvintes dos alertas, para que quando algum aler-
tar for recebido pela TV ele também possa ser recebido em uma
aplicação.
3.1
Visão geral da solução
A solução proposta possibilita que aplicações locais a TV ou não lo-
cais (externas à TV) se registrem para receber alertas de emergência,
desde que estejam autenticadas e conectadas a TV. Deste modo, as
aplicações clientes (locais ou externas) podem requisitar e acessar
as informações de alerta de emergência. A partir disso, diversos
cenários podem ser explorados, incluindo os voltados para a aces-
sibilidade. No caso de telespectadores com deficiência auditiva, o
alerta recebido, pode ser exibido no formato de Língua de Sinais na
TV ou no dispositivo do usuário. Para as pessoas com deficiência
visual, o alerta pode ser exibido com suporte a audiodescrição.
Na TV 3.0, a recepção dos alertas de emergência é independente
de Internet, bastando que a TV esteja sintonizada em alguma emis-
sora para que o alerta seja recebido. Desta forma, em situações
extremas, onde não há Internet e nem mesmo sinal de celular, mas
em que o sinal de TV e a energia elétrica ainda estejam presentes,
será possível receber esses alertas na TV, e, até mesmo no celular,
caso este esteja conectado a TV por uma rede local.
Em uma situação em que a TV esteja desligada, mas conectada
a rede de energia elétrica, quando um alerta de emergência for
transmitido pela emissora, a TV consegue monitorá-lo, e poderá ser
automaticamente ligada, assumindo as definições adequadas para o
tipo de alerta recebido. Para isso, é necessário que, mesmo no estado
de espera (standby), seja realizado um monitoramento mínimo do
sinal da emissora. É importante mencionar, inclusive, que esse tipo
de monitoramento tem baixíssimo impacto no consumo de energia
elétrica [1].
A proposta de arquitetura do sistema de alerta está apresentada
na Figura 1. Conforme indicado, os órgãos de coordenação de alertas
de emergência, como, por exemplo, o CENAD e o CEMADEM,
monitoram e emitem os alertas no formato CAP, o qual contém
dados detalhados sobre as circunstâncias da emergência. Uma vez
emitidos, os alertas são recebidos pelas as emissoras de TV. Estas,
por sua vez, inicialmente, traduzem os campos de texto que estão em
português para glosa1 para possibilitar a acessibilidade em língua
de sinais do conteúdo do alerta.
Figure 1: Arquitetura do sistema proposto.
Os campos do CAP que são traduzidos para glosa são os atributos
<headline>, <description> e <instruction>; seus equivalentes no
AEA são <EventDesc> e <AEAText>. Este último é repetido para
representar os valores de <description> e <instruction>. O AEAText
possui um subatributo @lang, que indica o idioma do texto [1]. Desta
forma, recomenda-se que, para uso no Brasil, cada AEAText possua
uma versão com o valor de @lang "pt" e outro com o valor "bzs"
(Brazilian Sign Language).
Após a tradução dos campos de texto, o restante do alerta CAP
[5] é convertido para o formato AEA [1]. Em seguida , os alertas são
preparados e filtrados de acordo com as políticas da estação e a área
geográfica do alerta. Por fim, os alertas são multiplexados transmi-
tidos pelas emissoras de TV, assegurando a rápida propagação das
informações de emergência para a população.
1Glosa é uma representação textual na gramática de língua de sinais
WTVDI’2024, Juiz de Fora/MG, Brazil
Costa et al.
No receptor, quando esse alerta for recebido e processado, o
alerta em formato AEA é decodificado pelo middleware Ginga,
confirme indicado na Figura 1. Em seguida, o player de língua de
sinais da TV 3.0 é acionado para renderizar o conteúdo da glosa
em uma animação 3D em língua de sinais para pessoas surdas, e o
player de áudio da TV 3.0 é acionado apresentar a audiodescrição
do alerta para pessoas cegas. Adicionalmente, o conteúdo do alerta
de emergência também é acessado pelo servidor webservice que
pode disponibilizar este alerta para aplicações locais ou externas
(dispositivo móvel) que estejam conectadas à TV do usuário.
3.2
API proposta de Webservice
Para que as aplicações locais ou externas conectadas a TV 3.0 pos-
sam acessar os alertas de emergência e personalizar a exibição
dessas informações de alerta, neste trabalho também é proposta
API para o servidor Webservice da TV3.0.
Essa nova API possibilita o encaminhamento do alerta de emergên-
cia para aplicações que se registrem como ouvintes dos alertas. A
proposta de rota é definida pela Listagem 1.
Listagem 1 Rota de alerta de emergência
http(s)://<host>/dtv/current-service/stream/emerge
ncy[&protocol=<websocket,udp>]
De acordo com A Listagem 1, nesta rota, tem-se o parâmetro
de query opcional protocol, o qual indica se o cliente deseja se
conectar ao Webservice através de um socket UDP ou através de
um websocket. Caso omitido é considerado que o socket padrão é
um socket UDP.
O retorno da requisição fornece as informações necessárias para
que o cliente se conecte ao servidor em formato JSON (JavaScript
Object Notation). A Listagem 2 apresenta o formato retornado pela
requisição.
Listagem 2 JSON de retorno da rota de alerta de emergência
{
"handle":"<handle>",
"url":"<streamUrl>
}
O parâmetro handle é um identificador da requisição que é uti-
lizado para a identificação, rastreamento ou gerenciamento de múlti-
plos alertas de emergência pelo Webservice.
A URL contém as informações necessárias para que o cliente
inicie uma conexão de socket ou websocket com o servidor, e fique
aguardando o recebimento de alertas, caso eles ocorram.
Quando um alerta é recebido pela televisão, o CCWS repassa
esse alerta para os clientes que se registraram através da conexão
de socket criada. O alerta é transmitido no mesmo formato XML
AEA, contendo todas as informações e mídias do alerta.
PROVA DE CONCEITO
Para validar a arquitetura proposta, foram desenvolvidas três apli-
cações que possibilitam a simulação de um sistema de distribuição,
recepção e transmissão dos alertas de emergência. Essas aplicações
e sua comunicações são apresentadas na Figura 2.
Figure 2: Aplicações da prova de conceito
A primeira aplicação desenvolvida foi o módulo de Envio de
Alertas, que simula o envio de alertas pela emissora.
O segunda aplicação é o Servidor Webservice. Ele implementa
algumas das rotas que estão sendo especificadas para a TV 3.0,
dentre as rotas está a apresentada na Seção 3.2. API proposta de
Webservice. Desta forma, recebe o alerta emitido pelo módulo de
envio de alertas e possibilita que a aplicação cliente receba esse
alerta.
A terceira aplicação é o módulo de Apresentação de Alertas,
que se registra no CCWS como ouvinte dos alertas de emergência.
Quando um alerta é recebido servidor Webservice, ele é transmitido
via websocket para o módulo de Apresentação de Alertas. Quando
o alerta é recebido, ele é apresentado de acordo com as suas especi-
ficidades.
4.1
Módulo de Envio de alertas
Para simular o envio de alertas da emissora, foi desenvolvida uma
aplicação web, na qual é possível cadastrar alertas no formato AEA.
Estes podem ser configurados e enviados diretamente para o servi-
dor Webservice da TV. Nesta aplicação, é possível configurar diver-
sos campos do alerta, como seu título, descrição, seus equivalentes
em glosa, as mídia de áudio, o tipo do alerta e sua prioridade.
O título e a descrição do alerta são traduzidos automaticamente
para glosa utilizando o tradutor do VLibras [9].
4.2
Módulo Servidor Webservice
A implementação parcial do servidor Webservice, para o presente
trabalho, tem como principal objetivo permitir que aplicações locais,
ou não locais, possam acessar os alertas de emergência transmitidos
por uma emissora de TV 3.0.
A rota proposta na Seção 3.2 foi implementada em um servidor
Node.js. O cliente ao se registrar como ouvinte recebe as infor-
mações sobre o socket TCP ou websocket para se conectar. Quando
um alerta de emergência é recebido, ele é repassado para todos os
clientes que tiverem se registrado. O alerta é repassado no mesmo
formato que é recebido, isto é, no formato AEA.
4.3
Módulo de Apresentação de Alertas
A aplicação do módulo de Apresentação de Alertas, se registra
como ouvinte de alertas no webservice, através da chamada a rota
proposta na Seção 3.2. Utilizando os dados do retorno da requisição
Suporte a Alertas de Emergência na TV 3.0 Brasileira
WTVDI’2024, Juiz de Fora/MG, Brazil
ao servidor, é iniciado uma conexão via websocket. Quando um
alerta é recebido, no servidor, ele é encaminhado para a aplicação
através da conexão que foi iniciada.
Quando o alerta é recebido ele é apresentado na tela da apli-
cação de acordo com sua prioridade. As quais são apresentadas nas
próximas seções.
4.3.1
Prioridade Mínima e Prioridade Baixa. Para os alertas de
menor criticidade, exibimos um alerta textual sem ação necessária,
garantindo que o foco do conteúdo da TV não seja afetado.
Esse tipo de alerta pode ser utilizado para mensagens periódicas,
testes, status, relógio meteorológico ou qualquer contexto que não
exija leitura ou ação obrigatória por parte de quem está assistindo
à TV no momento.
As Figuras 3 e 4 apresentam exemplos desse alertas.
Figure 3: Prioridade Mínima
Figure 4: Prioridade Baixa
4.3.2
Prioridade Moderada. O alerta moderado traz mais destaque
à mensagem, podendo ocupar mais espaço e incluir um acionável
associado para visualizar mais detalhes ou iniciar algum serviço
ou aplicação. Ao contrário dos alertas anteriores, que se fecham
automaticamente, esse tipo de alerta precisa ser fechado direta-
mente por uma ação do usuário, garantindo sua persistência na tela,
confirme indicado na Figura 5.
Figure 5: Prioridade Moderada
A partir do alerta moderado, os recursos de acessibilidade como
Língua de Sinais e audiodescrição ficam disponíveis. Caso seja trans-
mitida mídia de áudio dentro do alerta, essa mídia será reproduzida.
Caso contrário, a aplicação utiliza um sintetizador de voz para
reproduzir o alerta textual em forma de áudio.
Se um usuário da aplicação estiver com o recurso de Língua de
Sinais ativo na TV, o alerta também será reproduzido em Língua de
Sinais, a partir do texto em glosa transmitido.
4.3.3
Prioridade Alta. O alerta alto, tem por objetivo chamar a
atenção do usuário para uma informação importante, tomando
grande parte do foco de tela durante sua chamada, apresentando
informações necessárias ao usuário, mas dando a ele opções de
interatividade, onde pode ver informações mais detalhadas sobre o
que lhe foi passado.
As Figura 6 apresenta a janela inicial do alerta, ao acessar o botão
"Ver Detalhes", ou clicar no controle remoto no botão ’1’, passa-se
para a tela de detalhes apresentada na Figura 7. Caso o recurso
de língua de sinais esteja ativo na TV, um avatar 3D apresenta o
conteúdo da glosa em formato de língua de sinais, confirme indicado
na Figura 8.
Figure 6: Prioridade Alta
Figure 7: Prioridade Alta Detalhes
Figure 8: Prioridade Alta com Língua de Sinais.
4.3.4
Prioridade Máxima. O alerta máximo por sua vez, representa
uma mensagem prioritariamente urgente, no qual é necessária in-
terromper toda a programação que possa estar sendo acessada pelo
usuário, dando informações críticas, com detalhes e imagens úteis.
A Figura 9 apresenta um exemplo dessa exibição.
WTVDI’2024, Juiz de Fora/MG, Brazil
Costa et al.
Figure 9: Prioridade Máxima
PRINCIPAIS DESAFIOS
Os principais desafios enfrentados durante o desenvolvimento deste
projeto incluíram:
Desenvolver uma arquitetura robusta para converter e transmitir
alertas de sistemas existentes, como CENAD e CEMADEN, para o
novo padrão de TV 3.0, garantindo compatibilidade com o proto-
colo CAP e o formato AEA. Além disso, implementar a tradução
automática de alertas para Língua de Sinais, utilizando tecnologias
como o tradutor VLibras e avatares 3D, exigiu colaboração entre
desenvolvedores e especialistas. A disponibilização offline do player
de língua de sinais e seu dicionário foi implementada e está fun-
cional. Contudo, ainda estão sendo pesquisadas formas de prover a
acessibilidade com avatar 3D sem a necessidade de ocupar espaço
de armazenamento com o dicionário de animações. Outro desafio
foi o trabalho de criar um sistema que mantivesse os alertas na tela
até serem manualmente fechados pelo usuário, especialmente para
alertas de alta prioridade, sem comprometer a usabilidade da TV.
CONCLUSÕES E TRABALHOS FUTUROS
O presente trabalho apresentou os requisitos de alertas de emergên-
cia, no âmbito no projeto TV 3.0. Verificou-se formas de apresen-
tação com acessibilidade, baseadas em língua de sinais e áudio
descrição. Foram geradas soluções para sua implementação, uma
proposta de extensão da API do Webservice, e de uma arquitetura
de sistema, o qual foi implementado como prova de conceito.
Como trabalhos futuros, pretende-se realizar testes em larga es-
cala para validar a eficácia do sistema em diferentes cenários de
emergência. Também pretende-se criar apps que integrem com a
API para exibir alertas sincronizados com a TV. Outro trabalho pos-
sível é de análise de impacto social dos alertas na TV 3.0, medindo
a eficácia e a satisfação dos usuários, especialmente entre pessoas
com deficiência.
Por fim, apesar de se encontrar ainda em fase de desenvolvimento,
é possível perceber a existência de uma contribuição científica,
tecnológica e social da proposta apresentada neste trabalho, uma
vez que essa solução, quando implementada na TV 3.0, pode trazer
benefícios para aproximadamente 8,8 milhões de deficientes visuais
e surdos brasileiros [4].

--- FIM DO ARQUIVO: 30511-829-24959-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30512-829-24960-1-10-20241001.txt ---
Sincronismo e Interatividade com a Segunda Tela frente à
Implementação da TV 3.0 no Brasil
Antonio Celestino
antonio.celestino@ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Brasil
Carlos Pernisa Júnior
carlos.pernisa@ufjf.br
Universidade Federal de Juiz de Fora
Juiz de Fora, Brasil
0.
A TV 3.0 tem em seu projeto potenciais avanços, entre os quais
a transmissão em alta definição com resolução em 4k e 8k, o áu-
dio imersivo que reverbera em mais de uma direção para que o
telespectador se sinta no ambiente da cena e a possibilidade de
integração com segundas telas representadas por dispositivos como
os smartphones. Também está sendo debatida a funcionalidade de
se criar um perfil de usuário para cada pessoa que utilizar a TV,
facilitando a recomendação de conteúdos e a continuidade de seu
consumo específico de programação. A navegação na TV 3.0 está
prevista para ser feita via aplicativos, de modo que o telespectador,
ao invés de escolher um canal numérico, irá acessar o aplicativo da
In: VII Workshop Futuro da TV Digital Interativa (WTVDI 2024). Anais Estendidos
do XXX Simpósio Brasileiro de Sistemas Multimídia e Web (WTVDI’2024). Juiz de
Fora/MG, Brazil. Porto Alegre: Brazilian Computer Society, 2024.
© 2024 SBC – Brazilian Computing Society.
ISSN 2596-1683
emissora de TV a exemplo do que é feito ao acessar uma plataforma
de streaming. Esse aplicativo dará entrada tanto para a programação
do fluxo normal quanto para os serviços e os conteúdos comple-
mentares que poderão ser disponibilizados. Essas funcionalidades
estarão disponíveis na TV aberta de forma gratuita.
Com relação aos conteúdos extras a serem disponibilizados pelas
emissoras, eles poderão ser demandados para aparecer na própria
tela da televisão ou serem direcionados para consumo em segundas
telas. A segunda tela pode ser um dispositivo de visualização que
tenha capacidade de receber conteúdos por rede ou pareamento e
citaremos, como exemplo, no decorrer deste artigo, os smartphones.
Defendemos que as informações extras exibidas em uma segunda
tela devam estar em sincronia com o fluxo transmitido na tela
principal. Para conceituação neste artigo, sincronia será entendida
como a condição de dois ou mais fenômenos ou fatos que ocorrem
simultaneamente.
Nossa questão principal é que a falta de sincronia entre a trans-
missão do fluxo principal e os conteúdos extras disponibilizados
pode prejudicar a experiência imersiva do telespectador. Nesse
ponto, transmitir os materiais extras no próprio sinal da TV daria
mais segurança ao sincronismo, ao invés de apenas disponibilizá-los
na Internet à mercê de delay, instabilidade ou até mesmo indisponi-
bilidade de sinal de rede do telespectador. Este artigo tem, portanto,
como objetivos: reforçar o papel da segunda tela em auxílio ao
consumo de informações complementares na TV 3.0 e explanar
sobre a importância da sincronia entre conteúdos para incremen-
tar a interatividade e potencializar a imersão do telespectador na
programação televisiva.
Antes, porém, de adentrarmos na abordagem da sincronia de
conteúdos entre telas na TV 3.0, cabe-nos explanar sobre o processo
de desenvolvimento, deliberação e escolha do padrão brasileiro para
a TV digital e, como decorrência, para a TV 3.0.
DESENVOLVIMENTO DA TV DIGITAL NO
BRASIL
Os estudos e deliberações para a implantação da TV 3.0 no Brasil
estão sendo conduzidos pelo Sistema Brasileiro de TV Digital Ter-
restre (SBTVD), que atua no desenvolvimento da TV digital no
Brasil desde o ano de 2003, quando foi criado por meio do de-
creto federal nº. 4.901 [3]. O SBTVD é vinculado ao Ministério das
Comunicações e tem foco na transmissão digital terrestre pelas
frequências hertzianas atmosféricas que é onde se concentram as
emissoras de sinal aberto de televisão no Brasil.
Os debates acerca da TV digital no Brasil ocorrem no Fórum do
Sistema Brasileiro de TV Digital Terrestre, que é uma associação
autônoma e sem fins lucrativos constituída para dar assistência ao
SBTVD no desenvolvimento de normas, padrões e regulamentação
WTVDI’2024, Juiz de Fora/MG, Brazil
Antonio Celestino and Carlos Pernisa Júnior
técnica voluntários ou obrigatórios. O Fórum constitui-se em um
ambiente de realização de debates, negociações e integração entre
representantes oriundos dos setores acadêmico (universidades e
centros de pesquisa), radiodifusão, recepção (fabricantes de televi-
sores e receptores), transmissão (fabricantes de transmissores) e
empresas de software, tendo em vista auxiliar no estabelecimento
de especificações técnicas da TV digital brasileira, obedecendo a
padrões e normas internacionais.
O Fórum do SBTVD institui, em seu estatuto social [11], o princí-
pio de estimular a melhoria no sistema de transmissão e recepção
de sons e imagens de televisão no Brasil, apoiando o governo na
adoção do sistema digital de televisão para o país, para que sejam
proporcionados padrões de qualidade de serviço compatíveis com
a exigência dos usuários e em benefício da população brasileira.
Em seu site, o Fórum traz a página TV 3.0 Project [14], que
tem o histórico das etapas que foram e estão sendo conduzidas no
desenvolvimento da transmissão terrestre de televisão no Brasil.
Etapas essas que começaram na TV 1.0 analógica em preto e branco,
evoluindo para a TV 1.5 analógica em cores, depois para a TV 2.0
digital, chegando à atual TV 2.5 digital com recursos de interativi-
dade até as fases do desenvolvimento da TV digital 3.0.
2.1
O padrão brasileiro de TV Digital
Na implantação do sinal de TV digital para as emissoras de sinal
aberto no país, o SBTVD adotou o padrão japonês ISDB-T (Inte-
grated Services Digital Broadcasting Terrestrial), que, adaptado ao
Brasil, originou o ISDB-TB. Já para estabelecer o padrão brasileiro
da TV digital 3.0, a Secretaria de Comunicação Social Eletrônica
do Ministério das Comunicações está analisando as propostas do
padrão norte-americano ATSC 3.0 e do padrão Advanced ISDB-T
que é japonês [12].
Essas duas propostas estão sendo analisadas na fase 3 do projeto
da TV 3.0, com testes complementares de laboratório e de campo
para avaliar a capacidade da configuração da tecnologia da sua
camada física de atender às demandas de transmissão no Brasil.
Ainda nesta fase 3, estão sendo desenvolvidas as adaptações e as
extensões necessárias para a especificação da camada de transporte
de dados, juntamente com uma implementação de referência para
os sistemas multiplexador (seletor de dados) e demultiplexador
(distribuidor de dados).
Para a finalização da fase 3 também está sendo avaliada a quali-
dade das tecnologias de codificação de vídeo TV 3.0, assim como o
desenvolvimento de adaptações e extensões para DTV Play for TV
3.0 com um Ambiente de Desenvolvimento Integrado (IDE). Junto
a esse desenvolvimento, está em curso a finalização das normas
técnicas, das diretrizes operacionais e dos testes de conformidade
para TV 3.0 para, em seguida, serem feitas as demonstrações de
sistema ponta a ponta.
Diante do avanço nas pesquisas e no desenvolvimento e imple-
mentação das tecnologias na fase 3, o governo tem expectativa de
que as primeiras transmissões em TV 3.0 no Brasil sejam realizadas
no ano de 2025 [2] [13], devendo os trabalhos de regulamentação
serem concluídos até 31 de dezembro de 2024 conforme disposto
no decreto federal nº 11.484 [4].
2.2
O Ginga e a Linguagem NCL
Para realizar a conexão do hardware dos aparelhos de televisão com
os aplicativos da TV digital, o SBTVD escolheu o middleware Ginga,
que foi criado em 2007 por meio de pesquisa feita em parceria entre a
Pontifícia Universidade Católica do Rio de Janeiro e a Universidade
Federal da Paraíba [8]. O Ginga usa a linguagem de programação
NCL (Nested Context Language) e provê os aparelhos de TV com o
ambiente declarativo Ginga-NCL.
A adoção do Ginga como padrão determina que os aparelhos
televisores de quaisquer fabricantes tenham esse software embar-
cado, o que garante ao consumidor poder usufruir dos recursos
básicos de interatividade e do uso da Internet na TV. O Ginga está,
inclusive, disponibilizado para uso em outros países e é reconhecido
pela União Internacional de Telecomunicações (UIT) como padrão
IBB – Integrated Broadcast-Broadband desde 2018 [8].
Muitos dos recursos pretendidos para a TV 3.0, embora ainda
não utilizados na transmissão de televisao aberta realizada atual-
mente no Brasil, já estão previstos na documentação da linguagem
NCL. Entre esses recursos podemos citar, dos elencados por Luiz
Fernando Soares e Simone Barbosa [15]: os casos de interação do
usuário, a adaptação do conteúdo e da forma como o conteúdo é
exibido, a edição dos conteúdos em tempo de exibição (ao vivo),
a adoção de múltiplos dispositivos de exibição e a definição, de
forma separada, do conteúdo dos objetos de mídia e do sincronismo
espacial e temporal em eles.
Para a implantação da TV Digital 3.0 no Brasil, o Ginga deverá ter
adaptações. Uma das mudanças diz respeito ao ponto de entrada dos
comandos que partem do telespectador. Até então, esses comandos
se dão por seleção de canais, mas passarão a ser feitos por seleção
de aplicativos.
2.3
A criação de um perfil para o telespectador
Outra mudança a ser implementada na TV 3.0 refere-se ao suporte
para a criação e seleção do perfil do usuário que irá assistir à TV.
Esse quesito é atendido pela versão superior da linguagem NCL
que foi demandada pelo governo, tendo sido recebida a proposta da
versão 4.0. De acordo com Fábio Barreto et al. [6], a NCL 4.0 propõe a
inserção, em seu código, do elemento geral <userBase>, o qual pode
conter vários elementos <userProfile> aos quais serão associados
os usuários. O elemento <userProfile> traz as características de um
determidado tipo de usuário (por exemplo: adulto ou infantil) e, a
partir dele, as pessoas poderão criar os seus perfis individuais que
irão conter dados como nome, idade, permissões e preferências de
programação, entre outros. Os dados são armazenados e executados
pelo Ginga que irá gerar dinamicamente os links para exibir os
perfis que irão acessar a TV. A criação de perfis, contudo, pode ser
opcional e, nesse caso, a TV deverá ter a opção de ser acessada com
um usuário default, que é padrão.
Na criação do perfil, os usuários serão classificados entre os tipos
definidos em <userProfile> para que o Ginga possa executar as
permissões programadas no código NCL. Essas permissões podem
habilitar, por exemplo, a exibição de determinados programas e
liberar ou não a realização de compras a partir das publicidades dos
produtos que aparecem na TV. A compra via TV e outros comandos
podem ser facilitados pela possibilidade do reconhecimento de
voz, a qual também estaria salva no perfil do usuário e a função
Sincronismo e Interatividade com a Segunda Tela frente à Implementação da TV 3.0 no Brasil
WTVDI’2024, Juiz de Fora/MG, Brazil
seria realizada pelo conector “onVoiceRecognitionSet” presente no
código NCL [6]. De acordo com a voz reconhecida, a TV saberia
qual pessoa está tentando realizar a compra e, após conferir as
permissões do perfil, habilitaria ou não o procedimento.
Frisamos que, para que a compra seja realizada, a emissora de
TV precisaria também disponibilizar em sua transmissão um canal
de retorno de dados para que o telespectador possa enviar a sua
solicitação de compra. Esse canal seria o que Luiz Fernando Soares e
Simone Barbosa [15] denominaram canal de retorno unidirecional,
o qual permite ao receptor apenas o envio de dados para solicitar as
compras ou votar em uma enquete, por exemplo. Essa ação deverá
ser feita em sincronia com a transmissão da TV.
O SINCRONISMO DA TV 3.0 COM A
SEGUNDA TELA
Na TV Digital 3.0, a transmissão do conteúdo televisivo será feita
com o sinal via radiodifusão e via banda larga de forma conjugada.
Essa integração potencializará uma maior interatividade, por per-
mitir ao telespectador buscar, sob demanda, conteúdos específicos
sobre a programação que está sendo veiculada no fluxo normal,
cujos resultados serão exibidos na própria tela da televisão ou em
seus smartphones. Com essa transmissão conjugada, a imersão em
conteúdos alternativos via banda larga tende a ser cada vez mais
automática dentro da TV 3.0.
Estabelece-se assim, a possibilidade de um programa televisivo
não-linear, que caracteriza-se como sendo um programa de TV que
é composto não apenas pelo áudio principal e vídeo principal, mas
também por outros dados extras que são transmitidos em conjunto.
Esses dados podem ser outros áudios e vídeos, imagens, textos, etc.,
os quais o telespectador pode acessar na segunda tela ou sobrepondo
os próprios áudios e vídeos principais na tela da TV. Para isso é
necessário que o aparelho de TV seja dotado de uma aplicação que
possibilite o relacionamento temporal e espacial desses objetos de
mídia, incluindo o vídeo principal e o áudio principal [15].
3.1
O auxílio da segunda tela
Apesar da potencialidade de se realizar várias atividades na própria
tela da TV, como uma compra por exemplo, consideramos útil
a utilização de uma segunda tela para interações individuais. A
segunda tela daria não somente mais privacidade para o usuário
como também evitaria ruidos de comunicação em um ambiente no
qual haja mais de uma pessoa assistindo à TV. Dada a variedade de
conteúdos interativos que podem ser produzidos e disponibilizados
em uma transmissão televisiva digital, em especial a projetada para
a TV 3.0, a segunda tela tem um papel importante no planejamento
da disposição desses conteúdos.
As características de disposição e de exibição de um objeto de mí-
dia televisiva (vídeo, imagem, áudio, etc.) é definida no documento
NCL, desde as suas primeiras versões, pela inserção do elemento
<regionBase>, que pode ainda adaptar o tamanho e posicionamento
dessas mídias através do seu subelemento <region> [15]. O ele-
mento <regionBase> terá como identificador o tipo de dispositivo
onde o conteúdo será exibido (tela da televisão, smartphone, etc.).
Para isso, o seu atributo device deve ser associado aos valores “sys-
temScreen(i)” ou “systemAudio(i)”, substituindo o i pelo índice
numérico do dispositivo a ser utilizado. Quando não houver es-
pecificação de outros dispositivos para a exibição, por padrão, a
televisão exibirá o conteúdo na sua própria tela.
Uma vez que a forma de exibição do conteúdo interativo seja
definida, é necessário que os objetos de mídia transmitidos estejam
em sincronia, para que possam ser consumidos em tempo real com
o fluxo normal da programação. Isso implica que, se for disponibi-
lizada uma informação extra sobre uma cena da TV ou o anúncio de
um produto mostrado por alguma personagem na tela, é necessário
que essa informação já esteja disponível para exibição enquanto a
cena está em curso.
Essa é uma condição subentendida quando se trata de exibição
na própria tela da TV, desde a transmissão linear onde o relaciona-
mento entre as mídias (inserção de publicidade, por exemplo) já vem
determinado dos estúdios da emissora até na transmissão não-linear
onde o telespectador decide, sob demanda, com quais conteúdos irá
interagir. Contudo, no caso de exibição dessa informação em uma
segunda tela, por haver transferência e interação de dados entre dis-
positivos distintos, há um desafio maior para o estabelecimento da
sincronia, motivo pelo qual defendemos que a segunda tela deverá
estar conectada diretamente à TV para que também possa acessar
os seus conteúdos em tempo real.
3.2
A conexão da segunda tela com a TV
Para que o smartphone funcione como segunda tela para receber
conteúdos sobre a programação veiculada na TV 3.0, ele precisa se
conectar ao aparelho de TV. Essa conexão poderá ser feita utilizando-
se um aplicativo do fabricante da TV, a ser instalado no smartphone
assim que o aparelho de TV for comprado. É uma ação semelhante
à que é feita no caso da lâmpada inteligente que, para ser contro-
lada pelo smartphone, exige que se baixe no aparelho o aplicativo
do fabricante. Carlos Pernisa Júnior et al. [5] ressaltam a neces-
sidade de que a conexão dos dispositivos de segunda tela com o
aparelho receptor de TV digital seja feita diretamente, por meio de
um aplicativo específico a ser instalado no smartphone.
Cada emissora poderá ter a sua maneira própria de disponibilizar
seus conteúdos extras ou de habilitar a interação do telespectador
em tempo real, mas a conexão do dispositivo de segunda tela com o
aparelho de TV terá que ser feita em primeiro lugar, para usufruir
dessa interatividade. Para isso, a TV deverá ter um software capaz
de receber e transmitir esses conteúdos a outros dispositivos. Karen
S. S. Oliveira et al. [7] destaca que a funcionalidade de suporte da
TV digital para a transmissão de conteúdos a múltiplos dispositivos
utilizando a linguagem NCL e o middleware Ginga não é nova, mas
ainda existe uma carência de uma definição clara de protocolos para
descoberta, registro e comunicação com dispositivos remotos.
A forma de conexão do smartphone com o Ginga do aparelho de
televisão no modelo atual da TV digital 2.5 é feita através do subsis-
tema Ginga Commom Core Web Services (CCWS), o qual deverá ser
mantido para a TV 3.0. Na sua versão atual, o Ginga CCWS já traz
a possibilidade de pareamento de dispositivos móveis através de
QRCode ou PIN, a sincronização de dados de perfil de usuário entre
a TV e o dispositivo móvel, a apresentação de conteúdo adicional
em segunda tela como replays ou imagens de câmeras distintas
e também a função T-commerce para a realização de compras no
smartphone de produtos anunciados na tela da TV [17].
WTVDI’2024, Juiz de Fora/MG, Brazil
Antonio Celestino and Carlos Pernisa Júnior
Visando privilegiar a experiência individual de cada usuário,
ressaltamos que, no momento da identificação do dispositivo de
segunda tela que está acessando a TV, também seja feita a requi-
sição de identificação do usuário. Dessa forma, mesmo que na TV
esteja sendo exibido um programa no perfil adulto, uma criança que
interaja pelo smartphone deverá ter o seu dispositivo identificado
como perfil infantil. Dentro do próprio perfil adulto também, se o
fluxo da tela principal estiver sendo exibido para o usuário A, as
múltiplas interações realizadas em segundas telas deverão ter os
seus usuários A, B, C, etc. identificados. Propostas nesse sentido
já existem, como a apresentada por Karen S. S. Oliveira et al. [7],
na qual o atributo “user” é utilizado, trazendo como parâmetro um
<userId> que identifica o usuário que demandou a interação.
Consideramos a conexão com uma segunda tela uma ação im-
portante a ser relizada na TV 3.0. Destacamos, nesse âmbito, dois
pontos relevantes. O primeiro refere-se à facilidade de executar na
tela do smartphone alguns comandos como preencher formulários e
receber informações de acessibilidade ou conteúdos extras sobre um
determinado programa. O segundo ponto diz respeito à dinâmica da
audiência coletiva pois, dado que a programação televisiva pode ser
assistida por mais de uma pessoa simultaneamente na tela principal,
trazer os conteúdos extras para a segunda tela não prejudicaria a
experiência de outro telespectador também presente no ambiente,
mas que esteja interessado em somente consumir o fluxo linear da
programação.
3.3
A sincronia como condição para a imersão
do telespectador
As emissoras podem disponibilizar para o telespectador informações
sobre os atores de uma novela, a classificação em tempo real de
um campeonato de futebol, trechos extras de vídeos ou até uma
enquete sobre um programa que esteja sendo exibido. Hoje essa
interação na TV aberta já pode ser realizada, contudo, não via
aplicativo de TV e sim com o telespectador utilizando-se do plano
de Internet do seu smartphone para buscar na web esses conteúdos.
Essa prática de disponibilizar as informações extras na web tem
pontos negativos, pois, além de exigir que o cidadão esteja com o
seu smartphone sempre conectado à Internet, ainda haveria o risco
de uma instabilidade no tráfego de dados prejudicar ou mesmo
inviabilizar o sincronismo entre o conteúdo que o telespectador
está recebendo e o que ele está vendo na tela principal.
No caso do consumo de informações na segunda tela, realizar a
transmissão do conteúdo interativo no próprio sinal emitido para
a TV 3.0, para que ocorra o seu posterior envio direto da TV para
o smartphone, minimizaria o risco de ocorrer assincronia e poten-
cializaria o seu consumo simultâneo. Tal ação pode melhorar a
experiência do telespectador, fato destacado por Stanley Teixeira
[16] ao evidenciar que a sincronia entre eventos presentes em vários
espaços estabelece os links necessários para uma continuidade nar-
rativa. Tomando como exemplo a transmissão de uma corrida de
Fórmula 1, a emissora poderia disponibilizar no seu sinal, para visu-
alização, uma tabela de classificação interativa, na qual a pontuação
dos pilotos se modifique em tempo real, de acordo com as suas
posições na pista e as ultrapassagens realizadas.
Essa transmissão no próprio sinal é importante, pois, da mesma
forma que o áudio imersivo que se pretende implementar na TV
3.0 vai dar ao usuário a sensação de entrar no ambiente da cena,
os conteúdos extras interativos disponibilizados também devem
contribuir para imergir ainda mais o telespectador na programação
transmitida. A sincronia entre esses conteúdos, seja na tela principal
ou com a utilização de segunda tela, seria, portanto, uma forma de
contribuir para essa pretendida imersão do telespectador.
No exemplo da corrida de Fórmula 1, o fato de ver na segunda
tela, em tempo real e interativo, as consequências do que está se
passando na tela principal, aumentaria sobremaneira a experiência
do telespectador com o conteúdo televisivo que está consumindo.
Para isso, a sincronia entre ambas as telas é fundamental para que
se dê essa imersão. Entendemos aqui como imersão a experiência
que, de acordo com Carlos Pernisa Júnior [9], faz o indivíduo entrar
no ambiente da história que está sendo contada. Essa “entrada”, por
tomar um ou mais sentidos, como audição e visão, decorreria em
uma suspensão temporária da realidade externa em benefício da
cena que estaria sendo transmitida na TV.
Tomando como base a divisão dos tipos de imersão estabelecida
por Dominic Arsenault [1] em sensorial, sistêmica e ficcional, con-
sideramos a imersão sensorial a pretendida no sistema da TV 3.0.
Massarolo e Mesquita [10] destacam que a imersão sensorial foca
os estímulos sensoriais vivenciados, sendo uma dimensão provo-
cada por ferramentas técnicas que dão, por meio de estímulos que
vão da visão ao tato, uma maior consistência sensorial ao mundo
construído; no nosso caso, ao programa veiculado.
Portanto, a nossa observação é a de que os conteúdos extras
deverão ser transmitidos no próprio sinal da TV 3.0 e enviados
para o smartphone diretamente da TV. Dessa forma, a sincronici-
dade estaria preservada, pois essas informações extras já estariam
disponíveis para consumo ao mesmo tempo em que a programação
principal, integrando-se conforme sejam demandadas. Assim, es-
taria sendo garantida a sincronia para que se busque manter o
telespectador imerso no ambiente da programação, pois levá-lo a
adentrar em outro ambiente web para obter esse conteúdo quebraria
a imersão sensorial pretendida.
DESAFIOS E APONTAMENTOS
Embora o Brasil viva um cenário onde existem municípios que
ainda não estejam usufruindo da digitalização do sinal de TV, o
avanço da TV Digital para a TV Digital 3.0 acontecerá dentro dos
próximos anos. É claro que essa mudança será gradativa, princi-
palmente pelos custos envolvidos tanto para a transmissão quanto
para a recepção do sinal. As normas para a fabricação de aparelhos
que suportem essa nova tecnologia ainda estão sendo debatidas
entre as representações presentes no Fórum SBTVD, procurando-se
atender as demandas dos fabricantes, das emissoras e do público
telespectador.
Para que a interatividade na TV 3.0 se dê sem risco de assincro-
nia, será recomendável que as emissoras transmitam o conteúdo
interativo no próprio sinal a ser recebido pela TV junto com o
fluxo normal, ao invés de simplesmente disponibilizá-lo na Internet.
Para isso, a rotina de produção deverá ser ajustada, para que sejam
produzidos e transmitidos no mesmo sinal do fluxo linear da progra-
mação conteúdos que deverão estar à disposição do telespectador
para consumo sob demanda.
Sincronismo e Interatividade com a Segunda Tela frente à Implementação da TV 3.0 no Brasil
WTVDI’2024, Juiz de Fora/MG, Brazil
Para os fabricantes, o desafio é produzir aparelhos que tenham
softwares capazes de realizar na tela da TV, de maneira individual,
o trabalho de edição que pode ser demandado pelo telespectador,
como redimensionamento de tela, sobreposição de vídeos, ampli-
ação de imagens, efeitos de transição, inserção de sons, etc. Também
deverá esse aparelho, como defendemos aqui, ter capacidade de se
conectar e transmitir conteúdos para segundas telas.
Com relação ao telespectador, um primeiro desafio pode ser o
custo de aquisição do aparelho de TV 3.0, o que exigirá do governo
uma política de transição gradual do sinal, assim como foi feito na
mudança do sinal analógico para o digital atual. Também caberá
ao cidadão acostumar-se com os recursos de interatividade e a
assistir à TV numa experiência que envolverá a tela principal e os
dispositivos de segunda tela.
Este artigo é uma provocação aos desafios de sincronia e de
interatividade na implantação da TV 3.0 no Brasil, questões entre
outras que estão sendo debatidas no Fórum do Sistema Brasileiro
de TV Digital Terrestre.

--- FIM DO ARQUIVO: 30512-829-24960-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30513-829-24961-1-10-20241001.txt ---
Ferramenta de Validação NCL: Aprimorando o processo de
desenvolvimento de aplicações de TV Digital
Iago Victor Silva Costa
iago.vsc@discente.ufma.br
Universidade Federal do Maranhão
Li-Chang Shuen
li.chang@ufma.br
Universidade Federal do Maranhão
Daniel de Sousa Moraes
danielmoraes@telemidia.puc-rio.br
TeleMídia Lab - PUC-Rio
Joel André Ferreira dos Santos
jsantos@eic.cefet-rj.br
CEFET/RJ
Débora C. Muchaluat-Saade
debora@midiacom.uff.br
Lab. MídiaCom
Universidade Federal Fluminense
Carlos de Salles Soares Neto
carlos.salles@ufma.br
Universidade Federal do Maranhão
WTVDI’2024, Juiz de Fora/MG, Brazil
Costa et al.
A Ferramenta de Validação NCL foi avaliada através da aplicação
de um roteiro de atividades formatado em um questionário divi-
dido por tópicos, no qual foram respondidas algumas perguntas de
múltipla escolha e feitos comentários sobre a ferramenta ao final
da realização das tarefas de cada uma dessas etapas.
Tal roteiro de atividades foi criado com base no modelo de ques-
tionário SUS (System Usability Scale), em que o participante da
pesquisa responde cada pergunta em uma escala de 1 (discorda
completamente) a 5 (concorda completamente). No contexto desta
pesquisa, a escala foi definida no aspecto de o quanto a ferramenta
ajudou na identificação de erros no código ao longo de sua escrita:
1 - Nada - a 5 - Completamente.
Por ter envolvido a participação de seres humanos durante o
processo de ponderação da ferramenta, foi necessário submeter a
pesquisa à avaliação pelo Comitê de Ética da Universidade Federal
de Juiz de Fora, por meio da Plataforma Brasil, registrada sob o
número 79915024.4.1001.5147.
Este artigo está dividido da seguinte forma: na Seção 2 são ex-
ploradas a etapa de levantamento de requisitos e a arquitetura da
Ferramenta de Validação NCL; já na Seção 3 são discutidos os re-
sultados obtidos pela aplicação do roteiro de atividades; na Seção 4,
são feitas as considerações finais sobre os resultados e a utilidade
da ferramenta desenvolvida.
FERRAMENTA DE VALIDAÇÃO NCL
A presente seção fornece uma discussão sobre o processo de le-
vantamento de requisitos e descreve a arquitetura proposta para a
ferramenta.
2.1
Levantamento de Requisitos
Uma das metas do projeto de TV 3.0 é a disponibilização de um
ambiente que facilite o processo de desenvolvimento de aplicações
interativas para TV Digital. Tal ambiente, além de outras funciona-
lidades, traz a validação em tempo real de código NCL, realizada
tanto pelo Schema XML que define a linguagem, quanto por uma
ferramenta de validação estrutural [1] [11]. A partir de reuniões
internas da equipe de Pesquisa e Desenvolvimento (P&D), foram
estabelecidos os seguintes tipos de validação estrutural que este
módulo deveria executar:
Validação de Atributos - Verifica se atributos relacionados
possuem valores corretos
Validação de Referências - Verifica se elemento referenciado
é do tipo correto
Validação de Composicionalidade - Verifica se referências
são feitas para elementos dentro da composição
Validação de Aninhamento de Composições - Verifica se
há loops de aninhamento causados por reúso
Validação de Reúso de elementos - Verifica se há loops de
reúso
A fim de atender a estas validações do ambiente de autoria,
inicialmente foram levantadas as tags que estão envolvidas em
cada um desses tipos de validação, bem como as regras e relações
associadas a tais tags. No decorrer da sondagem por estas tags,
também fez-se importante entender o funcionamento de algumas
áreas funcionais da NCL, visando delimitar a abrangência de cada
Tabela 1: Áreas funcionais da NCL 3.0 (1/3)
Tabela 2: Áreas funcionais da NCL 3.0 (2/3)
tipo de validação. Tomou-se como base a tabela apresentada nas
Tabelas 1, 2 e 3, retiradas do livro Programando em NCL 3.0 [14].
Durante todo o processo de estudo da linguagem, foi gerada
uma tabela, acessível a partir do link no apêndice, que consistiu
em reunir algumas das tags da linguagem NCL e seus respectivos
atributos para determinar quais estavam envolvidos no que se quis
verificar.
Para a primeira versão da ferramenta de validação, apenas os ele-
mentos utilizados com mais frequência foram considerados, tendo-
se a intenção de acrescentar mais à medida que o ambiente for
atualizado. Na tabela A, as células marcadas em cinza indicam ele-
mentos que possuem atributos relacionados a alguma validação,
Ferramenta de Validação NCL: Aprimorando o processo de desenvolvimento de aplicações de TV Digital
WTVDI’2024, Juiz de Fora/MG, Brazil
Tabela 3: Áreas funcionais da NCL 3.0 (3/3)
enquanto as marcadas em amarelo apontam os atributos propri-
amente ditos. Na coluna “VALIDAÇÃO” estão descritas o que foi
validado com respeito ao atributo correspondente.
2.2
Arquitetura da Ferramenta de Validação
NCL
A Ferramenta de Validação NCL está sendo desenvolvida como
uma extensão para o editor de texto Visual Studio Code [7] e é
compatível com as versões 1.82.0 em diante. Extensões permitem
adicionar linguagens, depuradores e ferramentas para dar suporte
ao trabalho executado.
A ferramenta deve ser instalada no Visual Studio Code como
uma extensão, que se comunica com a API de extensões disponi-
bilizada pelo editor para personalizar seu comportamento - neste
caso, indicar textual e visualmente erros e/ou ações inesperadas
diretamente no código-fonte. O diagrama de componentes ilustrado
na Figura 1, descreve a intregação da Ferramenta de Validação NCL
com a API de extensões fornecida pelo VS Code.
Visual Studio Code
Funções para customizar
o funcionamento do VS
Code (Mensagens no
código-fonte, novos
componentes na UI, etc.)
Aplicação NCL
Mensagens
de alerta/erro
no código-fonte
API
Extensões
Ferramenta
de
Validação
Arquivo
NCL
import
import
Arquivo
NCL
Arquivo
NCL
Figura 1: Diagrama de Componentes
Conforme mostra a Figura 2, ao abrir um arquivo do tipo NCL
(“arquivo.ncl”, por exemplo) e/ou realizar alguma forma de edição,
como apagar e escrever texto ou salvar alterações feitas, a ferra-
menta guarda uma representação hierárquica de todo o documento
- a árvore DOM [4] -, contendo cada tag, suas tags “filhas”, atributos
e outros, a partir dos quais verifica o atendimento do código às
regras estabelecidas na norma da linguagem NCL.
Abrir arquivo NCL no VS Code
onDidOpenTextDocument()
new domParser().parseFromString()
Ferramenta de
Validação
API Extensões
[Validação de Documento]
Iago Victor | August 28, 2024
Desenvolvedor
[document.fileName.endsWith('.ncl')] createDiags()
domParser
Objeto XMLDocument
Representação
Hierárquica
diagnostics
Mensagens
de
Erro/Alerta
diagCollection.set(docURI, diagnostics)
Laço
[onDidChangeTextDocument()]
Figura 2: Diagrama de Sequência (Validação de Documento)
Figura 3: Diagnóstico de Erro
Na Figura 3, a ferramenta aponta que o identificador referenciado
pelo atributo component da tag <port> não existe. Neste exemplo,
o atributo deveria referenciar a tag <media> imediatamente abaixo,
mas o identificador referenciado está escrito sem a letra “i”. Por-
tanto, não havendo nenhuma tag com id “vdeo1”, existe um erro
em <port>.
RESULTADOS
Após a criação e testes do roteiro de avaliação da Ferramenta de
Validação NCL, foi solicitada a realização individual das atividades
nele presente a alunos de graduação e pós-graduação em cursos da
área de Computação de diferentes universidades que soubessem
codificar utilizando a linguagem NCL em um nível ao menos básico
- criação de aplicações com comportamentos definidos por elos.
Foram obtidas sete respostas ao roteiro, as quais configuraram o
quadro de avaliações da Figura 4, onde cada seção é representada
por uma barra.
Note que, para o participante 6, a avaliação da seção sobre Reuso
em NCL é zero, opção referente à impossibilidade de completar
todos os passos. Tal opção foi adicionada devido à abrangência das
validações definidas durante o projeto da ferramenta, que levou a
um certo salto de dificuldade nas tarefas a serem executadas. Uma
situação semelhante ocorreu com o participante 7, com a diferença
de que, além da questão de conhecimento da linguagem, também
há o fato de que ele utilizou o OSS Code, uma versão de código
aberto do Visual Studio Code sem código proprietário da Microsoft,
que possivelmente causou erros internos durante a execução da
extensão da ferramenta.
WTVDI’2024, Juiz de Fora/MG, Brazil
Costa et al.
Avaliação da Ferramenta
Participante
Region & Descriptor
Link & Connector
Composicionalidade
Reuso
Avaliação Geral
Figura 4: Avaliações em cada Seção por Participante
Participante
Média de Avaliação
Média das Avaliações do Participante
Avaliação Geral (Seção 5) do Participante
Figura 5: Média de Avaliação por Participante
Apesar destes impasses, a maioria das avaliações foi de pontuação
4 - a ferramenta ajudou muito na identificação de erros (maioria
deles) - e 5 - todos os erros foram identificados e corrigidos devido
à ferramenta -, apontando para a satisfação de se ter um validador
que assinale falhas em tempo real.
Como se pode observar na Figura 5, não houve disparidade entre
as médias das avaliações (primeiras 4 seções) de cada participante e
suas respectivas avaliações gerais (dadas na seção 5 do roteiro) para
a maioria dos casos, o que demonstra constância tanto do impacto da
Ferramenta de Validação ao longo das áreas funcionais consideradas
nesta primeira versão quanto das respostas dos voluntários deste
estudo, refletindo sua veracidade.
Em relação ao caso do participante 7, a média de suas avaliações
foi baixa devido à não completude dos passos em 2 seções, derivando
daí avaliação 0 em ambas. No entanto, considerando-se apenas as
outras duas seções, que foram completamente feitas, a média passa
a ser de 4.5/5, bem mais próxima à classificação geral.
Segundo a Figura 6, apesar da dificuldade acentuada dos passos
do roteiro a partir da seção sobre Composicionalidade (seção 3), a
menor média que a ferramenta atingiu foi de 3.1/5, enquanto as
outras se aproximaram mais de 4.
Por fim, apesar das avaliações quantitativas de média alta, dois
problemas pontuais foram explicitados por meio de comentários
feitos pelos participantes: (i) É acusado erro na importação de do-
cumentos caso o valor do atributo documentURI contenha os ca-
racteres "./", usados comumente para se referir ao diretório onde o
documento aberto se encontra, o que causa a notificação incorreta
de erro tanto na importação quanto nas referências aos elementos
importados, mesmo que estejam corretas; (ii) Os erros marcados
não são exibidos na aba Problems do VS Code, sendo possível visua-
lizar suas descrições apenas ao posicionar o cursor em cima de sua
indicação visual.
4,7
3,7
4,3
3,1
4,3
Seção
Média de Avaliação
0,0
1,0
2,0
3,0
4,0
5,0
Figura 6: Média de Avaliação por Seção
CONSIDERAÇÕES FINAIS
O objetivo deste trabalho foi avaliar a utilidade de um validador
para a linguagem NCL, que facilita e acelera a criação de aplicações
interativas de Televisão. Sendo parte do Ambiente de Autoria, tec-
nologia proposta a fim de suprir um dos requisitos do projeto TV
3.0, este validador, referido como Ferramenta de Validação NCL,
poderá impulsionar o uso da linguagem na produção de software
voltado à TV Digital brasileira.
Tanto no decorrer da elaboração deste trabalho, quando outros
integrantes das equipes de Pesquisa & Desenvolvimento do projeto
TV 3.0 puderam utilizar a ferramenta, quanto no momento da aná-
lise das respostas do roteiro, observou-se que ter acesso simples e
rápido a uma ferramenta que norteie o desenvolvedor durante a
escrita de cada parte do programa é de grande valia.
Os resultados apresentados neste trabalho são qualitativos e com-
postos de uma pequena amostra de voluntários. Não é tão simples
alcançar um número maior de voluntários, dadas as especificidades
necessárias para tal. No entanto, como trabalho futuro espera-se
Ferramenta de Validação NCL: Aprimorando o processo de desenvolvimento de aplicações de TV Digital
WTVDI’2024, Juiz de Fora/MG, Brazil
confirmar os resultados preliminares apresentados neste artigo com
um grupo maior de usuários.
Outro desdobramento possível é fazer uma avaliação sobre os be-
nefícios na integração da ferramenta com extensões NCLua e HTML
da IDE Microsoft Visual Studio. Neste estudo futuro pretende-se
medir o impacto de se ter uma IDE integrada que permita a criação
de aplicações de TV digital que sejam tanto declarativas quanto
imperativas.
AGRADECIMENTOS
Os autores agradecem ao Ministério da Comunicações, Fórum
SBTVD e RNP pelo suporte financeiro para este trabalho.

--- FIM DO ARQUIVO: 30513-829-24961-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30514-829-24962-1-10-20241001.txt ---
Geração automática de SDK em ES6 para APIs do Ginga CCWS
Raphael Abreu
raphael.abreu@midiacom.uff.br
Laboratorio MídiaCom
Universidade Federal Fluminense
Niterói, Brasil
Joel A. Ferreira dos Santos
jsantos@eic.cefet-rj.br
CEFET/RJ
Rio de Janeiro, Brasil
Débora C. Muchaluat-Saade
debora@midiacom.uff.br
Laboratorio MídiaCom
Universidade Federal Fluminense
Niterói, Brasil
WTVDI’2024, Juiz de Fora/MG, Brazil
Raphael Abreu, Joel A. Ferreira dos Santos, and Débora C. Muchaluat-Saade
Além dos trabalhos acadêmicos, diversas iniciativas governamen-
tais têm adotado o OpenAPI como padrão para a descrição de suas
APIs. No Brasil, o Portal Brasileiro de Transparência1 utiliza o Ope-
nAPI para documentar e disponibilizar seus serviços, facilitando o
acesso e a integração com dados públicos.
A disponibilização de especificações OpenAPI para serviços gov-
ernamentais e padrões tecnológicos, como a norma Ginga CCWS,
representa um passo importante para a comunidade de desenvolve-
dores. Essa prática promove a transparência, a interoperabilidade e a
inovação, facilitando a criação de aplicações e serviços que utilizam
esses recursos. A geração de SDKs a partir dessas especificações,
como o SDK JavaScript apresentado neste trabalho, simplifica ainda
mais o processo de desenvolvimento, permitindo que os desenvolve-
dores se concentrem na lógica de suas aplicações e contribuam para
um ecossistema mais rico e diversificado.
ESPECIFICAÇÃO OPENAPI DO GINGA
CCWS DA TV 2.5
Como primeira contribuição deste trabalho foi feita uma especifi-
cação OpenAPI para a norma Ginga CCWS [2] da TV 2.5. Ela de-
screve os endpoints, métodos, parâmetros, tipos de dados e respostas
esperadas da API. Ela serve como base para a geração automática do
SDK e garante a interoperabilidade entre diferentes implementações.
A especificação está disponível em formato YAML e pode ser visual-
izada em https://drive.google.com/file/d/1Bb0UV41ysQXhyEurePsOEaDl3VsvC_
Uc/view?usp=sharing testada utilizando ferramentas como o Swag-
ger Editor2.
O trecho de código na Figura 1 descreve um endpoint na API
Ginga CCWS: o /𝑑𝑡𝑣/𝑎𝑢𝑡ℎ𝑜𝑟𝑖𝑧𝑒, acessado via método GET. Ele serve
para solicitar o estado de autorização de um cliente, identificado pelo
parâmetro obrigatório 𝑐𝑙𝑖𝑒𝑛𝑡𝑖𝑑. Adicionalmente, o 𝑑𝑖𝑠𝑝𝑙𝑎𝑦−𝑛𝑎𝑚𝑒
é usado para apresentar o nome do cliente ao usuário durante o
processo de autorização. Parâmetros opcionais como 𝑝𝑚, 𝑘𝑥𝑝e
𝑘𝑒𝑦permitem lidar com diferentes métodos de pareamento e segu-
rança para clientes não locais. A especificação detalha as possíveis
respostas, incluindo sucesso (código 200) com exemplos de dife-
rentes cenários de acesso, e erro “não encontrado” (código 404) com
referências a exemplos de erros específicos.
De posse de uma da especificação OpenAPI, é possível gerar
documentação interativa e de fácil navegação, como a ilustrada
na Figura 2. Essa documentação permite que desenvolvedores ex-
plorem os endpoints da API, seus parâmetros e as respostas esper-
adas, facilitando a compreensão e o uso da API. Adicionalmente, a
especificação OpenAPI pode ser utilizada para gerar automatica-
mente testes que validam o comportamento da API, garantindo sua
qualidade e confiabilidade.
Ao clicar em um endpoint específico, é possível visualizar uma
descrição detalhada da rota, incluindo os parâmetros necessários,
exemplos de requisições e respostas, e até mesmo a possibilidade de
testar a rota diretamente na documentação. O endpoint /𝑑𝑡𝑣/𝑎𝑢𝑡ℎ𝑜𝑟𝑖𝑧𝑒
pode ser visualizado na Figura 3 agilizando o processo de desen-
volvimento e integração. Adicionalmente, a especificação OpenAPI
pode ser utilizada para gerar automaticamente testes que validam o
comportamento da API, garantindo sua qualidade e confiabilidade.
1https://api.portaldatransparencia.gov.br
2https://editor.swagger.io/
Figure 1: Exemplo especificação OpenAPI para a rota de au-
torização de clientes.
Figure 2: Exemplo de visualização de lista de APIs OpenAPI.
É importante ressaltar que a especificação OpenAPI permite a
flexibilidade na definição do host da API. Ao utilizar o SDK, o de-
senvolvedor poderá configurar o host da API de acordo com suas
Geração automática de SDK em ES6 para APIs do Ginga CCWS
WTVDI’2024, Juiz de Fora/MG, Brazil
Figure 3: Exemplo de visualização do endpoint /𝑑𝑡𝑣/𝑎𝑢𝑡ℎ𝑜𝑟𝑖𝑧𝑒
na interface gráfica.
necessidades, seja em um ambiente de desenvolvimento, homolo-
gação ou produção. Essa flexibilidade é fundamental para garantir
a adaptabilidade do SDK a diferentes cenários e infraestruturas.
GERADOR DE SDK CCWS
O gerador de SDK é um script Node.js que utiliza a especificação
OpenAPI do Ginga CCWS como entrada e gera automaticamente
o código JavaScript do SDK. Ele automatiza a criação de méto-
dos para cada endpoint da API, simplificando o uso dos serviços
Ginga CCWS. O código do gerador realiza as seguintes etapas: lê
o arquivo JSON da especificação OpenAPI da Ginga CCWS 2.5,
analisa-o para identificar os endpoints, métodos, parâmetros e
respostas da API, gera o código JavaScript do SDK com um ob-
jeto API contendo métodos para cada endpoint, e salva o código
em um arquivo sdk.js. A classe FetchWrapper encapsula a lógica
de comunicação HTTP usando a função fetch do JavaScript. O
gerador de SDK encontra-se em https://drive.google.com/file/d/
1XpBi6iEQU3cQAWF5ZCnvIwX0jObSWojR/view?usp=sharing
4.1
Usando o SDK gerado
O SDK gerado oferece uma API onde cada endpoint é representado
por um método com parâmetros correspondentes à especificação
OpenAPI. A Listagem 1 demonstra um exemplo de uso, importando
o SDK e realizando uma chamada à função dtvAuthorizeGet. Note
que é possível customizar o host da API, caso necessário.
Listing 1: Exemplo de uso do SDK gerado
import API from
' . / sdk . js ' ;
API . host =
' < ip >: < porta > ' ;
/ /
opcional
await API . dtvAuthorizeGet ( {
c l i e n t i d :
' xpto ' ,
displayName :
' xpto ' ,
pm:
' qrcode '
} )
. then ( ( value ) => {
console . log ( value ) ;
} )
. catch ( e r r o r => {
console . log ( e r r o r ) ;
} ) ;
Além disso, o gerador permite customizar a especificação Ope-
nAPI antes da geração do SDK, possibilitando a adaptação da API
às necessidades específicas do projeto, adicionando ou removendo
funcionalidades conforme necessário.
CONCLUSÃO
Este artigo apresentou um gerador de SDK em JavaScript para a
norma Ginga CCWS. Foi criada a especificação OpenAPI da norma
referente ao Ginga CCWS da TV 2.5 para usar como base do gerador.
O SDK gerado tem potencial de simplificar o desenvolvimento de
aplicações interativas para a TV Digital, promovendo a interoper-
abilidade e a adaptabilidade a futuras versões da norma.
A utilização do OpenAPI e do JavaScript permite que os desen-
volvedores aproveitem as vantagens de uma linguagem moderna
e amplamente utilizada, além de garantir a aderência aos padrões
da indústria e a facilidade de integração com outras ferramentas e
plataformas.
WTVDI’2024, Juiz de Fora/MG, Brazil
Raphael Abreu, Joel A. Ferreira dos Santos, and Débora C. Muchaluat-Saade
Como trabalho futuro pretende-se expandir o gerador de SDK
para outras linguagens de programação, como Python. Além disso,
com a evolução da norma Ginga para a TV 3.0, planeja-se atualizar
a especificação OpenAPI e o gerador de SDK para suportar as novas
funcionalidades e serviços que serão introduzidos.
Por fim, um passo crucial será a realização de testes práticos do
SDK com desenvolvedores de aplicações para a TV Digital, a fim
de coletar feedback sobre a usabilidade, a eficiência e a capacidade
do SDK de atender às necessidades reais do desenvolvimento de
aplicações Ginga.

--- FIM DO ARQUIVO: 30514-829-24962-1-10-20241001.txt ---

--- INÍCIO DO ARQUIVO: 30515-829-24963-1-10-20241001.txt ---
Uma Ferramenta de Autoria para
Interação Multimodal em Aplicações NCL 4.0
Pedro Alves Valentim
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, Rio de Janeiro, Brasil
pedroalvesvalentim@midiacom.uff.br
Débora Christina Muchaluat-Saade
Laboratório MídiaCom
Universidade Federal Fluminense
Niterói, Rio de Janeiro, Brasil
debora@midiacom.uff.br
0. The
tool, implemented as a VSCode extension, automates the process
of converting remote control interactions into multimodal links,
thus reducing the authoring effort. By integrating voice, gesture,
and facial expression interactions, the tool allows developers to
focus more on enhancing user experience rather than technical
implementation details. The paper details the functionalities and
implementation of the tool, demonstrating how it can streamline
the development process and improve the flexibility and usability of
NCL 4.0 applications. Future work includes further testing and ex-
ploration of additional interaction modalities to enhance the tool’s
adaptability and inclusiveness.
0.
O restante do texto se estrutura da seguinte maneira: a Seção 2
descreve brevemente as novidades da linguagem NCL, como seus
novos papéis de conectores; a Seção 3 descreve as funcionalidades da
ferramenta; a Seção 4 descreve como essas funcionalidades foram
implementadas na extensão; finalmente, a Seção 5 recapitula as
contribuições do trabalhos e sugere trabalhos futuros.
INTERAÇÃO MULTIMODAL EM NCL 4.0
O modelo NCM (Nested Context Model) [4, 9] estrutura a represen-
tação de documentos multimídia através de entidades como nós,
conectores hipermídia [6] e elos. Cada nó é um conjunto de infor-
mações a ser apresentado, enquanto conectores e elos estabelecem
as relações entre esses nós. A máquina de estados no NCM, que é
fundamental para o modelo, opera com base em uma sincronização
de eventos [7, 9]. Esta abordagem permite que documentos hiper-
mídia incluam ações relacionadas a eventos temporais, espaciais e
assíncronos, como interações do usuário. À medida que os eventos
ocorrem, a máquina de estados altera seus estados conforme as con-
dições associadas aos conectores e elos são atendidas, acionando as
ações especificadas. A Figura 1 apresenta a máquina de estados dos
eventos NCM.
O modelo NCM já inclui diversos tipos de eventos relacionados
à interação do usuário, com atributos que descrevem características
WTVDI’2024, Juiz de Fora/MG, Brazil
Pedro Alves Valentim and Débora Christina Muchaluat-Saade
Figura 1: Máquina de estados dos eventos NCM.
específicas desses eventos. Por exemplo, o atributo “key” em um
evento de seleção define a tecla pressionada. A linguagem NCL, em
sua versão 4.0, apresenta novos tipos de eventos para ampliar os mo-
dos de interação. Entre os novos eventos estão “voiceRecognition”,
“handPoseRecognition” e “faceRecognition” — para interações via
voz, gestos e expressões faciais, respectivamente —, que utilizam o
atributo “key” de maneira diferente. Cada evento possui um papel
associado — por exemplo, o evento “voiceRecognition” é direta-
mente relacionado ao papel “onVoiceRecognition”. A listagem 1
apresenta um exemplo de conector e elo para interação por voz.
No contexto dos eventos multimodais, o atributo “key” funciona
como uma chave que fornece informações adicionais sobre o evento;
por exemplo, no evento “handPoseRecognition”, um “key” igual a
“OPEN” indica o gesto de mão aberta.
1 <connectorBase>
<causalConnector id="onVoiceRecognitionStart">
<connectorParam name="key"/>
<simpleCondition role="onVoiceRecognition" key=
"$key"/>
<simpleAction role="start" />
</causalConnector>
7 </connectorBase>
8 ...
9 <link xconnector="onVoiceRecognitionStart">
<bind role="onVoiceRecognition" component="
botanicalGardenImage">
<bindParam name="key" value="play"/>
</bind>
<bind role="start" component="botanicalGardenVideo"
/>
14 </link>
Listing 1: Exemplo de conector e elo para interação por voz
É possível configurar múltiplas condições num mesmo conector,
permitindo, por exemplo, combinar diferentes modos de interação
em um mesmo conector, com condição composta por operador
booleano “OR”. A listagem 2 apresenta um exemplo de conector
com condição composta.
1 <causalConnector id="
onKeySelectionOrVoiceRecognitionStart">
<connectorParam name="key" />
<connectorParam name="key_voice" />
<compoundCondition operator="or">
<simpleCondition role="onSelection" key="$key
"/>
<simpleCondition role="onVoiceRecognition"
key="$key_voice" />
</compoundCondition>
<simpleAction role="start" max="unbounded" />
9 </causalConnector>
Listing 2: Exemplo de conector que pode ser ativado tanto
por controle remoto quanto por voz.
PROPOSTA DE FERRAMENTA DE AUTORIA
A ferramenta de autoria para integração multimodal apresentada
neste trabalho visa facilitar a criação de aplicativos NCL 4.0 que
combina diferentes modos de interação, incluindo voz, gesto e ex-
pressão facial. Para isso, desenvolvemos uma extensão VSCode
que automatiza o processo de conversão de elos de interação via
controle remoto e outros modos de interação em elos multimodais.
A ferramenta utiliza o recurso Code Actions do VSCode para in-
dicar ao usuário que é possível adicionar outros modos de interação
a elos presentes no código. Este recurso tem a aparência de uma
lâmpada no começo da linha do editor. Ao clicar na lâmpada, é
aberto um pequeno menu com os modos de interação que podem
ser adicionados ao elo. A Figura 2 é um recorte de captura de tela
que apresenta tal recurso.
Figura 2: Aparência do elemento de Code Action para adição
de interação multimodal a um elo de interação via controle
remoto.
Após a seleção, abre-se uma caixa de entrada de texto para que
o usuário insira o valor esperado “key” para o novo papel do elo.
Feita esta atribuição, a extensão verifica se há na base de conectores
um conector correspondente ao novo elo. Caso não haja, ele é adici-
onado. Finalmente, é reescrito o elo, mantendo os modos originais
de interação e o novo, recém-escolhido. Com essa ferramenta, o
autor de aplicações NCL não precisa mais se preocupar com a im-
plementação manual da lógica de interação multimodal, reduzindo
consideravelmente o esforço de autoria.
Com essa ferramenta, é possível criar aplicativos NCL 4.0 que
integrem diferentes modos de interação, tornando-os mais flexíveis
e fáceis de usar. Além disso, a redução do esforço de autoria permite
que os desenvolvedores se concentrem em melhorar a experiência
do usuário, ao invés de se preocupar com aspectos técnicos de
sintaxe e dependências.
IMPLEMENTAÇÃO NO VSCODE
A extensão é escrita em TypeScript [3] e utiliza a API do VSCode
para registrar comandos, fornecer ações de código e manipular o
Uma Ferramenta de Autoria para Interação Multimodal em Aplicações NCL 4.0
WTVDI’2024, Juiz de Fora/MG, Brazil
conteúdo dos documentos abertos no editor. A ativação da extensão
ocorre por meio da função “activate”, que registra os comandos e
ações a serem disponibilizados aos usuários.
O comando principal da extensão, “editor.writeLink”, é registrado
durante a ativação da extensão. Esse comando é responsável por
lidar com a escrita de elos multimodais no código NCL. Quando
acionado, abre uma caixa de entrada de texto para que o usuário que
insira o valor “key” para a nova interação. Após o usuário fornecer
o valor, o comando constrói uma nova tag de elo que incorpora o
novo modo de interação e verifica se o conector correspondente
já existe no documento. Se o conector não existir, ele é criado e
inserido na base de conectores.
A extensão inclui uma função que verifica se um conector es-
pecífico já está presente no documento. Isso é feito ao comparar
o conteúdo do documento com uma expressão regular que busca
pelo identificador do conector. Se o conector necessário não esti-
ver presente, outra função é responsável por compor o conector e
inseri-lo na base de conectores. Esta composição inclui a constru-
ção de uma tag de conector causal que integra a nova ação simples
correspondente ao tipo de interação adicionada.
A extensão utiliza o recurso Code Actions do VSCode para forne-
cer sugestões de interação ao usuário, indicando que é possível adi-
cionar modos de interação a elos presentes no código. Um provedor
de ações de código verifica se a seleção atual no editor corresponde
a um elo de interação. Se corresponder, ele cria ações de código
que permitem ao usuário adicionar interações de voz, gestos ou
expressão facial ao elo selecionado.
A extensão automatiza o processo de adição de interações mul-
timodais, de modo que o usuário pode simplesmente selecionar e
definir novos modos de interação para os elos presentes no código.
Ao fazer isso, a extensão verifica e atualiza automaticamente os co-
nectores necessários, assegurando coesão sintática no documento.
A Figura 3 apresenta um diagrama de atividades para o sistema.
A versão inicial da extensão está disponível em: ncl-multimodal-
qol-0.0.1.vsix
CONCLUSÃO
Neste trabalho, desenvolvemos uma ferramenta de autoria voltada
para a criação de aplicações NCL 4.0 com interação multimodal,
visando facilitar a programação e melhorar a experiência do usuário.
Nossa ferramenta permite a integração de diferentes modos de
interação, como voz, gestos e expressões faciais, automatizando a
conversão de elos de interação e simplificando o processo para os
autores.
As principais contribuições deste trabalho incluem a implemen-
tação de uma extensão para o VSCode que utiliza o recurso Code
Actions para sugerir e adicionar novos modos de interação de ma-
neira intuitiva. Esta abordagem reduz significativamente o esforço
necessário para programar aplicações multimodais, permitindo que
os autores se concentrem em aspectos mais criativos e na melhoria
da experiência do usuário.
Além disso, discutimos a estrutura do modelo NCM e como a lin-
guagem NCL 4.0 foi ampliada para suportar novos tipos de eventos
multimodais. A ferramenta implementada verifica a presença de co-
nectores necessários e, caso não existam, os cria automaticamente,
garantindo a coesão sintática do documento NCL.
Usuário seleciona linha
em elo de interação
Usuário abre opções
de Code Actions
Usuário seleciona tipo
da interação a adicionar
Usuário digita valor key
para a nova interação
Não
Sim
Conector
correspondente
existe na base
de conectores?
Cria conector correspondente
e adiciona à base de conectores
Modifica elo original,
com nova interação
Figura 3: Diagrama de atividades da extensão.
Para trabalhos futuros, sugerimos a realização de testes adici-
onais da ferramenta em diferentes cenários e com um conjunto
maior de dados para validar sua robustez e versatilidade. Outra
direção promissora é a exploração de modalidades de interação
adicionais, como o uso do evento “eyeGaze” (fixação do olhar), que
pode proporcionar uma experiência de usuário ainda mais inclu-
siva. A implementação de novos recursos que permitam uma maior
personalização e adaptabilidade da ferramenta às necessidades es-
pecíficas dos usuários também é um caminho a ser explorado.
Acreditamos que esta ferramenta pode trazer contribuições sig-
nificativas para a comunidade de desenvolvimento de aplicações
NCL, promovendo uma experiência de TV digital mais interativa e
personalizada.
AGRADECIMENTOS
Os autores agradecem o apoio recebido da FAPERJ, CAPES, CAPES
PRINT, CNPq, MCOM e RNP.

