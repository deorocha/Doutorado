iRev: Um framework de avalia√ß√£o de sistemas de recomenda√ß√£o
baseados coment√°rios textuais
Guilherme Bittencourt
bittenkourt.gmf@aluno.ufsj.edu.br
UFSJ - Minas Gerais - Brasil
Naan Vasconcelos
naan.vasconcelos@aluno.ufsj.edu.br
UFSJ - Minas Gerais - Brasil
Leonardo Rocha
lcrocha@ufsj.edu.br
UFSJ - Minas Gerais - Brasil
2023. Realizamos um estudo detal-
hado dos principais avan√ßos, dos principais conjuntos de dados
e das m√©tricas utilizadas. Observamos que as cole√ß√µes de dados
In: IV Concurso de Trabalhos de Inicia√ß√£o Cient√≠fica (CTIC 2024). Anais Estendidos do
XXX Simp√≥sio Brasileiro de Sistemas Multim√≠dia e Web (CTIC‚Äô2024). Juiz de Fora/MG,
Brazil. Porto Alegre: Brazilian Computer Society, 2024.
¬© 2024 SBC ‚Äì Sociedade Brasileira de Computa√ß√£o.
ISSN 2596-1683
mais utilizadas dentre os trabalhos relevantes s√£o as bases de da-
dos de produtos da Amazon e de pontos de interesse da Yelp, por
disponibilizarem as intera√ß√µes usu√°rio/item e tamb√©m os comen-
t√°rios provenientes das intera√ß√µes. Em rela√ß√£o √†s m√©tricas de avali-
a√ß√£o, √© poss√≠vel observar que m√©tricas de erro s√£o as formas de met-
rifica√ß√£o mais utilizadas pelos trabalhos, seguidas pelas m√©tricas de
avalia√ß√£o de ranking. Por√©m, apesar do consenso na comunidade de
recomenda√ß√£o de que √© necess√°rio mais do que precis√£o para avaliar
a efic√°cia dos SsR, a grande maioria dos trabalhos priorizam a pre-
cis√£o sobre outras dimens√µes de qualidade, tais como serendipidade
e diversidade. Outra limita√ß√£o surge em rela√ß√£o aos algoritmos que
s√£o considerados estado-da-arte e suas configura√ß√µes. N√£o existe
um consenso das linhas de bases a serem consideradas, cada artigo
utiliza um conjunto distinto e as configura√ß√µes dos par√¢metros
raramente s√£o reportadas. Menos de 50% dos trabalhos analisa-
dos disponibiliza c√≥digo fonte de suas propostas, e menos de 30%
fornece as configura√ß√µes de par√¢metros dos algoritmos propostos.
Nesse sentido, como segunda contribui√ß√£o, implementamos os
10 principais algoritmos de recomenda√ß√£o review-aware, consoli-
dando todos os c√≥digos fontes gerados, bem como todos os artefatos
levantados durante o mapeamento sistem√°tico (m√©tricas e bases
de dados) em um framework aberto e publicamente dispon√≠vel1,
denominado iRev, com o objetivo de facilitar a pesquisa e com-
para√ß√µes entre abordagens na √°rea de review aware. Por fim, como
terceira contribui√ß√£o, realizamos uma an√°lise experimental de difer-
entes algoritmos, com diversas cole√ß√µes e m√©tricas, destacando as
principais dire√ß√µes para desenvolvimentos futuros.
Todas as implementa√ß√µes, execu√ß√µes e avalia√ß√µes dos resultados
foram realizadas pelo aluno Guilherme Bittencourt, com aux√≠lio do
aluno Naan Vasconcelos, sob a orienta√ß√£o do professor Leonardo Rocha.
MAPEAMENTO SISTEM√ÅTICO
Nesta se√ß√£o abordamos o processo de coleta, filtragem e sele√ß√£o
dos artigos relacionados a RARSs.
2.1
Fase 1: Quest√µes de pesquisa, palavras de
busca e fontes digitais
As quest√µes de pesquisa que dever√£o ser respondidas s√£o:
‚Ä¢ QP1: Como os algoritmos de recomenda√ß√£o utilizam t√©cnicas de
PLN para definir as prefer√™ncias dos usu√°rios por meio de seus
coment√°rios?
‚Ä¢ QP2: Quais s√£o as bases de dados e m√©tricas mais prevalentes uti-
lizadas na avalia√ß√£o de algoritmos em estudos relacionados a esse
tema?
‚Ä¢ QP3: Como as avalia√ß√µes experimentais s√£o conduzidas nos estudos
analisados, considerando estados da arte, configura√ß√µes e par√¢met-
ros dos modelos?
1https://github.com/guibitten03/iRev
CTIC‚Äô2024, Juiz de Fora/MG, Brazil
Bittencourt G. et al.
Recorremos ao mecanismo de pesquisa do Google Scholar para
realizar as tr√™s consultas abaixo:
‚Ä¢ SS-Q1: ("review based‚Äù OR ‚Äúreview aware‚Äù OR "review modeling")
AND ("recommender systems" OR "recommendation systems" OR
"recommender system") AND (‚Äútext‚Äù OR "textual" OR "review")
‚Ä¢ SS-Q2: (‚Äúreview based‚Äù OR ‚Äúreview aware‚Äù OR "review modeling")
AND (‚Äúrecommender systems‚Äú OR ‚Äúrecommendation systems‚Äú OR
‚Äúrecommender system‚Äú) AND (‚Äúevaluation‚Äù OR "measure" OR "met-
rics")
‚Ä¢ SS-Q3: (‚Äúreview based‚Äù OR ‚Äúreview aware‚Äù OR "review modeling")
AND ("recommender systems" OR "recommendation systems" OR
"recommender system") AND (‚Äúsource code‚Äù OR ‚Äúreproducibility‚Äù
OR ‚Äúempirical‚Äù OR ‚Äúexperimental‚Äù)
2.2
Fase 2: Sele√ß√£o de trabalhos relevantes
Ap√≥s a coleta, aplicamos um filtro de data entre 2014 e 2023, assegu-
rando um escopo de 10 anos de publica√ß√µes, acumulando um total
de 1.190 artigos. Eliminamos as duplicadas e aplicamos um segundo
filtro considerando apenas artigos das 100 confer√™ncias de maior
fator de impacto de acordo com a research.com), tais como RecSys,
WWW, WSDM, SIGIR, etc., resultando em 681 artigos. Por fim, em-
pregamos um filtro avan√ßado com crit√©rios de inclus√£o e exclus√£o.
Realizamos uma an√°lise manual de cada artigo, classificando-os
como relevantes ou n√£o, de acordo com os crit√©rios estabelecidos:
Crit√©rios de Inclus√£o
‚Ä¢ O m√©todo principal empregado para realizar as recomenda√ß√µes √©
o uso dos coment√°rios dos usu√°rios, considerando as avalia√ß√µes
num√©ricas como um suporte adicional.
‚Ä¢ Prop√µem avan√ßos e inova√ß√µes no dom√≠nio, n√£o se limitando √† otimiza-
√ß√£o de algoritmos preexistentes.
‚Ä¢ Realizam avalia√ß√µes experimentais comparativas entre os algorit-
mos que utilizam coment√°rios do usu√°rio e o m√©todo proposto nos
artigos correspondentes.
Crit√©rios de Exclus√£o
‚Ä¢ Al√©m dos coment√°rios, empregam outras fontes de informa√ß√£o, como
imagens, √°udios ou v√≠deos para prever as prefer√™ncias do usu√°rio.
‚Ä¢ S√£o surveys, casos de estudo, revis√µes sistem√°ticas ou experimentais
sobre os algoritmos do cen√°rio.
‚Ä¢ Utilizam os coment√°rios exclusivamente para justificar as recomen-
da√ß√µes, focando na explicabilidade..
Ap√≥s a aplica√ß√£o desses crit√©rios, restaram 117 artigos que foram
identificados como mais relevantes.
2.3
Fase 3: Extra√ß√£o das informa√ß√µes dos artigos
Realizamos uma leitura detalhada dos 117 artigos restantes para
identificar as principais caracter√≠sticas das solu√ß√µes propostas, suas
principais inova√ß√µes e contribui√ß√µes para a literatura e as metodolo-
gias de avalia√ß√£o utilizadas. A seguir, detalhamos o resultado dessa
an√°lise visando responder as tr√™s quest√µes de pesquisas levantadas
no in√≠cio desta se√ß√£o.
AVALIA√á√ÉO SISTEM√ÅTICA
Essa se√ß√£o detalha como os SsR v√™m sendo avaliados por meio de
uma inspe√ß√£o das avalia√ß√µes experimentais dos 117 artigos sele-
cionados.
3.1
Bases de dados
Conforme podemos observar na Figura 1, que as bases de dados da
Amazon e da Yelp s√£o as mais utilizadas. Ambas tratam de cen√°rios
cl√°ssicos para an√°lises de recomenda√ß√£o review aware devido ao
grande n√∫mero de usu√°rios que comentam sobre os itens. A Ama-
zon √© composta de subcole√ß√µes de acordo com a categoria de item
e a Yelp por diferentes cidades. A grande maioria dos trabalhos n√£o
especifica qual categoria/cidade utilizada nos experimentos. Outra
quest√£o cr√≠tica √© que essas cole√ß√µes t√™m cortes temporais que tam-
b√©m n√£o s√£o mencionados. Essas quest√µes impactam negativamente
na reprodutibilidade desses trabalhos.
Figure 1: Frequ√™ncia de bases de dados em experimenta√ß√µes.
3.2
M√©tricas
Conforme podemos observar na Figura 2, h√° um constante interesse
sob a precis√£o dos ratings preditos. O consenso na comunidade de
SR √© que a precis√£o por si s√≥ n√£o √© suficiente para avaliar a efic√°cia
pr√°tica e o valor agregado das recomenda√ß√µes, sendo necess√°rio
outras t√©cnicas de avalia√ß√£o como diversidade e serendipidade. N√£o
identificamos nenhum trabalho dentre os 117 analisado que busca
avaliar os modelos nessas dimens√µes, sendo esse um importante
ponto fraco que avalia√ß√µes futuras precisam considerar.
Figure 2: Frequ√™ncia de m√©tricas em experimenta√ß√µes.
3.3
Algoritmos e Configura√ß√£o de Par√¢metros
Com respeito a disponibilidade do c√≥digo fonte dos algoritmos
propostos, temos que pelo menos 50% dos trabalhos disponibiliza
c√≥digo fonte, o que dificulta a utiliza√ß√£o dos mesmos como linhas
de base. Um novo algoritmo √© comparado, em m√©dia, com apenas
tr√™s outros algoritmos e, no m√°ximo, nove. Outra observa√ß√£o im-
portante √© sobre o processo de calibra√ß√£o dos algoritmos, em que
menos de 30% dos trabalhos apresentam em detalhes desse processo.
Todas essas quest√µes impacta na replicabilidade dos trabalhos.
iRev: Um framework de avalia√ß√£o de sistemas de recomenda√ß√£o baseados coment√°rios textuais
CTIC‚Äô2024, Juiz de Fora/MG, Brazil
IREV
Nessa se√ß√£o detalhamos nossa proposta de um framework de avali-
a√ß√£o de SsR baseados coment√°rios textuais: iRev (dispon√≠vel em
https://github.com/guibitten03/iRevRS) .
4.1
Algoritmos Implementados
Dos 117 algoritmos examinados, selecionamos todas as abordagens
utilizadas em pelo menos dois artigos diferentes. Os 10 algoritmos
selecionados s√£o apresentados na Tabela 1, onde a coluna ‚ÄôLinhas de
Base‚Äô representa quantas vezes o algoritmo foi utilizado em outros
trabalhos.
Algoritmo
# Linhas de Base
# Cita√ß√µes
DeepCoNN
Narre
D-ATTN
Daml
MPCN
CARL
ANR
CARP
HRDR
RGNN
Table 1: Algoritmos mais utilizados como linhas de base.
O DeepCoNN utiliza redes neurais convolucionais para capturar
as informa√ß√µes relevantes nos coment√°rios [10]. O MPCN, por sua
vez, emprega uma arquitetura de co-aten√ß√£o multi-pontual para
capturar o contexto em diferentes n√≠veis de granularidade [8]. O
D-ATTN adota uma rede neural de aten√ß√£o dupla considerando
os coment√°rios e as caracter√≠sticas do usu√°rio/item [7]. O NARRE
utiliza uma abordagem baseada em redes neurais para modelar
a aten√ß√£o e as intera√ß√µes entre aspectos nos coment√°rios [1]. O
DAML prop√µe uma abordagem de aprendizado m√∫tuo de aten√ß√£o
entre avalia√ß√µes e coment√°rios [4]. O CARL utiliza redes neurais
convolucionais em c√°psulas para gerar recomenda√ß√µes e fornecer
explica√ß√µes sobre as prefer√™ncias do usu√°rio [9]. O CARP introduz
uma estrutura de rede neural para incorporar a aten√ß√£o contextual
na modelagem de avalia√ß√µes e coment√°rios [3]. O ANR adota uma
abordagem baseada em aspectos para recomenda√ß√£o, capturando
a rela√ß√£o entre aspectos e usu√°rios/itens [2]. O HRDR realiza uma
abordagem conjunta de representa√ß√µes de aprendizado profundo
de avalia√ß√µes e coment√°rios [5]. Por fim, o RGNN prop√µe uma rep-
resenta√ß√£o hier√°rquica de coment√°rios de avalia√ß√µes em forma de
grafo para aprimorar a precis√£o das recomenda√ß√µes [6].
4.2
Configura√ß√£o dos Algoritmos
Utilizamos os c√≥digos dos algoritmos provenientes no GitHub dos
respectivos autores e realizamos uma tunagem de par√¢metros, de
acordo com o apresentado na Tabela 2.
4.3
Cole√ß√µes de Dados
A Tabela 3 apresenta alguns detalhes sobre as cole√ß√µes disponibi-
lizadas pelo iRev. Para garantir reprodutibilidade, todas elas s√£o
divididas em subconjuntos de treino, teste e valida√ß√£o. O conjunto
de treino √© composto por 80% dos dados, enquanto os conjuntos de
valida√ß√£o e teste possuem 10% cada. Os reviews presentes nos dados
foram pr√©-processados utilizando a biblioteca NLTK, que permitiu
Par√¢metros
Valores
√âpocas de treinamento
10, 20, 50
Fun√ß√£o de perda
MSE
Otimizador
ADAM
Dimens√µes dos vetores de usu√°rio e item
Dimens√µes dos vetores de palavras
Codificadores utilizados
TF-IDF, Word2Vec e FastText.
Taxa de dropout
0.5
Weight decay
1ùëí‚àí3
Tamanho do lote
128 a 32
Tamanho m√°ximo dos documentos
500 palavras
Taxa de aprendizado
2ùëí‚àí3
# filtros nas camadas convolucionais
Table 2: Configura√ß√µes dos par√¢metros dos algoritmos
realizar tratamentos nos textos, tais como remo√ß√£o de stopwords e
lematiza√ß√£o.
Cole√ß√£o
# Usu√°rios
# Itens
Esparsidade
Amazon - Video Games
10.000
17.005
99.99%
Yelp - Tampa
18.437
8.664
99,99%
Yelp - Philadelphia
32.376
14.226
99.99%
Table 3: Vis√£o geral das cole√ß√µes utilizadas na avalia√ß√£o.
4.4
M√©tricas
Para avaliar as recomenda√ß√µes dos algoritmos consideramos quatro
m√©tricas de precis√£o: duas m√©tricas de erro (i.e., MSE e MAE) que
avaliam a diferen√ßa entre o rating real e o previsto pelos algorit-
mos; e duas de efetividade (i.e. Accuracy e F1 Score) que avaliam
o qu√£o bem o algoritmo aprendeu o comportamento do usu√°rio.
O consenso na comunidade de SsR √© que a precis√£o por si s√≥ n√£o
√© suficiente para avaliar a efic√°cia pr√°tica e o valor agregado das
recomenda√ß√µes. Assim, al√©m da precis√£o, exclusivamente consid-
erada em praticamente todos revisados neste artigo, consideramos
outras duas m√©tricas: serendipidade e diversidade. A serendipidade
se refere a descoberta de itens √∫teis e inesperados e a diversidade
aos itens recomendados diferentes do hist√≥rico de consumo.
4.5
Avalia√ß√£o Experimental
Com o objetivo de validar o framework proposto, realizamos uma
avalia√ß√£o experimental de todos os algoritmos implementados, con-
siderando as cinco m√©tricas nas tr√™s cole√ß√µes disponibilizadas e os
resultados s√£o apresentados na Tabela 4 apresentamos os resultados
obtidos. Observamos que n√£o h√° um destaque √∫nico. Na cole√ß√£o
Amazon, por exemplo, dos 10 algoritmos analisados, cinco deles se
destacaram em distintas m√©tricas. Enquanto os algoritmos HRDR,
D-ATTN e o NARRE se destacaram em m√©tricas de precis√£o, o Deep-
CONN e o ANR se destacaram em diversidade e serendipidade.
Os resultados tamb√©m variam de acordo com as cole√ß√µes. Na
Yelp - Tampa, o segundo algoritmo mais recente proposto, HRDR,
obteve melhores resultados nas m√©tricas de precis√£o, corroborando
com os experimentos mencionados no artigo original. Al√©m disso,
observou-se que os algoritmos CARP, CARL e MPCN n√£o obtiveram
resultados significativos nessas mesmas m√©tricas, o que contradiz
as afirma√ß√µes de seus respectivos artigos. Por outro lado, esses al-
goritmos foram destaque em termos de serendipidade e diversidade.
O ANR n√£o obteve o melhor resultado em nenhuma m√©trica, mas
CTIC‚Äô2024, Juiz de Fora/MG, Brazil
Bittencourt G. et al.
Cole√ß√£o
Amazon - Video Games
Yelp - Tampa
Yelp - Philadelphia
Measure
MSE
MAE
Acc
F1@10
Ser
MSE
MAE
Acc
F1@10
Ser
MSE
MAE
Acc
F1@10
Ser
DeepCoNN
1.541
0.928
0.206
0.323
0.141
0.197‚ñ≤
1.337
0.892
0.361‚ñ≤
0.216
0.147
0.097
1.561
0.994
0.300
0.125
0.159‚Ä¢ 0.224‚Ä¢
D ATTN
1.127
0.727‚ñ≤
0.228
0.428
0.048
0.060
1.346
0.912
0.329
0.197
0.159
0.088
1.207
0.871
0.343
0.198
0.141
0.099
MPCN
1.636
0.993
0.121
0.262
0.069
0.1598
1.447
0.965
0.288
0.124
0.164‚Ä¢
0.163
1.322
0.913
0.323
0.121
0.145
0.125
NARRE
1.075
0.691
0.255
0.459‚ñ≤
0.061
0.141
1.302
0.892
0.348
0.218
0.147
0.049
1.172
0.844
0.364
0.218
0.146
0.063
DAML
1.149
0.744
0.234
0.411
0.035
0.094
1.364
0.935
0.308
0.177
0.146
0.037
1.270
0.899
0.329
0.173
0.152
0.037
CARL
1.286
0.839
0.326
0.152
0.021
0.081
1.525
0.995
0.293
0.134
0.166‚Ä¢
0.125
1.306
0.921
0.332
0.171
0.153‚Ä¢ 0.224‚Ä¢
CARP
1.262
0.824
0.340
0.170
0.213
0.124
1.583
0.987
0.285
0.107
0.147
0.225‚ñ≤
1.336
0.902
0.324
0.136
0.152
0,172
ANR
1.171
0.780
0.381
0.226
0.242‚ñ≤
0.071
1.288
0.899
0.338
0.215
0.148
0.061
1.115‚ñ≤0.813‚ñ≤0.397‚ñ≤0.267‚ñ≤
0.124
0.050
HRDR
1.039‚ñ≤
0.751
0.456‚ñ≤
0.248
0.082
0.081
1.257‚ñ≤0.879‚ñ≤
0.355
0.249‚ñ≤
0.146
0.050
1.482
0.964
0.324
0.209
0.146
0.057
RGNN
1.179
0.792
0.297
0.150
0.101
0.148
1.364
0.916
0.314
0.186
0.142
0.124
1.296
0.896
0.287
0.128
0.150
0.075
Table 4: Resultados validados com o teste de Wilcoxon com um valor p = 0,05. ‚ñ≤representa ganhos significativos e ‚Ä¢ empates estat√≠sticos.
apresentou resultados consistentes e est√°veis em termos de desem-
penho. O algoritmo RGNN, embora seja o mais recente em termos
de proposta, apresentou resultados inferiores a muitas outras es-
trat√©gias avaliadas. Na cole√ß√£o Yelp - Philadelphia, o algoritmo ANR
mostrou os melhores resultados nas quatro m√©tricas relacionadas
√† efetividade, mais uma vez alinhados com o que foi apresentado
no artigo original. O algoritmo CARL foi melhor nessa cole√ß√£o em
compara√ß√£o com a anterior, obtendo os melhores resultados de
serendipidade e diversidade, juntamente com o DeepCoNN.
Grande parte dos algoritmos apresentaram resultados consis-
tentes com seus estudos originais. No entanto, alguns algoritmos
n√£o obtiveram bons resultados em compara√ß√£o com o que foi de-
scrito pelos autores. Algoritmos como CARL, CARP, MPCN e RGNN,
que teoricamente deveriam superar metodologias como DATTN,
ANR e NARRE, n√£o tiveram sucesso em nossa avalia√ß√£o emp√≠rica.
Esses resultados refor√ßam a import√¢ncia do iRev por avan√ßar na
quest√£o da reprodutibilidade, por ser um reposit√≥rio p√∫blico n√£o
apenas de cole√ß√µes de dados, como tamb√©m dos pr√≥prios algoritmos
e seus processos de tunagem de par√¢metros.
CONCLUS√ïES E TRABALHOS FUTUROS
Como primeira contribui√ß√£o, esse trabalho apresenta um mapea-
mento sistem√°tico dos estudos sobre sistemas de recomenda√ß√£o
review-aware (RARSs) selecionando e investigando os 117 artigos
relevantes publicados nos principais ve√≠culos da √°rea (e.g., Rec-
Sys, SIGIR, WWW, etc.), identificando esfor√ßos, resultados, con-
tribui√ß√µes e limita√ß√µes relevantes. A partir desse levantamento,
propomos e disponibilizamos um framework, denominado iRev,
contendo a implementa√ß√£o dos 10 principais RARSs, bem como
todos os artefatos levantados durante o mapeamento sistem√°tico
(m√©tricas e bases de dados) com o intuito de mitigar a limita√ß√£o at-
ual de falta de reprodutibilidade devido √† aus√™ncia de c√≥digos fontes
e de confiabilidade devido √† aus√™ncia de distintas m√©tricas de avali-
a√ß√£o. Para validar o iRev, realizamos uma avalia√ß√£o completa das
principais abordagens, considerando diferentes cole√ß√µes de dados e
m√©tricas. Nossos resultados mostram que as SsR baseadas em redes
neurais, especialmente as que utilizam mecanismos de extra√ß√£o de
aten√ß√£o e aspecto, obtiveram os resultados mais competitivos. Por
outro lado, tais resultados tamb√©m refor√ßam que n√£o h√° um √∫nico
algoritmo que se destaque de forma absoluta, deixando claro que
ainda h√° espa√ßo de melhora consider√°vel a ser explorado por novas
estrat√©gias. Como trabalhos futuros, visamos complementar o iRev
com a implementa√ß√£o de outras estrat√©gias, tornando o reposit√≥rio
uma refer√™ncia para que pesquisadores da √°rea.
AGRADECIMENTOS
Este trabalho foi financiado por CNPq, CAPES, Fapemig, FAPESP e
AWS.
